{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to skforecast","text":""},{"location":"index.html#about-the-project","title":"About The Project","text":"<p>Skforecast is a Python library for time series forecasting using machine learning models. It works with any regressor compatible with the scikit-learn API, including popular options like LightGBM, XGBoost, CatBoost, Keras, and many others.</p>"},{"location":"index.html#why-use-skforecast","title":"Why use skforecast?","text":"<p>Skforecast simplifies time series forecasting with machine learning by providing:</p> <ul> <li> Seamless integration with any scikit-learn compatible regressor (e.g., LightGBM, XGBoost, CatBoost, etc.).</li> <li> Flexible workflows that allow for both single and multi-series forecasting.</li> <li> Comprehensive tools for feature engineering, model selection, hyperparameter tuning, and more.</li> <li> Production-ready models with interpretability and validation methods for backtesting and realistic performance evaluation.</li> </ul> <p>Whether you're building quick prototypes or deploying models in production, skforecast ensures a fast, reliable, and scalable experience.</p>"},{"location":"index.html#get-involved","title":"Get Involved","text":"<p>We value your input! Here are a few ways you can participate:</p> <ul> <li>Report bugs and suggest new features on our GitHub Issues page.</li> <li>Contribute to the project by submitting code, adding new features, or improving the documentation.</li> <li>Share your feedback on LinkedIn to help spread the word about skforecast!</li> </ul> <p>Together, we can make time series forecasting accessible to everyone. Discover more in our contribution contribution guide</p>"},{"location":"index.html#installation-dependencies","title":"Installation &amp; Dependencies","text":"<p>To install the basic version of <code>skforecast</code> with core dependencies, run the following:</p> <pre><code>pip install skforecast\n</code></pre> <p>For more installation options, including dependencies and additional features, check out our Installation Guide.</p>"},{"location":"index.html#forecasters","title":"Forecasters","text":"<p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>The skforecast library offers a variety of forecaster types, each tailored to specific requirements such as single or multiple time series, direct or recursive strategies, or custom predictors. Regardless of the specific forecaster type, all instances share the same API.</p> Forecaster Single series Multiple series Recursive strategy Direct strategy Probabilistic prediction Time series differentiation Exogenous features Window features ForecasterRecursive \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterDirect \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterRecursiveMultiSeries \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterDirectMultiVariate \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterRNN \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterSarimax \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f"},{"location":"index.html#features","title":"Features","text":"<p>Skforecast provides a set of key features designed to make time series forecasting with machine learning easy and efficient. For a detailed overview, see the User Guides.</p>"},{"location":"index.html#examples-and-tutorials","title":"Examples and tutorials","text":"<p>Explore our extensive list of examples and tutorials (English and Spanish) to get you started with skforecast. You can find them here.</p>"},{"location":"index.html#how-to-contribute","title":"How to contribute","text":"<p>Primarily, skforecast development consists of adding and creating new Forecasters, new validation strategies, or improving the performance of the current code. However, there are many other ways to contribute:</p> <ul> <li>Submit a bug report or feature request on GitHub Issues.</li> <li>Contribute a Jupyter notebook to our examples.</li> <li>Write unit or integration tests for our project.</li> <li>Answer questions on our issues, Stack Overflow, and elsewhere.</li> <li>Translate our documentation into another language.</li> <li>Write a blog post, tweet, or share our project with others.</li> </ul> <p>For more information on how to contribute to skforecast, see our Contribution Guide.</p> <p>Visit our About section to meet the people behind skforecast.</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use skforecast for a scientific publication, we would appreciate citations to the published software.</p> <p>Zenodo</p> <pre><code>Amat Rodrigo, Joaquin, &amp; Escobar Ortiz, Javier. (2025). skforecast (v0.17.0). Zenodo. https://doi.org/10.5281/zenodo.8382788\n</code></pre> <p>APA: <pre><code>Amat Rodrigo, J., &amp; Escobar Ortiz, J. (2025). skforecast (Version 0.17.0) [Computer software]. https://doi.org/10.5281/zenodo.8382788\n</code></pre></p> <p>BibTeX: <pre><code>@software{skforecast,\n  author  = {Amat Rodrigo, Joaquin and Escobar Ortiz, Javier},\n  title   = {skforecast},\n  version = {0.17.0},\n  month   = {8},\n  year    = {2025},\n  license = {BSD-3-Clause},\n  url     = {https://skforecast.org/},\n  doi     = {10.5281/zenodo.8382788}\n}\n</code></pre></p>"},{"location":"index.html#publications-citing-skforecast","title":"Publications citing skforecast","text":"<ul> <li><p> Chamara Hewage, H., Rostami-Tabar, B., Syntetos, A., Liberatore, F., and Milano, G., \u201cA Novel Hybrid Approach to Contraceptive Demand Forecasting: Integrating Point Predictions with Probabilistic Distributions\u201d, arXiv e-prints, Art. no. arXiv:2502.09685, 2025. doi:10.48550/arXiv.2502.09685. </p></li> <li><p> Kuthe, S., Persson, C. and Glaser, B. (2025), Physics-Informed Data-Driven Prediction of Submerged Entry Nozzle Clogging with the Aid of Ab Initio Repository. steel research int. 2400800. https://doi.org/10.1002/srin.202400800 </p></li> <li><p> Chatzikonstantinidis, K., Afxentiou, N., Giama, E., Fokaides, P. A., &amp; Papadopoulos, A. M. (2025). Energy management of smart buildings during crises and digital twins as an optimisation tool for sustainable urban environment. International Journal of Sustainable Energy, 44(1). https://doi.org/10.1080/14786451.2025.2455134 </p></li> <li><p> Sanan, O., Sperling, J., Greene, D., &amp; Greer, R. (2024, April). Forecasting Weather and Energy Demand for Optimization of Renewable Energy and Energy Storage Systems for Water Desalination. In 2024 IEEE Conference on Technologies for Sustainability (SusTech) (pp. 175-182). IEEE. https://doi.org/10.1109/SusTech60925.2024.10553570 </p></li> <li><p> Bojer, A. K., Biru, B. H., Al-Quraishi, A. M. F., Debelee, T. G., Negera, W. G., Woldesillasie, F. F., &amp; Esubalew, S. Z. (2024). Machine learning and remote sensing based time series analysis for drought risk prediction in Borena Zone, Southwest Ethiopia. Journal of Arid Environments, 222, 105160. https://doi.org/10.1016/j.jaridenv.2024.105160 </p></li> <li><p> V. Negri, A. Mingotti, R. Tinarelli and L. Peretto, \"Comparison Between the Machine Learning and the Statistical Approach to the Forecasting of Voltage, Current, and Frequency,\" 2023 IEEE 13th International Workshop on Applied Measurements for Power Systems (AMPS), Bern, Switzerland, 2023, pp. 01-06, doi: 10.1109/AMPS59207.2023.10297192. https://doi.org/10.1109/AMPS59207.2023.10297192 </p></li> <li><p> Marcillo Vera, F., Rosado, R., Zambrano, P., Velastegui, J., Morales, G., Lagla, L., &amp; Herrera, A. (2024). Forecasting con Python, caso de estudio: visitas a las redes sociales en Ecuador con machine learning. CONECTIVIDAD, 5(2), 15-29. <li><p>OUKHOUYA, H., KADIRI, H., EL HIMDI, K., &amp; GUERBAZ, R. (2023). Forecasting International Stock Market Trends: XGBoost, LSTM, LSTM-XGBoost, and Backtesting XGBoost Models. Statistics, Optimization &amp; Information Computing, 12(1), 200-209. https://doi.org/10.19139/soic-2310-5070-1822</p> </li> <li><p>DUDZIK, S., &amp; Kowalczyk, B. (2023). Prognozowanie produkcji energii fotowoltaicznej z wykorzystaniem platformy NEXO i VRM Portal. Przeglad Elektrotechniczny, 2023(11). doi:10.15199/48.2023.11.41 </p> </li> <li><p>Polo J, Mart\u00edn-Chivelet N, Alonso-Abella M, Sanz-Saiz C, Cuenca J, de la Cruz M. Exploring the PV Power Forecasting at Building Fa\u00e7ades Using Gradient Boosting Methods. Energies. 2023; 16(3):1495. https://doi.org/10.3390/en16031495</p> </li> <li><p>Pop\u0142awski T, Dudzik S, Szel\u0105g P. Forecasting of Energy Balance in Prosumer Micro-Installations Using Machine Learning Models. Energies. 2023; 16(18):6726. https://doi.org/10.3390/en16186726</p> </li> <li><p>Harrou F, Sun Y, Taghezouit B, Dairi A. Artificial Intelligence Techniques for Solar Irradiance and PV Modeling and Forecasting. Energies. 2023; 16(18):6731. https://doi.org/10.3390/en16186731</p> </li> <li><p>Amara-Ouali, Y., Goude, Y., Doum\u00e8che, N., Veyret, P., Thomas, A., Hebenstreit, D., ... &amp; Phe-Neau, T. (2023). Forecasting Electric Vehicle Charging Station Occupancy: Smarter Mobility Data Challenge. arXiv preprint arXiv:2306.06142.</p> </li> <li><p>Emami, P., Sahu, A., &amp; Graf, P. (2023). BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. arXiv preprint arXiv:2307.00142.</p> </li> <li><p>Dang, HA., Dao, VD. (2023). Building Power Demand Forecasting Using Machine Learning: Application for an Office Building in Danang. In: Nguyen, D.C., Vu, N.P., Long, B.T., Puta, H., Sattler, KU. (eds) Advances in Engineering Research and Application. ICERA 2022. Lecture Notes in Networks and Systems, vol 602. Springer, Cham. https://doi.org/10.1007/978-3-031-22200-9_32</p> </li> <li><p>Morate del Moral, Iv\u00e1n (2023). Predici\u00f3n de llamadas realizadas a un Call Center. Proyecto Fin de Carrera / Trabajo Fin de Grado, E.T.S.I. de Sistemas Inform\u00e1ticos (UPM), Madrid.</p> </li> <li><p>Lopez Vega, A., &amp; Villanueva Vargas, R. A. (2022). Sistema para la automatizaci\u00f3n de procesos hospitalarios de control para pacientes para COVID-19 usando machine learning para el Centro de Salud San Fernando.</p> </li> <li><p>Garc\u00eda \u00c1lvarez, J. D. (2022). Modelo predictivo de rentabilidad de criptomonedas para un futuro cercano.</p> </li> <li><p>Chilet Vera, \u00c1. (2023). Elaboraci\u00f3n de un algoritmo predictivo para la reposici\u00f3n de hipoclorito en los dep\u00f3sitos mediante t\u00e9cnicas de Machine Learning (Doctoral dissertation, Universitat Polit\u00e8cnica de Val\u00e8ncia).</p> </li> <li><p>Bustinza Barrial, A. A., Bautista Abanto, A. M., Alva Alfaro, D. A., Villena Sotomayor, G. M., &amp; Trujillo Sabrera, J. M. (2022). Predicci\u00f3n de los valores de la demanda m\u00e1xima de energ\u00eda el\u00e9ctrica empleando t\u00e9cnicas de machine learning para la empresa Nexa Resources\u2013Cajamarquilla.</p> </li> <li><p>Morgado, K. Desarrollo de una t\u00e9cnica de gesti\u00f3n de activos para transformadores de distribuci\u00f3n basada en sistema de monitoreo (Doctoral dissertation, Universidad Nacional de Colombia).</p> </li> <li><p>Zafeiriou A., Chantzis G., Jonkaitis T., Fokaides P., Papadopoulos A., 2023, Smart Energy Strategy - A Comparative Study of Energy Consumption Forecasting Machine Learning Models, Chemical Engineering Transactions, 103, 691-696.</p> </li>"},{"location":"index.html#donating","title":"Donating","text":"<p>If you found skforecast useful, you can support us with a donation. Your contribution will help us continue developing, maintaining, and improving this project. Every contribution, no matter the size, makes a difference. Thank you for your support!</p> <p> </p> <p></p>"},{"location":"index.html#license","title":"License","text":"<p>Skforecast software: BSD-3-Clause License</p> <p>Skforecast documentation: CC BY-NC-SA 4.0</p> <p>Trademark: The trademark skforecast is registered with the European Union Intellectual Property Office (EUIPO) under the application number 019109684. Unauthorized use of this trademark, its logo, or any associated visual identity elements is strictly prohibited without the express consent of the owner.</p>"},{"location":"api/ForecasterDirect.html","title":"<code>ForecasterDirect</code>","text":""},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect","title":"skforecast.direct._forecaster_direct.ForecasterDirect","text":"<pre><code>ForecasterDirect(\n    regressor,\n    steps,\n    lags=None,\n    window_features=None,\n    transformer_y=None,\n    transformer_exog=None,\n    weight_func=None,\n    differentiation=None,\n    fit_kwargs=None,\n    binner_kwargs=None,\n    n_jobs=\"auto\",\n    forecaster_id=None,\n)\n</code></pre> <p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a autoregressive direct multi-step forecaster. A separate model is created for each forecast time step. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>steps</code> <code>int</code> <p>Maximum number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value should be defined before training.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>None</code> <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>None</code> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>None</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Before returning a prediction, the differencing operation is reversed.</p> <code>None</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>None</code> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code> used to discretize  the residuals into k bins according to the predicted values associated  with each residual. Available arguments are: <code>n_bins</code>, <code>method</code>, <code>subsample</code>, <code>random_state</code> and <code>dtype</code>. Argument <code>method</code> is passed internally to the function <code>numpy.percentile</code>. New in version 0.15.0</p> <code>None</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster.</p> <code>`'auto'`</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API. An instance of this regressor is trained for each step. All of them  are stored in <code>self.regressors_</code>.</p> <code>regressors_</code> <code>dict</code> <p>Dictionary with regressors trained for each step. They are initialized  as a copy of <code>regressor</code>.</p> <code>steps</code> <code>numpy array</code> <p>Future steps the forecaster will predict when using method <code>predict()</code>.  Since a different model is created for each step, this value should be  defined before training.</p> <code>max_step</code> <code>int</code> <p>Maximum step the forecaster is able to predict. It is the maximum value included in <code>steps</code>.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>lags_names</code> <code>list</code> <p>Names of the lags used as predictors.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_features</code> <code>list</code> <p>Class or list of classes used to create window features.</p> <code>window_features_names</code> <code>list</code> <p>Names of the window features to be included in the <code>X_train</code> matrix.</p> <code>window_features_class_names</code> <code>list</code> <p>Names of the classes used to create the window features.</p> <code>max_size_window_features</code> <code>int</code> <p>Maximum window size required by the window features.</p> <code>window_size</code> <code>int</code> <p>The window size needed to create the predictors. It is calculated as the  maximum value between <code>max_lag</code> and <code>max_size_window_features</code>. If  differentiation is used, <code>window_size</code> is increased by n units equal to  the order of differentiation so that predictors can be generated correctly.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the  forecaster.</p> <code>differentiation_max</code> <code>int</code> <p>Maximum order of differentiation. For this Forecaster, it is equal to the value of the <code>differentiation</code> parameter.</p> <code>differentiator</code> <code>TimeSeriesDifferentiator</code> <p>Skforecast object used to differentiate the time series.</p> <code>last_window_</code> <code>pandas DataFrame</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the values needed to predict the next step immediately after the training data. These values are stored in the original scale of the time series before undergoing any transformations or differentiation. When <code>differentiation</code> parameter is specified, the dimensions of the <code>last_window_</code> are expanded as many values as the order of differentiation. For example, if <code>lags</code> = 7 and <code>differentiation</code> = 1, <code>last_window_</code> will have 8 values.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_name_in_</code> <code>str</code> <p>Names of the series provided by the user during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous data (pandas Series or DataFrame) used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_direct_exog_names_out_</code> <code>list</code> <p>Same as <code>X_train_exog_names_out_</code> but using the direct format. The same  exogenous variable is repeated for each step.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 10_000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after differentiation.</p> <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>. If  <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed  scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after  differentiation.  New in version 0.15.0</p> <code>out_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non-training data. Only stored up to 10_000 values. Use <code>set_out_sample_residuals()</code> method to set values. If  <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed  scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after  differentiation.</p> <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>. If  <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed  scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after  differentiation.  New in version 0.15.0</p> <code>binner</code> <code>QuantileBinner</code> <p><code>QuantileBinner</code> used to discretize residuals into k bins according  to the predicted values associated with each residual. New in version 0.15.0</p> <code>binner_intervals_</code> <code>dict</code> <p>Intervals used to discretize residuals into k bins according to the predicted values associated with each residual. New in version 0.15.0</p> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code>. New in version 0.15.0</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>_probabilistic_mode</code> <code>(str, bool)</code> <p>Private attribute used to indicate whether the forecaster should perform  some calculations during backtesting.</p> Notes <p>A separate model is created for each forecasting time step. It is important to note that all models share the same parameter and hyperparameter configuration.</p> <p>Methods:</p> Name Description <code>create_train_X_y</code> <p>Create training matrices from univariate time series and exogenous</p> <code>filter_train_X_y_for_step</code> <p>Select the columns needed to train a forecaster for a specific step.  </p> <code>create_sample_weights</code> <p>Create weights for each observation according to the forecaster's attribute</p> <code>fit</code> <p>Training Forecaster.</p> <code>create_predict_X</code> <p>Create the predictors needed to predict <code>steps</code> ahead.</p> <code>predict</code> <p>Predict n steps ahead.</p> <code>predict_bootstrapping</code> <p>Generate multiple forecasting predictions using a bootstrapping process. </p> <code>predict_interval</code> <p>Predict n steps ahead and estimate prediction intervals using either </p> <code>predict_quantiles</code> <p>Bootstrapping based predicted quantiles.</p> <code>predict_dist</code> <p>Fit a given probability distribution for each step. After generating </p> <code>set_params</code> <p>Set new values to the parameters of the scikit-learn model stored in the</p> <code>set_fit_kwargs</code> <p>Set new values for the additional keyword arguments passed to the <code>fit</code> </p> <code>set_lags</code> <p>Set new value to the attribute <code>lags</code>. Attributes <code>lags_names</code>, </p> <code>set_window_features</code> <p>Set new value to the attribute <code>window_features</code>. Attributes </p> <code>set_in_sample_residuals</code> <p>Set in-sample residuals in case they were not calculated during the</p> <code>set_out_sample_residuals</code> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample</p> <code>get_feature_importances</code> <p>Return feature importance of the model stored in the forecaster for a</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    steps: int,\n    lags: int | list[int] | np.ndarray[int] | range[int] | None = None,\n    window_features: object | list[object] | None = None,\n    transformer_y: object | None = None,\n    transformer_exog: object | None = None,\n    weight_func: Callable | None = None,\n    differentiation: int | None = None,\n    fit_kwargs: dict[str, object] | None = None,\n    binner_kwargs: dict[str, object] | None = None,\n    n_jobs: int | str = 'auto',\n    forecaster_id: str | int | None = None\n) -&gt; None:\n\n    self.regressor                          = copy(regressor)\n    self.transformer_y                      = transformer_y\n    self.transformer_exog                   = transformer_exog\n    self.weight_func                        = weight_func\n    self.source_code_weight_func            = None\n    self.differentiation                    = differentiation\n    self.differentiation_max                = None\n    self.differentiator                     = None\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.series_name_in_                    = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.exog_dtypes_out_                   = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_direct_exog_names_out_     = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.out_sample_residuals_              = None\n    self.in_sample_residuals_by_bin_        = None\n    self.out_sample_residuals_by_bin_       = None\n    self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted                          = False\n    self.fit_date                           = None\n    self.skforecast_version                 = skforecast.__version__\n    self.python_version                     = sys.version.split(\" \")[0]\n    self.forecaster_id                      = forecaster_id\n    self._probabilistic_mode                = \"binned\"\n\n    if not isinstance(steps, int):\n        raise TypeError(\n            f\"`steps` argument must be an int greater than or equal to 1. \"\n            f\"Got {type(steps)}.\"\n        )\n\n    if steps &lt; 1:\n        raise ValueError(\n            f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n        )\n\n    self.steps    = np.arange(steps) + 1\n    self.max_step = steps\n\n    self.regressors_ = {step: clone(self.regressor) for step in self.steps}\n    self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_features, self.window_features_names, self.max_size_window_features = (\n        initialize_window_features(window_features)\n    )\n    if self.window_features is None and self.lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ]\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    if differentiation is not None:\n        if not isinstance(differentiation, int) or differentiation &lt; 1:\n            raise ValueError(\n                f\"Argument `differentiation` must be an integer equal to or \"\n                f\"greater than 1. Got {differentiation}.\"\n            )\n        self.differentiation = differentiation\n        self.differentiation_max = differentiation\n        self.window_size += differentiation\n        self.differentiator = TimeSeriesDifferentiator(\n            order=differentiation, window_size=self.window_size\n        )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.binner_kwargs = binner_kwargs\n    if binner_kwargs is None:\n        self.binner_kwargs = {\n            'n_bins': 10, 'method': 'linear', 'subsample': 200000,\n            'random_state': 789654, 'dtype': np.float64\n        }\n    self.binner = QuantileBinner(**self.binner_kwargs)\n    self.binner_intervals_ = None\n\n    if n_jobs == 'auto':\n        self.n_jobs = select_n_jobs_fit_forecaster(\n                          forecaster_name = type(self).__name__,\n                          regressor       = self.regressor\n                      )\n    else:\n        if not isinstance(n_jobs, int):\n            raise TypeError(\n                f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\"\n            )\n        self.n_jobs = n_jobs if n_jobs &gt; 0 else cpu_count()\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.regressor","title":"regressor  <code>instance-attribute</code>","text":"<pre><code>regressor = copy(regressor)\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.transformer_y","title":"transformer_y  <code>instance-attribute</code>","text":"<pre><code>transformer_y = transformer_y\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.transformer_exog","title":"transformer_exog  <code>instance-attribute</code>","text":"<pre><code>transformer_exog = transformer_exog\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.weight_func","title":"weight_func  <code>instance-attribute</code>","text":"<pre><code>weight_func = weight_func\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.source_code_weight_func","title":"source_code_weight_func  <code>instance-attribute</code>","text":"<pre><code>source_code_weight_func = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.differentiation","title":"differentiation  <code>instance-attribute</code>","text":"<pre><code>differentiation = differentiation\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.differentiation_max","title":"differentiation_max  <code>instance-attribute</code>","text":"<pre><code>differentiation_max = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.differentiator","title":"differentiator  <code>instance-attribute</code>","text":"<pre><code>differentiator = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.last_window_","title":"last_window_  <code>instance-attribute</code>","text":"<pre><code>last_window_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.index_type_","title":"index_type_  <code>instance-attribute</code>","text":"<pre><code>index_type_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.index_freq_","title":"index_freq_  <code>instance-attribute</code>","text":"<pre><code>index_freq_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.training_range_","title":"training_range_  <code>instance-attribute</code>","text":"<pre><code>training_range_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.series_name_in_","title":"series_name_in_  <code>instance-attribute</code>","text":"<pre><code>series_name_in_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.exog_in_","title":"exog_in_  <code>instance-attribute</code>","text":"<pre><code>exog_in_ = False\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.exog_names_in_","title":"exog_names_in_  <code>instance-attribute</code>","text":"<pre><code>exog_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.exog_type_in_","title":"exog_type_in_  <code>instance-attribute</code>","text":"<pre><code>exog_type_in_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.exog_dtypes_in_","title":"exog_dtypes_in_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_in_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.exog_dtypes_out_","title":"exog_dtypes_out_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.X_train_window_features_names_out_","title":"X_train_window_features_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_window_features_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.X_train_exog_names_out_","title":"X_train_exog_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_exog_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.X_train_direct_exog_names_out_","title":"X_train_direct_exog_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_direct_exog_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.X_train_features_names_out_","title":"X_train_features_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_features_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.in_sample_residuals_","title":"in_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.out_sample_residuals_","title":"out_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.in_sample_residuals_by_bin_","title":"in_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.out_sample_residuals_by_bin_","title":"out_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.creation_date","title":"creation_date  <code>instance-attribute</code>","text":"<pre><code>creation_date = strftime('%Y-%m-%d %H:%M:%S')\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.is_fitted","title":"is_fitted  <code>instance-attribute</code>","text":"<pre><code>is_fitted = False\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.fit_date","title":"fit_date  <code>instance-attribute</code>","text":"<pre><code>fit_date = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.skforecast_version","title":"skforecast_version  <code>instance-attribute</code>","text":"<pre><code>skforecast_version = __version__\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.python_version","title":"python_version  <code>instance-attribute</code>","text":"<pre><code>python_version = split(' ')[0]\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.forecaster_id","title":"forecaster_id  <code>instance-attribute</code>","text":"<pre><code>forecaster_id = forecaster_id\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect._probabilistic_mode","title":"_probabilistic_mode  <code>instance-attribute</code>","text":"<pre><code>_probabilistic_mode = 'binned'\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.steps","title":"steps  <code>instance-attribute</code>","text":"<pre><code>steps = arange(steps) + 1\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.max_step","title":"max_step  <code>instance-attribute</code>","text":"<pre><code>max_step = steps\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.regressors_","title":"regressors_  <code>instance-attribute</code>","text":"<pre><code>regressors_ = {step: (clone(regressor))for step in (steps)}\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = max(\n    [\n        ws\n        for ws in [max_lag, max_size_window_features]\n        if ws is not None\n    ]\n)\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.window_features_class_names","title":"window_features_class_names  <code>instance-attribute</code>","text":"<pre><code>window_features_class_names = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.fit_kwargs","title":"fit_kwargs  <code>instance-attribute</code>","text":"<pre><code>fit_kwargs = check_select_fit_kwargs(\n    regressor=regressor, fit_kwargs=fit_kwargs\n)\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.binner_kwargs","title":"binner_kwargs  <code>instance-attribute</code>","text":"<pre><code>binner_kwargs = binner_kwargs\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.binner","title":"binner  <code>instance-attribute</code>","text":"<pre><code>binner = QuantileBinner(**(binner_kwargs))\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.binner_intervals_","title":"binner_intervals_  <code>instance-attribute</code>","text":"<pre><code>binner_intervals_ = None\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.n_jobs","title":"n_jobs  <code>instance-attribute</code>","text":"<pre><code>n_jobs = select_n_jobs_fit_forecaster(\n    forecaster_name=__name__, regressor=regressor\n)\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def _repr_html_(self):\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    (\n        params,\n        _,\n        _,\n        exog_names_in_,\n        _,\n    ) = self._preprocess_repr(\n            regressor      = self.regressor,\n            exog_names_in_ = self.exog_names_in_\n        )\n\n    style, unique_id = get_style_repr_html(self.is_fitted)\n\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Regressor:&lt;/strong&gt; {type(self.regressor).__name__}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Maximum steps to predict:&lt;/strong&gt; {self.max_step}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Series name:&lt;/strong&gt; {self.series_name_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skforecast version:&lt;/strong&gt; {self.skforecast_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n            &lt;ul&gt;\n                {exog_names_in_}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Data Transformations&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Training Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Regressor Parameters&lt;/summary&gt;\n            &lt;ul&gt;\n                {params}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n            &lt;ul&gt;\n                {self.fit_kwargs}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterdirect.html\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/direct-multi-step-forecasting.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    # Return the combined style and content\n    return style + content\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect._create_lags","title":"_create_lags","text":"<pre><code>_create_lags(y, X_as_pandas=False, train_index=None)\n</code></pre> <p>Create the lagged values and their target variable from a time series.</p> <p>Note that the returned matrix <code>X_data</code> contains the lag 1 in the first  column, the lag 2 in the in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>Training time series values.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_data</code> is a pandas DataFrame.</p> <code>False</code> <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_data</code> when <code>X_as_pandas</code> is <code>True</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray, pandas DataFrame, None</code> <p>Lagged values (predictors).</p> <code>y_data</code> <code>numpy ndarray</code> <p>Values of the time series related to each row of <code>X_data</code>.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray,\n    X_as_pandas: bool = False,\n    train_index: pd.Index | None = None\n) -&gt; tuple[np.ndarray | pd.DataFrame | None, np.ndarray]:\n    \"\"\"\n    Create the lagged values and their target variable from a time series.\n\n    Note that the returned matrix `X_data` contains the lag 1 in the first \n    column, the lag 2 in the in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        Training time series values.\n    X_as_pandas : bool, default False\n        If `True`, the returned matrix `X_data` is a pandas DataFrame.\n    train_index : pandas Index, default None\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_data` when `X_as_pandas` is `True`.\n\n    Returns\n    -------\n    X_data : numpy ndarray, pandas DataFrame, None\n        Lagged values (predictors).\n    y_data : numpy ndarray\n        Values of the time series related to each row of `X_data`.\n\n    \"\"\"\n\n    n_rows = len(y) - self.window_size - (self.max_step - 1)\n\n    X_data = None\n    if self.lags is not None:\n        X_data = np.full(\n            shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n        )\n        for i, lag in enumerate(self.lags):\n            X_data[:, i] = y[self.window_size - lag : -(lag + self.max_step - 1)] \n\n        if X_as_pandas:\n            X_data = pd.DataFrame(\n                         data    = X_data,\n                         columns = self.lags_names,\n                         index   = train_index\n                     )\n\n    y_data = np.full(\n        shape=(n_rows, self.max_step), fill_value=np.nan, order='F', dtype=float\n    )\n    for step in range(self.max_step):\n        y_data[:, step] = y[self.window_size + step : self.window_size + step + n_rows]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect._create_window_features","title":"_create_window_features","text":"<pre><code>_create_window_features(y, train_index, X_as_pandas=False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_train_window_features</code> when <code>X_as_pandas</code> is <code>True</code>.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_train_window_features</code> is a  pandas DataFrame.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train_window_features</code> <code>list</code> <p>List of numpy ndarrays or pandas DataFrames with the window features.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def _create_window_features(\n    self, \n    y: pd.Series,\n    train_index: pd.Index,\n    X_as_pandas: bool = False,\n) -&gt; tuple[list[np.ndarray | pd.DataFrame], list[str]]:\n    \"\"\"\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    train_index : pandas Index\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_train_window_features` when `X_as_pandas` is `True`.\n    X_as_pandas : bool, default False\n        If `True`, the returned matrix `X_train_window_features` is a \n        pandas DataFrame.\n\n    Returns\n    -------\n    X_train_window_features : list\n        List of numpy ndarrays or pandas DataFrames with the window features.\n    X_train_window_features_names_out_ : list\n        Names of the window features.\n\n    \"\"\"\n\n    len_train_index = len(train_index)\n    X_train_window_features = []\n    X_train_window_features_names_out_ = []\n    for wf in self.window_features:\n        X_train_wf = wf.transform_batch(y)\n        if not isinstance(X_train_wf, pd.DataFrame):\n            raise TypeError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a pandas DataFrame.\"\n            )\n        X_train_wf = X_train_wf.iloc[-len_train_index:]\n        if not len(X_train_wf) == len_train_index:\n            raise ValueError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a DataFrame with the same number of rows as \"\n                f\"the input time series - (`window_size` + (`steps` - 1)): {len_train_index}.\"\n            )\n        X_train_wf.index = train_index\n\n        X_train_window_features_names_out_.extend(X_train_wf.columns)\n        if not X_as_pandas:\n            X_train_wf = X_train_wf.to_numpy()     \n        X_train_window_features.append(X_train_wf)\n\n    return X_train_window_features, X_train_window_features_names_out_\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect._create_train_X_y","title":"_create_train_X_y","text":"<pre><code>_create_train_X_y(y, exog=None)\n</code></pre> <p>Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and  predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the <code>filter_train_X_y_for_step</code> method.</p> <code>y_train</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_train</code> for each  step in the form {step: y_step_[i]}.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of the columns of the matrix created internally for training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def _create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[\n    pd.DataFrame, \n    dict[int, pd.Series], \n    list[str], \n    list[str], \n    list[str], \n    dict[str, type], \n    dict[str, type]\n]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables. The resulting matrices contain the target variable and \n    predictors needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the `filter_train_X_y_for_step` method.\n    y_train : dict\n        Values of the time series related to each row of `X_train` for each \n        step in the form {step: y_step_[i]}.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of the columns of the matrix created internally for training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training before the transformation\n        applied by `transformer_exog`. If `transformer_exog` is not used, it\n        is equal to `exog_dtypes_out_`.\n    exog_dtypes_out_ : dict\n        Type of each exogenous variable/s used in training after the transformation \n        applied by `transformer_exog`. If `transformer_exog` is not used, it \n        is equal to `exog_dtypes_in_`.\n\n    \"\"\"\n\n    check_y(y=y)\n    y = input_to_frame(data=y, input_name='y')\n\n    if len(y) &lt; self.window_size + self.max_step:\n        raise ValueError(\n            f\"Minimum length of `y` for training this forecaster is \"\n            f\"{self.window_size + self.max_step}. Reduce the number of \"\n            f\"predicted steps, {self.max_step}, or the maximum \"\n            f\"window_size, {self.window_size}, if no more data is available.\\n\"\n            f\"    Length `y`: {len(y)}.\\n\"\n            f\"    Max step : {self.max_step}.\\n\"\n            f\"    Max window size: {self.window_size}.\\n\"\n            f\"    Lags window size: {self.max_lag}.\\n\"\n            f\"    Window features window size: {self.max_size_window_features}.\"\n        )\n\n    fit_transformer = False if self.is_fitted else True\n    y = transform_dataframe(\n            df                = y, \n            transformer       = self.transformer_y,\n            fit               = fit_transformer,\n            inverse_transform = False,\n        )\n    y_values, y_index = check_extract_values_and_index(data=y, data_label='`y`')\n\n    if self.differentiation is not None:\n        if not self.is_fitted:\n            y_values = self.differentiator.fit_transform(y_values)\n        else:\n            differentiator = copy(self.differentiator)\n            y_values = differentiator.fit_transform(y_values)\n\n    exog_names_in_ = None\n    exog_dtypes_in_ = None\n    exog_dtypes_out_ = None\n    X_as_pandas = False\n    if exog is not None:\n        check_exog(exog=exog, allow_nan=True)\n        exog = input_to_frame(data=exog, input_name='exog')\n        _, exog_index = check_extract_values_and_index(\n            data=exog, data_label='`exog`', ignore_freq=True, return_values=False\n        )\n\n        y_index_no_ws = y_index[self.window_size:]\n        len_y = len(y_values)\n        len_y_no_ws = len_y - self.window_size\n        len_exog = len(exog)\n        if not len_exog == len_y and not len_exog == len_y_no_ws:\n            raise ValueError(\n                f\"Length of `exog` must be equal to the length of `y` (if index is \"\n                f\"fully aligned) or length of `y` - `window_size` (if `exog` \"\n                f\"starts after the first `window_size` values).\\n\"\n                f\"    `exog`              : ({exog_index[0]} -- {exog_index[-1]})  (n={len_exog})\\n\"\n                f\"    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n\"\n                f\"    `y` - `window_size` : ({y_index_no_ws[0]} -- {y_index_no_ws[-1]})  (n={len_y_no_ws})\"\n            )\n\n        # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n        self.exog_in_ = True\n        exog_names_in_ = exog.columns.to_list()\n        exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = fit_transformer,\n                   inverse_transform = False\n               )\n\n        check_exog_dtypes(exog, call_check_exog=True)\n        exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n        X_as_pandas = any(\n            not pd.api.types.is_numeric_dtype(dtype) or pd.api.types.is_bool_dtype(dtype) \n            for dtype in set(exog.dtypes)\n        )\n\n        if len_exog == len_y:\n            if not (exog_index == y_index).all():\n                raise ValueError(\n                    \"When `exog` has the same length as `y`, the index of \"\n                    \"`exog` must be aligned with the index of `y` \"\n                    \"to ensure the correct alignment of values.\"\n                )\n            # The first `self.window_size` positions have to be removed from \n            # exog since they are not in X_train.\n            exog = exog.iloc[self.window_size:, ]\n        else:\n            if not (exog_index == y_index_no_ws).all():\n                raise ValueError(\n                    \"When `exog` doesn't contain the first `window_size` observations, \"\n                    \"the index of `exog` must be aligned with the index of `y` minus \"\n                    \"the first `window_size` observations to ensure the correct \"\n                    \"alignment of values.\"\n                )\n\n    X_train = []\n    X_train_features_names_out_ = []\n    train_index = y_index[self.window_size + (self.max_step - 1):]\n    len_train_index = len(train_index)\n\n    X_train_lags, y_train = self._create_lags(\n        y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n    )\n    if X_train_lags is not None:\n        X_train.append(X_train_lags)\n        X_train_features_names_out_.extend(self.lags_names)\n\n    X_train_window_features_names_out_ = None\n    if self.window_features is not None:\n        n_diff = 0 if self.differentiation is None else self.differentiation\n        end_wf = None if self.max_step == 1 else -(self.max_step - 1)\n        y_window_features = pd.Series(\n            y_values[n_diff:end_wf], index=y_index[n_diff:end_wf]\n        )\n        X_train_window_features, X_train_window_features_names_out_ = (\n            self._create_window_features(\n                y           = y_window_features, \n                X_as_pandas = X_as_pandas, \n                train_index = train_index\n            )\n        )\n        X_train.extend(X_train_window_features)\n        X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n    # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n    self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n\n    X_train_exog_names_out_ = None\n    if exog is not None:\n        X_train_exog_names_out_ = exog.columns.to_list()\n        if X_as_pandas:\n            exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(\n                exog=exog, steps=self.max_step\n            )\n            exog_direct.index = train_index\n        else:\n            exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(\n                exog=exog, steps=self.max_step\n            )\n\n        # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n        self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n\n        X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n        X_train.append(exog_direct)\n\n    if len(X_train) == 1:\n        X_train = X_train[0]\n    else:\n        if X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n\n    if X_as_pandas:\n        X_train.index = train_index\n    else:\n        X_train = pd.DataFrame(\n                      data    = X_train,\n                      index   = train_index,\n                      columns = X_train_features_names_out_\n                  )\n\n    y_train = {\n        step: pd.Series(\n                  data  = y_train[:, step - 1], \n                  index = y_index[self.window_size + step - 1:][:len_train_index],\n                  name  = f\"y_step_{step}\"\n              )\n        for step in self.steps\n    }\n\n    return (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_\n    )\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.create_train_X_y","title":"create_train_X_y","text":"<pre><code>create_train_X_y(y, exog=None)\n</code></pre> <p>Create training matrices from univariate time series and exogenous variables. The resulting matrices contain the target variable and  predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the <code>filter_train_X_y_for_step</code> method.</p> <code>y_train</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_train</code> for each  step in the form {step: y_step_[i]}.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None,\n) -&gt; tuple[pd.DataFrame, dict[int, pd.Series]]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables. The resulting matrices contain the target variable and \n    predictors needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the `filter_train_X_y_for_step` method.\n    y_train : dict\n        Values of the time series related to each row of `X_train` for each \n        step in the form {step: y_step_[i]}.\n\n    \"\"\"\n\n    output = self._create_train_X_y(y=y, exog=exog)\n\n    X_train = output[0]\n    y_train = output[1]\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.filter_train_X_y_for_step","title":"filter_train_X_y_for_step","text":"<pre><code>filter_train_X_y_for_step(\n    step, X_train, y_train, remove_suffix=False\n)\n</code></pre> <p>Select the columns needed to train a forecaster for a specific step. The input matrices should be created using <code>create_train_X_y</code> method.  This method updates the index of <code>X_train</code> to the corresponding one  according to <code>y_train</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\"  will be removed from the column names. </p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Step for which columns must be selected. Starts at 1.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <code>y_train</code> <code>dict</code> <p>Dict created with the <code>create_train_X_y</code> method, second return.</p> required <code>remove_suffix</code> <code>bool</code> <p>If True, suffix \"_step_i\" is removed from the column names.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train_step</code> <code>pandas DataFrame</code> <p>Training values (predictors) for the selected step.</p> <code>y_train_step</code> <code>pandas Series</code> <p>Values of the time series related to each row of <code>X_train</code>.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def filter_train_X_y_for_step(\n    self,\n    step: int,\n    X_train: pd.DataFrame,\n    y_train: dict[int, pd.Series],\n    remove_suffix: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Select the columns needed to train a forecaster for a specific step.  \n    The input matrices should be created using `create_train_X_y` method. \n    This method updates the index of `X_train` to the corresponding one \n    according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n    will be removed from the column names. \n\n    Parameters\n    ----------\n    step : int\n        Step for which columns must be selected. Starts at 1.\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n    y_train : dict\n        Dict created with the `create_train_X_y` method, second return.\n    remove_suffix : bool, default False\n        If True, suffix \"_step_i\" is removed from the column names.\n\n    Returns\n    -------\n    X_train_step : pandas DataFrame\n        Training values (predictors) for the selected step.\n    y_train_step : pandas Series\n        Values of the time series related to each row of `X_train`.\n\n    \"\"\"\n\n    if (step &lt; 1) or (step &gt; self.max_step):\n        raise ValueError(\n            f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n            f\"and the maximum step is {self.max_step}.\"\n        )\n\n    y_train_step = y_train[step]\n\n    # Matrix X_train starts at index 0.\n    if not self.exog_in_:\n        X_train_step = X_train\n    else:\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = (\n            len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        )\n        idx_columns_autoreg = np.arange(n_lags + n_window_features)\n        n_exog = len(self.X_train_direct_exog_names_out_) / self.max_step\n        idx_columns_exog = (\n            np.arange((step - 1) * n_exog, (step) * n_exog) + idx_columns_autoreg[-1] + 1\n        )\n        idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n        X_train_step = X_train.iloc[:, idx_columns]\n\n    X_train_step.index = y_train_step.index\n\n    if remove_suffix:\n        X_train_step.columns = [\n            col_name.replace(f\"_step_{step}\", \"\")\n            for col_name in X_train_step.columns\n        ]\n        y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n    return X_train_step, y_train_step\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect._train_test_split_one_step_ahead","title":"_train_test_split_one_step_ahead","text":"<pre><code>_train_test_split_one_step_ahead(\n    y, initial_train_size, exog=None\n)\n</code></pre> <p>Create matrices needed to train and test the forecaster for one-step-ahead predictions.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>initial_train_size</code> <code>int</code> <p>Initial size of the training set. It is the number of observations used to train the forecaster before making the first prediction.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Predictor values used to train the model.</p> <code>y_train</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_train</code> for each  step in the form {step: y_step_[i]}.</p> <code>X_test</code> <code>pandas DataFrame</code> <p>Predictor values used to test the model.</p> <code>y_test</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_test</code> for each  step in the form {step: y_step_[i]}.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def _train_test_split_one_step_ahead(\n    self,\n    y: pd.Series,\n    initial_train_size: int,\n    exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, dict[int, pd.Series], pd.DataFrame, dict[int, pd.Series]]:\n    \"\"\"\n    Create matrices needed to train and test the forecaster for one-step-ahead\n    predictions.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    initial_train_size : int\n        Initial size of the training set. It is the number of observations used\n        to train the forecaster before making the first prediction.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : dict\n        Values of the time series related to each row of `X_train` for each \n        step in the form {step: y_step_[i]}.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : dict\n        Values of the time series related to each row of `X_test` for each \n        step in the form {step: y_step_[i]}.\n\n    \"\"\"\n\n    is_fitted = self.is_fitted\n    self.is_fitted = False\n    X_train, y_train, *_ = self._create_train_X_y(\n        y    = y.iloc[: initial_train_size],\n        exog = exog.iloc[: initial_train_size] if exog is not None else None\n    )\n\n    test_init = initial_train_size - self.window_size\n    self.is_fitted = True\n    X_test, y_test, *_ = self._create_train_X_y(\n        y    = y.iloc[test_init:],\n        exog = exog.iloc[test_init:] if exog is not None else None\n    )\n\n    self.is_fitted = is_fitted\n\n    return X_train, y_train, X_test, y_test\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.create_sample_weights","title":"create_sample_weights","text":"<pre><code>create_sample_weights(X_train)\n</code></pre> <p>Create weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with <code>create_train_X_y</code> and <code>filter_train_X_y_for_step</code> methods, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n) -&gt; np.ndarray:\n    \"\"\"\n    Create weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with `create_train_X_y` and `filter_train_X_y_for_step`\n        methods, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                \"The resulting `sample_weight` cannot be normalized because \"\n                \"the sum of the weights is zero.\"\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.fit","title":"fit","text":"<pre><code>fit(\n    y,\n    exog=None,\n    store_last_window=True,\n    store_in_sample_residuals=False,\n    random_state=123,\n)\n</code></pre> <p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>store_last_window</code> <code>bool</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <code>True</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None,\n    store_last_window: bool = True,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_last_window : bool, default True\n        Whether or not to store the last window (`last_window_`) of training data.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.series_name_in_                    = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.exog_dtypes_out_                   = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_direct_exog_names_out_     = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.in_sample_residuals_by_bin_        = None\n    self.binner_intervals_                  = None\n    self.is_fitted                          = False\n    self.fit_date                           = None\n\n    (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_\n    ) = self._create_train_X_y(y=y, exog=exog)\n\n    def fit_forecaster(regressor, X_train, y_train, step):\n        \"\"\"\n        Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n        Parameters\n        ----------\n        regressor : object\n            Regressor to be fitted.\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `create_train_X_y` method, second return.\n        step : int\n            Step of the forecaster to be fitted.\n\n        Returns\n        -------\n        Tuple with the step, fitted regressor, true values and predicted \n        values for the step.\n\n        \"\"\"\n\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n        sample_weight = self.create_sample_weights(X_train=X_train_step)\n        if sample_weight is not None:\n            regressor.fit(\n                X             = X_train_step,\n                y             = y_train_step,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            regressor.fit(\n                X = X_train_step,\n                y = y_train_step,\n                **self.fit_kwargs\n            )\n\n        # NOTE: This is done to save time during fit in functions such as backtesting()\n        y_true_step = None\n        y_pred_step = None\n        if self._probabilistic_mode is not False:\n            y_true_step = y_train_step.to_numpy()\n            y_pred_step = regressor.predict(X_train_step)\n\n        return step, regressor, y_true_step, y_pred_step\n\n    results_fit = (\n        Parallel(n_jobs=self.n_jobs)\n        (delayed(fit_forecaster)\n        (\n            regressor = copy(self.regressor),\n            X_train   = X_train,\n            y_train   = y_train,\n            step      = step\n        )\n        for step in self.steps)\n    )\n\n    self.regressors_ = {step: regressor for step, regressor, *_ in results_fit}\n\n    if self._probabilistic_mode is not False:\n        y_true, y_pred = zip(*[(y_true, y_pred) for *_, y_true, y_pred in results_fit])\n        self._binning_in_sample_residuals(\n            y_true                    = np.concatenate(y_true),\n            y_pred                    = np.concatenate(y_pred),\n            store_in_sample_residuals = store_in_sample_residuals,\n            random_state              = random_state\n        )\n\n    self.X_train_features_names_out_ = X_train_features_names_out_\n\n    self.is_fitted = True\n    self.series_name_in_ = y.name if y.name is not None else 'y'\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = y.index[[0, -1]]\n    self.index_type_ = type(y.index)\n    if isinstance(y.index, pd.DatetimeIndex):\n        self.index_freq_ = y.index.freqstr\n    else: \n        self.index_freq_ = y.index.step\n\n    if exog is not None:\n        self.exog_in_ = True\n        self.exog_type_in_ = type(exog)\n        self.exog_names_in_ = exog_names_in_\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n    if store_last_window:\n        self.last_window_ = (\n            y.iloc[-self.window_size:]\n            .copy()\n            .to_frame(name=y.name if y.name is not None else 'y')\n        )\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect._binning_in_sample_residuals","title":"_binning_in_sample_residuals","text":"<pre><code>_binning_in_sample_residuals(\n    y_true,\n    y_pred,\n    store_in_sample_residuals=False,\n    random_state=123,\n)\n</code></pre> <p>Bin residuals according to the predicted value each residual is associated with. First a <code>skforecast.preprocessing.QuantileBinner</code> object is fitted to the predicted values. Then, residuals are binned according to the predicted value each residual is associated with. Residuals are stored in the forecaster object as <code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code>.</p> <p><code>y_true</code> and <code>y_pred</code> assumed to be differentiated and or transformed according to the attributes <code>differentiation</code> and <code>transformer_y</code>. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code>. The total number of residuals stored is <code>10_000</code>. New in version 0.15.0</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>numpy ndarray</code> <p>True values of the time series.</p> required <code>y_pred</code> <code>numpy ndarray</code> <p>Predicted values of the time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def _binning_in_sample_residuals(\n    self,\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Bin residuals according to the predicted value each residual is\n    associated with. First a `skforecast.preprocessing.QuantileBinner` object\n    is fitted to the predicted values. Then, residuals are binned according\n    to the predicted value each residual is associated with. Residuals are\n    stored in the forecaster object as `in_sample_residuals_` and\n    `in_sample_residuals_by_bin_`.\n\n    `y_true` and `y_pred` assumed to be differentiated and or transformed\n    according to the attributes `differentiation` and `transformer_y`.\n    The number of residuals stored per bin is limited to \n    `10_000 // self.binner.n_bins_`. The total number of residuals stored is\n    `10_000`.\n    **New in version 0.15.0**\n\n    Parameters\n    ----------\n    y_true : numpy ndarray\n        True values of the time series.\n    y_pred : numpy ndarray\n        Predicted values of the time series.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    residuals = y_true - y_pred\n\n    if self._probabilistic_mode == \"binned\":\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        self.binner.fit(y_pred)\n        self.binner_intervals_ = self.binner.intervals_\n\n    if store_in_sample_residuals:\n        rng = np.random.default_rng(seed=random_state)\n        if self._probabilistic_mode == \"binned\":\n            data['bin'] = self.binner.transform(y_pred).astype(int)\n            self.in_sample_residuals_by_bin_ = (\n                data.groupby('bin')['residuals'].apply(np.array).to_dict()\n            )\n\n            max_sample = 10_000 // self.binner.n_bins_\n            for k, v in self.in_sample_residuals_by_bin_.items():\n                if len(v) &gt; max_sample:\n                    sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                    self.in_sample_residuals_by_bin_[k] = sample\n\n        if len(residuals) &gt; 10_000:\n            residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=10_000)\n            ]\n\n        self.in_sample_residuals_ = residuals\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect._create_predict_inputs","title":"_create_predict_inputs","text":"<pre><code>_create_predict_inputs(\n    steps=None,\n    last_window=None,\n    exog=None,\n    predict_probabilistic=False,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    check_inputs=True,\n)\n</code></pre> <p>Create the inputs needed for the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>predict_probabilistic</code> <code>bool</code> <p>If <code>True</code>, the necessary checks for probabilistic predictions will be  performed.</p> <code>False</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Xs</code> <code>list</code> <p>List of numpy arrays with the predictors for each step.</p> <code>Xs_col_names</code> <code>list</code> <p>Names of the columns of the matrix created internally for prediction.</p> <code>steps</code> <code>list</code> <p>Steps to predict.</p> <code>prediction_index</code> <code>pandas Index</code> <p>Index of the predictions.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    predict_probabilistic: bool = False,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    check_inputs: bool = True\n) -&gt; tuple[list[np.ndarray], list[str], list[int], pd.Index]:\n    \"\"\"\n    Create the inputs needed for the prediction process.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    predict_probabilistic : bool, default False\n        If `True`, the necessary checks for probabilistic predictions will be \n        performed.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    Xs : list\n        List of numpy arrays with the predictors for each step.\n    Xs_col_names : list\n        Names of the columns of the matrix created internally for prediction.\n    steps : list\n        Steps to predict.\n    prediction_index : pandas Index\n        Index of the predictions.\n\n    \"\"\"\n\n    steps = prepare_steps_direct(\n                max_step = self.max_step,\n                steps    = steps\n            )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name = type(self).__name__,\n            steps           = steps,\n            is_fitted       = self.is_fitted,\n            exog_in_        = self.exog_in_,\n            index_type_     = self.index_type_,\n            index_freq_     = self.index_freq_,\n            window_size     = self.window_size,\n            last_window     = last_window,\n            exog            = exog,\n            exog_names_in_  = self.exog_names_in_,\n            interval        = None,\n            max_step        = self.max_step\n        )\n\n        if predict_probabilistic:\n            check_residuals_input(\n                forecaster_name              = type(self).__name__,\n                use_in_sample_residuals      = use_in_sample_residuals,\n                in_sample_residuals_         = self.in_sample_residuals_,\n                out_sample_residuals_        = self.out_sample_residuals_,\n                use_binned_residuals         = use_binned_residuals,\n                in_sample_residuals_by_bin_  = self.in_sample_residuals_by_bin_,\n                out_sample_residuals_by_bin_ = self.out_sample_residuals_by_bin_\n            )\n\n    last_window_values = (\n        last_window.iloc[-self.window_size:].to_numpy(copy=True).ravel()\n    )\n    last_window_values = transform_numpy(\n                             array             = last_window_values,\n                             transformer       = self.transformer_y,\n                             fit               = False,\n                             inverse_transform = False\n                         )\n    if self.differentiation is not None:\n        last_window_values = self.differentiator.fit_transform(last_window_values)\n\n    X_autoreg = []\n    Xs_col_names = []\n    if self.lags is not None:\n        X_lags = last_window_values[-self.lags]\n        X_autoreg.append(X_lags)\n        Xs_col_names.extend(self.lags_names)\n\n    if self.window_features is not None:\n        n_diff = 0 if self.differentiation is None else self.differentiation\n        X_window_features = np.concatenate(\n            [\n                wf.transform(last_window_values[n_diff:])\n                for wf in self.window_features\n            ]\n        )\n        X_autoreg.append(X_window_features)\n        Xs_col_names.extend(self.X_train_window_features_names_out_)\n\n    X_autoreg = np.concatenate(X_autoreg).reshape(1, -1)\n    if exog is not None:\n        exog = input_to_frame(data=exog, input_name='exog')\n        exog = exog[self.exog_names_in_]\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )\n        check_exog_dtypes(exog=exog)\n        exog_values, _ = exog_to_direct_numpy(\n                             exog  = exog.to_numpy()[:max(steps)],\n                             steps = max(steps)\n                         )\n        exog_values = exog_values[0]\n\n        n_exog = exog.shape[1]\n        Xs = [\n            np.concatenate(\n                [\n                    X_autoreg,\n                    exog_values[(step - 1) * n_exog : step * n_exog].reshape(1, -1),\n                ],\n                axis=1\n            )\n            for step in steps\n        ]\n        # HACK: This is not the best way to do it. Can have any problem\n        # if the exog_columns are not in the same order as the\n        # self.window_features_names.\n        Xs_col_names = Xs_col_names + exog.columns.to_list()\n    else:\n        Xs = [X_autoreg] * len(steps)\n\n    prediction_index = expand_index(\n                           index = last_window.index,\n                           steps = max(steps)\n                       )[np.array(steps) - 1]\n    if isinstance(last_window.index, pd.DatetimeIndex) and np.array_equal(\n        steps, np.arange(min(steps), max(steps) + 1)\n    ):\n        prediction_index.freq = last_window.index.freq\n\n    # HACK: Why no use self.X_train_features_names_out_ as Xs_col_names?\n    return Xs, Xs_col_names, steps, prediction_index\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.create_predict_X","title":"create_predict_X","text":"<pre><code>create_predict_X(\n    steps=None,\n    last_window=None,\n    exog=None,\n    check_inputs=True,\n)\n</code></pre> <p>Create the predictors needed to predict <code>steps</code> ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>X_predict</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the predictors for each step. The index  is the same as the prediction index.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def create_predict_X(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    check_inputs: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create the predictors needed to predict `steps` ahead.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    X_predict : pandas DataFrame\n        Pandas DataFrame with the predictors for each step. The index \n        is the same as the prediction index.\n\n    \"\"\"\n\n    (\n        Xs,\n        Xs_col_names,\n        steps,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs\n        )\n\n    X_predict = pd.DataFrame(\n                    data    = np.concatenate(Xs, axis=0), \n                    columns = Xs_col_names, \n                    index   = prediction_index\n                )\n\n    if self.exog_in_:\n        categorical_features = any(\n            not pd.api.types.is_numeric_dtype(dtype) or pd.api.types.is_bool_dtype(dtype) \n            for dtype in set(self.exog_dtypes_out_)\n        )\n        if categorical_features:\n            X_predict = X_predict.astype(self.exog_dtypes_out_)\n\n    if self.transformer_y is not None or self.differentiation is not None:\n        warnings.warn(\n            \"The output matrix is in the transformed scale due to the \"\n            \"inclusion of transformations or differentiation in the Forecaster. \"\n            \"As a result, any predictions generated using this matrix will also \"\n            \"be in the transformed scale. Please refer to the documentation \"\n            \"for more details: \"\n            \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n            DataTransformationWarning\n        )\n\n    return X_predict\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.predict","title":"predict","text":"<pre><code>predict(\n    steps=None,\n    last_window=None,\n    exog=None,\n    check_inputs=True,\n)\n</code></pre> <p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def predict(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    check_inputs: bool = True\n) -&gt; pd.Series:\n    \"\"\"\n    Predict n steps ahead.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    (\n        Xs,\n        _,\n        steps,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs,\n        )\n\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = np.array([\n            regressor.predict(X).ravel()[0] \n            for regressor, X in zip(regressors, Xs)\n        ])\n\n    if self.differentiation is not None:\n        predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n    predictions = transform_numpy(\n                      array             = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = prediction_index,\n                      name  = 'pred'\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.predict_bootstrapping","title":"predict_bootstrapping","text":"<pre><code>predict_bootstrapping(\n    steps=None,\n    last_window=None,\n    exog=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n)\n</code></pre> <p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the References section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the References section for more information. \n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    (\n        Xs,\n        _,\n        steps,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps                   = steps, \n            last_window             = last_window, \n            exog                    = exog,\n            predict_probabilistic   = True, \n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    # NOTE: Predictors and residuals are transformed and differentiated\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = np.array([\n            regressor.predict(X).ravel()[0] \n            for regressor, X in zip(regressors, Xs)\n        ])\n\n    rng = np.random.default_rng(seed=random_state)\n    if not use_binned_residuals:\n        sampled_residuals = residuals[\n            rng.integers(low=0, high=residuals.size, size=(len(steps), n_boot))\n        ]\n    else:\n        predicted_bins = self.binner.transform(predictions)\n        sampled_residuals = np.full(\n                                shape      = (predicted_bins.size, n_boot),\n                                fill_value = np.nan,\n                                order      = 'C',\n                                dtype      = float\n                            )\n        for i, bin in enumerate(predicted_bins):\n            sampled_residuals[i, :] = residuals_by_bin[bin][\n                rng.integers(low=0, high=residuals_by_bin[bin].size, size=n_boot)\n            ]\n\n    boot_predictions = np.tile(predictions, (n_boot, 1)).T\n    boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n    boot_predictions = boot_predictions + sampled_residuals\n\n    if self.differentiation is not None:\n        boot_predictions = (\n            self.differentiator.inverse_transform_next_window(boot_predictions)\n        )\n\n    if self.transformer_y:\n        boot_predictions = np.apply_along_axis(\n                               func1d            = transform_numpy,\n                               axis              = 0,\n                               arr               = boot_predictions,\n                               transformer       = self.transformer_y,\n                               fit               = False,\n                               inverse_transform = True\n                           )\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = prediction_index,\n                           columns = boot_columns\n                       )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect._predict_interval_conformal","title":"_predict_interval_conformal","text":"<pre><code>_predict_interval_conformal(\n    steps=None,\n    last_window=None,\n    exog=None,\n    nominal_coverage=0.95,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n)\n</code></pre> <p>Generate prediction intervals using the conformal prediction  split method [1]_.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>nominal_coverage</code> <code>float</code> <p>Nominal coverage, also known as expected coverage, of the prediction intervals. Must be between 0 and 1.</p> <code>0.95</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def _predict_interval_conformal(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    nominal_coverage: float = 0.95,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate prediction intervals using the conformal prediction \n    split method [1]_.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    nominal_coverage : float, default 0.95\n        Nominal coverage, also known as expected coverage, of the prediction\n        intervals. Must be between 0 and 1.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    References\n    ----------\n    .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    (\n        Xs,\n        _,\n        steps,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps                   = steps, \n            last_window             = last_window, \n            exog                    = exog,\n            predict_probabilistic   = True, \n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    # NOTE: Predictors and residuals are transformed and differentiated  \n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = np.array([\n            regressor.predict(X).ravel()[0] \n            for regressor, X in zip(regressors, Xs)\n        ])\n\n    if use_binned_residuals:\n        correction_factor_by_bin = {\n            k: np.quantile(np.abs(v), nominal_coverage)\n            for k, v in residuals_by_bin.items()\n        }\n        replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n        predictions_bin = self.binner.transform(predictions)\n        correction_factor = replace_func(predictions_bin)\n    else:\n        correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n    lower_bound = predictions - correction_factor\n    upper_bound = predictions + correction_factor\n    predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n    if self.differentiation is not None:\n        predictions = (\n            self.differentiator.inverse_transform_next_window(predictions)\n        )\n\n    if self.transformer_y:\n        predictions = np.apply_along_axis(\n                          func1d            = transform_numpy,\n                          axis              = 0,\n                          arr               = predictions,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n    predictions = pd.DataFrame(\n                      data    = predictions,\n                      index   = prediction_index,\n                      columns = [\"pred\", \"lower_bound\", \"upper_bound\"]\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.predict_interval","title":"predict_interval","text":"<pre><code>predict_interval(\n    steps=None,\n    last_window=None,\n    exog=None,\n    method=\"bootstrapping\",\n    interval=[5, 95],\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n)\n</code></pre> <p>Predict n steps ahead and estimate prediction intervals using either  bootstrapping or conformal prediction methods. Refer to the References  section for additional details on these methods.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction  intervals [1]_.</li> <li>'conformal': Employs the conformal prediction split method for  interval estimation [2]_.</li> </ul> <code>'bootstrapping'</code> <code>interval</code> <code>(float, list, tuple)</code> <p>Confidence level of the prediction interval. Interpretation depends  on the method used:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0  and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code>  percentiles.</li> <li>If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which  must be between 0 and 100 inclusive. For example, interval  of 95% should be as <code>interval = [2.5, 97.5]</code>.</li> <li>When using <code>method='conformal'</code>, the interval must be a float or  a list/tuple defining a symmetric interval.</li> </ul> <code>[5, 95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> <p>.. [2] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    method: str = 'bootstrapping',\n    interval: float | list[float] | tuple[float] = [5, 95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using either \n    bootstrapping or conformal prediction methods. Refer to the References \n    section for additional details on these methods.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    method : str, default 'bootstrapping'\n        Technique used to estimate prediction intervals. Available options:\n\n        - 'bootstrapping': Bootstrapping is used to generate prediction \n        intervals [1]_.\n        - 'conformal': Employs the conformal prediction split method for \n        interval estimation [2]_.\n    interval : float, list, tuple, default [5, 95]\n        Confidence level of the prediction interval. Interpretation depends \n        on the method used:\n\n        - If `float`, represents the nominal (expected) coverage (between 0 \n        and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]` \n        percentiles.\n        - If `list` or `tuple`, defines the exact percentiles to compute, which \n        must be between 0 and 100 inclusive. For example, interval \n        of 95% should be as `interval = [2.5, 97.5]`.\n        - When using `method='conformal'`, the interval must be a float or \n        a list/tuple defining a symmetric interval.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    if method == \"bootstrapping\":\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=False)\n            interval = np.array(interval) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal='interval')\n            interval = np.array([0.5 - interval / 2, 0.5 + interval / 2])\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    elif method == 'conformal':\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal='interval')\n            nominal_coverage = interval\n\n        predictions = self._predict_interval_conformal(\n                          steps                   = steps,\n                          last_window             = last_window,\n                          exog                    = exog,\n                          nominal_coverage        = nominal_coverage,\n                          use_in_sample_residuals = use_in_sample_residuals,\n                          use_binned_residuals    = use_binned_residuals\n                      )\n    else:\n        raise ValueError(\n            f\"Invalid `method` '{method}'. Choose 'bootstrapping' or 'conformal'.\"\n        )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.predict_quantiles","title":"predict_quantiles","text":"<pre><code>predict_quantiles(\n    steps=None,\n    last_window=None,\n    exog=None,\n    quantiles=[0.05, 0.5, 0.95],\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n)\n</code></pre> <p>Bootstrapping based predicted quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>quantiles</code> <code>(list, tuple)</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>[0.05, 0.5, 0.95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating quantiles.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Quantiles predicted by the forecaster.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def predict_quantiles(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    quantiles: list[float] | tuple[float] = [0.05, 0.5, 0.95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Bootstrapping based predicted quantiles.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    quantiles : list, tuple, default [0.05, 0.5, 0.95]\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating quantiles.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Quantiles predicted by the forecaster.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    check_interval(quantiles=quantiles)\n\n    predictions = self.predict_bootstrapping(\n                      steps                   = steps,\n                      last_window             = last_window,\n                      exog                    = exog,\n                      n_boot                  = n_boot,\n                      random_state            = random_state,\n                      use_in_sample_residuals = use_in_sample_residuals,\n                      use_binned_residuals    = use_binned_residuals\n                  )\n\n    predictions = predictions.quantile(q=quantiles, axis=1).transpose()\n    predictions.columns = [f'q_{q}' for q in quantiles]\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.predict_dist","title":"predict_dist","text":"<pre><code>predict_dist(\n    distribution,\n    steps=None,\n    last_window=None,\n    exog=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n)\n</code></pre> <p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>object</code> <p>A distribution object from scipy.stats with methods <code>_pdf</code> and <code>fit</code>.  For example scipy.stats.norm.</p> required <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def predict_dist(\n    self,\n    distribution: object,\n    steps: int | list[int] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    distribution : object\n        A distribution object from scipy.stats with methods `_pdf` and `fit`. \n        For example scipy.stats.norm.\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    if not hasattr(distribution, \"_pdf\") or not callable(getattr(distribution, \"fit\", None)):\n        raise TypeError(\n            \"`distribution` must be a valid probability distribution object \"\n            \"from scipy.stats, with methods `_pdf` and `fit`.\"\n        )\n\n    predictions = self.predict_bootstrapping(\n                      steps                   = steps,\n                      last_window             = last_window,\n                      exog                    = exog,\n                      n_boot                  = n_boot,\n                      random_state            = random_state,\n                      use_in_sample_residuals = use_in_sample_residuals,\n                      use_binned_residuals    = use_binned_residuals\n                  )       \n\n    param_names = [\n        p for p in inspect.signature(distribution._pdf).parameters\n        if not p == 'x'\n    ] + [\"loc\", \"scale\"]\n\n    predictions[param_names] = (\n        predictions.apply(\n            lambda x: distribution.fit(x), axis=1, result_type='expand'\n        )\n    )\n    predictions = predictions[param_names]\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.set_params","title":"set_params","text":"<pre><code>set_params(params)\n</code></pre> <p>Set new values to the parameters of the scikit-learn model stored in the forecaster. It is important to note that all models share the same  configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def set_params(\n    self, \n    params: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the scikit-learn model stored in the\n    forecaster. It is important to note that all models share the same \n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.regressors_ = {\n        step: clone(self.regressor)\n        for step in self.steps\n    }\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.set_fit_kwargs","title":"set_fit_kwargs","text":"<pre><code>set_fit_kwargs(fit_kwargs)\n</code></pre> <p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.set_lags","title":"set_lags","text":"<pre><code>set_lags(lags=None)\n</code></pre> <p>Set new value to the attribute <code>lags</code>. Attributes <code>lags_names</code>,  <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def set_lags(\n    self, \n    lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `lags`. Attributes `lags_names`, \n    `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range, default None\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if self.window_features is None and lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation\n        self.differentiator.set_params(window_size=self.window_size)\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.set_window_features","title":"set_window_features","text":"<pre><code>set_window_features(window_features=None)\n</code></pre> <p>Set new value to the attribute <code>window_features</code>. Attributes  <code>max_size_window_features</code>, <code>window_features_names</code>,  <code>window_features_class_names</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def set_window_features(\n    self, \n    window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `window_features`. Attributes \n    `max_size_window_features`, `window_features_names`, \n    `window_features_class_names` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    window_features : object, list, default None\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if window_features is None and self.lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.window_features, self.window_features_names, self.max_size_window_features = (\n        initialize_window_features(window_features)\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ] \n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation   \n        self.differentiator.set_params(window_size=self.window_size)\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.set_in_sample_residuals","title":"set_in_sample_residuals","text":"<pre><code>set_in_sample_residuals(y, exog=None, random_state=123)\n</code></pre> <p>Set in-sample residuals in case they were not calculated during the training process. </p> <p>In-sample residuals are calculated as the difference between the true  values and the predictions made by the forecaster using the training  data. The following internal attributes are updated:</p> <ul> <li><code>in_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>binner_intervals_</code>: intervals used to bin the residuals are calculated using the quantiles of the predicted values.</li> <li><code>in_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the intervals of the predicted values and the values are the residuals associated with that range. </li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>in_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def set_in_sample_residuals(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process. \n\n    In-sample residuals are calculated as the difference between the true \n    values and the predictions made by the forecaster using the training \n    data. The following internal attributes are updated:\n\n    + `in_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `binner_intervals_`: intervals used to bin the residuals are calculated\n    using the quantiles of the predicted values.\n    + `in_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the intervals of the predicted values and the values are\n    the residuals associated with that range. \n\n    A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    check_y(y=y)\n    y_index_range = check_extract_values_and_index(\n        data=y, data_label='`y`', return_values=False\n    )[1][[0, -1]]\n    if not y_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `y` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {y_index_range}\"\n        )\n\n    # NOTE: This attributes are modified in _create_train_X_y, store original values\n    original_exog_in_ = self.exog_in_\n    original_X_train_window_features_names_out_ = self.X_train_window_features_names_out_\n    original_X_train_direct_exog_names_out_ = self.X_train_direct_exog_names_out_\n\n    (\n        X_train,\n        y_train,\n        _,\n        _,\n        X_train_features_names_out_,\n        *_\n    ) = self._create_train_X_y(y=y, exog=exog)\n\n    if not X_train_features_names_out_ == self.X_train_features_names_out_:\n\n        # NOTE: Reset attributes modified in _create_train_X_y to their original values\n        self.exog_in_ = original_exog_in_\n        self.X_train_window_features_names_out_ = original_X_train_window_features_names_out_\n        self.X_train_direct_exog_names_out_ = original_X_train_direct_exog_names_out_\n\n        raise ValueError(\n            f\"Feature mismatch detected after matrix creation. The features \"\n            f\"generated from the provided data do not match those used during \"\n            f\"the training process. To correctly set in-sample residuals, \"\n            f\"ensure that the same data and preprocessing steps are applied.\\n\"\n            f\"    Expected output : {self.X_train_features_names_out_}\\n\"\n            f\"    Current output  : {X_train_features_names_out_}\"\n        )\n\n    y_true_steps = []\n    y_pred_steps = []\n    self.in_sample_residuals_ = {}\n    for step in self.steps:\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n\n        y_true_steps.append(y_train_step.to_numpy())\n        y_pred_steps.append(self.regressors_[step].predict(X_train_step))\n\n    self._binning_in_sample_residuals(\n        y_true                    = np.concatenate(y_true_steps),\n        y_pred                    = np.concatenate(y_pred_steps),\n        store_in_sample_residuals = True,\n        random_state              = random_state\n    )\n\n    # NOTE: Reset attributes modified in _create_train_X_y to their original values\n    self.exog_in_ = original_exog_in_\n    self.X_train_window_features_names_out_ = original_X_train_window_features_names_out_\n    self.X_train_direct_exog_names_out_ = original_X_train_direct_exog_names_out_\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.set_out_sample_residuals","title":"set_out_sample_residuals","text":"<pre><code>set_out_sample_residuals(\n    y_true, y_pred, append=False, random_state=123\n)\n</code></pre> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. <code>y_true</code> and <code>y_pred</code> are expected to be in the original scale of the time series. Residuals are calculated as <code>y_true</code> - <code>y_pred</code>, after applying the necessary transformations and differentiations if the forecaster includes them (<code>self.transformer_y</code> and <code>self.differentiation</code>). Two internal attributes are updated:</p> <ul> <li><code>out_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>out_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the  intervals of the predicted values and the values are the residuals associated with that range. If a bin binning is empty, it is filled with a random sample of residuals from other bins. This is done to ensure that all bins have at least one residual and can be used in the prediction process.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>out_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>numpy ndarray, pandas Series</code> <p>True values of the time series from which the residuals have been calculated.</p> required <code>y_pred</code> <code>numpy ndarray, pandas Series</code> <p>Predicted values of the time series.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the forecaster. If after appending the new residuals, the limit of <code>10_000 // self.binner.n_bins_</code> values per bin is reached, a random sample of residuals is stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    append: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. `y_true` and `y_pred` are expected\n    to be in the original scale of the time series. Residuals are calculated\n    as `y_true` - `y_pred`, after applying the necessary transformations and\n    differentiations if the forecaster includes them (`self.transformer_y`\n    and `self.differentiation`). Two internal attributes are updated:\n\n    + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `out_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the  intervals of the predicted values and the values are\n    the residuals associated with that range. If a bin binning is empty, it\n    is filled with a random sample of residuals from other bins. This is done\n    to ensure that all bins have at least one residual and can be used in the\n    prediction process.\n\n    A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y_true : numpy ndarray, pandas Series\n        True values of the time series from which the residuals have been\n        calculated.\n    y_pred : numpy ndarray, pandas Series\n        Predicted values of the time series.\n    append : bool, default False\n        If `True`, new residuals are added to the once already stored in the\n        forecaster. If after appending the new residuals, the limit of\n        `10_000 // self.binner.n_bins_` values per bin is reached, a random\n        sample of residuals is stored.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if len(y_true) != len(y_pred):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same length. \"\n            f\"Got {len(y_true)} and {len(y_pred)}.\"\n        )\n\n    if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n        if not y_true.index.equals(y_pred.index):\n            raise ValueError(\n                \"`y_true` and `y_pred` must have the same index.\"\n            )\n\n    y_true = deepcopy(y_true)\n    y_pred = deepcopy(y_pred)\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.to_numpy()\n\n    if self.transformer_y:\n        y_true = transform_numpy(\n                     array             = y_true,\n                     transformer       = self.transformer_y,\n                     fit               = False,\n                     inverse_transform = False\n                 )\n        y_pred = transform_numpy(\n                     array             = y_pred,\n                     transformer       = self.transformer_y,\n                     fit               = False,\n                     inverse_transform = False\n                 )\n\n    if self.differentiation is not None:\n        differentiator = copy(self.differentiator)\n        differentiator.set_params(window_size=None)\n        y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n        y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n\n    data = pd.DataFrame(\n        {'prediction': y_pred, 'residuals': y_true - y_pred}\n    ).dropna()\n    y_pred = data['prediction'].to_numpy()\n    residuals = data['residuals'].to_numpy()\n\n    data['bin'] = self.binner.transform(y_pred).astype(int)\n    residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n    out_sample_residuals = (\n        np.array([]) \n        if self.out_sample_residuals_ is None\n        else self.out_sample_residuals_\n    )\n    out_sample_residuals_by_bin = (\n        {} \n        if self.out_sample_residuals_by_bin_ is None\n        else self.out_sample_residuals_by_bin_\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    max_samples = 10_000 // self.binner.n_bins_\n    rng = np.random.default_rng(seed=random_state)\n    for k, v in out_sample_residuals_by_bin.items():\n        if len(v) &gt; max_samples:\n            sample = rng.choice(a=v, size=max_samples, replace=False)\n            out_sample_residuals_by_bin[k] = sample\n\n    bin_keys = (\n        []\n        if self.binner_intervals_ is None\n        else self.binner_intervals_.keys()\n    )\n    for k in bin_keys:\n        if k not in out_sample_residuals_by_bin:\n            out_sample_residuals_by_bin[k] = np.array([])\n\n    empty_bins = [\n        k for k, v in out_sample_residuals_by_bin.items() \n        if v.size == 0\n    ]\n    if empty_bins:\n        warnings.warn(\n            f\"The following bins have no out of sample residuals: {empty_bins}. \"\n            f\"No predicted values fall in the interval \"\n            f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n            f\"Empty bins will be filled with a random sample of residuals.\",\n            ResidualsUsageWarning\n        )\n        empty_bin_size = min(max_samples, len(out_sample_residuals))\n        for k in empty_bins:\n            out_sample_residuals_by_bin[k] = rng.choice(\n                a       = out_sample_residuals,\n                size    = empty_bin_size,\n                replace = False\n            )\n\n    if len(out_sample_residuals) &gt; 10_000:\n        out_sample_residuals = rng.choice(\n            a       = out_sample_residuals, \n            size    = 10_000, \n            replace = False\n        )\n\n    self.out_sample_residuals_ = out_sample_residuals\n    self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/ForecasterDirect.html#skforecast.direct._forecaster_direct.ForecasterDirect.get_feature_importances","title":"get_feature_importances","text":"<pre><code>get_feature_importances(step, sort_importance=True)\n</code></pre> <p>Return feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\direct\\_forecaster_direct.py</code> <pre><code>def get_feature_importances(\n    self, \n    step: int,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importance of the model stored in the forecaster for a\n    specific step. Since a separate model is created for each forecast time\n    step, it is necessary to select the model from which retrieve information.\n    Only valid when regressor stores internally the feature importances in\n    the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n    `None`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n    sort_importance: bool, default True\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not isinstance(step, int):\n        raise TypeError(f\"`step` must be an integer. Got {type(step)}.\")\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `get_feature_importances()`.\"\n        )\n\n    if (step &lt; 1) or (step &gt; self.max_step):\n        raise ValueError(\n            f\"The step must have a value from 1 to the maximum number of steps \"\n            f\"({self.max_step}). Got {step}.\"\n        )\n\n    if isinstance(self.regressor, Pipeline):\n        estimator = self.regressors_[step][-1]\n    else:\n        estimator = self.regressors_[step]\n\n    n_lags = len(self.lags) if self.lags is not None else 0\n    n_window_features = (\n        len(self.window_features_names) if self.window_features is not None else 0\n    )\n    idx_columns_autoreg = np.arange(n_lags + n_window_features)\n    if not self.exog_in_:\n        idx_columns = idx_columns_autoreg\n    else:\n        n_exog = len(self.X_train_direct_exog_names_out_) / self.max_step\n        idx_columns_exog = (\n            np.arange((step - 1) * n_exog, (step) * n_exog) + idx_columns_autoreg[-1] + 1\n        )\n        idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n\n    idx_columns = [int(x) for x in idx_columns]  # Required since numpy 2.0\n    feature_names = [\n        self.X_train_features_names_out_[i].replace(f\"_step_{step}\", \"\") \n        for i in idx_columns\n    ]\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            f\"Impossible to access feature importances for regressor of type \"\n            f\"{type(estimator)}. This method is only valid when the \"\n            f\"regressor stores internally the feature importances in the \"\n            f\"attribute `feature_importances_` or `coef_`.\"\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': feature_names,\n                                  'importance': feature_importances\n                              })\n        if sort_importance:\n            feature_importances = feature_importances.sort_values(\n                                      by='importance', ascending=False\n                                  )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html","title":"<code>ForecasterDirectMultiVariate</code>","text":""},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate","title":"skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate","text":"<pre><code>ForecasterDirectMultiVariate(\n    regressor,\n    level,\n    steps,\n    lags=None,\n    window_features=None,\n    transformer_series=StandardScaler(),\n    transformer_exog=None,\n    weight_func=None,\n    differentiation=None,\n    fit_kwargs=None,\n    binner_kwargs=None,\n    n_jobs=\"auto\",\n    forecaster_id=None,\n)\n</code></pre> <p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a autoregressive multivariate direct multi-step forecaster. A separate model  is created for each forecast time step. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>level</code> <code>str</code> <p>Name of the time series to be predicted.</p> required <code>steps</code> <code>int</code> <p>Maximum number of future steps the forecaster will predict when using method <code>predict()</code>. Since a different model is created for each step, this value must be defined before training.</p> required <code>lags</code> <code>int, list, numpy ndarray, range, dict</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>dict</code>: create different lags for each series. {'series_column_name': lags}.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>None</code> <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>None</code> <code>transformer_series</code> <code>(transformer(preprocessor), dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>`sklearn.preprocessing.StandardScaler`</code> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Before returning a prediction, the differencing operation is reversed.</p> <code>None</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>None</code> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code> used to discretize  the residuals into k bins according to the predicted values associated  with each residual. Available arguments are: <code>n_bins</code>, <code>method</code>, <code>subsample</code>, <code>random_state</code> and <code>dtype</code>. Argument <code>method</code> is passed internally to the function <code>numpy.percentile</code>. New in version 0.15.0</p> <code>None</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster.</p> <code>'auto'</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API. An instance of this regressor is trained for each step. All of them  are stored in <code>self.regressors_</code>.</p> <code>regressors_</code> <code>dict</code> <p>Dictionary with regressors trained for each step. They are initialized  as a copy of <code>regressor</code>.</p> <code>steps</code> <code>numpy array</code> <p>Future steps the forecaster will predict when using method <code>predict()</code>.  Since a different model is created for each step, this value should be  defined before training.</p> <code>max_step</code> <code>int</code> <p>Maximum step the forecaster is able to predict. It is the maximum value included in <code>steps</code>.</p> <code>lags</code> <code>numpy ndarray, dict</code> <p>Lags used as predictors.</p> <code>lags_</code> <code>dict</code> <p>Dictionary with the lags of each series. Created from <code>lags</code> when  creating the training matrices and used internally to avoid overwriting.</p> <code>lags_names</code> <code>dict</code> <p>Names of the lags of each series.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_features</code> <code>list</code> <p>Class or list of classes used to create window features.</p> <code>window_features_names</code> <code>list</code> <p>Names of the window features to be included in the <code>X_train</code> matrix.</p> <code>window_features_class_names</code> <code>list</code> <p>Names of the classes used to create the window features.</p> <code>max_size_window_features</code> <code>int</code> <p>Maximum window size required by the window features.</p> <code>window_size</code> <code>int</code> <p>The window size needed to create the predictors. It is calculated as the  maximum value between <code>max_lag</code> and <code>max_size_window_features</code>. If  differentiation is used, <code>window_size</code> is increased by n units equal to  the order of differentiation so that predictors can be generated correctly.</p> <code>transformer_series</code> <code>transformer (preprocessor), dict, default None</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the  forecaster.</p> <code>differentiation_max</code> <code>int</code> <p>Maximum order of differentiation. For this Forecaster, it is equal to the value of the <code>differentiation</code> parameter.</p> <code>differentiator</code> <code>TimeSeriesDifferentiator</code> <p>Skforecast object used to differentiate the time series.</p> <code>differentiator_</code> <code>dict</code> <p>Dictionary with the <code>differentiator</code> for each series. It is created cloning the objects in <code>differentiator</code> and is used internally to avoid overwriting.</p> <code>last_window_</code> <code>pandas DataFrame</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the values needed to predict the next step immediately after the training data. These values are stored in the original scale of the time series before undergoing any transformations or differentiation. When <code>differentiation</code> parameter is specified, the dimensions of the <code>last_window_</code> are expanded as many values as the order of differentiation. For example, if <code>lags</code> = 7 and <code>differentiation</code> = 1, <code>last_window_</code> will have 8 values.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series used during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series added to <code>X_train</code> when creating the training matrices with <code>_create_train_X_y</code> method. It is a subset of  <code>series_names_in_</code>.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_direct_exog_names_out_</code> <code>list</code> <p>Same as <code>X_train_exog_names_out_</code> but using the direct format. The same  exogenous variable is repeated for each step.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting training data. Only stored up  to 10_000 values per series in the form <code>{series: residuals}</code>. If  <code>transformer_series</code> is not <code>None</code>, residuals are stored in the  transformed scale. If <code>differentiation</code> is not <code>None</code>, residuals are  stored after differentiation.</p> <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> per series in the form <code>{series: residuals}</code>. If <code>transformer_series</code> is not <code>None</code>, residuals are stored in the  transformed scale. If <code>differentiation</code> is not <code>None</code>, residuals are  stored after differentiation.  New in version 0.15.0</p> <code>out_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting non-training data. Only stored up  to 10_000 values per series in the form <code>{series: residuals}</code>. Use  <code>set_out_sample_residuals()</code> method to set values. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale. If  <code>differentiation</code> is not <code>None</code>, residuals are stored after differentiation. </p> <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> per series in the form <code>{series: residuals}</code>. If <code>transformer_series</code> is not <code>None</code>, residuals are stored in the  transformed scale. If <code>differentiation</code> is not <code>None</code>, residuals are  stored after differentiation.  New in version 0.15.0</p> <code>binner</code> <code>dict</code> <p>Dictionary of <code>skforecast.preprocessing.QuantileBinner</code> used to discretize residuals of each series into k bins according to the predicted values  associated with each residual. In the form <code>{series: binner}</code>. New in version 0.15.0</p> <code>binner_intervals_</code> <code>dict</code> <p>Intervals used to discretize residuals into k bins according to the predicted values associated with each residual. In the form <code>{series: binner_intervals_}</code>. New in version 0.15.0</p> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code>. New in version 0.15.0</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>_probabilistic_mode</code> <code>(str, bool)</code> <p>Private attribute used to indicate whether the forecaster should perform  some calculations during backtesting.</p> <code>dropna_from_series</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>encoding</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> Notes <p>A separate model is created for each forecasting time step. It is important to note that all models share the same parameter and hyperparameter configuration.</p> <p>Methods:</p> Name Description <code>create_train_X_y</code> <p>Create training matrices from multiple time series and exogenous</p> <code>filter_train_X_y_for_step</code> <p>Select the columns needed to train a forecaster for a specific step.  </p> <code>create_sample_weights</code> <p>Create weights for each observation according to the forecaster's attribute</p> <code>fit</code> <p>Training Forecaster.</p> <code>create_predict_X</code> <p>Create the predictors needed to predict <code>steps</code> ahead.</p> <code>predict</code> <p>Predict n steps ahead</p> <code>predict_bootstrapping</code> <p>Generate multiple forecasting predictions using a bootstrapping process. </p> <code>predict_interval</code> <p>Predict n steps ahead and estimate prediction intervals using either </p> <code>predict_quantiles</code> <p>Bootstrapping based predicted quantiles.</p> <code>predict_dist</code> <p>Fit a given probability distribution for each step. After generating </p> <code>set_params</code> <p>Set new values to the parameters of the scikit-learn model stored in the</p> <code>set_fit_kwargs</code> <p>Set new values for the additional keyword arguments passed to the <code>fit</code> </p> <code>set_lags</code> <p>Set new value to the attribute <code>lags</code>. Attributes <code>lags_names</code>, </p> <code>set_window_features</code> <p>Set new value to the attribute <code>window_features</code>. Attributes </p> <code>set_in_sample_residuals</code> <p>Set in-sample residuals in case they were not calculated during the</p> <code>set_out_sample_residuals</code> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample</p> <code>get_feature_importances</code> <p>Return feature importance of the model stored in the forecaster for a</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    level: str,\n    steps: int,\n    lags: int | list[int] | np.ndarray[int] | range[int] | dict[str, int | list] | None = None,\n    window_features: object | list[object] | None = None,\n    transformer_series: object | dict[str, object] | None = StandardScaler(),\n    transformer_exog: object | None = None,\n    weight_func: Callable | None = None,\n    differentiation: int | None = None,\n    fit_kwargs: dict[str, object] | None = None,\n    binner_kwargs: dict[str, object] | None = None,\n    n_jobs: int | str = 'auto',\n    forecaster_id: str | int | None = None\n) -&gt; None:\n\n    self.regressor                          = copy(regressor)\n    self.level                              = level\n    self.lags_                              = None\n    self.transformer_series                 = transformer_series\n    self.transformer_series_                = None\n    self.transformer_exog                   = transformer_exog\n    self.weight_func                        = weight_func\n    self.source_code_weight_func            = None\n    self.differentiation                    = differentiation\n    self.differentiation_max                = None\n    self.differentiator                     = None\n    self.differentiator_                    = None\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.series_names_in_                   = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.exog_dtypes_out_                   = None\n    self.X_train_series_names_in_           = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_direct_exog_names_out_     = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.out_sample_residuals_              = None\n    self.in_sample_residuals_by_bin_        = None\n    self.out_sample_residuals_by_bin_       = None\n    self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted                          = False\n    self.fit_date                           = None\n    self.skforecast_version                 = skforecast.__version__\n    self.python_version                     = sys.version.split(\" \")[0]\n    self.forecaster_id                      = forecaster_id\n    self._probabilistic_mode                = \"binned\"\n    self.dropna_from_series                 = False  # Ignored in this forecaster\n    self.encoding                           = None   # Ignored in this forecaster\n\n    if not isinstance(level, str):\n        raise TypeError(\n            f\"`level` argument must be a str. Got {type(level)}.\"\n        )\n\n    if not isinstance(steps, int):\n        raise TypeError(\n            f\"`steps` argument must be an int greater than or equal to 1. \"\n            f\"Got {type(steps)}.\"\n        )\n\n    if steps &lt; 1:\n        raise ValueError(\n            f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n        )\n\n    self.steps    = np.arange(steps) + 1\n    self.max_step = steps\n\n    self.regressors_ = {step: clone(self.regressor) for step in self.steps}\n\n    if isinstance(lags, dict):\n        self.lags = {}\n        self.lags_names = {}\n        list_max_lags = []\n        for key in lags:\n            if lags[key] is None:\n                self.lags[key] = None\n                self.lags_names[key] = None\n            else:\n                self.lags[key], lags_names, max_lag = initialize_lags(\n                    forecaster_name = type(self).__name__,\n                    lags            = lags[key]\n                )\n                self.lags_names[key] = (\n                    [f'{key}_{lag}' for lag in lags_names] \n                     if lags_names is not None \n                     else None\n                )\n                if max_lag is not None:\n                    list_max_lags.append(max_lag)\n\n        self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n    else:\n        self.lags, self.lags_names, self.max_lag = initialize_lags(\n            forecaster_name = type(self).__name__, \n            lags            = lags\n        )\n\n    self.window_features, self.window_features_names, self.max_size_window_features = (\n        initialize_window_features(window_features)\n    )\n    if self.window_features is None and (self.lags is None or self.max_lag is None):\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ]\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    if differentiation is not None:\n        if not isinstance(differentiation, int) or differentiation &lt; 1:\n            raise ValueError(\n                f\"Argument `differentiation` must be an integer equal to or \"\n                f\"greater than 1. Got {differentiation}.\"\n            )\n        self.differentiation = differentiation\n        self.differentiation_max = differentiation\n        self.window_size += differentiation\n        self.differentiator = TimeSeriesDifferentiator(\n            order=differentiation, window_size=self.window_size\n        )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.binner = {}\n    self.binner_intervals_ = {}\n    self.binner_kwargs = binner_kwargs\n    if binner_kwargs is None:\n        self.binner_kwargs = {\n            'n_bins': 10, 'method': 'linear', 'subsample': 200000,\n            'random_state': 789654, 'dtype': np.float64\n        }\n\n    if n_jobs == 'auto':\n        self.n_jobs = select_n_jobs_fit_forecaster(\n                          forecaster_name = type(self).__name__,\n                          regressor       = self.regressor\n                      )\n    else:\n        if not isinstance(n_jobs, int):\n            raise TypeError(\n                f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\"\n            )\n        self.n_jobs = n_jobs if n_jobs &gt; 0 else cpu_count()\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.regressor","title":"regressor  <code>instance-attribute</code>","text":"<pre><code>regressor = copy(regressor)\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.level","title":"level  <code>instance-attribute</code>","text":"<pre><code>level = level\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.lags_","title":"lags_  <code>instance-attribute</code>","text":"<pre><code>lags_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.transformer_series","title":"transformer_series  <code>instance-attribute</code>","text":"<pre><code>transformer_series = transformer_series\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.transformer_series_","title":"transformer_series_  <code>instance-attribute</code>","text":"<pre><code>transformer_series_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.transformer_exog","title":"transformer_exog  <code>instance-attribute</code>","text":"<pre><code>transformer_exog = transformer_exog\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.weight_func","title":"weight_func  <code>instance-attribute</code>","text":"<pre><code>weight_func = weight_func\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.source_code_weight_func","title":"source_code_weight_func  <code>instance-attribute</code>","text":"<pre><code>source_code_weight_func = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.differentiation","title":"differentiation  <code>instance-attribute</code>","text":"<pre><code>differentiation = differentiation\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.differentiation_max","title":"differentiation_max  <code>instance-attribute</code>","text":"<pre><code>differentiation_max = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.differentiator","title":"differentiator  <code>instance-attribute</code>","text":"<pre><code>differentiator = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.differentiator_","title":"differentiator_  <code>instance-attribute</code>","text":"<pre><code>differentiator_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.last_window_","title":"last_window_  <code>instance-attribute</code>","text":"<pre><code>last_window_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.index_type_","title":"index_type_  <code>instance-attribute</code>","text":"<pre><code>index_type_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.index_freq_","title":"index_freq_  <code>instance-attribute</code>","text":"<pre><code>index_freq_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.training_range_","title":"training_range_  <code>instance-attribute</code>","text":"<pre><code>training_range_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.series_names_in_","title":"series_names_in_  <code>instance-attribute</code>","text":"<pre><code>series_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.exog_in_","title":"exog_in_  <code>instance-attribute</code>","text":"<pre><code>exog_in_ = False\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.exog_names_in_","title":"exog_names_in_  <code>instance-attribute</code>","text":"<pre><code>exog_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.exog_type_in_","title":"exog_type_in_  <code>instance-attribute</code>","text":"<pre><code>exog_type_in_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.exog_dtypes_in_","title":"exog_dtypes_in_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_in_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.exog_dtypes_out_","title":"exog_dtypes_out_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.X_train_series_names_in_","title":"X_train_series_names_in_  <code>instance-attribute</code>","text":"<pre><code>X_train_series_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.X_train_window_features_names_out_","title":"X_train_window_features_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_window_features_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.X_train_exog_names_out_","title":"X_train_exog_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_exog_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.X_train_direct_exog_names_out_","title":"X_train_direct_exog_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_direct_exog_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.X_train_features_names_out_","title":"X_train_features_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_features_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.in_sample_residuals_","title":"in_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.out_sample_residuals_","title":"out_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.in_sample_residuals_by_bin_","title":"in_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.out_sample_residuals_by_bin_","title":"out_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.creation_date","title":"creation_date  <code>instance-attribute</code>","text":"<pre><code>creation_date = strftime('%Y-%m-%d %H:%M:%S')\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.is_fitted","title":"is_fitted  <code>instance-attribute</code>","text":"<pre><code>is_fitted = False\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.fit_date","title":"fit_date  <code>instance-attribute</code>","text":"<pre><code>fit_date = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.skforecast_version","title":"skforecast_version  <code>instance-attribute</code>","text":"<pre><code>skforecast_version = __version__\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.python_version","title":"python_version  <code>instance-attribute</code>","text":"<pre><code>python_version = split(' ')[0]\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.forecaster_id","title":"forecaster_id  <code>instance-attribute</code>","text":"<pre><code>forecaster_id = forecaster_id\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._probabilistic_mode","title":"_probabilistic_mode  <code>instance-attribute</code>","text":"<pre><code>_probabilistic_mode = 'binned'\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.dropna_from_series","title":"dropna_from_series  <code>instance-attribute</code>","text":"<pre><code>dropna_from_series = False\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.steps","title":"steps  <code>instance-attribute</code>","text":"<pre><code>steps = arange(steps) + 1\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.max_step","title":"max_step  <code>instance-attribute</code>","text":"<pre><code>max_step = steps\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.regressors_","title":"regressors_  <code>instance-attribute</code>","text":"<pre><code>regressors_ = {step: (clone(regressor))for step in (steps)}\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.lags","title":"lags  <code>instance-attribute</code>","text":"<pre><code>lags = {}\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.lags_names","title":"lags_names  <code>instance-attribute</code>","text":"<pre><code>lags_names = {}\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.max_lag","title":"max_lag  <code>instance-attribute</code>","text":"<pre><code>max_lag = (\n    max(list_max_lags) if len(list_max_lags) != 0 else None\n)\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = max(\n    [\n        ws\n        for ws in [max_lag, max_size_window_features]\n        if ws is not None\n    ]\n)\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.window_features_class_names","title":"window_features_class_names  <code>instance-attribute</code>","text":"<pre><code>window_features_class_names = None\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.fit_kwargs","title":"fit_kwargs  <code>instance-attribute</code>","text":"<pre><code>fit_kwargs = check_select_fit_kwargs(\n    regressor=regressor, fit_kwargs=fit_kwargs\n)\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.binner","title":"binner  <code>instance-attribute</code>","text":"<pre><code>binner = {}\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.binner_intervals_","title":"binner_intervals_  <code>instance-attribute</code>","text":"<pre><code>binner_intervals_ = {}\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.binner_kwargs","title":"binner_kwargs  <code>instance-attribute</code>","text":"<pre><code>binner_kwargs = binner_kwargs\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.n_jobs","title":"n_jobs  <code>instance-attribute</code>","text":"<pre><code>n_jobs = select_n_jobs_fit_forecaster(\n    forecaster_name=__name__, regressor=regressor\n)\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def _repr_html_(self):\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    (\n        params,\n        _,\n        series_names_in_,\n        exog_names_in_,\n        transformer_series,\n    ) = self._preprocess_repr(\n            regressor          = self.regressor,\n            series_names_in_   = self.series_names_in_,\n            exog_names_in_     = self.exog_names_in_,\n            transformer_series = self.transformer_series,\n        )\n\n    style, unique_id = get_style_repr_html(self.is_fitted)\n\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Regressor:&lt;/strong&gt; {type(self.regressor).__name__}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Target series (level):&lt;/strong&gt; {self.level}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Maximum steps to predict:&lt;/strong&gt; {self.max_step}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skforecast version:&lt;/strong&gt; {self.skforecast_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n            &lt;ul&gt;\n                {exog_names_in_}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Data Transformations&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for series:&lt;/strong&gt; {transformer_series}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Training Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Target series (level):&lt;/strong&gt; {self.level}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Multivariate series:&lt;/strong&gt; {series_names_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Regressor Parameters&lt;/summary&gt;\n            &lt;ul&gt;\n                {params}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n            &lt;ul&gt;\n                {self.fit_kwargs}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterdirectmultivariate.html\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/dependent-multi-series-multivariate-forecasting.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    # Return the combined style and content\n    return style + content\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._create_data_to_return_dict","title":"_create_data_to_return_dict","text":"<pre><code>_create_data_to_return_dict(series_names_in_)\n</code></pre> <p>Create <code>data_to_return_dict</code> based on series names and lags configuration. The dictionary contains the information to decide what data to return in  the <code>_create_lags</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>series_names_in_</code> <code>list</code> <p>Names of the series used during training.</p> required <p>Returns:</p> Name Type Description <code>data_to_return_dict</code> <code>dict</code> <p>Dictionary with the information to decide what data to return in the <code>_create_lags</code> method. Options are 'X', 'y' or 'both'.</p> <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series added to <code>X_train</code> when creating the training matrices with <code>_create_train_X_y</code> method. It is a subset of  <code>series_names_in_</code>.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def _create_data_to_return_dict(\n    self, \n    series_names_in_: list[str]\n) -&gt; tuple[dict[str, str], list[str]]:\n    \"\"\"\n    Create `data_to_return_dict` based on series names and lags configuration.\n    The dictionary contains the information to decide what data to return in \n    the `_create_lags` method.\n\n    Parameters\n    ----------\n    series_names_in_ : list\n        Names of the series used during training.\n\n    Returns\n    -------\n    data_to_return_dict : dict\n        Dictionary with the information to decide what data to return in the\n        `_create_lags` method. Options are 'X', 'y' or 'both'.\n    X_train_series_names_in_ : list\n        Names of the series added to `X_train` when creating the training\n        matrices with `_create_train_X_y` method. It is a subset of \n        `series_names_in_`.\n\n    \"\"\"\n\n    if isinstance(self.lags, dict):\n        lags_keys = list(self.lags.keys())\n        if set(lags_keys) != set(series_names_in_):  # Set to avoid order\n            raise ValueError(\n                f\"When `lags` parameter is a `dict`, its keys must be the \"\n                f\"same as `series` column names. If don't want to include lags, \"\n                 \"add '{column: None}' to the lags dict.\\n\"\n                f\"  Lags keys        : {lags_keys}.\\n\"\n                f\"  `series` columns : {series_names_in_}.\"\n            )\n        self.lags_ = copy(self.lags)\n    else:\n        self.lags_ = {series: self.lags for series in series_names_in_}\n        if self.lags is not None:\n            # Defined `lags_names` here to avoid overwriting when fit and then create_train_X_y\n            lags_names = [f'lag_{i}' for i in self.lags]\n            self.lags_names = {\n                series: [f'{series}_{lag}' for lag in lags_names]\n                for series in series_names_in_\n            }\n        else:\n            self.lags_names = {series: None for series in series_names_in_}\n\n    X_train_series_names_in_ = series_names_in_\n    if self.lags is None:\n        data_to_return_dict = {self.level: 'y'}\n    else:\n        # If col is not level and has lags, create 'X' if no lags don't include\n        # If col is level, create 'both' (`X` and `y`)\n        data_to_return_dict = {\n            col: ('both' if col == self.level else 'X')\n            for col in series_names_in_\n            if col == self.level or self.lags_.get(col) is not None\n        }\n\n        # Adjust 'level' in case self.lags_[level] is None\n        if self.lags_.get(self.level) is None:\n            data_to_return_dict[self.level] = 'y'\n\n        if self.window_features is None:\n            # X_train_series_names_in_ include series that will be added to X_train\n            X_train_series_names_in_ = [\n                col for col in data_to_return_dict.keys()\n                if data_to_return_dict[col] in ['X', 'both']\n            ]\n\n    return data_to_return_dict, X_train_series_names_in_\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._create_lags","title":"_create_lags","text":"<pre><code>_create_lags(y, lags, data_to_return='both')\n</code></pre> <p>Create the lagged values and their target variable from a time series.</p> <p>Note that the returned matrix <code>X_data</code> contains the lag 1 in the first  column, the lag 2 in the in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>Training time series values.</p> required <code>lags</code> <code>numpy ndarray</code> <p>lags to create.</p> required <code>data_to_return</code> <code>str</code> <p>Specifies which data to return. Options are 'X', 'y', 'both' or None.</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray, None</code> <p>Lagged values (predictors).</p> <code>y_data</code> <code>numpy ndarray, None</code> <p>Values of the time series related to each row of <code>X_data</code>.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray,\n    lags: np.ndarray,\n    data_to_return: str | None = 'both'\n) -&gt; tuple[np.ndarray | None, np.ndarray | None]:\n    \"\"\"\n    Create the lagged values and their target variable from a time series.\n\n    Note that the returned matrix `X_data` contains the lag 1 in the first \n    column, the lag 2 in the in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        Training time series values.\n    lags : numpy ndarray\n        lags to create.\n    data_to_return : str, default 'both'\n        Specifies which data to return. Options are 'X', 'y', 'both' or None.\n\n    Returns\n    -------\n    X_data : numpy ndarray, None\n        Lagged values (predictors).\n    y_data : numpy ndarray, None\n        Values of the time series related to each row of `X_data`.\n\n    \"\"\"\n\n    X_data = None\n    y_data = None\n    if data_to_return is not None:\n\n        n_rows = len(y) - self.window_size - (self.max_step - 1)\n\n        if data_to_return != 'y':\n            # If `data_to_return` is not 'y', it means is 'X' or 'both', X_data is created\n            X_data = np.full(\n                shape=(n_rows, len(lags)), fill_value=np.nan, order='F', dtype=float\n            )\n            for i, lag in enumerate(lags):\n                X_data[:, i] = y[self.window_size - lag : -(lag + self.max_step - 1)]\n\n        if data_to_return != 'X':\n            # If `data_to_return` is not 'X', it means is 'y' or 'both', y_data is created\n            y_data = np.full(\n                shape=(n_rows, self.max_step), fill_value=np.nan, order='F', dtype=float\n            )\n            for step in range(self.max_step):\n                y_data[:, step] = y[self.window_size + step : self.window_size + step + n_rows]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._create_window_features","title":"_create_window_features","text":"<pre><code>_create_window_features(y, train_index, X_as_pandas=False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_train_window_features</code> when <code>X_as_pandas</code> is <code>True</code>.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_train_window_features</code> is a  pandas DataFrame.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train_window_features</code> <code>list</code> <p>List of numpy ndarrays or pandas DataFrames with the window features.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def _create_window_features(\n    self, \n    y: pd.Series,\n    train_index: pd.Index,\n    X_as_pandas: bool = False,\n) -&gt; tuple[list[np.ndarray | pd.DataFrame], list[str]]:\n    \"\"\"\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    train_index : pandas Index\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_train_window_features` when `X_as_pandas` is `True`.\n    X_as_pandas : bool, default False\n        If `True`, the returned matrix `X_train_window_features` is a \n        pandas DataFrame.\n\n    Returns\n    -------\n    X_train_window_features : list\n        List of numpy ndarrays or pandas DataFrames with the window features.\n    X_train_window_features_names_out_ : list\n        Names of the window features.\n\n    \"\"\"\n\n    len_train_index = len(train_index)\n    X_train_window_features = []\n    X_train_window_features_names_out_ = []\n    for wf in self.window_features:\n        X_train_wf = wf.transform_batch(y)\n        if not isinstance(X_train_wf, pd.DataFrame):\n            raise TypeError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a pandas DataFrame.\"\n            )\n        X_train_wf = X_train_wf.iloc[-len_train_index:]\n        if not len(X_train_wf) == len_train_index:\n            raise ValueError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a DataFrame with the same number of rows as \"\n                f\"the input time series - (`window_size` + (`steps` - 1)): {len_train_index}.\"\n            )\n        X_train_wf.index = train_index\n\n        X_train_wf.columns = [f'{y.name}_{col}' for col in X_train_wf.columns]\n        X_train_window_features_names_out_.extend(X_train_wf.columns)\n        if not X_as_pandas:\n            X_train_wf = X_train_wf.to_numpy()     \n        X_train_window_features.append(X_train_wf)\n\n    return X_train_window_features, X_train_window_features_names_out_\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._create_train_X_y","title":"_create_train_X_y","text":"<pre><code>_create_train_X_y(series, exog=None)\n</code></pre> <p>Create training matrices from multiple time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the filter_train_X_y_for_step method.</p> <code>y_train</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_train</code> for each  step in the form {step: y_step_[i]}.</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series used during training.</p> <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series added to <code>X_train</code> when creating the training matrices with <code>_create_train_X_y</code> method. It is a subset of  <code>series_names_in_</code>.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables included in the training matrices.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of the columns of the matrix created internally for training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def _create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[\n    pd.DataFrame, \n    dict[int, pd.Series], \n    list[str], \n    list[str], \n    list[str], \n    list[str], \n    list[str], \n    dict[str, type], \n    dict[str, type]\n]:\n    \"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. The resulting matrices contain the target variable and predictors\n    needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the filter_train_X_y_for_step method.\n    y_train : dict\n        Values of the time series related to each row of `X_train` for each \n        step in the form {step: y_step_[i]}.\n    series_names_in_ : list\n        Names of the series used during training.\n    X_train_series_names_in_ : list\n        Names of the series added to `X_train` when creating the training\n        matrices with `_create_train_X_y` method. It is a subset of \n        `series_names_in_`.\n    exog_names_in_ : list\n        Names of the exogenous variables included in the training matrices.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of the columns of the matrix created internally for training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training before the transformation\n        applied by `transformer_exog`. If `transformer_exog` is not used, it\n        is equal to `exog_dtypes_out_`.\n    exog_dtypes_out_ : dict\n        Type of each exogenous variable/s used in training after the transformation \n        applied by `transformer_exog`. If `transformer_exog` is not used, it \n        is equal to `exog_dtypes_in_`.\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(\n            f\"`series` must be a pandas DataFrame. Got {type(series)}.\"\n        )\n\n    if len(series) &lt; self.window_size + self.max_step:\n        raise ValueError(\n            f\"Minimum length of `series` for training this forecaster is \"\n            f\"{self.window_size + self.max_step}. Reduce the number of \"\n            f\"predicted steps, {self.max_step}, or the maximum \"\n            f\"window_size, {self.window_size}, if no more data is available.\\n\"\n            f\"    Length `series`: {len(series)}.\\n\"\n            f\"    Max step : {self.max_step}.\\n\"\n            f\"    Max window size: {self.window_size}.\\n\"\n            f\"    Lags window size: {self.max_lag}.\\n\"\n            f\"    Window features window size: {self.max_size_window_features}.\"\n        )\n\n    _, series_index = check_extract_values_and_index(\n        data=series, data_label=\"`series`\", return_values=False\n    )\n    series_names_in_ = list(series.columns)\n\n    if self.level not in series_names_in_:\n        raise ValueError(\n            f\"One of the `series` columns must be named as the `level` of the forecaster.\\n\"\n            f\"  Forecaster `level` : {self.level}.\\n\"\n            f\"  `series` columns   : {series_names_in_}.\"\n        )\n\n    data_to_return_dict, X_train_series_names_in_ = (\n        self._create_data_to_return_dict(series_names_in_=series_names_in_)\n    )\n\n    series_to_create_autoreg_features_and_y = [\n        col for col in series_names_in_ \n        if col in X_train_series_names_in_ + [self.level]\n    ]\n\n    fit_transformer = False\n    if not self.is_fitted:\n        fit_transformer = True\n        self.transformer_series_ = initialize_transformer_series(\n                                       forecaster_name    = type(self).__name__,\n                                       series_names_in_   = series_to_create_autoreg_features_and_y,\n                                       transformer_series = self.transformer_series\n                                   )\n\n    if self.differentiation is None:\n        self.differentiator_ = {\n            serie: None for serie in series_to_create_autoreg_features_and_y\n        }\n    else:\n        if not self.is_fitted:\n            self.differentiator_ = {\n                serie: copy(self.differentiator)\n                for serie in series_to_create_autoreg_features_and_y\n            }\n\n    exog_names_in_ = None\n    exog_dtypes_in_ = None\n    exog_dtypes_out_ = None\n    X_as_pandas = False\n    if exog is not None:\n        check_exog(exog=exog, allow_nan=True)\n        exog = input_to_frame(data=exog, input_name='exog')\n        _, exog_index = check_extract_values_and_index(\n            data=exog, data_label='`exog`', ignore_freq=True, return_values=False\n        )\n\n        series_index_no_ws = series_index[self.window_size:]\n        len_series = len(series)\n        len_series_no_ws = len_series - self.window_size\n        len_exog = len(exog)\n        if not len_exog == len_series and not len_exog == len_series_no_ws:\n            raise ValueError(\n                f\"Length of `exog` must be equal to the length of `series` (if \"\n                f\"index is fully aligned) or length of `series` - `window_size` \"\n                f\"(if `exog` starts after the first `window_size` values).\\n\"\n                f\"    `exog`                   : ({exog_index[0]} -- {exog_index[-1]})  (n={len_exog})\\n\"\n                f\"    `series`                 : ({series_index[0]} -- {series_index[-1]})  (n={len_series})\\n\"\n                f\"    `series` - `window_size` : ({series_index_no_ws[0]} -- {series_index_no_ws[-1]})  (n={len_series_no_ws})\"\n            )\n\n        exog_names_in_ = exog.columns.to_list()\n        if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n            raise ValueError(\n                f\"`exog` cannot contain a column named the same as one of \"\n                f\"the series (column names of series).\\n\"\n                f\"  `series` columns : {series_names_in_}.\\n\"\n                f\"  `exog`   columns : {exog_names_in_}.\"\n            )\n\n        # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n        self.exog_in_ = True\n        exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = fit_transformer,\n                   inverse_transform = False\n               )\n\n        check_exog_dtypes(exog, call_check_exog=True)\n        exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n        X_as_pandas = any(\n            not pd.api.types.is_numeric_dtype(dtype) or pd.api.types.is_bool_dtype(dtype) \n            for dtype in set(exog.dtypes)\n        )\n\n        if len_exog == len_series:\n            if not (exog_index == series_index).all():\n                raise ValueError(\n                    \"When `exog` has the same length as `series`, the index \"\n                    \"of `exog` must be aligned with the index of `series` \"\n                    \"to ensure the correct alignment of values.\"\n                )\n            # The first `self.window_size` positions have to be removed from \n            # exog since they are not in X_train.\n            exog = exog.iloc[self.window_size:, ]\n        else:\n            if not (exog_index == series_index_no_ws).all():\n                raise ValueError(\n                    \"When `exog` doesn't contain the first `window_size` \"\n                    \"observations, the index of `exog` must be aligned with \"\n                    \"the index of `series` minus the first `window_size` \"\n                    \"observations to ensure the correct alignment of values.\"\n                )\n\n    X_train_autoreg = []\n    X_train_window_features_names_out_ = [] if self.window_features is not None else None\n    X_train_features_names_out_ = []\n    train_index = series_index[self.window_size + (self.max_step - 1):]\n    for col in series_to_create_autoreg_features_and_y:\n\n        y_values = series[col].to_numpy(copy=True).ravel()\n        if np.isnan(y_values).any():\n            raise ValueError(f\"Column '{col}' has missing values.\")\n\n        y_values = transform_numpy(\n                       array             = y_values,\n                       transformer       = self.transformer_series_[col],\n                       fit               = fit_transformer,\n                       inverse_transform = False\n                   )\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator_[col].fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator_[col])\n                y_values = differentiator.fit_transform(y_values)\n\n        X_train_autoreg_col = []\n\n        X_train_lags, y_train_values = self._create_lags(\n            y=y_values, lags=self.lags_[col], data_to_return=data_to_return_dict.get(col, None)\n        )\n        if X_train_lags is not None:\n            X_train_autoreg_col.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names[col])\n\n        if col == self.level:\n            y_train = y_train_values\n\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            end_wf = None if self.max_step == 1 else -(self.max_step - 1)\n            y_window_features = pd.Series(\n                y_values[n_diff:end_wf], index=series_index[n_diff:end_wf], name=col\n            )\n            X_train_window_features, X_train_wf_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features, X_as_pandas=False, train_index=train_index\n                )\n            )\n            X_train_autoreg_col.extend(X_train_window_features)\n            X_train_window_features_names_out_.extend(X_train_wf_names_out_)\n            X_train_features_names_out_.extend(X_train_wf_names_out_)\n\n        if X_train_autoreg_col:\n            if len(X_train_autoreg_col) == 1:\n                X_train_autoreg_col = X_train_autoreg_col[0]\n            else:\n                X_train_autoreg_col = np.concatenate(X_train_autoreg_col, axis=1)\n\n            X_train_autoreg.append(X_train_autoreg_col)\n\n    X_train = []\n    len_train_index = len(train_index)\n    if X_as_pandas:\n        if len(X_train_autoreg) == 1:\n            X_train_autoreg = X_train_autoreg[0]\n        else:\n            X_train_autoreg = np.concatenate(X_train_autoreg, axis=1)\n        X_train_autoreg = pd.DataFrame(\n                              data    = X_train_autoreg,\n                              columns = X_train_features_names_out_,\n                              index   = train_index\n                          )\n        X_train.append(X_train_autoreg)\n    else:\n        X_train.extend(X_train_autoreg)\n\n    # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n    self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n\n    X_train_exog_names_out_ = None\n    if exog is not None:\n        X_train_exog_names_out_ = exog.columns.to_list()\n        if X_as_pandas:\n            exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(\n                exog=exog, steps=self.max_step\n            )\n            exog_direct.index = train_index\n        else:\n            exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(\n                exog=exog, steps=self.max_step\n            )\n\n        # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n        self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n\n        X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n        X_train.append(exog_direct)\n\n    if len(X_train) == 1:\n        X_train = X_train[0]\n    else:\n        if X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n\n    if X_as_pandas:\n        X_train.index = train_index\n    else:\n        X_train = pd.DataFrame(\n                      data    = X_train,\n                      index   = train_index,\n                      columns = X_train_features_names_out_\n                  )\n\n    y_train = {\n        step: pd.Series(\n                  data  = y_train[:, step - 1], \n                  index = series_index[self.window_size + step - 1:][:len_train_index],\n                  name  = f\"{self.level}_step_{step}\"\n              )\n        for step in self.steps\n    }\n\n    return (\n        X_train,\n        y_train,\n        series_names_in_,\n        X_train_series_names_in_,\n        exog_names_in_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_\n    )\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.create_train_X_y","title":"create_train_X_y","text":"<pre><code>create_train_X_y(\n    series, exog=None, suppress_warnings=False\n)\n</code></pre> <p>Create training matrices from multiple time series and exogenous variables. The resulting matrices contain the target variable and predictors needed to train all the regressors (one per step).</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the creation of the training matrices. See skforecast.exceptions.warn_skforecast_categories  for more information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors) for each step. Note that the index  corresponds to that of the last step. It is updated for the corresponding  step in the filter_train_X_y_for_step method.</p> <code>y_train</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_train</code> for each  step in the form {step: y_step_[i]}.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: pd.Series | pd.DataFrame | None = None,\n    suppress_warnings: bool = False\n) -&gt; tuple[pd.DataFrame, dict[int, pd.Series]]:\n    \"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. The resulting matrices contain the target variable and predictors\n    needed to train all the regressors (one per step).\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the creation\n        of the training matrices. See skforecast.exceptions.warn_skforecast_categories \n        for more information.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors) for each step. Note that the index \n        corresponds to that of the last step. It is updated for the corresponding \n        step in the filter_train_X_y_for_step method.\n    y_train : dict\n        Values of the time series related to each row of `X_train` for each \n        step in the form {step: y_step_[i]}.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    output = self._create_train_X_y(\n                 series = series, \n                 exog   = exog\n             )\n\n    X_train = output[0]\n    y_train = output[1]\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.filter_train_X_y_for_step","title":"filter_train_X_y_for_step","text":"<pre><code>filter_train_X_y_for_step(\n    step, X_train, y_train, remove_suffix=False\n)\n</code></pre> <p>Select the columns needed to train a forecaster for a specific step. The input matrices should be created using <code>_create_train_X_y</code> method.  This method updates the index of <code>X_train</code> to the corresponding one  according to <code>y_train</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\"  will be removed from the column names. </p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>step for which columns must be selected. Starts at 1.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>_create_train_X_y</code> method, first return.</p> required <code>y_train</code> <code>dict</code> <p>Dict created with the <code>_create_train_X_y</code> method, second return.</p> required <code>remove_suffix</code> <code>bool</code> <p>If True, suffix \"_step_i\" is removed from the column names.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train_step</code> <code>pandas DataFrame</code> <p>Training values (predictors) for the selected step.</p> <code>y_train_step</code> <code>pandas Series</code> <p>Values of the time series related to each row of <code>X_train</code>.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def filter_train_X_y_for_step(\n    self,\n    step: int,\n    X_train: pd.DataFrame,\n    y_train: dict[int, pd.Series],\n    remove_suffix: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Select the columns needed to train a forecaster for a specific step.  \n    The input matrices should be created using `_create_train_X_y` method. \n    This method updates the index of `X_train` to the corresponding one \n    according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n    will be removed from the column names. \n\n    Parameters\n    ----------\n    step : int\n        step for which columns must be selected. Starts at 1.\n    X_train : pandas DataFrame\n        Dataframe created with the `_create_train_X_y` method, first return.\n    y_train : dict\n        Dict created with the `_create_train_X_y` method, second return.\n    remove_suffix : bool, default False\n        If True, suffix \"_step_i\" is removed from the column names.\n\n    Returns\n    -------\n    X_train_step : pandas DataFrame\n        Training values (predictors) for the selected step.\n    y_train_step : pandas Series\n        Values of the time series related to each row of `X_train`.\n\n    \"\"\"\n\n    if (step &lt; 1) or (step &gt; self.max_step):\n        raise ValueError(\n            f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n            f\"and the maximum step is {self.max_step}.\"\n        )\n\n    y_train_step = y_train[step]\n\n    # Matrix X_train starts at index 0.\n    if not self.exog_in_:\n        X_train_step = X_train\n    else:\n        n_lags = len(list(\n            chain(*[v for v in self.lags_.values() if v is not None])\n        ))\n        n_window_features = (\n            len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        )\n        idx_columns_autoreg = np.arange(n_lags + n_window_features)\n        n_exog = len(self.X_train_direct_exog_names_out_) / self.max_step\n        idx_columns_exog = (\n            np.arange((step - 1) * n_exog, (step) * n_exog) + idx_columns_autoreg[-1] + 1 \n        )\n        idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n        X_train_step = X_train.iloc[:, idx_columns]\n\n    X_train_step.index = y_train_step.index\n\n    if remove_suffix:\n        X_train_step.columns = [\n            col_name.replace(f\"_step_{step}\", \"\")\n            for col_name in X_train_step.columns\n        ]\n        y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n    return X_train_step, y_train_step\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._train_test_split_one_step_ahead","title":"_train_test_split_one_step_ahead","text":"<pre><code>_train_test_split_one_step_ahead(\n    series, initial_train_size, exog=None\n)\n</code></pre> <p>Create matrices needed to train and test the forecaster for one-step-ahead predictions.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>initial_train_size</code> <code>int</code> <p>Initial size of the training set. It is the number of observations used to train the forecaster before making the first prediction.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Predictor values used to train the model.</p> <code>y_train</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_train</code> for each  step in the form {step: y_step_[i]}.</p> <code>X_test</code> <code>pandas DataFrame</code> <p>Predictor values used to test the model.</p> <code>y_test</code> <code>dict</code> <p>Values of the time series related to each row of <code>X_test</code> for each  step in the form {step: y_step_[i]}.</p> <code>X_train_encoding</code> <code>pandas Series</code> <p>Series identifiers for each row of <code>X_train</code>.</p> <code>X_test_encoding</code> <code>pandas Series</code> <p>Series identifiers for each row of <code>X_test</code>.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def _train_test_split_one_step_ahead(\n    self,\n    series: pd.DataFrame,\n    initial_train_size: int,\n    exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[\n    pd.DataFrame, \n    dict[int, pd.Series], \n    pd.DataFrame, \n    dict[int, pd.Series], \n    pd.Series, \n    pd.Series\n]:\n    \"\"\"\n    Create matrices needed to train and test the forecaster for one-step-ahead\n    predictions.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    initial_train_size : int\n        Initial size of the training set. It is the number of observations used\n        to train the forecaster before making the first prediction.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : dict\n        Values of the time series related to each row of `X_train` for each \n        step in the form {step: y_step_[i]}.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : dict\n        Values of the time series related to each row of `X_test` for each \n        step in the form {step: y_step_[i]}.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n\n    \"\"\"\n\n    span_index = series.index\n\n    fold = [\n        [0, initial_train_size],\n        [initial_train_size - self.window_size, initial_train_size],\n        [initial_train_size - self.window_size, len(span_index)],\n        [0, 0],  # Dummy value\n        True\n    ]\n    data_fold = _extract_data_folds_multiseries(\n                    series             = series,\n                    folds              = [fold],\n                    span_index         = span_index,\n                    window_size        = self.window_size,\n                    exog               = exog,\n                    dropna_last_window = self.dropna_from_series,\n                    externally_fitted  = False\n                )\n    series_train, _, levels_last_window, exog_train, exog_test, _ = next(data_fold)\n\n    start_test_idx = initial_train_size - self.window_size\n    series_test = series.iloc[start_test_idx:, :]\n    series_test = series_test.loc[:, levels_last_window]\n    series_test = series_test.dropna(axis=1, how='all')\n\n    _is_fitted = self.is_fitted\n    _series_names_in_ = self.series_names_in_\n    _exog_names_in_ = self.exog_names_in_\n\n    self.is_fitted = False\n    X_train, y_train, series_names_in_, _, exog_names_in_, *_ = (\n        self._create_train_X_y(\n            series = series_train,\n            exog   = exog_train,\n        )\n    )\n    self.series_names_in_ = series_names_in_\n    if exog is not None:\n        self.exog_names_in_ = exog_names_in_\n    self.is_fitted = True\n\n    X_test, y_test, *_ = self._create_train_X_y(\n                             series = series_test,\n                             exog   = exog_test,\n                         )\n    self.is_fitted = _is_fitted\n    self.series_names_in_ = _series_names_in_\n    self.exog_names_in_ = _exog_names_in_\n\n    X_train_encoding = pd.Series(self.level, index=X_train.index)\n    X_test_encoding = pd.Series(self.level, index=X_test.index)\n\n    return X_train, y_train, X_test, y_test, X_train_encoding, X_test_encoding\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.create_sample_weights","title":"create_sample_weights","text":"<pre><code>create_sample_weights(X_train)\n</code></pre> <p>Create weights for each observation according to the forecaster's attribute <code>weight_func</code>. </p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with <code>_create_train_X_y</code> and filter_train_X_y_for_step` methods, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame\n) -&gt; np.ndarray:\n    \"\"\"\n    Create weights for each observation according to the forecaster's attribute\n    `weight_func`. \n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with `_create_train_X_y` and filter_train_X_y_for_step`\n        methods, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                (\"The resulting `sample_weight` cannot be normalized because \"\n                 \"the sum of the weights is zero.\")\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.fit","title":"fit","text":"<pre><code>fit(\n    series,\n    exog=None,\n    store_last_window=True,\n    store_in_sample_residuals=False,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>None</code> <code>store_last_window</code> <code>bool</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <code>True</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the training  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    exog: pd.Series | pd.DataFrame | None = None,\n    store_last_window: bool = True,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n    store_last_window : bool, default True\n        Whether or not to store the last window (`last_window_`) of training data.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the training \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    # Reset values in case the forecaster has already been fitted.\n    self.lags_                              = None\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.series_names_in_                   = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.exog_dtypes_out_                   = None\n    self.X_train_series_names_in_           = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_direct_exog_names_out_     = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.in_sample_residuals_by_bin_        = None\n    self.binner                             = {}\n    self.binner_intervals_                  = {}\n    self.is_fitted                          = False\n    self.fit_date                           = None\n\n    (\n        X_train,\n        y_train,\n        series_names_in_,\n        X_train_series_names_in_,\n        exog_names_in_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_\n    ) = self._create_train_X_y(series=series, exog=exog)\n\n    def fit_forecaster(regressor, X_train, y_train, step):\n        \"\"\"\n        Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n        Parameters\n        ----------\n        regressor : object\n            Regressor to be fitted.\n        X_train : pandas DataFrame\n            Dataframe created with the `_create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `_create_train_X_y` method, second return.\n        step : int\n            Step of the forecaster to be fitted.\n\n        Returns\n        -------\n        Tuple with the step, fitted regressor, in-sample residuals, true values\n        and predicted values for the step.\n\n        \"\"\"\n\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n        sample_weight = self.create_sample_weights(X_train=X_train_step)\n        if sample_weight is not None:\n            regressor.fit(\n                X             = X_train_step,\n                y             = y_train_step,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            regressor.fit(\n                X = X_train_step,\n                y = y_train_step,\n                **self.fit_kwargs\n            )\n\n        # NOTE: This is done to save time during fit in functions such as backtesting()\n        y_true_step = None\n        y_pred_step = None\n        if self._probabilistic_mode is not False:\n            y_true_step = y_train_step.to_numpy()\n            y_pred_step = regressor.predict(X_train_step)\n\n        return step, regressor, y_true_step, y_pred_step\n\n    results_fit = (\n        Parallel(n_jobs=self.n_jobs)\n        (delayed(fit_forecaster)\n        (\n            regressor = copy(self.regressor),\n            X_train   = X_train,\n            y_train   = y_train,\n            step      = step\n        )\n        for step in self.steps)\n    )\n\n    self.regressors_ = {step: regressor for step, regressor, *_ in results_fit}\n\n    self.in_sample_residuals_ = {}\n    self.in_sample_residuals_by_bin_ = {}\n    if self._probabilistic_mode is not False:\n        for level in [self.level]:\n            y_true_level, y_pred_level = zip(\n                *[(y_true, y_pred) for *_, y_true, y_pred in results_fit]\n            )\n            self._binning_in_sample_residuals(\n                level                     = level,\n                y_true                    = np.concatenate(y_true_level),\n                y_pred                    = np.concatenate(y_pred_level),\n                store_in_sample_residuals = store_in_sample_residuals,\n                random_state              = random_state\n            )\n\n    if not store_in_sample_residuals:\n        for level in [self.level]:\n            self.in_sample_residuals_[level] = None\n            self.in_sample_residuals_by_bin_[level] = None\n\n    self.series_names_in_ = series_names_in_\n    self.X_train_series_names_in_ = X_train_series_names_in_\n    self.X_train_features_names_out_ = X_train_features_names_out_\n\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = series.index[[0, -1]]\n    self.index_type_ = type(series.index)\n    if isinstance(series.index, pd.DatetimeIndex):\n        self.index_freq_ = series.index.freqstr\n    else: \n        self.index_freq_ = series.index.step\n\n    if exog is not None:\n        self.exog_in_ = True\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog)\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n    if store_last_window:\n        self.last_window_ = series.iloc[-self.window_size:, ][\n            self.X_train_series_names_in_\n        ].copy()\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._binning_in_sample_residuals","title":"_binning_in_sample_residuals","text":"<pre><code>_binning_in_sample_residuals(\n    level,\n    y_true,\n    y_pred,\n    store_in_sample_residuals=False,\n    random_state=123,\n)\n</code></pre> <p>Bin residuals according to the predicted value each residual is associated with. First a <code>skforecast.preprocessing.QuantileBinner</code> object is fitted to the predicted values. Then, residuals are binned according to the predicted value each residual is associated with. Residuals are stored in the forecaster object as <code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code>.</p> <p><code>y_true</code> and <code>y_pred</code> assumed to be differentiated and/or transformed according to the attributes <code>differentiation</code> and <code>transformer_series</code>. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code>. The total number of residuals stored is <code>10_000</code>. New in version 0.15.0</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Name of the series (level) to store the residuals.</p> required <code>y_true</code> <code>numpy ndarray</code> <p>True values of the time series.</p> required <code>y_pred</code> <code>numpy ndarray</code> <p>Predicted values of the time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def _binning_in_sample_residuals(\n    self,\n    level: str,\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Bin residuals according to the predicted value each residual is\n    associated with. First a `skforecast.preprocessing.QuantileBinner` object\n    is fitted to the predicted values. Then, residuals are binned according\n    to the predicted value each residual is associated with. Residuals are\n    stored in the forecaster object as `in_sample_residuals_` and\n    `in_sample_residuals_by_bin_`.\n\n    `y_true` and `y_pred` assumed to be differentiated and/or transformed\n    according to the attributes `differentiation` and `transformer_series`.\n    The number of residuals stored per bin is limited to \n    `10_000 // self.binner.n_bins_`. The total number of residuals stored is\n    `10_000`.\n    **New in version 0.15.0**\n\n    Parameters\n    ----------\n    level : str\n        Name of the series (level) to store the residuals.\n    y_true : numpy ndarray\n        True values of the time series.\n    y_pred : numpy ndarray\n        Predicted values of the time series.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    residuals = y_true - y_pred\n\n    if self._probabilistic_mode == \"binned\":\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        self.binner[level] = QuantileBinner(**self.binner_kwargs)\n        self.binner[level].fit(y_pred)\n        self.binner_intervals_[level] = self.binner[level].intervals_\n\n    if store_in_sample_residuals:\n        rng = np.random.default_rng(seed=random_state)\n        if self._probabilistic_mode == \"binned\":\n            data['bin'] = self.binner[level].transform(y_pred).astype(int)\n            self.in_sample_residuals_by_bin_[level] = (\n                data.groupby('bin')['residuals'].apply(np.array).to_dict()\n            )\n\n            max_sample = 10_000 // self.binner[level].n_bins_\n            for k, v in self.in_sample_residuals_by_bin_[level].items():\n                if len(v) &gt; max_sample:\n                    sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                    self.in_sample_residuals_by_bin_[level][k] = sample\n        else:\n            self.in_sample_residuals_by_bin_[level] = None\n\n        if len(residuals) &gt; 10_000:\n            residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=10_000)\n            ]\n        self.in_sample_residuals_[level] = residuals\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._create_predict_inputs","title":"_create_predict_inputs","text":"<pre><code>_create_predict_inputs(\n    steps=None,\n    last_window=None,\n    exog=None,\n    predict_probabilistic=False,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    check_inputs=True,\n)\n</code></pre> <p>Create the inputs needed for the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>predict_probabilistic</code> <code>bool</code> <p>If <code>True</code>, the necessary checks for probabilistic predictions will be  performed.</p> <code>False</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Xs</code> <code>list</code> <p>List of numpy arrays with the predictors for each step.</p> <code>Xs_col_names</code> <code>list</code> <p>Names of the columns of the matrix created internally for prediction.</p> <code>steps</code> <code>list</code> <p>Steps to predict.</p> <code>prediction_index</code> <code>pandas Index</code> <p>Index of the predictions.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    predict_probabilistic: bool = False,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    check_inputs: bool = True\n) -&gt; tuple[list[np.ndarray], list[str], list[int], pd.Index]:\n    \"\"\"\n    Create the inputs needed for the prediction process.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    predict_probabilistic : bool, default False\n        If `True`, the necessary checks for probabilistic predictions will be \n        performed.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    Xs : list\n        List of numpy arrays with the predictors for each step.\n    Xs_col_names : list\n        Names of the columns of the matrix created internally for prediction.\n    steps : list\n        Steps to predict.\n    prediction_index : pandas Index\n        Index of the predictions.\n\n    \"\"\"\n\n    steps = prepare_steps_direct(\n                steps    = steps,\n                max_step = self.max_step\n            )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name  = type(self).__name__,\n            steps            = steps,\n            is_fitted        = self.is_fitted,\n            exog_in_         = self.exog_in_,\n            index_type_      = self.index_type_,\n            index_freq_      = self.index_freq_,\n            window_size      = self.window_size,\n            last_window      = last_window,\n            exog             = exog,\n            exog_names_in_   = self.exog_names_in_,\n            interval         = None,\n            max_step         = self.max_step,\n            series_names_in_ = self.X_train_series_names_in_\n        )\n\n        if predict_probabilistic:\n            check_residuals_input(\n                forecaster_name              = type(self).__name__,\n                use_in_sample_residuals      = use_in_sample_residuals,\n                in_sample_residuals_         = self.in_sample_residuals_,\n                out_sample_residuals_        = self.out_sample_residuals_,\n                use_binned_residuals         = use_binned_residuals,\n                in_sample_residuals_by_bin_  = self.in_sample_residuals_by_bin_,\n                out_sample_residuals_by_bin_ = self.out_sample_residuals_by_bin_,\n                levels                       = [self.level],\n            )\n\n    last_window = last_window.iloc[\n        -self.window_size:, last_window.columns.get_indexer(self.X_train_series_names_in_)\n    ].copy()\n\n    X_autoreg = []\n    Xs_col_names = []\n    for series in self.X_train_series_names_in_:\n        last_window_series = transform_numpy(\n                                 array             = last_window[series].to_numpy(),\n                                 transformer       = self.transformer_series_[series],\n                                 fit               = False,\n                                 inverse_transform = False\n                             )\n\n        if self.differentiation is not None:\n            last_window_series = self.differentiator_[series].fit_transform(last_window_series)\n\n        if self.lags is not None:\n            X_lags = last_window_series[-self.lags_[series]]\n            X_autoreg.append(X_lags)\n            Xs_col_names.extend(self.lags_names[series])\n\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            X_window_features = np.concatenate(\n                [\n                    wf.transform(last_window_series[n_diff:]) \n                    for wf in self.window_features\n                ]\n            )\n            X_autoreg.append(X_window_features)\n            # HACK: This is not the best way to do it. Can have any problem\n            # if the window_features are not in the same order as the\n            # self.window_features_names.\n            Xs_col_names.extend([f\"{series}_{wf}\" for wf in self.window_features_names])\n\n    X_autoreg = np.concatenate(X_autoreg).reshape(1, -1)\n    if exog is not None:\n        exog = input_to_frame(data=exog, input_name='exog')\n        exog = exog[self.exog_names_in_]\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )\n        check_exog_dtypes(exog=exog)\n        exog_values, _ = exog_to_direct_numpy(\n                             exog  = exog.to_numpy()[:max(steps)],\n                             steps = max(steps)\n                         )\n        exog_values = exog_values[0]\n\n        n_exog = exog.shape[1]\n        Xs = [\n            np.concatenate(\n                [\n                    X_autoreg, \n                    exog_values[(step - 1) * n_exog : step * n_exog].reshape(1, -1)\n                ],\n                axis=1\n            )\n            for step in steps\n        ]\n        # HACK: This is not the best way to do it. Can have any problem\n        # if the exog_columns are not in the same order as the\n        # self.window_features_names.\n        Xs_col_names = Xs_col_names + exog.columns.to_list()\n    else:\n        Xs = [X_autoreg] * len(steps)\n\n    prediction_index = expand_index(\n                           index = last_window.index,\n                           steps = max(steps)\n                       )[np.array(steps) - 1]\n    if isinstance(last_window.index, pd.DatetimeIndex) and np.array_equal(\n        steps, np.arange(min(steps), max(steps) + 1)\n    ):\n        prediction_index.freq = last_window.index.freq\n\n    # HACK: Why no use self.X_train_features_names_out_ as Xs_col_names?\n    return Xs, Xs_col_names, steps, prediction_index\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.create_predict_X","title":"create_predict_X","text":"<pre><code>create_predict_X(\n    steps=None,\n    last_window=None,\n    exog=None,\n    suppress_warnings=False,\n    check_inputs=True,\n    levels=None,\n)\n</code></pre> <p>Create the predictors needed to predict <code>steps</code> ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_predict</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the predictors for each step. The index  is the same as the prediction index.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def create_predict_X(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    suppress_warnings: bool = False,\n    check_inputs: bool = True,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create the predictors needed to predict `steps` ahead.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_predict : pandas DataFrame\n        Pandas DataFrame with the predictors for each step. The index \n        is the same as the prediction index.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    (\n        Xs,\n        Xs_col_names,\n        steps,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs\n        )\n\n    X_predict = pd.DataFrame(\n                    data    = np.concatenate(Xs, axis=0), \n                    columns = Xs_col_names, \n                    index   = prediction_index\n                )\n    X_predict.insert(0, 'level', np.tile([self.level], len(steps)))\n\n    if self.exog_in_:\n        categorical_features = any(\n            not pd.api.types.is_numeric_dtype(dtype) or pd.api.types.is_bool_dtype(dtype) \n            for dtype in set(self.exog_dtypes_out_)\n        )\n        if categorical_features:\n            X_predict = X_predict.astype(self.exog_dtypes_out_)\n\n    if self.transformer_series is not None or self.differentiation is not None:\n        warnings.warn(\n            \"The output matrix is in the transformed scale due to the \"\n            \"inclusion of transformations or differentiation in the Forecaster. \"\n            \"As a result, any predictions generated using this matrix will also \"\n            \"be in the transformed scale. Please refer to the documentation \"\n            \"for more details: \"\n            \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n            DataTransformationWarning\n        )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_predict\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.predict","title":"predict","text":"<pre><code>predict(\n    steps=None,\n    last_window=None,\n    exog=None,\n    suppress_warnings=False,\n    check_inputs=True,\n    levels=None,\n)\n</code></pre> <p>Predict n steps ahead</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the predictions. The columns are <code>level</code> and <code>pred</code>.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def predict(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    suppress_warnings: bool = False,\n    check_inputs: bool = True,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Long-format DataFrame with the predictions. The columns are `level`\n        and `pred`.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    (\n        Xs,\n        _,\n        steps,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs,\n        )\n\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = np.array([\n            regressor.predict(X).ravel()[0] \n            for regressor, X in zip(regressors, Xs)\n        ])\n\n    if self.differentiation is not None:\n        predictions = (\n            self.differentiator_[self.level]\n            .inverse_transform_next_window(predictions)\n        )\n\n    predictions = transform_numpy(\n                      array             = predictions,\n                      transformer       = self.transformer_series_[self.level],\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    # TODO: This DataFrame has freq because it only contain 1 level\n    # TODO: Adapt to multiple levels\n    # n_steps, n_levels = predictions.shape\n    # predictions = pd.DataFrame(\n    #     {\"level\": np.tile(levels, n_steps), \"pred\": predictions.ravel()},\n    #     index = np.repeat(prediction_index, n_levels),\n    # )\n    predictions = pd.DataFrame(\n        {\"level\": np.tile([self.level], len(steps)), \"pred\": predictions},\n        index = prediction_index,\n    )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.predict_bootstrapping","title":"predict_bootstrapping","text":"<pre><code>predict_bootstrapping(\n    steps=None,\n    last_window=None,\n    exog=None,\n    n_boot=250,\n    random_state=123,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    suppress_warnings=False,\n    levels=None,\n)\n</code></pre> <p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the References section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the bootstrapping predictions. The columns are <code>level</code>, <code>pred_boot_0</code>, <code>pred_boot_1</code>, ..., <code>pred_boot_n_boot</code>.</p> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    suppress_warnings: bool = False,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the References section for more information. \n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.     \n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.            \n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.   \n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Long-format DataFrame with the bootstrapping predictions. The columns\n        are `level`, `pred_boot_0`, `pred_boot_1`, ..., `pred_boot_n_boot`.\n\n    References\n    ----------\n    .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    (\n        Xs,\n        _,\n        steps,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps                   = steps, \n            last_window             = last_window, \n            exog                    = exog,\n            predict_probabilistic   = True, \n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_[self.level]\n        residuals_by_bin = self.in_sample_residuals_by_bin_[self.level]\n    else:\n        residuals = self.out_sample_residuals_[self.level]\n        residuals_by_bin = self.out_sample_residuals_by_bin_[self.level]\n\n    # NOTE: Predictors and residuals are transformed and differentiated\n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = np.array([\n            regressor.predict(X).ravel()[0] \n            for regressor, X in zip(regressors, Xs)\n        ])\n\n    rng = np.random.default_rng(seed=random_state)\n    if not use_binned_residuals:\n        sampled_residuals = residuals[\n            rng.integers(low=0, high=residuals.size, size=(len(steps), n_boot))\n        ]\n    else:\n        predicted_bins = self.binner[self.level].transform(predictions)\n        sampled_residuals = np.full(\n                                shape      = (predicted_bins.size, n_boot),\n                                fill_value = np.nan,\n                                order      = 'C',\n                                dtype      = float\n                            )\n        for i, bin in enumerate(predicted_bins):\n            sampled_residuals[i, :] = residuals_by_bin[bin][\n                rng.integers(low=0, high=residuals_by_bin[bin].size, size=n_boot)\n            ]\n\n    boot_predictions = np.tile(predictions, (n_boot, 1)).T\n    boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n    boot_predictions = boot_predictions + sampled_residuals\n\n    if self.differentiation is not None:\n        boot_predictions = (\n            self.differentiator_[self.level]\n            .inverse_transform_next_window(boot_predictions)\n        )\n\n    if self.transformer_series_[self.level]:\n        boot_predictions = np.apply_along_axis(\n                               func1d            = transform_numpy,\n                               axis              = 0,\n                               arr               = boot_predictions,\n                               transformer       = self.transformer_series_[self.level],\n                               fit               = False,\n                               inverse_transform = True\n                           )\n\n    # TODO: This DataFrame has freq because it only contain 1 level\n    # TODO: Adapt to multiple levels\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = prediction_index,\n                           columns = boot_columns\n                       )\n    boot_predictions.insert(0, 'level', np.tile([self.level], len(steps)))\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate._predict_interval_conformal","title":"_predict_interval_conformal","text":"<pre><code>_predict_interval_conformal(\n    steps=None,\n    last_window=None,\n    exog=None,\n    nominal_coverage=0.95,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n)\n</code></pre> <p>Generate prediction intervals using the conformal prediction  split method [1]_.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>nominal_coverage</code> <code>float</code> <p>Nominal coverage, also known as expected coverage, of the prediction intervals. Must be between 0 and 1.</p> <code>0.95</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def _predict_interval_conformal(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    nominal_coverage: float = 0.95,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate prediction intervals using the conformal prediction \n    split method [1]_.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    nominal_coverage : float, default 0.95\n        Nominal coverage, also known as expected coverage, of the prediction\n        intervals. Must be between 0 and 1.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    References\n    ----------\n    .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    (\n        Xs,\n        _,\n        steps,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps                   = steps, \n            last_window             = last_window, \n            exog                    = exog,\n            predict_probabilistic   = True, \n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_[self.level]\n        residuals_by_bin = self.in_sample_residuals_by_bin_[self.level]\n    else:\n        residuals = self.out_sample_residuals_[self.level]\n        residuals_by_bin = self.out_sample_residuals_by_bin_[self.level]\n\n    # NOTE: Predictors and residuals are transformed and differentiated  \n    regressors = [self.regressors_[step] for step in steps]\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = np.array([\n            regressor.predict(X).ravel()[0] \n            for regressor, X in zip(regressors, Xs)\n        ])\n\n    if use_binned_residuals:\n        correction_factor_by_bin = {\n            k: np.quantile(np.abs(v), nominal_coverage)\n            for k, v in residuals_by_bin.items()\n        }\n        replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n        predictions_bin = self.binner[self.level].transform(predictions)\n        correction_factor = replace_func(predictions_bin)\n    else:\n        correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n    lower_bound = predictions - correction_factor\n    upper_bound = predictions + correction_factor\n    predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n    if self.differentiation is not None:\n        predictions = (\n            self.differentiator_[self.level]\n            .inverse_transform_next_window(predictions)\n        )\n\n    if self.transformer_series_[self.level]:\n        predictions = np.apply_along_axis(\n                          func1d            = transform_numpy,\n                          axis              = 0,\n                          arr               = predictions,\n                          transformer       = self.transformer_series_[self.level],\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n    predictions = pd.DataFrame(\n                      data    = predictions,\n                      index   = prediction_index,\n                      columns = [\"pred\", \"lower_bound\", \"upper_bound\"]\n                  )\n    predictions.insert(0, 'level', np.tile([self.level], len(steps)))\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.predict_interval","title":"predict_interval","text":"<pre><code>predict_interval(\n    steps=None,\n    last_window=None,\n    exog=None,\n    method=\"conformal\",\n    interval=[5, 95],\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    suppress_warnings=False,\n    levels=None,\n)\n</code></pre> <p>Predict n steps ahead and estimate prediction intervals using either  bootstrapping or conformal prediction methods. Refer to the References  section for additional details on these methods.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction  intervals [1]_.</li> <li>'conformal': Employs the conformal prediction split method for  interval estimation [2]_.</li> </ul> <code>'conformal'</code> <code>interval</code> <code>(float, list, tuple)</code> <p>Confidence level of the prediction interval. Interpretation depends  on the method used:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0  and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code>  percentiles.</li> <li>If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which  must be between 0 and 100 inclusive. For example, interval  of 95% should be as <code>interval = [2.5, 97.5]</code>.</li> <li>When using <code>method='conformal'</code>, the interval must be a float or  a list/tuple defining a symmetric interval.</li> </ul> <code>[5, 95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the predictions and the lower and upper bounds of the estimated interval. The columns are <code>level</code>, <code>pred</code>, <code>lower_bound</code>, <code>upper_bound</code>.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> <p>.. [2] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    method: str = 'conformal',\n    interval: float | list[float] | tuple[float] = [5, 95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    suppress_warnings: bool = False,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using either \n    bootstrapping or conformal prediction methods. Refer to the References \n    section for additional details on these methods.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    method : str, default 'conformal'\n        Technique used to estimate prediction intervals. Available options:\n\n        - 'bootstrapping': Bootstrapping is used to generate prediction \n        intervals [1]_.\n        - 'conformal': Employs the conformal prediction split method for \n        interval estimation [2]_.\n    interval : float, list, tuple, default [5, 95]\n        Confidence level of the prediction interval. Interpretation depends \n        on the method used:\n\n        - If `float`, represents the nominal (expected) coverage (between 0 \n        and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]` \n        percentiles.\n        - If `list` or `tuple`, defines the exact percentiles to compute, which \n        must be between 0 and 100 inclusive. For example, interval \n        of 95% should be as `interval = [2.5, 97.5]`.\n        - When using `method='conformal'`, the interval must be a float or \n        a list/tuple defining a symmetric interval.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Long-format DataFrame with the predictions and the lower and upper\n        bounds of the estimated interval. The columns are `level`, `pred`,\n        `lower_bound`, `upper_bound`.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    if method == \"bootstrapping\":\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=False)\n            interval = np.array(interval) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal='interval')\n            interval = np.array([0.5 - interval / 2, 0.5 + interval / 2])\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        boot_predictions[['lower_bound', 'upper_bound']] = (\n            boot_predictions.iloc[:, 1:].quantile(q=interval, axis=1).transpose()\n        )\n        predictions = pd.concat([\n            predictions, boot_predictions[['lower_bound', 'upper_bound']]\n        ], axis=1)\n\n    elif method == 'conformal':\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal='interval')\n            nominal_coverage = interval\n\n        predictions = self._predict_interval_conformal(\n                          steps                   = steps,\n                          last_window             = last_window,\n                          exog                    = exog,\n                          nominal_coverage        = nominal_coverage,\n                          use_in_sample_residuals = use_in_sample_residuals,\n                          use_binned_residuals    = use_binned_residuals\n                      )\n    else:\n        raise ValueError(\n            f\"Invalid `method` '{method}'. Choose 'bootstrapping' or 'conformal'.\"\n        )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.predict_quantiles","title":"predict_quantiles","text":"<pre><code>predict_quantiles(\n    steps=None,\n    last_window=None,\n    exog=None,\n    quantiles=[0.05, 0.5, 0.95],\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    suppress_warnings=False,\n    levels=None,\n)\n</code></pre> <p>Bootstrapping based predicted quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>quantiles</code> <code>(list, tuple)</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>[0.05, 0.5, 0.95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating quantiles.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the quantiles predicted by the forecaster. For example, if <code>quantiles = [0.05, 0.5, 0.95]</code>, the columns are <code>level</code>, <code>q_0.05</code>, <code>q_0.5</code>, <code>q_0.95</code>.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def predict_quantiles(\n    self,\n    steps: int | list[int] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    quantiles: list[float] | tuple[float] = [0.05, 0.5, 0.95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    suppress_warnings: bool = False,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Bootstrapping based predicted quantiles.\n\n    Parameters\n    ----------\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    quantiles : list, tuple, default [0.05, 0.5, 0.95]\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating quantiles.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Long-format DataFrame with the quantiles predicted by the forecaster.\n        For example, if `quantiles = [0.05, 0.5, 0.95]`, the columns are\n        `level`, `q_0.05`, `q_0.5`, `q_0.95`.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    check_interval(quantiles=quantiles)\n\n    predictions = self.predict_bootstrapping(\n                      steps                   = steps,\n                      last_window             = last_window,\n                      exog                    = exog,\n                      n_boot                  = n_boot,\n                      random_state            = random_state,\n                      use_in_sample_residuals = use_in_sample_residuals,\n                      use_binned_residuals    = use_binned_residuals\n                  )\n\n    quantiles_cols = [f'q_{q}' for q in quantiles]\n    predictions[quantiles_cols] = (\n        predictions.iloc[:, 1:].quantile(q=quantiles, axis=1).transpose()\n    )\n    predictions = predictions[['level'] + quantiles_cols]\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.predict_dist","title":"predict_dist","text":"<pre><code>predict_dist(\n    distribution,\n    steps=None,\n    last_window=None,\n    exog=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    suppress_warnings=False,\n    levels=None,\n)\n</code></pre> <p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>object</code> <p>A distribution object from scipy.stats with methods <code>_pdf</code> and <code>fit</code>.  For example scipy.stats.norm.</p> required <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>levels</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the parameters of the fitted distribution for each step. The columns are <code>level</code>, <code>param_0</code>, <code>param_1</code>, ...,  <code>param_n</code>, where <code>param_i</code> are the parameters of the distribution.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def predict_dist(\n    self,\n    distribution: object,\n    steps: int | list[int] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    suppress_warnings: bool = False,\n    levels: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    distribution : object\n        A distribution object from scipy.stats with methods `_pdf` and `fit`. \n        For example scipy.stats.norm.\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    levels : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Long-format DataFrame with the parameters of the fitted distribution\n        for each step. The columns are `level`, `param_0`, `param_1`, ..., \n        `param_n`, where `param_i` are the parameters of the distribution.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    if not hasattr(distribution, \"_pdf\") or not callable(getattr(distribution, \"fit\", None)):\n        raise TypeError(\n            \"`distribution` must be a valid probability distribution object \"\n            \"from scipy.stats, with methods `_pdf` and `fit`.\"\n        )\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    predictions = self.predict_bootstrapping(\n                      steps                   = steps,\n                      last_window             = last_window,\n                      exog                    = exog,\n                      n_boot                  = n_boot,\n                      random_state            = random_state,\n                      use_in_sample_residuals = use_in_sample_residuals,\n                      use_binned_residuals    = use_binned_residuals\n                  )\n\n    param_names = [\n        p for p in inspect.signature(distribution._pdf).parameters if not p == \"x\"\n    ] + [\"loc\", \"scale\"]\n\n    predictions[param_names] = (\n        predictions.iloc[:, 1:].apply(\n            lambda x: distribution.fit(x), axis=1, result_type='expand'\n        )\n    )\n    predictions = predictions[['level'] + param_names]\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.set_params","title":"set_params","text":"<pre><code>set_params(params)\n</code></pre> <p>Set new values to the parameters of the scikit-learn model stored in the forecaster. It is important to note that all models share the same  configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def set_params(\n    self, \n    params: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the scikit-learn model stored in the\n    forecaster. It is important to note that all models share the same \n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.regressors_ = {\n        step: clone(self.regressor)\n        for step in self.steps\n    }\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.set_fit_kwargs","title":"set_fit_kwargs","text":"<pre><code>set_fit_kwargs(fit_kwargs)\n</code></pre> <p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.set_lags","title":"set_lags","text":"<pre><code>set_lags(lags=None)\n</code></pre> <p>Set new value to the attribute <code>lags</code>. Attributes <code>lags_names</code>,  <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range, dict</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>dict</code>: create different lags for each series. {'series_column_name': lags}.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def set_lags(\n    self, \n    lags: int | list[int] | np.ndarray[int] | range[int] | dict[str, int | list] | None = None,\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `lags`. Attributes `lags_names`, \n    `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range, dict, default None\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `dict`: create different lags for each series. {'series_column_name': lags}.\n        - `None`: no lags are included as predictors. \n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if self.window_features is None and lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    if isinstance(lags, dict):\n        self.lags = {}\n        self.lags_names = {}\n        list_max_lags = []\n        for key in lags:\n            if lags[key] is None:\n                self.lags[key] = None\n                self.lags_names[key] = None\n            else:\n                self.lags[key], lags_names, max_lag = initialize_lags(\n                    forecaster_name = type(self).__name__,\n                    lags            = lags[key]\n                )\n                self.lags_names[key] = (\n                    [f'{key}_{lag}' for lag in lags_names] \n                     if lags_names is not None \n                     else None\n                )\n                if max_lag is not None:\n                    list_max_lags.append(max_lag)\n\n        self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n    else:\n        self.lags, self.lags_names, self.max_lag = initialize_lags(\n            forecaster_name = type(self).__name__, \n            lags            = lags\n        )\n\n    # Repeated here in case of lags is a dict with all values as None\n    if self.window_features is None and (lags is None or self.max_lag is None):\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation\n        self.differentiator.set_params(window_size=self.window_size)\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.set_window_features","title":"set_window_features","text":"<pre><code>set_window_features(window_features=None)\n</code></pre> <p>Set new value to the attribute <code>window_features</code>. Attributes  <code>max_size_window_features</code>, <code>window_features_names</code>,  <code>window_features_class_names</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def set_window_features(\n    self, \n    window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `window_features`. Attributes \n    `max_size_window_features`, `window_features_names`, \n    `window_features_class_names` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    window_features : object, list, default None\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if window_features is None and self.max_lag is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.window_features, self.window_features_names, self.max_size_window_features = (\n        initialize_window_features(window_features)\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ] \n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation\n        self.differentiator.set_params(window_size=self.window_size)\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.set_in_sample_residuals","title":"set_in_sample_residuals","text":"<pre><code>set_in_sample_residuals(\n    series,\n    exog=None,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Set in-sample residuals in case they were not calculated during the training process. </p> <p>In-sample residuals are calculated as the difference between the true  values and the predictions made by the forecaster using the training  data. The following internal attributes are updated:</p> <ul> <li><code>in_sample_residuals_</code>: Dictionary containing a numpy ndarray with the residuals for each series in the form <code>{series: residuals}</code>.</li> <li><code>binner_intervals_</code>: intervals used to bin the residuals are calculated using the quantiles of the predicted values.</li> <li><code>in_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the intervals of the predicted values and the values are the residuals associated with that range. </li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>in_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the sampling  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def set_in_sample_residuals(\n    self,\n    series: pd.DataFrame,\n    exog: pd.Series | pd.DataFrame | None = None,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process. \n\n    In-sample residuals are calculated as the difference between the true \n    values and the predictions made by the forecaster using the training \n    data. The following internal attributes are updated:\n\n    + `in_sample_residuals_`: Dictionary containing a numpy ndarray with the\n    residuals for each series in the form `{series: residuals}`.\n    + `binner_intervals_`: intervals used to bin the residuals are calculated\n    using the quantiles of the predicted values.\n    + `in_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the intervals of the predicted values and the values are\n    the residuals associated with that range. \n\n    A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the sampling \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    check_y(y=series[self.level], series_id='`series`')\n    series_index_range = check_extract_values_and_index(\n        data=series, data_label='`series`', return_values=False\n    )[1][[0, -1]]\n    if not series_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `series` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {series_index_range}\"\n        )\n\n    # NOTE: This attributes are modified in _create_train_X_y, store original values\n    original_exog_in_ = self.exog_in_\n    original_X_train_window_features_names_out_ = self.X_train_window_features_names_out_\n    original_X_train_direct_exog_names_out_ = self.X_train_direct_exog_names_out_\n\n    (\n        X_train,\n        y_train,\n        _,\n        _,\n        _,\n        _,\n        X_train_features_names_out_,\n        *_\n    ) = self._create_train_X_y(series=series, exog=exog)\n\n    if not X_train_features_names_out_ == self.X_train_features_names_out_:\n\n        # NOTE: Reset attributes modified in _create_train_X_y to their original values\n        self.exog_in_ = original_exog_in_\n        self.X_train_window_features_names_out_ = original_X_train_window_features_names_out_\n        self.X_train_direct_exog_names_out_ = original_X_train_direct_exog_names_out_\n\n        raise ValueError(\n            f\"Feature mismatch detected after matrix creation. The features \"\n            f\"generated from the provided data do not match those used during \"\n            f\"the training process. To correctly set in-sample residuals, \"\n            f\"ensure that the same data and preprocessing steps are applied.\\n\"\n            f\"    Expected output : {self.X_train_features_names_out_}\\n\"\n            f\"    Current output  : {X_train_features_names_out_}\"\n        )\n\n    y_true_steps = []\n    y_pred_steps = []\n    self.in_sample_residuals_ = {}\n    for step in self.steps:\n        X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                         step          = step,\n                                         X_train       = X_train,\n                                         y_train       = y_train,\n                                         remove_suffix = True\n                                     )\n\n        y_true_steps.append(y_train_step.to_numpy())\n        y_pred_steps.append(self.regressors_[step].predict(X_train_step))\n\n    self._binning_in_sample_residuals(\n        level                     = self.level,\n        y_true                    = np.concatenate(y_true_steps),\n        y_pred                    = np.concatenate(y_pred_steps),\n        store_in_sample_residuals = True,\n        random_state              = random_state\n    )\n\n    # NOTE: Reset attributes modified in _create_train_X_y to their original values\n    self.exog_in_ = original_exog_in_\n    self.X_train_window_features_names_out_ = original_X_train_window_features_names_out_\n    self.X_train_direct_exog_names_out_ = original_X_train_direct_exog_names_out_\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.set_out_sample_residuals","title":"set_out_sample_residuals","text":"<pre><code>set_out_sample_residuals(\n    y_true, y_pred, append=False, random_state=123\n)\n</code></pre> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. <code>y_true</code> and <code>y_pred</code> are expected to be in the original scale of the time series. Residuals are calculated as <code>y_true</code> - <code>y_pred</code>, after applying the necessary transformations and differentiations if the forecaster includes them (<code>self.transformer_series</code> and <code>self.differentiation</code>).</p> <p>A total of 10_000 residuals are stored in the attribute <code>out_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>dict</code> <p>Dictionary of numpy ndarrays or pandas Series with the true values of the time series for each series in the form {series: y_true}.</p> required <code>y_pred</code> <code>dict</code> <p>Dictionary of numpy ndarrays or pandas Series with the predicted values of the time series for each series in the form {series: y_pred}.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals_</code>. If after appending the new residuals, the limit of 10_000 samples is exceeded, a random sample of 10_000 is kept.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: dict[str, np.ndarray | pd.Series],\n    y_pred: dict[str, np.ndarray | pd.Series],\n    append: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. `y_true` and `y_pred` are expected\n    to be in the original scale of the time series. Residuals are calculated\n    as `y_true` - `y_pred`, after applying the necessary transformations and\n    differentiations if the forecaster includes them (`self.transformer_series`\n    and `self.differentiation`).\n\n    A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored.\n\n    Parameters\n    ----------\n    y_true : dict\n        Dictionary of numpy ndarrays or pandas Series with the true values of\n        the time series for each series in the form {series: y_true}.\n    y_pred : dict\n        Dictionary of numpy ndarrays or pandas Series with the predicted values\n        of the time series for each series in the form {series: y_pred}.\n    append : bool, default False\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals_`. If after appending the new residuals,\n        the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n        kept.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, dict):\n        raise TypeError(\n            f\"`y_true` must be a dictionary of numpy ndarrays or pandas Series. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, dict):\n        raise TypeError(\n            f\"`y_pred` must be a dictionary of numpy ndarrays or pandas Series. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if not set(y_true.keys()) == set(y_pred.keys()):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same keys. \"\n            f\"Got {set(y_true.keys())} and {set(y_pred.keys())}.\"\n        )\n\n    for k in y_true.keys():\n        if not isinstance(y_true[k], (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"Values of `y_true` must be numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_true[k])} for series {k}.\"\n            )\n        if not isinstance(y_pred[k], (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"Values of `y_pred` must be numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_pred[k])} for series {k}.\"\n            )\n        if len(y_true[k]) != len(y_pred[k]):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true[k])} and {len(y_pred[k])} for series {k}.\"\n            )\n        if isinstance(y_true[k], pd.Series) and isinstance(y_pred[k], pd.Series):\n            if not y_true[k].index.equals(y_pred[k].index):\n                raise ValueError(\n                    f\"When containing pandas Series, elements in `y_true` and \"\n                    f\"`y_pred` must have the same index. Error in series {k}.\"\n                )\n\n    if not set(y_pred.keys()) == {self.level}:\n        raise ValueError(\n            f\"`y_pred` and `y_true` must have only the key '{self.level}'. \" \n            f\"Got {set(y_pred.keys())}.\"\n        )\n\n    y_true = deepcopy(y_true[self.level])\n    y_pred = deepcopy(y_pred[self.level])\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.to_numpy()\n\n    if self.transformer_series:\n        y_true = transform_numpy(\n                     array             = y_true,\n                     transformer       = self.transformer_series_[self.level],\n                     fit               = False,\n                     inverse_transform = False\n                 )\n        y_pred = transform_numpy(\n                     array             = y_pred,\n                     transformer       = self.transformer_series_[self.level],\n                     fit               = False,\n                     inverse_transform = False\n                 )\n\n    if self.differentiation is not None:\n        differentiator = copy(self.differentiator)\n        differentiator.set_params(window_size=None)\n        y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n        y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n\n    data = pd.DataFrame(\n        {'prediction': y_pred, 'residuals': y_true - y_pred}\n    ).dropna()\n    y_pred = data['prediction'].to_numpy()\n    residuals = data['residuals'].to_numpy()\n\n    data['bin'] = self.binner[self.level].transform(y_pred).astype(int)\n    residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n    if self.out_sample_residuals_ is None:\n        self.out_sample_residuals_ = {self.level: None}\n        self.out_sample_residuals_by_bin_ = {self.level: None}\n\n    out_sample_residuals = (\n        np.array([]) \n        if self.out_sample_residuals_[self.level] is None\n        else self.out_sample_residuals_[self.level]\n    )\n    out_sample_residuals_by_bin = (\n        {} \n        if self.out_sample_residuals_by_bin_[self.level] is None\n        else self.out_sample_residuals_by_bin_[self.level]\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    max_samples = 10_000 // self.binner[self.level].n_bins_\n    rng = np.random.default_rng(seed=random_state)\n    for k, v in out_sample_residuals_by_bin.items():\n        if len(v) &gt; max_samples:\n            sample = rng.choice(a=v, size=max_samples, replace=False)\n            out_sample_residuals_by_bin[k] = sample\n\n    for k in self.binner_intervals_.get(self.level, {}).keys():\n        if k not in out_sample_residuals_by_bin:\n            out_sample_residuals_by_bin[k] = np.array([])\n\n    empty_bins = [\n        k for k, v in out_sample_residuals_by_bin.items() \n        if v.size == 0\n    ]\n    if empty_bins:\n        warnings.warn(\n            f\"The following bins of level '{self.level}' have no out of sample residuals: \"\n            f\"{empty_bins}. No predicted values fall in the interval \"\n            f\"{[self.binner_intervals_[self.level][bin] for bin in empty_bins]}. \"\n            f\"Empty bins will be filled with a random sample of residuals.\", \n            ResidualsUsageWarning\n        )\n        empty_bin_size = min(max_samples, len(out_sample_residuals))\n        for k in empty_bins:\n            out_sample_residuals_by_bin[k] = rng.choice(\n                a       = out_sample_residuals,\n                size    = empty_bin_size,\n                replace = False\n            )\n\n    if len(out_sample_residuals) &gt; 10_000:\n        out_sample_residuals = rng.choice(\n            a       = out_sample_residuals, \n            size    = 10_000, \n            replace = False\n        )\n\n    self.out_sample_residuals_[self.level] = out_sample_residuals\n    self.out_sample_residuals_by_bin_[self.level] = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/ForecasterDirectMultiVariate.html#skforecast.direct._forecaster_direct_multivariate.ForecasterDirectMultiVariate.get_feature_importances","title":"get_feature_importances","text":"<pre><code>get_feature_importances(step, sort_importance=True)\n</code></pre> <p>Return feature importance of the model stored in the forecaster for a specific step. Since a separate model is created for each forecast time step, it is necessary to select the model from which retrieve information. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, it returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Model from which retrieve information (a separate model is created  for each forecast time step). First step is 1.</p> required <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\direct\\_forecaster_direct_multivariate.py</code> <pre><code>def get_feature_importances(\n    self,\n    step: int,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importance of the model stored in the forecaster for a\n    specific step. Since a separate model is created for each forecast time\n    step, it is necessary to select the model from which retrieve information.\n    Only valid when regressor stores internally the feature importances in\n    the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n    `None`.\n\n    Parameters\n    ----------\n    step : int\n        Model from which retrieve information (a separate model is created \n        for each forecast time step). First step is 1.\n    sort_importance: bool, default True\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not isinstance(step, int):\n        raise TypeError(\n            f\"`step` must be an integer. Got {type(step)}.\"\n        )\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `get_feature_importances()`.\"\n        )\n\n    if (step &lt; 1) or (step &gt; self.max_step):\n        raise ValueError(\n            f\"The step must have a value from 1 to the maximum number of steps \"\n            f\"({self.max_step}). Got {step}.\"\n        )\n\n    if isinstance(self.regressor, Pipeline):\n        estimator = self.regressors_[step][-1]\n    else:\n        estimator = self.regressors_[step]\n\n    n_lags = len(list(\n        chain(*[v for v in self.lags_.values() if v is not None])\n    ))\n    n_window_features = (\n        len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n    )\n    idx_columns_autoreg = np.arange(n_lags + n_window_features)\n    if self.exog_in_:\n        idx_columns_exog = np.flatnonzero(\n            [name.endswith(f\"step_{step}\") for name in self.X_train_features_names_out_]\n        )\n    else:\n        idx_columns_exog = np.array([], dtype=int)\n\n    idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n    idx_columns = [int(x) for x in idx_columns]  # Required since numpy 2.0\n    feature_names = [\n        self.X_train_features_names_out_[i].replace(f\"_step_{step}\", \"\") \n        for i in idx_columns\n    ]\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            f\"Impossible to access feature importances for regressor of type \"\n            f\"{type(estimator)}. This method is only valid when the \"\n            f\"regressor stores internally the feature importances in the \"\n            f\"attribute `feature_importances_` or `coef_`.\"\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': feature_names,\n                                  'importance': feature_importances\n                              })\n        if sort_importance:\n            feature_importances = feature_importances.sort_values(\n                                      by='importance', ascending=False\n                                  )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html","title":"<code>ForecasterEquivalentDate</code>","text":""},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate","title":"skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate","text":"<pre><code>ForecasterEquivalentDate(\n    offset,\n    n_offsets=1,\n    agg_func=np.mean,\n    binner_kwargs=None,\n    forecaster_id=None,\n)\n</code></pre> <p>This forecaster predicts future values based on the most recent equivalent date. It also allows to aggregate multiple past values of the equivalent date using a function (e.g. mean, median, max, min, etc.). The equivalent date is calculated by moving back in time a specified number of steps (offset). The offset can be defined as an integer or as a pandas DateOffset. This approach is useful as a baseline, but it is a simplistic method and may not capture complex underlying patterns.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period. If <code>offset</code> is an integer, it represents the number of steps to go back in time. For example, if the frequency of the time series is daily,  <code>offset = 7</code> means that the most recent data similar to the target period is the value observed 7 days ago. Pandas DateOffsets can also be used to move forward a given number of  valid dates. For example, Bday(2) can be used to move back two business  days. If the date does not start on a valid date, it is first moved to a  valid date. For example, if the date is a Saturday, it is moved to the  previous Friday. Then, the offset is applied. If the result is a non-valid  date, it is moved to the next valid date. For example, if the date is a Sunday, it is moved to the next Monday.  For more information about offsets, see https://pandas.pydata.org/docs/reference/offset_frequency.html.</p> required <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction. If <code>n_offsets</code> is greater than 1, the values at the equivalent dates are aggregated using the <code>agg_func</code> function. For example, if the frequency of the time series is daily, <code>offset = 7</code>, <code>n_offsets = 2</code> and <code>agg_func = np.mean</code>, the predicted value will be the mean of the values observed 7 and 14 days ago.</p> <code>1</code> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1.</p> <code>np.mean</code> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code> used to discretize  the residuals into k bins according to the predicted values associated  with each residual. Available arguments are: <code>n_bins</code>, <code>method</code>, <code>subsample</code>, <code>random_state</code> and <code>dtype</code>. Argument <code>method</code> is passed internally to the function <code>numpy.percentile</code>. New in version 0.17.0</p> <code>None</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>offset</code> <code>(int, DateOffset)</code> <p>Number of steps to go back in time to find the most recent equivalent date to the target period. If <code>offset</code> is an integer, it represents the number of steps to go back in time. For example, if the frequency of the time series is daily,  <code>offset = 7</code> means that the most recent data similar to the target period is the value observed 7 days ago. Pandas DateOffsets can also be used to move forward a given number of  valid dates. For example, Bday(2) can be used to move back two business  days. If the date does not start on a valid date, it is first moved to a  valid date. For example, if the date is a Saturday, it is moved to the  previous Friday. Then, the offset is applied. If the result is a non-valid  date, it is moved to the next valid date. For example, if the date is a Sunday, it is moved to the next Monday.  For more information about offsets, see https://pandas.pydata.org/docs/reference/offset_frequency.html.</p> <code>n_offsets</code> <code>int</code> <p>Number of equivalent dates (multiple of offset) used in the prediction. If <code>offset</code> is greater than 1, the value at the equivalent dates is aggregated using the <code>agg_func</code> function. For example, if the frequency of the time series is daily, <code>offset = 7</code>, <code>n_offsets = 2</code> and <code>agg_func = np.mean</code>, the predicted value will be the mean of the values observed 7 and 14 days ago.</p> <code>agg_func</code> <code>Callable</code> <p>Function used to aggregate the values of the equivalent dates when the number of equivalent dates (<code>n_offsets</code>) is greater than 1.</p> <code>window_size</code> <code>int</code> <p>Number of past values needed to include the last equivalent dates according to the <code>offset</code> and <code>n_offsets</code>.</p> <code>last_window_</code> <code>pandas Series</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the past values needed to include the last equivalent date according the <code>offset</code> and <code>n_offsets</code>.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_name_in_</code> <code>str</code> <p>Names of the series provided by the user during training.</p> <code>in_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 10_000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after differentiation.</p> <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>. If  <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed  scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after  differentiation. </p> <code>out_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non-training data. Only stored up to 10_000 values. Use <code>set_out_sample_residuals()</code> method to set values. If  <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed  scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after  differentiation.</p> <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>. If  <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed  scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after  differentiation. </p> <code>binner</code> <code>QuantileBinner</code> <p><code>QuantileBinner</code> used to discretize residuals into k bins according  to the predicted values associated with each residual.</p> <code>binner_intervals_</code> <code>dict</code> <p>Intervals used to discretize residuals into k bins according to the predicted values associated with each residual.</p> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code>.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>_probabilistic_mode</code> <code>(str, bool)</code> <p>Private attribute used to indicate whether the forecaster should perform  some calculations during backtesting.</p> <code>regressor</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiation</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiation_max</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Training Forecaster.</p> <code>predict</code> <p>Predict n steps ahead.</p> <code>predict_interval</code> <p>Predict n steps ahead and estimate prediction intervals using conformal </p> <code>set_in_sample_residuals</code> <p>Set in-sample residuals in case they were not calculated during the</p> <code>set_out_sample_residuals</code> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample</p> <code>summary</code> <p>Show forecaster information.</p> Source code in <code>skforecast\\recursive\\_forecaster_equivalent_date.py</code> <pre><code>def __init__(\n    self,\n    offset: int | pd.tseries.offsets.DateOffset,\n    n_offsets: int = 1,\n    agg_func: Callable = np.mean,\n    binner_kwargs: dict[str, object] | None = None,\n    forecaster_id: str | int | None = None\n) -&gt; None:\n\n    self.offset                       = offset\n    self.n_offsets                    = n_offsets\n    self.agg_func                     = agg_func\n    self.last_window_                 = None\n    self.index_type_                  = None\n    self.index_freq_                  = None\n    self.training_range_              = None\n    self.series_name_in_              = None\n    self.in_sample_residuals_         = None\n    self.out_sample_residuals_        = None\n    self.in_sample_residuals_by_bin_  = None\n    self.out_sample_residuals_by_bin_ = None\n    self.creation_date                = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted                    = False\n    self.fit_date                     = None\n    self.skforecast_version           = skforecast.__version__\n    self.python_version               = sys.version.split(\" \")[0]\n    self.forecaster_id                = forecaster_id\n    self._probabilistic_mode          = \"binned\"\n    self.regressor                    = None\n    self.differentiation              = None\n    self.differentiation_max          = None\n\n    if not isinstance(self.offset, (int, pd.tseries.offsets.DateOffset)):\n        raise TypeError(\n            \"`offset` must be an integer greater than 0 or a \"\n            \"pandas.tseries.offsets. Find more information about offsets in \"\n            \"https://pandas.pydata.org/docs/reference/offset_frequency.html\"\n        )\n\n    self.window_size = self.offset * self.n_offsets\n\n    self.binner_kwargs = binner_kwargs\n    if binner_kwargs is None:\n        self.binner_kwargs = {\n            'n_bins': 10, 'method': 'linear', 'subsample': 200000,\n            'random_state': 789654, 'dtype': np.float64\n        }\n    self.binner = QuantileBinner(**self.binner_kwargs)\n    self.binner_intervals_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.offset","title":"offset  <code>instance-attribute</code>","text":"<pre><code>offset = offset\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.n_offsets","title":"n_offsets  <code>instance-attribute</code>","text":"<pre><code>n_offsets = n_offsets\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.agg_func","title":"agg_func  <code>instance-attribute</code>","text":"<pre><code>agg_func = agg_func\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.last_window_","title":"last_window_  <code>instance-attribute</code>","text":"<pre><code>last_window_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.index_type_","title":"index_type_  <code>instance-attribute</code>","text":"<pre><code>index_type_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.index_freq_","title":"index_freq_  <code>instance-attribute</code>","text":"<pre><code>index_freq_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.training_range_","title":"training_range_  <code>instance-attribute</code>","text":"<pre><code>training_range_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.series_name_in_","title":"series_name_in_  <code>instance-attribute</code>","text":"<pre><code>series_name_in_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.in_sample_residuals_","title":"in_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.out_sample_residuals_","title":"out_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.in_sample_residuals_by_bin_","title":"in_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.out_sample_residuals_by_bin_","title":"out_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.creation_date","title":"creation_date  <code>instance-attribute</code>","text":"<pre><code>creation_date = strftime('%Y-%m-%d %H:%M:%S')\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.is_fitted","title":"is_fitted  <code>instance-attribute</code>","text":"<pre><code>is_fitted = False\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.fit_date","title":"fit_date  <code>instance-attribute</code>","text":"<pre><code>fit_date = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.skforecast_version","title":"skforecast_version  <code>instance-attribute</code>","text":"<pre><code>skforecast_version = __version__\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.python_version","title":"python_version  <code>instance-attribute</code>","text":"<pre><code>python_version = split(' ')[0]\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.forecaster_id","title":"forecaster_id  <code>instance-attribute</code>","text":"<pre><code>forecaster_id = forecaster_id\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate._probabilistic_mode","title":"_probabilistic_mode  <code>instance-attribute</code>","text":"<pre><code>_probabilistic_mode = 'binned'\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.regressor","title":"regressor  <code>instance-attribute</code>","text":"<pre><code>regressor = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.differentiation","title":"differentiation  <code>instance-attribute</code>","text":"<pre><code>differentiation = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.differentiation_max","title":"differentiation_max  <code>instance-attribute</code>","text":"<pre><code>differentiation_max = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = offset * n_offsets\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.binner_kwargs","title":"binner_kwargs  <code>instance-attribute</code>","text":"<pre><code>binner_kwargs = binner_kwargs\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.binner","title":"binner  <code>instance-attribute</code>","text":"<pre><code>binner = QuantileBinner(**(binner_kwargs))\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.binner_intervals_","title":"binner_intervals_  <code>instance-attribute</code>","text":"<pre><code>binner_intervals_ = None\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\recursive\\_forecaster_equivalent_date.py</code> <pre><code>def _repr_html_(self):\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    style, unique_id = get_style_repr_html(self.is_fitted)\n\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Regressor:&lt;/strong&gt; {type(self.regressor).__name__}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Offset:&lt;/strong&gt; {self.offset}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Number of offsets:&lt;/strong&gt; {self.n_offsets}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Aggregation function:&lt;/strong&gt; {self.agg_func.__name__}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skforecast version:&lt;/strong&gt; {self.skforecast_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Training Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterequivalentdate.html\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/forecasting-baseline.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return style + content\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.fit","title":"fit","text":"<pre><code>fit(\n    y,\n    store_in_sample_residuals=False,\n    random_state=123,\n    exog=None,\n)\n</code></pre> <p>Training Forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_equivalent_date.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123,\n    exog: Any = None\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"`y` must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        if not isinstance(y.index, pd.DatetimeIndex):\n            raise TypeError(\n                \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                \"pandas DatetimeIndex with frequency.\"\n            )\n        elif y.index.freq is None:\n            raise TypeError(\n                \"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                \"pandas DatetimeIndex with frequency.\"\n            )\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_    = None\n    self.index_type_     = None\n    self.index_freq_     = None\n    self.training_range_ = None\n    self.series_name_in_ = None\n    self.is_fitted       = False\n\n    _, y_index = check_extract_values_and_index(\n        data=y, data_label='`y`', return_values=False\n    )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        # Calculate the window_size in steps for compatibility with the\n        # check_predict_input function. This is not a exact calculation\n        # because the offset follows the calendar rules and the distance\n        # between two dates may not be constant.\n        first_valid_index = (y_index[-1] - self.offset * self.n_offsets)\n\n        try:\n            window_size_idx_start = y_index.get_loc(first_valid_index)\n            window_size_idx_end = y_index.get_loc(y_index[-1])\n            self.window_size = window_size_idx_end - window_size_idx_start\n        except KeyError:\n            raise ValueError(\n                f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                f\"to the window size ({self.window_size}). This is because  \"\n                f\"the offset ({self.offset}) is larger than the available \"\n                f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `y`.\"\n            )\n    else:\n        if len(y) &lt;= self.window_size:\n            raise ValueError(\n                f\"Length of `y` must be greater than the maximum window size \"\n                f\"needed by the forecaster. This is because  \"\n                f\"the offset ({self.offset}) is larger than the available \"\n                f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `y`.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n            )\n\n    self.is_fitted = True\n    self.series_name_in_ = y.name if y.name is not None else 'y'\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = y_index[[0, -1]]\n    self.index_type_ = type(y_index)\n    self.index_freq_ = (\n        y_index.freqstr if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n    )\n\n    # NOTE: This is done to save time during fit in functions such as backtesting()\n    if self._probabilistic_mode is not False:\n        self._binning_in_sample_residuals(\n            y                         = y,\n            store_in_sample_residuals = store_in_sample_residuals,\n            random_state              = random_state\n        )\n\n    # The last time window of training data is stored so that equivalent\n    # dates are available when calling the `predict` method.\n    # Store the whole series to avoid errors when the offset is larger \n    # than the data available.\n    self.last_window_ = y.copy()\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate._binning_in_sample_residuals","title":"_binning_in_sample_residuals","text":"<pre><code>_binning_in_sample_residuals(\n    y, store_in_sample_residuals=False, random_state=123\n)\n</code></pre> <p>Bin residuals according to the predicted value each residual is associated with. First a <code>skforecast.preprocessing.QuantileBinner</code> object is fitted to the predicted values. Then, residuals are binned according to the predicted value each residual is associated with. Residuals are stored in the forecaster object as <code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code>.</p> <p>The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code>. The total number of residuals stored is <code>10_000</code>. New in version 0.17.0</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_equivalent_date.py</code> <pre><code>def _binning_in_sample_residuals(\n    self,\n    y: pd.Series,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Bin residuals according to the predicted value each residual is\n    associated with. First a `skforecast.preprocessing.QuantileBinner` object\n    is fitted to the predicted values. Then, residuals are binned according\n    to the predicted value each residual is associated with. Residuals are\n    stored in the forecaster object as `in_sample_residuals_` and\n    `in_sample_residuals_by_bin_`.\n\n    The number of residuals stored per bin is limited to \n    `10_000 // self.binner.n_bins_`. The total number of residuals stored is\n    `10_000`.\n    **New in version 0.17.0**\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n        y_preds = []\n        for n_off in range(1, self.n_offsets + 1):\n            idx = y.index - self.offset * n_off\n            mask = idx &gt;= y.index[0]\n            y_pred = y.loc[idx[mask]]\n            y_pred.index = y.index[-mask.sum():]\n            y_preds.append(y_pred)\n\n        y_preds = pd.concat(y_preds, axis=1).to_numpy()\n        y_true = y.to_numpy()[-len(y_preds):]\n\n    else:\n        y_preds = [\n            y.shift(self.offset * n_off)[self.window_size:]\n            for n_off in range(1, self.n_offsets + 1)\n        ]\n        y_preds = np.column_stack(y_preds)\n        y_true = y.to_numpy()[self.window_size:]\n\n    y_pred = np.apply_along_axis(\n                 self.agg_func,\n                 axis = 1,\n                 arr  = y_preds\n             )\n\n    residuals = y_true - y_pred\n\n    if self._probabilistic_mode == \"binned\":\n        data = pd.DataFrame(\n            {'prediction': y_pred, 'residuals': residuals}\n        ).dropna()\n        y_pred = data['prediction'].to_numpy()\n        residuals = data['residuals'].to_numpy()\n\n        self.binner.fit(y_pred)\n        self.binner_intervals_ = self.binner.intervals_\n\n    if store_in_sample_residuals:\n        rng = np.random.default_rng(seed=random_state)\n        if self._probabilistic_mode == \"binned\":\n            data['bin'] = self.binner.transform(y_pred).astype(int)\n            self.in_sample_residuals_by_bin_ = (\n                data.groupby('bin')['residuals'].apply(np.array).to_dict()\n            )\n\n            max_sample = 10_000 // self.binner.n_bins_\n            for k, v in self.in_sample_residuals_by_bin_.items():\n                if len(v) &gt; max_sample:\n                    sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                    self.in_sample_residuals_by_bin_[k] = sample\n\n            for k in self.binner_intervals_.keys():\n                if k not in self.in_sample_residuals_by_bin_:\n                    self.in_sample_residuals_by_bin_[k] = np.array([])\n\n            empty_bins = [\n                k for k, v in self.in_sample_residuals_by_bin_.items() \n                if v.size == 0\n            ]\n            if empty_bins:\n                empty_bin_size = min(max_sample, len(residuals))\n                for k in empty_bins:\n                    self.in_sample_residuals_by_bin_[k] = rng.choice(\n                        a       = residuals,\n                        size    = empty_bin_size,\n                        replace = False\n                    )\n\n        if len(residuals) &gt; 10_000:\n            residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=10_000)\n            ]\n\n        self.in_sample_residuals_ = residuals\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.predict","title":"predict","text":"<pre><code>predict(\n    steps, last_window=None, check_inputs=True, exog=None\n)\n</code></pre> <p>Predict n steps ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Past values needed to select the last equivalent dates according to  the offset. If <code>last_window = None</code>, the values stored in  <code>self.last_window_</code> are used and the predictions start immediately  after the training data.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast\\recursive\\_forecaster_equivalent_date.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    check_inputs: bool = True,\n    exog: Any = None\n) -&gt; pd.Series:\n    \"\"\"\n    Predict n steps ahead.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    last_window : pandas Series, default None\n        Past values needed to select the last equivalent dates according to \n        the offset. If `last_window = None`, the values stored in \n        `self.last_window_` are used and the predictions start immediately \n        after the training data.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name = type(self).__name__,\n            steps           = steps,\n            is_fitted       = self.is_fitted,\n            exog_in_        = False,\n            index_type_     = self.index_type_,\n            index_freq_     = self.index_freq_,\n            window_size     = self.window_size,\n            last_window     = last_window\n        )\n\n    prediction_index = expand_index(index=last_window.index, steps=steps)\n\n    if isinstance(self.offset, int):\n\n        last_window_values = last_window.to_numpy(copy=True).ravel()\n        equivalent_indexes = np.tile(\n                                 np.arange(-self.offset, 0),\n                                 int(np.ceil(steps / self.offset))\n                             )\n        equivalent_indexes = equivalent_indexes[:steps]\n\n        if self.n_offsets == 1:\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = equivalent_values.ravel()\n\n        if self.n_offsets &gt; 1:\n            equivalent_indexes = [\n                equivalent_indexes - n * self.offset \n                for n in np.arange(self.n_offsets)\n            ]\n            equivalent_indexes = np.vstack(equivalent_indexes)\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = np.apply_along_axis(\n                              self.agg_func,\n                              axis = 0,\n                              arr  = equivalent_values\n                          )\n\n        predictions = pd.Series(\n                          data  = predictions,\n                          index = prediction_index,\n                          name  = 'pred'\n                      )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n        last_window = last_window.copy()\n        max_allowed_date = last_window.index[-1]\n\n        # For every date in prediction_index, calculate the n offsets\n        offset_dates = []\n        for date in prediction_index:\n            selected_offsets = []\n            while len(selected_offsets) &lt; self.n_offsets:\n                offset_date = date - self.offset\n                if offset_date &lt;= max_allowed_date:\n                    selected_offsets.append(offset_date)\n                date = offset_date\n            offset_dates.append(selected_offsets)\n\n        offset_dates = np.array(offset_dates)\n\n        # Select the values of the time series corresponding to the each\n        # offset date. If the offset date is not in the time series, the\n        # value is set to NaN.\n        equivalent_values = (\n            last_window.\n            reindex(offset_dates.ravel())\n            .to_numpy()\n            .reshape(-1, self.n_offsets)\n        )\n        equivalent_values = pd.DataFrame(\n                                data    = equivalent_values,\n                                index   = prediction_index,\n                                columns = [f'offset_{i}' for i in range(self.n_offsets)]\n                            )\n\n        # Error if all values are missing\n        if equivalent_values.isnull().all().all():\n            raise ValueError(\n                f\"All equivalent values are missing. This is caused by using \"\n                f\"an offset ({self.offset}) larger than the available data. \"\n                f\"Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `last_window`. In backtesting, this error may be \"\n                f\"caused by using an `initial_train_size` too small.\"\n            )\n\n        # Warning if equivalent values are missing\n        incomplete_offsets = equivalent_values.isnull().any(axis=1)\n        incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n        if not incomplete_offsets.empty:\n            warnings.warn(\n                f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                f\"To avoid this, increase the `last_window` size or decrease \"\n                f\"the number of `n_offsets`. The current configuration requires \" \n                f\"a total offset of {self.offset * self.n_offsets}.\",\n                MissingValuesWarning\n            )\n\n        aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n        predictions = aggregate_values.rename('pred')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.predict_interval","title":"predict_interval","text":"<pre><code>predict_interval(\n    steps,\n    last_window=None,\n    method=\"conformal\",\n    interval=[5, 95],\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=None,\n    exog=None,\n    n_boot=None,\n)\n</code></pre> <p>Predict n steps ahead and estimate prediction intervals using conformal  prediction method. Refer to the References section for additional  details on this method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Past values needed to select the last equivalent dates according to  the offset. If <code>last_window = None</code>, the values stored in  <code>self.last_window_</code> are used and the predictions start immediately  after the training data.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'conformal': Employs the conformal prediction split method for  interval estimation [1]_.</li> </ul> <code>'conformal'</code> <code>interval</code> <code>(float, list, tuple)</code> <p>Confidence level of the prediction interval. Interpretation depends  on the method used:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0  and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code>  percentiles.</li> <li>If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which  must be between 0 and 100 inclusive. For example, interval  of 95% should be as <code>interval = [2.5, 97.5]</code>.</li> <li>When using <code>method='conformal'</code>, the interval must be a float or  a list/tuple defining a symmetric interval.</li> </ul> <code>[5, 95]</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>n_boot</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\recursive\\_forecaster_equivalent_date.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    method: str = 'conformal',\n    interval: float | list[float] | tuple[float] = [5, 95],\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: Any = None,\n    exog: Any = None,\n    n_boot: Any = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using conformal \n    prediction method. Refer to the References section for additional \n    details on this method.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict.\n    last_window : pandas Series, default None\n        Past values needed to select the last equivalent dates according to \n        the offset. If `last_window = None`, the values stored in \n        `self.last_window_` are used and the predictions start immediately \n        after the training data.\n    method : str, default 'conformal'\n        Technique used to estimate prediction intervals. Available options:\n\n        - 'conformal': Employs the conformal prediction split method for \n        interval estimation [1]_.\n    interval : float, list, tuple, default [5, 95]\n        Confidence level of the prediction interval. Interpretation depends \n        on the method used:\n\n        - If `float`, represents the nominal (expected) coverage (between 0 \n        and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]` \n        percentiles.\n        - If `list` or `tuple`, defines the exact percentiles to compute, which \n        must be between 0 and 100 inclusive. For example, interval \n        of 95% should be as `interval = [2.5, 97.5]`.\n        - When using `method='conformal'`, the interval must be a float or \n        a list/tuple defining a symmetric interval.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : Ignored\n        Not used, present here for API consistency by convention.\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n    n_boot : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    References\n    ----------        \n    .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    if method != 'conformal':\n        raise ValueError(\n            f\"Method '{method}' is not supported. Only 'conformal' is available.\"\n        )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    check_predict_input(\n        forecaster_name = type(self).__name__,\n        steps           = steps,\n        is_fitted       = self.is_fitted,\n        exog_in_        = False,\n        index_type_     = self.index_type_,\n        index_freq_     = self.index_freq_,\n        window_size     = self.window_size,\n        last_window     = last_window\n    )\n\n    check_residuals_input(\n        forecaster_name              = type(self).__name__,\n        use_in_sample_residuals      = use_in_sample_residuals,\n        in_sample_residuals_         = self.in_sample_residuals_,\n        out_sample_residuals_        = self.out_sample_residuals_,\n        use_binned_residuals         = use_binned_residuals,\n        in_sample_residuals_by_bin_  = self.in_sample_residuals_by_bin_,\n        out_sample_residuals_by_bin_ = self.out_sample_residuals_by_bin_\n    )\n\n    if isinstance(interval, (list, tuple)):\n        check_interval(interval=interval, ensure_symmetric_intervals=True)\n        nominal_coverage = (interval[1] - interval[0]) / 100\n    else:\n        check_interval(alpha=interval, alpha_literal='interval')\n        nominal_coverage = interval\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    prediction_index = expand_index(index=last_window.index, steps=steps)\n\n    if isinstance(self.offset, int):\n\n        last_window_values = last_window.to_numpy(copy=True).ravel()\n        equivalent_indexes = np.tile(\n                                 np.arange(-self.offset, 0),\n                                 int(np.ceil(steps / self.offset))\n                             )\n        equivalent_indexes = equivalent_indexes[:steps]\n\n        if self.n_offsets == 1:\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = equivalent_values.ravel()\n\n        if self.n_offsets &gt; 1:\n            equivalent_indexes = [\n                equivalent_indexes - n * self.offset \n                for n in np.arange(self.n_offsets)\n            ]\n            equivalent_indexes = np.vstack(equivalent_indexes)\n            equivalent_values = last_window_values[equivalent_indexes]\n            predictions = np.apply_along_axis(\n                              self.agg_func,\n                              axis = 0,\n                              arr  = equivalent_values\n                          )\n\n    if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n        last_window = last_window.copy()\n        max_allowed_date = last_window.index[-1]\n\n        # For every date in prediction_index, calculate the n offsets\n        offset_dates = []\n        for date in prediction_index:\n            selected_offsets = []\n            while len(selected_offsets) &lt; self.n_offsets:\n                offset_date = date - self.offset\n                if offset_date &lt;= max_allowed_date:\n                    selected_offsets.append(offset_date)\n                date = offset_date\n            offset_dates.append(selected_offsets)\n\n        offset_dates = np.array(offset_dates)\n\n        # Select the values of the time series corresponding to the each\n        # offset date. If the offset date is not in the time series, the\n        # value is set to NaN.\n        equivalent_values = (\n            last_window.\n            reindex(offset_dates.ravel())\n            .to_numpy()\n            .reshape(-1, self.n_offsets)\n        )\n        equivalent_values = pd.DataFrame(\n                                data    = equivalent_values,\n                                index   = prediction_index,\n                                columns = [f'offset_{i}' for i in range(self.n_offsets)]\n                            )\n\n        # Error if all values are missing\n        if equivalent_values.isnull().all().all():\n            raise ValueError(\n                f\"All equivalent values are missing. This is caused by using \"\n                f\"an offset ({self.offset}) larger than the available data. \"\n                f\"Try to decrease the size of the offset ({self.offset}), \"\n                f\"the number of `n_offsets` ({self.n_offsets}) or increase the \"\n                f\"size of `last_window`. In backtesting, this error may be \"\n                f\"caused by using an `initial_train_size` too small.\"\n            )\n\n        # Warning if equivalent values are missing\n        incomplete_offsets = equivalent_values.isnull().any(axis=1)\n        incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n        if not incomplete_offsets.empty:\n            warnings.warn(\n                f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                f\"are calculated with less than {self.n_offsets} `n_offsets`. \"\n                f\"To avoid this, increase the `last_window` size or decrease \"\n                f\"the number of `n_offsets`. The current configuration requires \" \n                f\"a total offset of {self.offset * self.n_offsets}.\",\n                MissingValuesWarning\n            )\n\n        aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n        predictions = aggregate_values.to_numpy()\n\n    if use_binned_residuals:\n        correction_factor_by_bin = {\n            k: np.quantile(np.abs(v), nominal_coverage)\n            for k, v in residuals_by_bin.items()\n        }\n        replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n        predictions_bin = self.binner.transform(predictions)\n        correction_factor = replace_func(predictions_bin)\n    else:\n        correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n    lower_bound = predictions - correction_factor\n    upper_bound = predictions + correction_factor\n    predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n    predictions = pd.DataFrame(\n                      data    = predictions,\n                      index   = prediction_index,\n                      columns = [\"pred\", \"lower_bound\", \"upper_bound\"]\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.set_in_sample_residuals","title":"set_in_sample_residuals","text":"<pre><code>set_in_sample_residuals(y, random_state=123, exog=None)\n</code></pre> <p>Set in-sample residuals in case they were not calculated during the training process. </p> <p>In-sample residuals are calculated as the difference between the true  values and the predictions made by the forecaster using the training  data. The following internal attributes are updated:</p> <ul> <li><code>in_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>binner_intervals_</code>: intervals used to bin the residuals are calculated using the quantiles of the predicted values.</li> <li><code>in_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the intervals of the predicted values and the values are the residuals associated with that range. </li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>in_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>exog</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_equivalent_date.py</code> <pre><code>def set_in_sample_residuals(\n    self,\n    y: pd.Series,\n    random_state: int = 123,\n    exog: Any = None\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process. \n\n    In-sample residuals are calculated as the difference between the true \n    values and the predictions made by the forecaster using the training \n    data. The following internal attributes are updated:\n\n    + `in_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `binner_intervals_`: intervals used to bin the residuals are calculated\n    using the quantiles of the predicted values.\n    + `in_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the intervals of the predicted values and the values are\n    the residuals associated with that range. \n\n    A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n    exog : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    check_y(y=y)\n    y_index_range = check_extract_values_and_index(\n        data=y, data_label='`y`', return_values=False\n    )[1][[0, -1]]\n    if not y_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `y` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {y_index_range}\"\n        )\n\n    self._binning_in_sample_residuals(\n        y                         = y,\n        store_in_sample_residuals = True,\n        random_state              = random_state\n    )\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.set_out_sample_residuals","title":"set_out_sample_residuals","text":"<pre><code>set_out_sample_residuals(\n    y_true, y_pred, append=False, random_state=123\n)\n</code></pre> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. Two internal attributes are updated:</p> <ul> <li><code>out_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>out_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the  intervals of the predicted values and the values are the residuals associated with that range. If a bin binning is empty, it is filled with a random sample of residuals from other bins. This is done to ensure that all bins have at least one residual and can be used in the prediction process.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>out_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>numpy ndarray, pandas Series</code> <p>True values of the time series from which the residuals have been calculated.</p> required <code>y_pred</code> <code>numpy ndarray, pandas Series</code> <p>Predicted values of the time series.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the forecaster. If after appending the new residuals, the limit of <code>10_000 // self.binner.n_bins_</code> values per bin is reached, a random sample of residuals is stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_equivalent_date.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    append: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. Two internal attributes are updated:\n\n    + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `out_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the  intervals of the predicted values and the values are\n    the residuals associated with that range. If a bin binning is empty, it\n    is filled with a random sample of residuals from other bins. This is done\n    to ensure that all bins have at least one residual and can be used in the\n    prediction process.\n\n    A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y_true : numpy ndarray, pandas Series\n        True values of the time series from which the residuals have been\n        calculated.\n    y_pred : numpy ndarray, pandas Series\n        Predicted values of the time series.\n    append : bool, default False\n        If `True`, new residuals are added to the once already stored in the\n        forecaster. If after appending the new residuals, the limit of\n        `10_000 // self.binner.n_bins_` values per bin is reached, a random\n        sample of residuals is stored.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if len(y_true) != len(y_pred):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same length. \"\n            f\"Got {len(y_true)} and {len(y_pred)}.\"\n        )\n\n    if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n        if not y_true.index.equals(y_pred.index):\n            raise ValueError(\n                \"`y_true` and `y_pred` must have the same index.\"\n            )\n\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.to_numpy()\n\n    data = pd.DataFrame(\n        {'prediction': y_pred, 'residuals': y_true - y_pred}\n    ).dropna()\n    y_pred = data['prediction'].to_numpy()\n    residuals = data['residuals'].to_numpy()\n\n    data['bin'] = self.binner.transform(y_pred).astype(int)\n    residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n    out_sample_residuals = (\n        np.array([]) \n        if self.out_sample_residuals_ is None\n        else self.out_sample_residuals_\n    )\n    out_sample_residuals_by_bin = (\n        {} \n        if self.out_sample_residuals_by_bin_ is None\n        else self.out_sample_residuals_by_bin_\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    max_samples = 10_000 // self.binner.n_bins_\n    rng = np.random.default_rng(seed=random_state)\n    for k, v in out_sample_residuals_by_bin.items():\n        if len(v) &gt; max_samples:\n            sample = rng.choice(a=v, size=max_samples, replace=False)\n            out_sample_residuals_by_bin[k] = sample\n\n    bin_keys = (\n        []\n        if self.binner_intervals_ is None\n        else self.binner_intervals_.keys()\n    )\n    for k in bin_keys:\n        if k not in out_sample_residuals_by_bin:\n            out_sample_residuals_by_bin[k] = np.array([])\n\n    empty_bins = [\n        k for k, v in out_sample_residuals_by_bin.items() \n        if v.size == 0\n    ]\n    if empty_bins:\n        warnings.warn(\n            f\"The following bins have no out of sample residuals: {empty_bins}. \"\n            f\"No predicted values fall in the interval \"\n            f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n            f\"Empty bins will be filled with a random sample of residuals.\",\n            ResidualsUsageWarning\n        )\n        empty_bin_size = min(max_samples, len(out_sample_residuals))\n        for k in empty_bins:\n            out_sample_residuals_by_bin[k] = rng.choice(\n                a       = out_sample_residuals,\n                size    = empty_bin_size,\n                replace = False\n            )\n\n    if len(out_sample_residuals) &gt; 10_000:\n        out_sample_residuals = rng.choice(\n            a       = out_sample_residuals, \n            size    = 10_000, \n            replace = False\n        )\n\n    self.out_sample_residuals_ = out_sample_residuals\n    self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/ForecasterEquivalentDate.html#skforecast.recursive._forecaster_equivalent_date.ForecasterEquivalentDate.summary","title":"summary","text":"<pre><code>summary()\n</code></pre> <p>Show forecaster information.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_equivalent_date.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"\n    Show forecaster information.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    print(self)\n</code></pre>"},{"location":"api/ForecasterRecursive.html","title":"<code>ForecasterRecursive</code>","text":""},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive","title":"skforecast.recursive._forecaster_recursive.ForecasterRecursive","text":"<pre><code>ForecasterRecursive(\n    regressor,\n    lags=None,\n    window_features=None,\n    transformer_y=None,\n    transformer_exog=None,\n    weight_func=None,\n    differentiation=None,\n    fit_kwargs=None,\n    binner_kwargs=None,\n    forecaster_id=None,\n)\n</code></pre> <p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>None</code> <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>None</code> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>None</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the forecaster. If <code>None</code>, no differencing is applied. The order of differentiation is the number of times the differencing operation is applied to a time series. Differencing involves computing the differences between consecutive data points in the series. Before returning a prediction, the differencing operation is reversed.</p> <code>None</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>None</code> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code> used to discretize  the residuals into k bins according to the predicted values associated  with each residual. Available arguments are: <code>n_bins</code>, <code>method</code>, <code>subsample</code>, <code>random_state</code> and <code>dtype</code>. Argument <code>method</code> is passed internally to the function <code>numpy.percentile</code>. New in version 0.14.0</p> <code>None</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>lags_names</code> <code>list</code> <p>Names of the lags used as predictors.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_features</code> <code>list</code> <p>Class or list of classes used to create window features.</p> <code>window_features_names</code> <code>list</code> <p>Names of the window features to be included in the <code>X_train</code> matrix.</p> <code>window_features_class_names</code> <code>list</code> <p>Names of the classes used to create the window features.</p> <code>max_size_window_features</code> <code>int</code> <p>Maximum window size required by the window features.</p> <code>window_size</code> <code>int</code> <p>The window size needed to create the predictors. It is calculated as the  maximum value between <code>max_lag</code> and <code>max_size_window_features</code>. If  differentiation is used, <code>window_size</code> is increased by n units equal to  the order of differentiation so that predictors can be generated correctly.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>Callable</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates. Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its <code>fit</code> method. The resulting <code>sample_weight</code> cannot have negative values.</p> <code>source_code_weight_func</code> <code>str</code> <p>Source code of the custom function used to create weights.</p> <code>differentiation</code> <code>int</code> <p>Order of differencing applied to the time series before training the  forecaster.</p> <code>differentiation_max</code> <code>int</code> <p>Maximum order of differentiation. For this Forecaster, it is equal to the value of the <code>differentiation</code> parameter.</p> <code>differentiator</code> <code>TimeSeriesDifferentiator</code> <p>Skforecast object used to differentiate the time series.</p> <code>last_window_</code> <code>pandas DataFrame</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the values needed to predict the next step immediately after the training data. These values are stored in the original scale of the time series before undergoing any transformations or differentiation. When <code>differentiation</code> parameter is specified, the dimensions of the <code>last_window_</code> are expanded as many values as the order of differentiation. For example, if <code>lags</code> = 7 and <code>differentiation</code> = 1, <code>last_window_</code> will have 8 values.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_name_in_</code> <code>str</code> <p>Names of the series provided by the user during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous data (pandas Series or DataFrame) used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting training data. Only stored up to 10_000 values. If <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after differentiation.</p> <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>. If  <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed  scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after  differentiation. </p> <code>out_sample_residuals_</code> <code>numpy ndarray</code> <p>Residuals of the model when predicting non-training data. Only stored up to 10_000 values. Use <code>set_out_sample_residuals()</code> method to set values. If  <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed  scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after  differentiation.</p> <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> in the form <code>{bin: residuals}</code>. If  <code>transformer_y</code> is not <code>None</code>, residuals are stored in the transformed  scale. If <code>differentiation</code> is not <code>None</code>, residuals are stored after  differentiation. </p> <code>binner</code> <code>QuantileBinner</code> <p><code>QuantileBinner</code> used to discretize residuals into k bins according  to the predicted values associated with each residual.</p> <code>binner_intervals_</code> <code>dict</code> <p>Intervals used to discretize residuals into k bins according to the predicted values associated with each residual.</p> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code>.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>_probabilistic_mode</code> <code>(str, bool)</code> <p>Private attribute used to indicate whether the forecaster should perform  some calculations during backtesting.</p> <p>Methods:</p> Name Description <code>create_train_X_y</code> <p>Create training matrices from univariate time series and exogenous</p> <code>create_sample_weights</code> <p>Create weights for each observation according to the forecaster's attribute</p> <code>fit</code> <p>Training Forecaster.</p> <code>create_predict_X</code> <p>Create the predictors needed to predict <code>steps</code> ahead. As it is a recursive</p> <code>predict</code> <p>Predict n steps ahead. It is an recursive process in which, each prediction,</p> <code>predict_bootstrapping</code> <p>Generate multiple forecasting predictions using a bootstrapping process.</p> <code>predict_interval</code> <p>Predict n steps ahead and estimate prediction intervals using either </p> <code>predict_quantiles</code> <p>Calculate the specified quantiles for each step. After generating </p> <code>predict_dist</code> <p>Fit a given probability distribution for each step. After generating </p> <code>set_params</code> <p>Set new values to the parameters of the scikit-learn model stored in the</p> <code>set_fit_kwargs</code> <p>Set new values for the additional keyword arguments passed to the <code>fit</code> </p> <code>set_lags</code> <p>Set new value to the attribute <code>lags</code>. Attributes <code>lags_names</code>, </p> <code>set_window_features</code> <p>Set new value to the attribute <code>window_features</code>. Attributes </p> <code>set_in_sample_residuals</code> <p>Set in-sample residuals in case they were not calculated during the</p> <code>set_out_sample_residuals</code> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample</p> <code>get_feature_importances</code> <p>Return feature importances of the regressor stored in the forecaster.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    lags: int | list[int] | np.ndarray[int] | range[int] | None = None,\n    window_features: object | list[object] | None = None,\n    transformer_y: object | None = None,\n    transformer_exog: object | None = None,\n    weight_func: Callable | None = None,\n    differentiation: int | None = None,\n    fit_kwargs: dict[str, object] | None = None,\n    binner_kwargs: dict[str, object] | None = None,\n    forecaster_id: str | int | None = None\n) -&gt; None:\n\n    self.regressor                          = copy(regressor)\n    self.transformer_y                      = transformer_y\n    self.transformer_exog                   = transformer_exog\n    self.weight_func                        = weight_func\n    self.source_code_weight_func            = None\n    self.differentiation                    = differentiation\n    self.differentiation_max                = None\n    self.differentiator                     = None\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.series_name_in_                    = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.exog_dtypes_out_                   = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.out_sample_residuals_              = None\n    self.in_sample_residuals_by_bin_        = None\n    self.out_sample_residuals_by_bin_       = None\n    self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted                          = False\n    self.fit_date                           = None\n    self.skforecast_version                 = skforecast.__version__\n    self.python_version                     = sys.version.split(\" \")[0]\n    self.forecaster_id                      = forecaster_id\n    self._probabilistic_mode                = \"binned\"\n\n    self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_features, self.window_features_names, self.max_size_window_features = (\n        initialize_window_features(window_features)\n    )\n    if self.window_features is None and self.lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ]\n\n    self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n        forecaster_name = type(self).__name__, \n        regressor       = regressor, \n        weight_func     = weight_func, \n        series_weights  = None\n    )\n\n    if differentiation is not None:\n        if not isinstance(differentiation, int) or differentiation &lt; 1:\n            raise ValueError(\n                f\"Argument `differentiation` must be an integer equal to or \"\n                f\"greater than 1. Got {differentiation}.\"\n            )\n        self.differentiation = differentiation\n        self.differentiation_max = differentiation\n        self.window_size += differentiation\n        self.differentiator = TimeSeriesDifferentiator(\n            order=differentiation, window_size=self.window_size\n        )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.binner_kwargs = binner_kwargs\n    if binner_kwargs is None:\n        self.binner_kwargs = {\n            'n_bins': 10, 'method': 'linear', 'subsample': 200000,\n            'random_state': 789654, 'dtype': np.float64\n        }\n    self.binner = QuantileBinner(**self.binner_kwargs)\n    self.binner_intervals_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.regressor","title":"regressor  <code>instance-attribute</code>","text":"<pre><code>regressor = copy(regressor)\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.transformer_y","title":"transformer_y  <code>instance-attribute</code>","text":"<pre><code>transformer_y = transformer_y\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.transformer_exog","title":"transformer_exog  <code>instance-attribute</code>","text":"<pre><code>transformer_exog = transformer_exog\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.weight_func","title":"weight_func  <code>instance-attribute</code>","text":"<pre><code>weight_func = weight_func\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.source_code_weight_func","title":"source_code_weight_func  <code>instance-attribute</code>","text":"<pre><code>source_code_weight_func = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.differentiation","title":"differentiation  <code>instance-attribute</code>","text":"<pre><code>differentiation = differentiation\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.differentiation_max","title":"differentiation_max  <code>instance-attribute</code>","text":"<pre><code>differentiation_max = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.differentiator","title":"differentiator  <code>instance-attribute</code>","text":"<pre><code>differentiator = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.last_window_","title":"last_window_  <code>instance-attribute</code>","text":"<pre><code>last_window_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.index_type_","title":"index_type_  <code>instance-attribute</code>","text":"<pre><code>index_type_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.index_freq_","title":"index_freq_  <code>instance-attribute</code>","text":"<pre><code>index_freq_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.training_range_","title":"training_range_  <code>instance-attribute</code>","text":"<pre><code>training_range_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.series_name_in_","title":"series_name_in_  <code>instance-attribute</code>","text":"<pre><code>series_name_in_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.exog_in_","title":"exog_in_  <code>instance-attribute</code>","text":"<pre><code>exog_in_ = False\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.exog_names_in_","title":"exog_names_in_  <code>instance-attribute</code>","text":"<pre><code>exog_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.exog_type_in_","title":"exog_type_in_  <code>instance-attribute</code>","text":"<pre><code>exog_type_in_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.exog_dtypes_in_","title":"exog_dtypes_in_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_in_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.exog_dtypes_out_","title":"exog_dtypes_out_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_out_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.X_train_window_features_names_out_","title":"X_train_window_features_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_window_features_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.X_train_exog_names_out_","title":"X_train_exog_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_exog_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.X_train_features_names_out_","title":"X_train_features_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_features_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.in_sample_residuals_","title":"in_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.out_sample_residuals_","title":"out_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.in_sample_residuals_by_bin_","title":"in_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.out_sample_residuals_by_bin_","title":"out_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.creation_date","title":"creation_date  <code>instance-attribute</code>","text":"<pre><code>creation_date = strftime('%Y-%m-%d %H:%M:%S')\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.is_fitted","title":"is_fitted  <code>instance-attribute</code>","text":"<pre><code>is_fitted = False\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.fit_date","title":"fit_date  <code>instance-attribute</code>","text":"<pre><code>fit_date = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.skforecast_version","title":"skforecast_version  <code>instance-attribute</code>","text":"<pre><code>skforecast_version = __version__\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.python_version","title":"python_version  <code>instance-attribute</code>","text":"<pre><code>python_version = split(' ')[0]\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.forecaster_id","title":"forecaster_id  <code>instance-attribute</code>","text":"<pre><code>forecaster_id = forecaster_id\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._probabilistic_mode","title":"_probabilistic_mode  <code>instance-attribute</code>","text":"<pre><code>_probabilistic_mode = 'binned'\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = max(\n    [\n        ws\n        for ws in [max_lag, max_size_window_features]\n        if ws is not None\n    ]\n)\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.window_features_class_names","title":"window_features_class_names  <code>instance-attribute</code>","text":"<pre><code>window_features_class_names = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.fit_kwargs","title":"fit_kwargs  <code>instance-attribute</code>","text":"<pre><code>fit_kwargs = check_select_fit_kwargs(\n    regressor=regressor, fit_kwargs=fit_kwargs\n)\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.binner_kwargs","title":"binner_kwargs  <code>instance-attribute</code>","text":"<pre><code>binner_kwargs = binner_kwargs\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.binner","title":"binner  <code>instance-attribute</code>","text":"<pre><code>binner = QuantileBinner(**(binner_kwargs))\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.binner_intervals_","title":"binner_intervals_  <code>instance-attribute</code>","text":"<pre><code>binner_intervals_ = None\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def _repr_html_(self):\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    (\n        params,\n        _,\n        _,\n        exog_names_in_,\n        _,\n    ) = self._preprocess_repr(\n            regressor      = self.regressor,\n            exog_names_in_ = self.exog_names_in_\n        )\n\n    style, unique_id = get_style_repr_html(self.is_fitted)\n\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Regressor:&lt;/strong&gt; {type(self.regressor).__name__}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Series name:&lt;/strong&gt; {self.series_name_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skforecast version:&lt;/strong&gt; {self.skforecast_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n            &lt;ul&gt;\n                {exog_names_in_}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Data Transformations&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Training Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Regressor Parameters&lt;/summary&gt;\n            &lt;ul&gt;\n                {params}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n            &lt;ul&gt;\n                {self.fit_kwargs}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return style + content\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._create_lags","title":"_create_lags","text":"<pre><code>_create_lags(y, X_as_pandas=False, train_index=None)\n</code></pre> <p>Create the lagged values and their target variable from a time series.</p> <p>Note that the returned matrix <code>X_data</code> contains the lag 1 in the first  column, the lag 2 in the in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>Training time series values.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_data</code> is a pandas DataFrame.</p> <code>False</code> <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_data</code> when <code>X_as_pandas</code> is <code>True</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray, pandas DataFrame, None</code> <p>Lagged values (predictors).</p> <code>y_data</code> <code>numpy ndarray</code> <p>Values of the time series related to each row of <code>X_data</code>.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def _create_lags(\n    self,\n    y: np.ndarray,\n    X_as_pandas: bool = False,\n    train_index: pd.Index | None = None\n) -&gt; tuple[np.ndarray | pd.DataFrame | None, np.ndarray]:\n    \"\"\"\n    Create the lagged values and their target variable from a time series.\n\n    Note that the returned matrix `X_data` contains the lag 1 in the first \n    column, the lag 2 in the in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        Training time series values.\n    X_as_pandas : bool, default False\n        If `True`, the returned matrix `X_data` is a pandas DataFrame.\n    train_index : pandas Index, default None\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_data` when `X_as_pandas` is `True`.\n\n    Returns\n    -------\n    X_data : numpy ndarray, pandas DataFrame, None\n        Lagged values (predictors).\n    y_data : numpy ndarray\n        Values of the time series related to each row of `X_data`.\n\n    \"\"\"\n\n    X_data = None\n    if self.lags is not None:\n        n_rows = len(y) - self.window_size\n        X_data = np.full(\n            shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n        )\n        for i, lag in enumerate(self.lags):\n            X_data[:, i] = y[self.window_size - lag: -lag]\n\n        if X_as_pandas:\n            X_data = pd.DataFrame(\n                         data    = X_data,\n                         columns = self.lags_names,\n                         index   = train_index\n                     )\n\n    y_data = y[self.window_size:]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._create_window_features","title":"_create_window_features","text":"<pre><code>_create_window_features(y, train_index, X_as_pandas=False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_train_window_features</code> when <code>X_as_pandas</code> is <code>True</code>.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_train_window_features</code> is a  pandas DataFrame.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train_window_features</code> <code>list</code> <p>List of numpy ndarrays or pandas DataFrames with the window features.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def _create_window_features(\n    self, \n    y: pd.Series,\n    train_index: pd.Index,\n    X_as_pandas: bool = False,\n) -&gt; tuple[list[np.ndarray | pd.DataFrame], list[str]]:\n    \"\"\"\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    train_index : pandas Index\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_train_window_features` when `X_as_pandas` is `True`.\n    X_as_pandas : bool, default False\n        If `True`, the returned matrix `X_train_window_features` is a \n        pandas DataFrame.\n\n    Returns\n    -------\n    X_train_window_features : list\n        List of numpy ndarrays or pandas DataFrames with the window features.\n    X_train_window_features_names_out_ : list\n        Names of the window features.\n\n    \"\"\"\n\n    len_train_index = len(train_index)\n    X_train_window_features = []\n    X_train_window_features_names_out_ = []\n    for wf in self.window_features:\n        X_train_wf = wf.transform_batch(y)\n        if not isinstance(X_train_wf, pd.DataFrame):\n            raise TypeError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a pandas DataFrame.\"\n            )\n        X_train_wf = X_train_wf.iloc[-len_train_index:]\n        if not len(X_train_wf) == len_train_index:\n            raise ValueError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a DataFrame with the same number of rows as \"\n                f\"the input time series - `window_size`: {len_train_index}.\"\n            )\n        if not (X_train_wf.index == train_index).all():\n            raise ValueError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a DataFrame with the same index as \"\n                f\"the input time series - `window_size`.\"\n            )\n\n        X_train_window_features_names_out_.extend(X_train_wf.columns)\n        if not X_as_pandas:\n            X_train_wf = X_train_wf.to_numpy()     \n        X_train_window_features.append(X_train_wf)\n\n    return X_train_window_features, X_train_window_features_names_out_\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._create_train_X_y","title":"_create_train_X_y","text":"<pre><code>_create_train_X_y(y, exog=None)\n</code></pre> <p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values of the time series related to each row of <code>X_train</code>.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of the columns of the matrix created internally for training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def _create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[\n    pd.DataFrame, \n    pd.Series, \n    list[str], \n    list[str], \n    list[str], \n    list[str], \n    dict[str, type],\n    dict[str, type]\n]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values of the time series related to each row of `X_train`.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of the columns of the matrix created internally for training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training before the transformation\n        applied by `transformer_exog`. If `transformer_exog` is not used, it\n        is equal to `exog_dtypes_out_`.\n    exog_dtypes_out_ : dict\n        Type of each exogenous variable/s used in training after the transformation\n        applied by `transformer_exog`. If `transformer_exog` is not used, it \n        is equal to `exog_dtypes_in_`.\n\n    \"\"\"\n\n    check_y(y=y)\n    y = input_to_frame(data=y, input_name='y')\n\n    if len(y) &lt;= self.window_size:\n        raise ValueError(\n            f\"Length of `y` must be greater than the maximum window size \"\n            f\"needed by the forecaster.\\n\"\n            f\"    Length `y`: {len(y)}.\\n\"\n            f\"    Max window size: {self.window_size}.\\n\"\n            f\"    Lags window size: {self.max_lag}.\\n\"\n            f\"    Window features window size: {self.max_size_window_features}.\"\n        )\n\n    fit_transformer = False if self.is_fitted else True\n    y = transform_dataframe(\n            df                = y, \n            transformer       = self.transformer_y,\n            fit               = fit_transformer,\n            inverse_transform = False,\n        )\n    y_values, y_index = check_extract_values_and_index(data=y, data_label='`y`')\n    train_index = y_index[self.window_size:]\n\n    if self.differentiation is not None:\n        if not self.is_fitted:\n            y_values = self.differentiator.fit_transform(y_values)\n        else:\n            differentiator = copy(self.differentiator)\n            y_values = differentiator.fit_transform(y_values)\n\n    exog_names_in_ = None\n    exog_dtypes_in_ = None\n    exog_dtypes_out_ = None\n    X_as_pandas = False\n    if exog is not None:\n        check_exog(exog=exog, allow_nan=True)\n        exog = input_to_frame(data=exog, input_name='exog')\n        _, exog_index = check_extract_values_and_index(\n            data=exog, data_label='`exog`', ignore_freq=True, return_values=False\n        )\n\n        len_y = len(y_values)\n        len_train_index = len(train_index)\n        len_exog = len(exog)\n        if not len_exog == len_y and not len_exog == len_train_index:\n            raise ValueError(\n                f\"Length of `exog` must be equal to the length of `y` (if index is \"\n                f\"fully aligned) or length of `y` - `window_size` (if `exog` \"\n                f\"starts after the first `window_size` values).\\n\"\n                f\"    `exog`              : ({exog_index[0]} -- {exog_index[-1]})  (n={len_exog})\\n\"\n                f\"    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n\"\n                f\"    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})\"\n            )\n\n        exog_names_in_ = exog.columns.to_list()\n        exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = fit_transformer,\n                   inverse_transform = False\n               )\n\n        check_exog_dtypes(exog, call_check_exog=True)\n        exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n        X_as_pandas = any(\n            not pd.api.types.is_numeric_dtype(dtype) or pd.api.types.is_bool_dtype(dtype) \n            for dtype in set(exog.dtypes)\n        )\n\n        if len_exog == len_y:\n            if not (exog_index == y_index).all():\n                raise ValueError(\n                    \"When `exog` has the same length as `y`, the index of \"\n                    \"`exog` must be aligned with the index of `y` \"\n                    \"to ensure the correct alignment of values.\"\n                )\n            # The first `self.window_size` positions have to be removed from \n            # exog since they are not in X_train.\n            exog = exog.iloc[self.window_size:, ]\n        else:\n            if not (exog_index == train_index).all():\n                raise ValueError(\n                    \"When `exog` doesn't contain the first `window_size` observations, \"\n                    \"the index of `exog` must be aligned with the index of `y` minus \"\n                    \"the first `window_size` observations to ensure the correct \"\n                    \"alignment of values.\"\n                )\n\n    X_train = []\n    X_train_features_names_out_ = []\n\n    X_train_lags, y_train = self._create_lags(\n        y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n    )\n    if X_train_lags is not None:\n        X_train.append(X_train_lags)\n        X_train_features_names_out_.extend(self.lags_names)\n\n    X_train_window_features_names_out_ = None\n    if self.window_features is not None:\n        n_diff = 0 if self.differentiation is None else self.differentiation\n        y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n        X_train_window_features, X_train_window_features_names_out_ = (\n            self._create_window_features(\n                y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index\n            )\n        )\n        X_train.extend(X_train_window_features)\n        X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n    X_train_exog_names_out_ = None\n    if exog is not None:\n        X_train_exog_names_out_ = exog.columns.to_list()  \n        if not X_as_pandas:\n            exog = exog.to_numpy()     \n        X_train_features_names_out_.extend(X_train_exog_names_out_)\n        X_train.append(exog)\n\n    if len(X_train) == 1:\n        X_train = X_train[0]\n    else:\n        if X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n\n    if X_as_pandas:\n        X_train.index = train_index\n    else:\n        X_train = pd.DataFrame(\n                      data    = X_train,\n                      index   = train_index,\n                      columns = X_train_features_names_out_\n                  )\n\n    y_train = pd.Series(\n                  data  = y_train,\n                  index = train_index,\n                  name  = 'y'\n              )\n\n    return (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_\n    )\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.create_train_X_y","title":"create_train_X_y","text":"<pre><code>create_train_X_y(y, exog=None)\n</code></pre> <p>Create training matrices from univariate time series and exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values of the time series related to each row of <code>X_data</code>.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def create_train_X_y(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None,\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values of the time series related to each row of `X_data`.\n\n    \"\"\"\n\n    output = self._create_train_X_y(y=y, exog=exog)\n\n    X_train = output[0]\n    y_train = output[1]\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._train_test_split_one_step_ahead","title":"_train_test_split_one_step_ahead","text":"<pre><code>_train_test_split_one_step_ahead(\n    y, initial_train_size, exog=None\n)\n</code></pre> <p>Create matrices needed to train and test the forecaster for one-step-ahead predictions.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>initial_train_size</code> <code>int</code> <p>Initial size of the training set. It is the number of observations used to train the forecaster before making the first prediction.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Predictor values used to train the model.</p> <code>y_train</code> <code>pandas Series</code> <p>Target values related to each row of <code>X_train</code>.</p> <code>X_test</code> <code>pandas DataFrame</code> <p>Predictor values used to test the model.</p> <code>y_test</code> <code>pandas Series</code> <p>Target values related to each row of <code>X_test</code>.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def _train_test_split_one_step_ahead(\n    self,\n    y: pd.Series,\n    initial_train_size: int,\n    exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create matrices needed to train and test the forecaster for one-step-ahead\n    predictions.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    initial_train_size : int\n        Initial size of the training set. It is the number of observations used\n        to train the forecaster before making the first prediction.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    \"\"\"\n\n    is_fitted = self.is_fitted\n    self.is_fitted = False\n    X_train, y_train, *_ = self._create_train_X_y(\n        y    = y.iloc[: initial_train_size],\n        exog = exog.iloc[: initial_train_size] if exog is not None else None\n    )\n\n    test_init = initial_train_size - self.window_size\n    self.is_fitted = True\n    X_test, y_test, *_ = self._create_train_X_y(\n        y    = y.iloc[test_init:],\n        exog = exog.iloc[test_init:] if exog is not None else None\n    )\n\n    self.is_fitted = is_fitted\n\n    return X_train, y_train, X_test, y_test\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.create_sample_weights","title":"create_sample_weights","text":"<pre><code>create_sample_weights(X_train)\n</code></pre> <p>Create weights for each observation according to the forecaster's attribute <code>weight_func</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <p>Returns:</p> Name Type Description <code>sample_weight</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def create_sample_weights(\n    self,\n    X_train: pd.DataFrame,\n) -&gt; np.ndarray:\n    \"\"\"\n    Create weights for each observation according to the forecaster's attribute\n    `weight_func`.\n\n    Parameters\n    ----------\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n\n    Returns\n    -------\n    sample_weight : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    sample_weight = None\n\n    if self.weight_func is not None:\n        sample_weight = self.weight_func(X_train.index)\n\n    if sample_weight is not None:\n        if np.isnan(sample_weight).any():\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have NaN values.\"\n            )\n        if np.any(sample_weight &lt; 0):\n            raise ValueError(\n                \"The resulting `sample_weight` cannot have negative values.\"\n            )\n        if np.sum(sample_weight) == 0:\n            raise ValueError(\n                \"The resulting `sample_weight` cannot be normalized because \"\n                \"the sum of the weights is zero.\"\n            )\n\n    return sample_weight\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.fit","title":"fit","text":"<pre><code>fit(\n    y,\n    exog=None,\n    store_last_window=True,\n    store_in_sample_residuals=False,\n    random_state=123,\n)\n</code></pre> <p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>store_last_window</code> <code>bool</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <code>True</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None,\n    store_last_window: bool = True,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_last_window : bool, default True\n        Whether or not to store the last window (`last_window_`) of training data.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # TODO: create a method reset_forecaster() to reset all attributes\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.series_name_in_                    = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.exog_dtypes_out_                   = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.in_sample_residuals_by_bin_        = None\n    self.binner_intervals_                  = None\n    self.is_fitted                          = False\n    self.fit_date                           = None\n\n    (\n        X_train,\n        y_train,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        X_train_features_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_\n    ) = self._create_train_X_y(y=y, exog=exog)\n    sample_weight = self.create_sample_weights(X_train=X_train)\n\n    if sample_weight is not None:\n        self.regressor.fit(\n            X             = X_train,\n            y             = y_train,\n            sample_weight = sample_weight,\n            **self.fit_kwargs\n        )\n    else:\n        self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n    self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n    self.X_train_features_names_out_ = X_train_features_names_out_\n\n    self.is_fitted = True\n    self.series_name_in_ = y.name if y.name is not None else 'y'\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = y.index[[0, -1]]\n    self.index_type_ = type(y.index)\n    if isinstance(y.index, pd.DatetimeIndex):\n        self.index_freq_ = y.index.freqstr\n    else: \n        self.index_freq_ = y.index.step\n\n    if exog is not None:\n        self.exog_in_ = True\n        self.exog_type_in_ = type(exog)\n        self.exog_names_in_ = exog_names_in_\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n    # NOTE: This is done to save time during fit in functions such as backtesting()\n    if self._probabilistic_mode is not False:\n        self._binning_in_sample_residuals(\n            y_true                    = y_train.to_numpy(),\n            y_pred                    = self.regressor.predict(X_train).ravel(),\n            store_in_sample_residuals = store_in_sample_residuals,\n            random_state              = random_state\n        )\n\n    if store_last_window:\n        self.last_window_ = (\n            y.iloc[-self.window_size:]\n            .copy()\n            .to_frame(name=y.name if y.name is not None else 'y')\n        )\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._binning_in_sample_residuals","title":"_binning_in_sample_residuals","text":"<pre><code>_binning_in_sample_residuals(\n    y_true,\n    y_pred,\n    store_in_sample_residuals=False,\n    random_state=123,\n)\n</code></pre> <p>Bin residuals according to the predicted value each residual is associated with. First a <code>skforecast.preprocessing.QuantileBinner</code> object is fitted to the predicted values. Then, residuals are binned according to the predicted value each residual is associated with. Residuals are stored in the forecaster object as <code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code>.</p> <p><code>y_true</code> and <code>y_pred</code> assumed to be differentiated and or transformed according to the attributes <code>differentiation</code> and <code>transformer_y</code>. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code>. The total number of residuals stored is <code>10_000</code>. New in version 0.14.0</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>numpy ndarray</code> <p>True values of the time series.</p> required <code>y_pred</code> <code>numpy ndarray</code> <p>Predicted values of the time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def _binning_in_sample_residuals(\n    self,\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Bin residuals according to the predicted value each residual is\n    associated with. First a `skforecast.preprocessing.QuantileBinner` object\n    is fitted to the predicted values. Then, residuals are binned according\n    to the predicted value each residual is associated with. Residuals are\n    stored in the forecaster object as `in_sample_residuals_` and\n    `in_sample_residuals_by_bin_`.\n\n    `y_true` and `y_pred` assumed to be differentiated and or transformed\n    according to the attributes `differentiation` and `transformer_y`.\n    The number of residuals stored per bin is limited to \n    `10_000 // self.binner.n_bins_`. The total number of residuals stored is\n    `10_000`.\n    **New in version 0.14.0**\n\n    Parameters\n    ----------\n    y_true : numpy ndarray\n        True values of the time series.\n    y_pred : numpy ndarray\n        Predicted values of the time series.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    residuals = y_true - y_pred\n\n    if self._probabilistic_mode == \"binned\":\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        self.binner.fit(y_pred)\n        self.binner_intervals_ = self.binner.intervals_\n\n    if store_in_sample_residuals:\n        rng = np.random.default_rng(seed=random_state)\n        if self._probabilistic_mode == \"binned\":\n            data['bin'] = self.binner.transform(y_pred).astype(int)\n            self.in_sample_residuals_by_bin_ = (\n                data.groupby('bin')['residuals'].apply(np.array).to_dict()\n            )\n\n            max_sample = 10_000 // self.binner.n_bins_\n            for k, v in self.in_sample_residuals_by_bin_.items():\n                if len(v) &gt; max_sample:\n                    sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                    self.in_sample_residuals_by_bin_[k] = sample\n\n        if len(residuals) &gt; 10_000:\n            residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=10_000)\n            ]\n\n        self.in_sample_residuals_ = residuals\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._create_predict_inputs","title":"_create_predict_inputs","text":"<pre><code>_create_predict_inputs(\n    steps,\n    last_window=None,\n    exog=None,\n    predict_probabilistic=False,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    check_inputs=True,\n)\n</code></pre> <p>Create the inputs needed for the first iteration of the prediction  process. As this is a recursive process, the last window is updated at  each iteration of the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, str, pandas Timestamp</code> <p>Number of steps to predict. </p> <ul> <li>If steps is int, number of steps to predict. </li> <li>If str or pandas Datetime, the prediction will be up to that date.</li> </ul> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>predict_probabilistic</code> <code>bool</code> <p>If <code>True</code>, the necessary checks for probabilistic predictions will be  performed.</p> <code>False</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>last_window_values</code> <code>numpy ndarray</code> <p>Series values used to create the predictors needed in the first  iteration of the prediction (t + 1).</p> <code>exog_values</code> <code>numpy ndarray, None</code> <p>Exogenous variable/s included as predictor/s.</p> <code>prediction_index</code> <code>pandas Index</code> <p>Index of the predictions.</p> <code>steps</code> <code>int</code> <p>Number of future steps predicted.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: int | str | pd.Timestamp, \n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    predict_probabilistic: bool = False,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    check_inputs: bool = True\n) -&gt; tuple[np.ndarray, np.ndarray | None, pd.Index, int]:\n    \"\"\"\n    Create the inputs needed for the first iteration of the prediction \n    process. As this is a recursive process, the last window is updated at \n    each iteration of the prediction process.\n\n    Parameters\n    ----------\n    steps : int, str, pandas Timestamp\n        Number of steps to predict. \n\n        - If steps is int, number of steps to predict. \n        - If str or pandas Datetime, the prediction will be up to that date.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    predict_probabilistic : bool, default False\n        If `True`, the necessary checks for probabilistic predictions will be \n        performed.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    last_window_values : numpy ndarray\n        Series values used to create the predictors needed in the first \n        iteration of the prediction (t + 1).\n    exog_values : numpy ndarray, None\n        Exogenous variable/s included as predictor/s.\n    prediction_index : pandas Index\n        Index of the predictions.\n    steps: int\n        Number of future steps predicted.\n\n    \"\"\"\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if self.is_fitted:\n        steps = date_to_index_position(\n                    index        = last_window.index,\n                    date_input   = steps,\n                    method       = 'prediction',\n                    date_literal = 'steps'\n                )\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name = type(self).__name__,\n            steps           = steps,\n            is_fitted       = self.is_fitted,\n            exog_in_        = self.exog_in_,\n            index_type_     = self.index_type_,\n            index_freq_     = self.index_freq_,\n            window_size     = self.window_size,\n            last_window     = last_window,\n            exog            = exog,\n            exog_names_in_  = self.exog_names_in_,\n            interval        = None\n        )\n\n        if predict_probabilistic:\n            check_residuals_input(\n                forecaster_name              = type(self).__name__,\n                use_in_sample_residuals      = use_in_sample_residuals,\n                in_sample_residuals_         = self.in_sample_residuals_,\n                out_sample_residuals_        = self.out_sample_residuals_,\n                use_binned_residuals         = use_binned_residuals,\n                in_sample_residuals_by_bin_  = self.in_sample_residuals_by_bin_,\n                out_sample_residuals_by_bin_ = self.out_sample_residuals_by_bin_\n            )\n\n    last_window_values = (\n        last_window.iloc[-self.window_size:].to_numpy(copy=True).ravel()\n    )\n    last_window_values = transform_numpy(\n                             array             = last_window_values,\n                             transformer       = self.transformer_y,\n                             fit               = False,\n                             inverse_transform = False\n                         )\n    if self.differentiation is not None:\n        last_window_values = self.differentiator.fit_transform(last_window_values)\n\n    if exog is not None:\n        exog = input_to_frame(data=exog, input_name='exog')\n        # TODO: only do the selections if columns are not already selected\n        # if not exog.columns.equals(pd.Index(self.exog_names_in_)):\n        #     exog = exog[self.exog_names_in_]\n        exog = exog[self.exog_names_in_]\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )\n        # TODO: only check dtypes if they are not the same as seen in training\n        # if not exog.dtypes.to_dict() == self.exog_dtypes_out_:\n        #   check_exog_dtypes(exog=exog)\n        # else:\n        #     check_exog(exog=exog, allow_nan=False, series_id=series_id)\n        check_exog_dtypes(exog=exog)\n        exog_values = exog.to_numpy()[:steps]\n    else:\n        exog_values = None\n\n    prediction_index = expand_index(\n                           index = last_window.index,\n                           steps = steps,\n                       )\n\n    return last_window_values, exog_values, prediction_index, steps\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._recursive_predict","title":"_recursive_predict","text":"<pre><code>_recursive_predict(\n    steps,\n    last_window_values,\n    exog_values=None,\n    residuals=None,\n    use_binned_residuals=True,\n)\n</code></pre> <p>Predict n steps ahead. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window_values</code> <code>numpy ndarray</code> <p>Series values used to create the predictors needed in the first  iteration of the prediction (t + 1).</p> required <code>exog_values</code> <code>numpy ndarray</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>residuals</code> <code>numpy ndarray, dict</code> <p>Residuals used to generate bootstrapping predictions.</p> <code>None</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    last_window_values: np.ndarray,\n    exog_values: np.ndarray | None = None,\n    residuals: np.ndarray | dict[str, np.ndarray] | None = None,\n    use_binned_residuals: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"\n    Predict n steps ahead. It is an iterative process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    last_window_values : numpy ndarray\n        Series values used to create the predictors needed in the first \n        iteration of the prediction (t + 1).\n    exog_values : numpy ndarray, default None\n        Exogenous variable/s included as predictor/s.\n    residuals : numpy ndarray, dict, default None\n        Residuals used to generate bootstrapping predictions.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    original_device = set_cpu_gpu_device(regressor=self.regressor, device='cpu')\n\n    n_lags = len(self.lags) if self.lags is not None else 0\n    n_window_features = (\n        len(self.X_train_window_features_names_out_)\n        if self.window_features is not None\n        else 0\n    )\n    n_exog = exog_values.shape[1] if exog_values is not None else 0\n\n    X = np.full(\n        shape=(n_lags + n_window_features + n_exog), fill_value=np.nan, dtype=float\n    )\n    predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n    last_window = np.concatenate((last_window_values, predictions))\n\n    for i in range(steps):\n\n        if self.lags is not None:\n            X[:n_lags] = last_window[-self.lags - (steps - i)]\n        if self.window_features is not None:\n            X[n_lags : n_lags + n_window_features] = np.concatenate(\n                [\n                    wf.transform(last_window[i : -(steps - i)])\n                    for wf in self.window_features\n                ]\n            )\n        if exog_values is not None:\n            X[n_lags + n_window_features:] = exog_values[i]\n\n        pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n\n        if residuals is not None:\n            if use_binned_residuals:\n                predicted_bin = self.binner.transform(pred).item()\n                step_residual = residuals[predicted_bin][i]\n            else:\n                step_residual = residuals[i]\n\n            pred += step_residual\n\n        predictions[i] = pred[0]\n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window[-(steps - i)] = pred[0]\n\n    set_cpu_gpu_device(regressor=self.regressor, device=original_device)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.create_predict_X","title":"create_predict_X","text":"<pre><code>create_predict_X(\n    steps, last_window=None, exog=None, check_inputs=True\n)\n</code></pre> <p>Create the predictors needed to predict <code>steps</code> ahead. As it is a recursive process, the predictors are created at each iteration of the prediction  process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, str, pandas Timestamp</code> <p>Number of steps to predict. </p> <ul> <li>If steps is int, number of steps to predict. </li> <li>If str or pandas Datetime, the prediction will be up to that date.</li> </ul> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>X_predict</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the predictors for each step. The index  is the same as the prediction index.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def create_predict_X(\n    self,\n    steps: int,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    check_inputs: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create the predictors needed to predict `steps` ahead. As it is a recursive\n    process, the predictors are created at each iteration of the prediction \n    process.\n\n    Parameters\n    ----------\n    steps : int, str, pandas Timestamp\n        Number of steps to predict. \n\n        - If steps is int, number of steps to predict. \n        - If str or pandas Datetime, the prediction will be up to that date.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    X_predict : pandas DataFrame\n        Pandas DataFrame with the predictors for each step. The index \n        is the same as the prediction index.\n\n    \"\"\"\n\n    (\n        last_window_values,\n        exog_values,\n        prediction_index,\n        steps\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs,\n        )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = self._recursive_predict(\n                          steps              = steps,\n                          last_window_values = last_window_values,\n                          exog_values        = exog_values\n                      )\n\n    X_predict = []\n    full_predictors = np.concatenate((last_window_values, predictions))\n\n    if self.lags is not None:\n        idx = np.arange(-steps, 0)[:, None] - self.lags\n        X_lags = full_predictors[idx + len(full_predictors)]\n        X_predict.append(X_lags)\n\n    if self.window_features is not None:\n        X_window_features = np.full(\n            shape      = (steps, len(self.X_train_window_features_names_out_)), \n            fill_value = np.nan, \n            order      = 'C',\n            dtype      = float\n        )\n        for i in range(steps):\n            X_window_features[i, :] = np.concatenate(\n                [wf.transform(full_predictors[i:-(steps - i)]) \n                 for wf in self.window_features]\n            )\n        X_predict.append(X_window_features)\n\n    if exog is not None:\n        X_predict.append(exog_values)\n\n    X_predict = pd.DataFrame(\n                    data    = np.concatenate(X_predict, axis=1),\n                    columns = self.X_train_features_names_out_,\n                    index   = prediction_index\n                )\n\n    if self.exog_in_:\n        categorical_features = any(\n            not pd.api.types.is_numeric_dtype(dtype) or pd.api.types.is_bool_dtype(dtype) \n            for dtype in set(self.exog_dtypes_out_)\n        )\n        if categorical_features:\n            X_predict = X_predict.astype(self.exog_dtypes_out_)\n\n    if self.transformer_y is not None or self.differentiation is not None:\n        warnings.warn(\n            \"The output matrix is in the transformed scale due to the \"\n            \"inclusion of transformations or differentiation in the Forecaster. \"\n            \"As a result, any predictions generated using this matrix will also \"\n            \"be in the transformed scale. Please refer to the documentation \"\n            \"for more details: \"\n            \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n            DataTransformationWarning\n        )\n\n    return X_predict\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.predict","title":"predict","text":"<pre><code>predict(\n    steps, last_window=None, exog=None, check_inputs=True\n)\n</code></pre> <p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, str, pandas Timestamp</code> <p>Number of steps to predict. </p> <ul> <li>If steps is int, number of steps to predict. </li> <li>If str or pandas Datetime, the prediction will be up to that date.</li> </ul> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def predict(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    check_inputs: bool = True\n) -&gt; pd.Series:\n    \"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int, str, pandas Timestamp\n        Number of steps to predict. \n\n        - If steps is int, number of steps to predict. \n        - If str or pandas Datetime, the prediction will be up to that date.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    (\n        last_window_values,\n        exog_values,\n        prediction_index,\n        steps\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs\n        )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = self._recursive_predict(\n                          steps              = steps,\n                          last_window_values = last_window_values,\n                          exog_values        = exog_values\n                      )\n\n    if self.differentiation is not None:\n        predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n    predictions = transform_numpy(\n                      array             = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n\n    predictions = pd.Series(\n                      data  = predictions,\n                      index = prediction_index,\n                      name  = 'pred'\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.predict_bootstrapping","title":"predict_bootstrapping","text":"<pre><code>predict_bootstrapping(\n    steps,\n    last_window=None,\n    exog=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n)\n</code></pre> <p>Generate multiple forecasting predictions using a bootstrapping process. By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  See the References section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, str, pandas Timestamp</code> <p>Number of steps to predict. </p> <ul> <li>If steps is int, number of steps to predict. </li> <li>If str or pandas Datetime, the prediction will be up to that date.</li> </ul> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Predictions generated by bootstrapping. Shape: (steps, n_boot)</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process.\n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    See the References section for more information. \n\n    Parameters\n    ----------\n    steps : int, str, pandas Timestamp\n        Number of steps to predict. \n\n        - If steps is int, number of steps to predict. \n        - If str or pandas Datetime, the prediction will be up to that date.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Predictions generated by bootstrapping.\n        Shape: (steps, n_boot)\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    (\n        last_window_values,\n        exog_values,\n        prediction_index,\n        steps\n    ) = self._create_predict_inputs(\n            steps                   = steps, \n            last_window             = last_window, \n            exog                    = exog,\n            predict_probabilistic   = True, \n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    rng = np.random.default_rng(seed=random_state)\n    if use_binned_residuals:\n        sampled_residuals = {\n            k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))]\n            for k, v in residuals_by_bin.items()\n        }\n    else:\n        sampled_residuals = residuals[\n            rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\n        ]\n\n    boot_columns = []\n    boot_predictions = np.full(\n                           shape      = (steps, n_boot),\n                           fill_value = np.nan,\n                           order      = 'F',\n                           dtype      = float\n                       )\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        for i in range(n_boot):\n\n            if use_binned_residuals:\n                boot_sampled_residuals = {\n                    k: v[:, i]\n                    for k, v in sampled_residuals.items()\n                }\n            else:\n                boot_sampled_residuals = sampled_residuals[:, i]\n\n            boot_columns.append(f\"pred_boot_{i}\")\n            boot_predictions[:, i] = self._recursive_predict(\n                steps                = steps,\n                last_window_values   = last_window_values,\n                exog_values          = exog_values,\n                residuals            = boot_sampled_residuals,\n                use_binned_residuals = use_binned_residuals,\n            )\n\n    if self.differentiation is not None:\n        boot_predictions = (\n            self.differentiator.inverse_transform_next_window(boot_predictions)\n        )\n\n    if self.transformer_y:\n        boot_predictions = np.apply_along_axis(\n                               func1d            = transform_numpy,\n                               axis              = 0,\n                               arr               = boot_predictions,\n                               transformer       = self.transformer_y,\n                               fit               = False,\n                               inverse_transform = True\n                           )\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions,\n                           index   = prediction_index,\n                           columns = boot_columns\n                       )\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive._predict_interval_conformal","title":"_predict_interval_conformal","text":"<pre><code>_predict_interval_conformal(\n    steps,\n    last_window=None,\n    exog=None,\n    nominal_coverage=0.95,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n)\n</code></pre> <p>Generate prediction intervals using the conformal prediction  split method [1]_.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, str, pandas Timestamp</code> <p>Number of steps to predict. </p> <ul> <li>If steps is int, number of steps to predict. </li> <li>If str or pandas Datetime, the prediction will be up to that date.</li> </ul> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>nominal_coverage</code> <code>float</code> <p>Nominal coverage, also known as expected coverage, of the prediction intervals. Must be between 0 and 1.</p> <code>0.95</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def _predict_interval_conformal(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    nominal_coverage: float = 0.95,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate prediction intervals using the conformal prediction \n    split method [1]_.\n\n    Parameters\n    ----------\n    steps : int, str, pandas Timestamp\n        Number of steps to predict. \n\n        - If steps is int, number of steps to predict. \n        - If str or pandas Datetime, the prediction will be up to that date.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    nominal_coverage : float, default 0.95\n        Nominal coverage, also known as expected coverage, of the prediction\n        intervals. Must be between 0 and 1.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    References\n    ----------\n    .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    (\n        last_window_values,\n        exog_values,\n        prediction_index,\n        steps\n    ) = self._create_predict_inputs(\n            steps                   = steps,\n            last_window             = last_window,\n            exog                    = exog,\n            predict_probabilistic   = True,\n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = self._recursive_predict(\n                          steps              = steps,\n                          last_window_values = last_window_values,\n                          exog_values        = exog_values\n                      )\n\n    if use_binned_residuals:\n        correction_factor_by_bin = {\n            k: np.quantile(np.abs(v), nominal_coverage)\n            for k, v in residuals_by_bin.items()\n        }\n        replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n        predictions_bin = self.binner.transform(predictions)\n        correction_factor = replace_func(predictions_bin)\n    else:\n        correction_factor = np.quantile(np.abs(residuals), nominal_coverage)\n\n    lower_bound = predictions - correction_factor\n    upper_bound = predictions + correction_factor\n    predictions = np.column_stack([predictions, lower_bound, upper_bound])\n\n    if self.differentiation is not None:\n        predictions = (\n            self.differentiator.inverse_transform_next_window(predictions)\n        )\n\n    if self.transformer_y:\n        predictions = np.apply_along_axis(\n                          func1d            = transform_numpy,\n                          axis              = 0,\n                          arr               = predictions,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n    predictions = pd.DataFrame(\n                      data    = predictions,\n                      index   = prediction_index,\n                      columns = [\"pred\", \"lower_bound\", \"upper_bound\"]\n                  )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.predict_interval","title":"predict_interval","text":"<pre><code>predict_interval(\n    steps,\n    last_window=None,\n    exog=None,\n    method=\"bootstrapping\",\n    interval=[5, 95],\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n)\n</code></pre> <p>Predict n steps ahead and estimate prediction intervals using either  bootstrapping or conformal prediction methods. Refer to the References  section for additional details on these methods.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, str, pandas Timestamp</code> <p>Number of steps to predict. </p> <ul> <li>If steps is int, number of steps to predict. </li> <li>If str or pandas Datetime, the prediction will be up to that date.</li> </ul> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction  intervals [1]_.</li> <li>'conformal': Employs the conformal prediction split method for  interval estimation [2]_.</li> </ul> <code>'bootstrapping'</code> <code>interval</code> <code>(float, list, tuple)</code> <p>Confidence level of the prediction interval. Interpretation depends  on the method used:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0  and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code>  percentiles.</li> <li>If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which  must be between 0 and 100 inclusive. For example, interval  of 95% should be as <code>interval = [2.5, 97.5]</code>.</li> <li>When using <code>method='conformal'</code>, the interval must be a float or  a list/tuple defining a symmetric interval.</li> </ul> <code>[5, 95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> <p>.. [2] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    method: str = 'bootstrapping',\n    interval: float | list[float] | tuple[float] = [5, 95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using either \n    bootstrapping or conformal prediction methods. Refer to the References \n    section for additional details on these methods.\n\n    Parameters\n    ----------\n    steps : int, str, pandas Timestamp\n        Number of steps to predict. \n\n        - If steps is int, number of steps to predict. \n        - If str or pandas Datetime, the prediction will be up to that date.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    method : str, default 'bootstrapping'\n        Technique used to estimate prediction intervals. Available options:\n\n        - 'bootstrapping': Bootstrapping is used to generate prediction \n        intervals [1]_.\n        - 'conformal': Employs the conformal prediction split method for \n        interval estimation [2]_.\n    interval : float, list, tuple, default [5, 95]\n        Confidence level of the prediction interval. Interpretation depends \n        on the method used:\n\n        - If `float`, represents the nominal (expected) coverage (between 0 \n        and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]` \n        percentiles.\n        - If `list` or `tuple`, defines the exact percentiles to compute, which \n        must be between 0 and 100 inclusive. For example, interval \n        of 95% should be as `interval = [2.5, 97.5]`.\n        - When using `method='conformal'`, the interval must be a float or \n        a list/tuple defining a symmetric interval.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    if method == \"bootstrapping\":\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=False)\n            interval = np.array(interval) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal='interval')\n            interval = np.array([0.5 - interval / 2, 0.5 + interval / 2])\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n    elif method == 'conformal':\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal='interval')\n            nominal_coverage = interval\n\n        predictions = self._predict_interval_conformal(\n                          steps                   = steps,\n                          last_window             = last_window,\n                          exog                    = exog,\n                          nominal_coverage        = nominal_coverage,\n                          use_in_sample_residuals = use_in_sample_residuals,\n                          use_binned_residuals    = use_binned_residuals\n                      )\n    else:\n        raise ValueError(\n            f\"Invalid `method` '{method}'. Choose 'bootstrapping' or 'conformal'.\"\n        )\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.predict_quantiles","title":"predict_quantiles","text":"<pre><code>predict_quantiles(\n    steps,\n    last_window=None,\n    exog=None,\n    quantiles=[0.05, 0.5, 0.95],\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n)\n</code></pre> <p>Calculate the specified quantiles for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  quantile is calculated for each step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, str, pandas Timestamp</code> <p>Number of steps to predict. </p> <ul> <li>If steps is int, number of steps to predict. </li> <li>If str or pandas Datetime, the prediction will be up to that date.</li> </ul> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>quantiles</code> <code>(list, tuple)</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>[0.05, 0.5, 0.95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating quantiles.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Quantiles predicted by the forecaster.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def predict_quantiles(\n    self,\n    steps: int | str | pd.Timestamp,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    quantiles: list[float] | tuple[float] = [0.05, 0.5, 0.95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the specified quantiles for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    quantile is calculated for each step.\n\n    Parameters\n    ----------\n    steps : int, str, pandas Timestamp\n        Number of steps to predict. \n\n        - If steps is int, number of steps to predict. \n        - If str or pandas Datetime, the prediction will be up to that date.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    quantiles : list, tuple, default [0.05, 0.5, 0.95]\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating quantiles.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Quantiles predicted by the forecaster.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    check_interval(quantiles=quantiles)\n\n    boot_predictions = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals,\n                           use_binned_residuals    = use_binned_residuals\n                       )\n\n    predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n    predictions.columns = [f'q_{q}' for q in quantiles]\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.predict_dist","title":"predict_dist","text":"<pre><code>predict_dist(\n    steps,\n    distribution,\n    last_window=None,\n    exog=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n)\n</code></pre> <p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, str, pandas Timestamp</code> <p>Number of steps to predict. </p> <ul> <li>If steps is int, number of steps to predict. </li> <li>If str or pandas Datetime, the prediction will be up to that date.</li> </ul> required <code>distribution</code> <code>object</code> <p>A distribution object from scipy.stats with methods <code>_pdf</code> and <code>fit</code>.  For example scipy.stats.norm.</p> required <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Distribution parameters estimated for each step.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int | str | pd.Timestamp,\n    distribution: object,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int, str, pandas Timestamp\n        Number of steps to predict. \n\n        - If steps is int, number of steps to predict. \n        - If str or pandas Datetime, the prediction will be up to that date.\n    distribution : object\n        A distribution object from scipy.stats with methods `_pdf` and `fit`. \n        For example scipy.stats.norm.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).  \n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Distribution parameters estimated for each step.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    if not hasattr(distribution, \"_pdf\") or not callable(getattr(distribution, \"fit\", None)):\n        raise TypeError(\n            \"`distribution` must be a valid probability distribution object \"\n            \"from scipy.stats, with methods `_pdf` and `fit`.\"\n        )\n\n    predictions = self.predict_bootstrapping(\n                      steps                   = steps,\n                      last_window             = last_window,\n                      exog                    = exog,\n                      n_boot                  = n_boot,\n                      random_state            = random_state,\n                      use_in_sample_residuals = use_in_sample_residuals,\n                      use_binned_residuals    = use_binned_residuals\n                  )       \n\n    param_names = [\n        p for p in inspect.signature(distribution._pdf).parameters\n        if not p == 'x'\n    ] + [\"loc\", \"scale\"]\n\n    predictions[param_names] = (\n        predictions.apply(\n            lambda x: distribution.fit(x), axis=1, result_type='expand'\n        )\n    )\n    predictions = predictions[param_names]\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.set_params","title":"set_params","text":"<pre><code>set_params(params)\n</code></pre> <p>Set new values to the parameters of the scikit-learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def set_params(\n    self, \n    params: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the scikit-learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.set_fit_kwargs","title":"set_fit_kwargs","text":"<pre><code>set_fit_kwargs(fit_kwargs)\n</code></pre> <p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.set_lags","title":"set_lags","text":"<pre><code>set_lags(lags=None)\n</code></pre> <p>Set new value to the attribute <code>lags</code>. Attributes <code>lags_names</code>,  <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def set_lags(\n    self, \n    lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `lags`. Attributes `lags_names`, \n    `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range, default None\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if self.window_features is None and lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation\n        self.differentiator.set_params(window_size=self.window_size)\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.set_window_features","title":"set_window_features","text":"<pre><code>set_window_features(window_features=None)\n</code></pre> <p>Set new value to the attribute <code>window_features</code>. Attributes  <code>max_size_window_features</code>, <code>window_features_names</code>,  <code>window_features_class_names</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def set_window_features(\n    self, \n    window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `window_features`. Attributes \n    `max_size_window_features`, `window_features_names`, \n    `window_features_class_names` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    window_features : object, list, default None\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if window_features is None and self.lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.window_features, self.window_features_names, self.max_size_window_features = (\n        initialize_window_features(window_features)\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ] \n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    if self.differentiation is not None:\n        self.window_size += self.differentiation\n        self.differentiator.set_params(window_size=self.window_size)\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.set_in_sample_residuals","title":"set_in_sample_residuals","text":"<pre><code>set_in_sample_residuals(y, exog=None, random_state=123)\n</code></pre> <p>Set in-sample residuals in case they were not calculated during the training process. </p> <p>In-sample residuals are calculated as the difference between the true  values and the predictions made by the forecaster using the training  data. The following internal attributes are updated:</p> <ul> <li><code>in_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>binner_intervals_</code>: intervals used to bin the residuals are calculated using the quantiles of the predicted values.</li> <li><code>in_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the intervals of the predicted values and the values are the residuals associated with that range. </li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>in_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def set_in_sample_residuals(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process. \n\n    In-sample residuals are calculated as the difference between the true \n    values and the predictions made by the forecaster using the training \n    data. The following internal attributes are updated:\n\n    + `in_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `binner_intervals_`: intervals used to bin the residuals are calculated\n    using the quantiles of the predicted values.\n    + `in_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the intervals of the predicted values and the values are\n    the residuals associated with that range. \n\n    A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    check_y(y=y)\n    y_index_range = check_extract_values_and_index(\n        data=y, data_label='`y`', return_values=False\n    )[1][[0, -1]]\n    if not y_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `y` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {y_index_range}\"\n        )\n\n    (\n        X_train,\n        y_train,\n        _,\n        _,\n        _,\n        X_train_features_names_out_,\n        *_\n    ) = self._create_train_X_y(y=y, exog=exog)\n\n    if not X_train_features_names_out_ == self.X_train_features_names_out_:\n        raise ValueError(\n            f\"Feature mismatch detected after matrix creation. The features \"\n            f\"generated from the provided data do not match those used during \"\n            f\"the training process. To correctly set in-sample residuals, \"\n            f\"ensure that the same data and preprocessing steps are applied.\\n\"\n            f\"    Expected output : {self.X_train_features_names_out_}\\n\"\n            f\"    Current output  : {X_train_features_names_out_}\"\n        )\n\n    self._binning_in_sample_residuals(\n        y_true                    = y_train.to_numpy(),\n        y_pred                    = self.regressor.predict(X_train).ravel(),\n        store_in_sample_residuals = True,\n        random_state              = random_state\n    )\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.set_out_sample_residuals","title":"set_out_sample_residuals","text":"<pre><code>set_out_sample_residuals(\n    y_true, y_pred, append=False, random_state=123\n)\n</code></pre> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. <code>y_true</code> and <code>y_pred</code> are expected to be in the original scale of the time series. Residuals are calculated as <code>y_true</code> - <code>y_pred</code>, after applying the necessary transformations and differentiations if the forecaster includes them (<code>self.transformer_y</code> and <code>self.differentiation</code>). Two internal attributes are updated:</p> <ul> <li><code>out_sample_residuals_</code>: residuals stored in a numpy ndarray.</li> <li><code>out_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary, where the keys are the  intervals of the predicted values and the values are the residuals associated with that range. If a bin binning is empty, it is filled with a random sample of residuals from other bins. This is done to ensure that all bins have at least one residual and can be used in the prediction process.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>out_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>numpy ndarray, pandas Series</code> <p>True values of the time series from which the residuals have been calculated.</p> required <code>y_pred</code> <code>numpy ndarray, pandas Series</code> <p>Predicted values of the time series.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the forecaster. If after appending the new residuals, the limit of <code>10_000 // self.binner.n_bins_</code> values per bin is reached, a random sample of residuals is stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    append: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. `y_true` and `y_pred` are expected\n    to be in the original scale of the time series. Residuals are calculated\n    as `y_true` - `y_pred`, after applying the necessary transformations and\n    differentiations if the forecaster includes them (`self.transformer_y`\n    and `self.differentiation`). Two internal attributes are updated:\n\n    + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n    + `out_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary, where\n    the keys are the  intervals of the predicted values and the values are\n    the residuals associated with that range. If a bin binning is empty, it\n    is filled with a random sample of residuals from other bins. This is done\n    to ensure that all bins have at least one residual and can be used in the\n    prediction process.\n\n    A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    y_true : numpy ndarray, pandas Series\n        True values of the time series from which the residuals have been\n        calculated.\n    y_pred : numpy ndarray, pandas Series\n        Predicted values of the time series.\n    append : bool, default False\n        If `True`, new residuals are added to the once already stored in the\n        forecaster. If after appending the new residuals, the limit of\n        `10_000 // self.binner.n_bins_` values per bin is reached, a random\n        sample of residuals is stored.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, (np.ndarray, pd.Series)):\n        raise TypeError(\n            f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if len(y_true) != len(y_pred):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same length. \"\n            f\"Got {len(y_true)} and {len(y_pred)}.\"\n        )\n\n    if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n        if not y_true.index.equals(y_pred.index):\n            raise ValueError(\n                \"`y_true` and `y_pred` must have the same index.\"\n            )\n\n    y_true = deepcopy(y_true)\n    y_pred = deepcopy(y_pred)\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = y_pred.to_numpy()\n    if not isinstance(y_true, np.ndarray):\n        y_true = y_true.to_numpy()\n\n    if self.transformer_y:\n        y_true = transform_numpy(\n                     array             = y_true,\n                     transformer       = self.transformer_y,\n                     fit               = False,\n                     inverse_transform = False\n                 )\n        y_pred = transform_numpy(\n                     array             = y_pred,\n                     transformer       = self.transformer_y,\n                     fit               = False,\n                     inverse_transform = False\n                 )\n\n    if self.differentiation is not None:\n        differentiator = copy(self.differentiator)\n        differentiator.set_params(window_size=None)\n        y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n        y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n\n    data = pd.DataFrame(\n        {'prediction': y_pred, 'residuals': y_true - y_pred}\n    ).dropna()\n    y_pred = data['prediction'].to_numpy()\n    residuals = data['residuals'].to_numpy()\n\n    data['bin'] = self.binner.transform(y_pred).astype(int)\n    residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n    out_sample_residuals = (\n        np.array([]) \n        if self.out_sample_residuals_ is None\n        else self.out_sample_residuals_\n    )\n    out_sample_residuals_by_bin = (\n        {} \n        if self.out_sample_residuals_by_bin_ is None\n        else self.out_sample_residuals_by_bin_\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    max_samples = 10_000 // self.binner.n_bins_\n    rng = np.random.default_rng(seed=random_state)\n    for k, v in out_sample_residuals_by_bin.items():\n        if len(v) &gt; max_samples:\n            sample = rng.choice(a=v, size=max_samples, replace=False)\n            out_sample_residuals_by_bin[k] = sample\n\n    bin_keys = (\n        []\n        if self.binner_intervals_ is None\n        else self.binner_intervals_.keys()\n    )\n    for k in bin_keys:\n        if k not in out_sample_residuals_by_bin:\n            out_sample_residuals_by_bin[k] = np.array([])\n\n    empty_bins = [\n        k for k, v in out_sample_residuals_by_bin.items() \n        if v.size == 0\n    ]\n    if empty_bins:\n        warnings.warn(\n            f\"The following bins have no out of sample residuals: {empty_bins}. \"\n            f\"No predicted values fall in the interval \"\n            f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n            f\"Empty bins will be filled with a random sample of residuals.\",\n            ResidualsUsageWarning\n        )\n        empty_bin_size = min(max_samples, len(out_sample_residuals))\n        for k in empty_bins:\n            out_sample_residuals_by_bin[k] = rng.choice(\n                a       = out_sample_residuals,\n                size    = empty_bin_size,\n                replace = False\n            )\n\n    if len(out_sample_residuals) &gt; 10_000:\n        out_sample_residuals = rng.choice(\n            a       = out_sample_residuals, \n            size    = 10_000, \n            replace = False\n        )\n\n    self.out_sample_residuals_ = out_sample_residuals\n    self.out_sample_residuals_by_bin_ = out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/ForecasterRecursive.html#skforecast.recursive._forecaster_recursive.ForecasterRecursive.get_feature_importances","title":"get_feature_importances","text":"<pre><code>get_feature_importances(sort_importance=True)\n</code></pre> <p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>. Otherwise, returns <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive.py</code> <pre><code>def get_feature_importances(\n    self,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n    Only valid when regressor stores internally the feature importances in the\n    attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n    Parameters\n    ----------\n    sort_importance: bool, default True\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `get_feature_importances()`.\"\n        )\n\n    if isinstance(self.regressor, Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            f\"Impossible to access feature importances for regressor of type \"\n            f\"{type(estimator)}. This method is only valid when the \"\n            f\"regressor stores internally the feature importances in the \"\n            f\"attribute `feature_importances_` or `coef_`.\"\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_features_names_out_,\n                                  'importance': feature_importances\n                              })\n        if sort_importance:\n            feature_importances = feature_importances.sort_values(\n                                      by='importance', ascending=False\n                                  )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html","title":"<code>ForecasterRecursiveMultiSeries</code>","text":""},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries","title":"skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries","text":"<pre><code>ForecasterRecursiveMultiSeries(\n    regressor,\n    lags=None,\n    window_features=None,\n    encoding=\"ordinal\",\n    transformer_series=None,\n    transformer_exog=None,\n    weight_func=None,\n    series_weights=None,\n    differentiation=None,\n    dropna_from_series=False,\n    fit_kwargs=None,\n    binner_kwargs=None,\n    forecaster_id=None,\n)\n</code></pre> <p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the scikit-learn API into a recursive autoregressive (multi-step) forecaster for multiple series.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>None</code> <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>None</code> <code>encoding</code> <code>(str, None)</code> <p>Encoding used to identify the different series. </p> <ul> <li>If <code>'ordinal'</code>, a single column is created with integer values from 0  to n_series - 1. </li> <li>If <code>'ordinal_category'</code>, a single column is created with integer  values from 0 to n_series - 1 and the column is transformed into  pandas.category dtype so that it can be used as a categorical variable. </li> <li>If <code>'onehot'</code>, a binary column is created for each series.</li> <li>If None, no column is created to identify the series. Internally, the series are identified as an integer from 0 to n_series - 1, but no column is created in the training matrices. Changed to 'ordinal' in version 0.14.0</li> </ul> <code>'ordinal'</code> <code>transformer_series</code> <code>(transformer(preprocessor), dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>None</code> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>weight_func</code> <code>(Callable, dict)</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present in  <code>weight_func</code>.</li> </ul> <code>None</code> <code>series_weights</code> <code>dict</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>None</code> <code>differentiation</code> <code>(int, dict)</code> <p>Order of differencing applied to the time series before training the forecaster. The order of differentiation is the number of times the differencing operation  is applied to a time series. Differencing involves computing the differences  between consecutive data points in the series. Before returning a prediction,  the differencing operation is reversed.</p> <ul> <li>If <code>int</code>, the same order of differentiation is applied to all series.</li> <li>If <code>dict</code>, a different order of differentiation (including None) can  be used for each series. The keys must be the names of the series used to fit the forecaster. If a series is not present in the dictionary, no differencing is applied.</li> <li>If <code>None</code>, no differencing is applied.</li> </ul> <code>None</code> <code>dropna_from_series</code> <code>bool</code> <p>Determine whether NaN detected in the training matrices will be dropped.</p> <ul> <li>If <code>True</code>, drop NaNs in X_train and same rows in y_train.</li> <li>If <code>False</code>, leave NaNs in X_train and warn the user. New in version 0.12.0</li> </ul> <code>False</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>None</code> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code> used to discretize  the residuals into k bins according to the predicted values associated  with each residual. Available arguments are: <code>n_bins</code>, <code>method</code>, <code>subsample</code>, <code>random_state</code> and <code>dtype</code>. Argument <code>method</code> is passed internally to the function <code>numpy.percentile</code>. New in version 0.14.0</p> <code>None</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>lags_names</code> <code>list</code> <p>Names of the lags used as predictors.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_features</code> <code>list</code> <p>Class or list of classes used to create window features.</p> <code>window_features_names</code> <code>list</code> <p>Names of the window features to be included in the <code>X_train</code> matrix.</p> <code>window_features_class_names</code> <code>list</code> <p>Names of the classes used to create the window features.</p> <code>max_size_window_features</code> <code>int</code> <p>Maximum window size required by the window features.</p> <code>window_size</code> <code>int</code> <p>The window size needed to create the predictors. It is calculated as the  maximum value between <code>max_lag</code> and <code>max_size_window_features</code>. If  differentiation is used, <code>window_size</code> is increased by n units equal to  the order of differentiation so that predictors can be generated correctly.</p> <code>encoding</code> <code>str</code> <p>Encoding used to identify the different series. </p> <ul> <li>If <code>'ordinal'</code>, a single column is created with integer values from 0  to n_series - 1. </li> <li>If <code>'ordinal_category'</code>, a single column is created with integer  values from 0 to n_series - 1 and the column is transformed into  pandas.category dtype so that it can be used as a categorical variable. </li> <li>If <code>'onehot'</code>, a binary column is created for each series.</li> <li>If None, no column is created to identify the series. Internally, the series are identified as an integer from 0 to n_series - 1, but no column is created in the training matrices. Changed to 'ordinal' in version 0.14.0</li> </ul> <code>encoder</code> <code>preprocessing</code> <p>Scikit-learn preprocessing encoder used to encode the series. New in version 0.12.0</p> <code>encoding_mapping_</code> <code>dict</code> <p>Mapping of the encoding used to identify the different series.</p> <code>transformer_series</code> <code>(transformer(preprocessor), dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer(preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>weight_func</code> <code>(Callable, dict)</code> <p>Function that defines the individual weights for each sample based on the index. For example, a function that assigns a lower weight to certain dates.  Ignored if <code>regressor</code> does not have the argument <code>sample_weight</code> in its  <code>fit</code> method. See Notes section for more details on the use of the weights.</p> <ul> <li>If single function: it is applied to all series. </li> <li>If <code>dict</code> {'series_column_name' : Callable}: a different function can be used for each series, a weight of 1 is given to all series not present  in <code>weight_func</code>.</li> </ul> <code>weight_func_</code> <code>dict</code> <p>Dictionary with the <code>weight_func</code> for each series. It is created cloning the  objects in <code>weight_func</code> and is used internally to avoid overwriting.</p> <code>source_code_weight_func</code> <code>(str, dict)</code> <p>Source code of the custom function(s) used to create weights.</p> <code>series_weights</code> <code>dict</code> <p>Weights associated with each series {'series_column_name' : float}. It is only applied if the <code>regressor</code> used accepts <code>sample_weight</code> in its <code>fit</code> method.  See Notes section for more details on the use of the weights.</p> <ul> <li>If a <code>dict</code> is provided, a weight of 1 is given to all series not present in <code>series_weights</code>.</li> <li>If <code>None</code>, all levels have the same weight.</li> </ul> <code>series_weights_</code> <code>dict</code> <p>Weights associated with each series. It is created as a clone of <code>series_weights</code> and is used internally to avoid overwriting.</p> <code>differentiation</code> <code>(int, dict)</code> <p>The order of differencing applied to the time series prior to training the  forecaster.</p> <code>differentiation_max</code> <code>int</code> <p>Maximum order of differentiation.</p> <code>differentiator</code> <code>(TimeSeriesDifferentiator, dict)</code> <p>Skforecast object (or dict of objects) used to differentiate the time series.</p> <code>differentiator_</code> <code>dict</code> <p>Dictionary with the <code>differentiator</code> for each series. It is created cloning the objects in <code>differentiator</code> and is used internally to avoid overwriting.</p> <code>dropna_from_series</code> <code>bool</code> <p>Determine whether NaN detected in the training matrices will be dropped.</p> <code>last_window_</code> <code>dict</code> <p>Last window of training data for each series. It stores the values  needed to predict the next <code>step</code> immediately after the training data.  These values are stored in the original scale of the time series before  undergoing any transformations or differentiation. When <code>differentiation</code>  parameter is specified, the dimensions of the <code>last_window_</code> are expanded  as many values as the order of differentiation. For example, if  <code>lags</code> = 7 and <code>differentiation</code> = 1, <code>last_window_</code> will have 8 values.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>dict</code> <p>First and last values of index of the data used during training for each  series.</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) provided by the user during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous data (pandas Series, DataFrame or dict) used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series (levels) included in the matrix <code>X_train</code> created internally for training. It can be different from <code>series_names_in_</code> if some series are dropped during the training process because of NaNs or  because they are not present in the training period.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>X_train_features_names_out_</code> <code>list</code> <p>Names of columns of the matrix created internally for training.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting training data. Only stored up  to 10_000 values per series in the form <code>{series: residuals}</code>. If  <code>transformer_series</code> is not <code>None</code>, residuals are stored in the  transformed scale. If <code>differentiation</code> is not <code>None</code>, residuals are  stored after differentiation.</p> <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> per series in the form <code>{series: residuals}</code>. If <code>transformer_series</code> is not <code>None</code>, residuals are stored in the  transformed scale. If <code>differentiation</code> is not <code>None</code>, residuals are  stored after differentiation.  New in version 0.15.0</p> <code>out_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting non-training data. Only stored up  to 10_000 values per series in the form <code>{series: residuals}</code>. Use  <code>set_out_sample_residuals()</code> method to set values. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale. If  <code>differentiation</code> is not <code>None</code>, residuals are stored after differentiation. </p> <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code> per series in the form <code>{series: residuals}</code>. If <code>transformer_series</code> is not <code>None</code>, residuals are stored in the  transformed scale. If <code>differentiation</code> is not <code>None</code>, residuals are  stored after differentiation.  New in version 0.15.0</p> <code>binner</code> <code>dict</code> <p>Dictionary of <code>skforecast.preprocessing.QuantileBinner</code> used to discretize residuals of each series into k bins according to the predicted values  associated with each residual. In the form <code>{series: binner}</code>. New in version 0.15.0</p> <code>binner_intervals_</code> <code>dict</code> <p>Intervals used to discretize residuals into k bins according to the predicted values associated with each residual. In the form <code>{series: binner_intervals_}</code>. New in version 0.15.0</p> <code>binner_kwargs</code> <code>dict</code> <p>Additional arguments to pass to the <code>QuantileBinner</code>. New in version 0.15.0</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>_probabilistic_mode</code> <code>(str, bool)</code> <p>Private attribute used to indicate whether the forecaster should perform  some calculations during backtesting.</p> Notes <p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterRecursiveMultiSeries</code> accepts two types of weights.  If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <ul> <li><code>series_weights</code> : controls the relative importance of each series. If a  series has twice as much weight as the others, the observations of that series  influence the training twice as much. The higher the weight of a series  relative to the others, the more the model will focus on trying to learn  that series.</li> <li><code>weight_func</code> : controls the relative importance of each observation  according to its index value. For example, a function that assigns a lower  weight to certain dates.</li> </ul> <p>Methods:</p> Name Description <code>create_train_X_y</code> <p>Create training matrices from multiple time series and exogenous</p> <code>create_sample_weights</code> <p>Create weights for each observation according to the forecaster's attributes</p> <code>fit</code> <p>Training Forecaster. See Notes section for more details depending on </p> <code>create_predict_X</code> <p>Create the predictors needed to predict <code>steps</code> ahead. As it is a recursive</p> <code>predict</code> <p>Predict n steps ahead. It is an recursive process in which, each prediction,</p> <code>predict_bootstrapping</code> <p>Generate multiple forecasting predictions using a bootstrapping process. </p> <code>predict_interval</code> <p>Predict n steps ahead and estimate prediction intervals using either </p> <code>predict_quantiles</code> <p>Calculate the specified quantiles for each step. After generating </p> <code>predict_dist</code> <p>Fit a given probability distribution for each step. After generating </p> <code>set_params</code> <p>Set new values to the parameters of the scikit-learn model stored in the</p> <code>set_fit_kwargs</code> <p>Set new values for the additional keyword arguments passed to the <code>fit</code> </p> <code>set_lags</code> <p>Set new value to the attribute <code>lags</code>. Attributes <code>lags_names</code>, </p> <code>set_window_features</code> <p>Set new value to the attribute <code>window_features</code>. Attributes </p> <code>set_in_sample_residuals</code> <p>Set in-sample residuals in case they were not calculated during the</p> <code>set_out_sample_residuals</code> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample</p> <code>get_feature_importances</code> <p>Return feature importances of the regressor stored in the</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    lags: int | list[int] | np.ndarray[int] | range[int] | None = None,\n    window_features: object | list[object] | None = None,\n    encoding: str | None = 'ordinal',\n    transformer_series: object | dict[str, object] | None = None,\n    transformer_exog: object | None = None,\n    weight_func: Callable | dict[str, Callable] | None = None,\n    series_weights: dict[str, float] | None = None,\n    differentiation: int | dict[str, int | None] | None = None,\n    dropna_from_series: bool = False,\n    fit_kwargs: dict[str, object] | None = None,\n    binner_kwargs: dict[str, object] | None = None,\n    forecaster_id: str | int | None = None\n) -&gt; None:\n\n    self.regressor                          = copy(regressor)\n    self.encoding                           = encoding\n    self.encoder                            = None\n    self.encoding_mapping_                  = {}\n    self.transformer_series                 = transformer_series\n    self.transformer_series_                = None\n    self.transformer_exog                   = transformer_exog\n    self.weight_func                        = weight_func\n    self.weight_func_                       = None\n    self.source_code_weight_func            = None\n    self.series_weights                     = series_weights\n    self.series_weights_                    = None\n    self.differentiation                    = differentiation\n    self.differentiation_max                = None\n    self.differentiator                     = None\n    self.differentiator_                    = None\n    self.dropna_from_series                 = dropna_from_series\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.series_names_in_                   = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.exog_dtypes_out_                   = None \n    self.X_train_series_names_in_           = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.in_sample_residuals_by_bin_        = None\n    self.out_sample_residuals_              = None\n    self.out_sample_residuals_by_bin_       = None\n    self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted                          = False\n    self.fit_date                           = None\n    self.skforecast_version                 = skforecast.__version__\n    self.python_version                     = sys.version.split(\" \")[0]\n    self.forecaster_id                      = forecaster_id\n    self._probabilistic_mode                = \"binned\"\n\n    self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_features, self.window_features_names, self.max_size_window_features = (\n        initialize_window_features(window_features)\n    )\n    if self.window_features is None and self.lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ]\n\n    if self.encoding not in ['ordinal', 'ordinal_category', 'onehot', None]:\n        raise ValueError(\n            f\"Argument `encoding` must be one of the following values: 'ordinal', \"\n            f\"'ordinal_category', 'onehot' or None. Got '{self.encoding}'.\"\n        )\n\n    if self.encoding == 'onehot':\n        self.encoder = OneHotEncoder(\n                           categories    = 'auto',\n                           sparse_output = False,\n                           drop          = None,\n                           dtype         = int\n                       ).set_output(transform='pandas')\n    else:\n        self.encoder = OrdinalEncoder(\n                           categories = 'auto',\n                           dtype      = int\n                       ).set_output(transform='pandas')\n\n    scaling_regressors = tuple(\n        member[1]\n        for member in inspect.getmembers(sklearn.linear_model, inspect.isclass)\n        + inspect.getmembers(sklearn.svm, inspect.isclass)\n    )\n\n    if self.transformer_series is None and isinstance(regressor, scaling_regressors):\n        warnings.warn(\n            \"When using a linear model, it is recommended to use a transformer_series \"\n            \"to ensure all series are in the same scale. You can use, for example, a \"\n            \"`StandardScaler` from sklearn.preprocessing.\",\n            DataTransformationWarning\n        )\n\n    if isinstance(self.transformer_series, dict):\n        if self.encoding is None:\n            raise TypeError(\n                \"When `encoding` is None, `transformer_series` must be a single \"\n                \"transformer (not `dict`) as it is applied to all series.\"\n            )\n        if '_unknown_level' not in self.transformer_series.keys():\n            raise ValueError(\n                \"If `transformer_series` is a `dict`, a transformer must be \"\n                \"provided to transform series that do not exist during training. \"\n                \"Add the key '_unknown_level' to `transformer_series`. \"\n                \"For example: {'_unknown_level': your_transformer}.\"\n            )\n\n    self.weight_func, self.source_code_weight_func, self.series_weights = (\n        initialize_weights(\n            forecaster_name = type(self).__name__,\n            regressor       = regressor,\n            weight_func     = weight_func,\n            series_weights  = series_weights,\n        )\n    )\n\n    if differentiation is not None:\n        if isinstance(differentiation, int):\n            if differentiation &lt; 1:\n                raise ValueError(\n                    f\"If `differentiation` is an integer, it must be equal \"\n                    f\"to or greater than 1. Got {differentiation}.\"\n                )\n            self.differentiation = differentiation\n            self.differentiation_max = differentiation\n            self.window_size += self.differentiation_max\n            self.differentiator = TimeSeriesDifferentiator(\n                order=differentiation, window_size=self.window_size\n            )\n        elif isinstance(differentiation, dict):\n\n            if self.encoding is None:\n                raise TypeError(\n                    \"When `encoding` is None, `differentiation` must be an \"\n                    \"integer equal to or greater than 1. Same differentiation \"\n                    \"must be applied to all series.\"\n                )\n            if '_unknown_level' not in differentiation.keys():\n                raise ValueError(\n                    \"If `differentiation` is a `dict`, an order must be provided \"\n                    \"to differentiate series that do not exist during training. \"\n                    \"Add the key '_unknown_level' to `differentiation`. \"\n                    \"For example: {'_unknown_level': 1}.\"\n                )\n\n            differentiation_max = []\n            for level, diff in differentiation.items():\n                if diff is not None:\n                    if not isinstance(diff, int) or diff &lt; 1:\n                        raise ValueError(\n                            f\"If `differentiation` is a dict, the values must be \"\n                            f\"None or integers equal to or greater than 1. \"\n                            f\"Got {diff} for series '{level}'.\"\n                        )\n                    differentiation_max.append(diff)\n\n            if len(differentiation_max) == 0:\n                raise ValueError(\n                    \"If `differentiation` is a dict, at least one value must be \"\n                    \"different from None. Got all values equal to None. If you \"\n                    \"do not want to differentiate any series, set `differentiation` \"\n                    \"to None.\"\n                )\n\n            self.differentiation = differentiation\n            self.differentiation_max = max(differentiation_max)\n            self.window_size += self.differentiation_max\n            self.differentiator = {\n                level: (\n                    TimeSeriesDifferentiator(order=diff, window_size=self.window_size)\n                    if diff is not None else None\n                )\n                for level, diff in differentiation.items()\n            }\n        else:\n            raise TypeError(\n                f\"When including `differentiation`, this argument must be \"\n                f\"an integer (equal to or greater than 1) or a dict of \"\n                f\"integers. Got {type(differentiation)}.\"\n            )\n\n    self.fit_kwargs = check_select_fit_kwargs(\n                          regressor  = regressor,\n                          fit_kwargs = fit_kwargs\n                      )\n\n    self.binner = {}\n    self.binner_intervals_ = {}\n    self.binner_kwargs = binner_kwargs\n    if binner_kwargs is None:\n        self.binner_kwargs = {\n            'n_bins': 10, 'method': 'linear', 'subsample': 200000,\n            'random_state': 789654, 'dtype': np.float64\n        }\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.regressor","title":"regressor  <code>instance-attribute</code>","text":"<pre><code>regressor = copy(regressor)\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding = encoding\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.encoding_mapping_","title":"encoding_mapping_  <code>instance-attribute</code>","text":"<pre><code>encoding_mapping_ = {}\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.transformer_series","title":"transformer_series  <code>instance-attribute</code>","text":"<pre><code>transformer_series = transformer_series\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.transformer_series_","title":"transformer_series_  <code>instance-attribute</code>","text":"<pre><code>transformer_series_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.transformer_exog","title":"transformer_exog  <code>instance-attribute</code>","text":"<pre><code>transformer_exog = transformer_exog\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.weight_func","title":"weight_func  <code>instance-attribute</code>","text":"<pre><code>weight_func = weight_func\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.weight_func_","title":"weight_func_  <code>instance-attribute</code>","text":"<pre><code>weight_func_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.source_code_weight_func","title":"source_code_weight_func  <code>instance-attribute</code>","text":"<pre><code>source_code_weight_func = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.series_weights","title":"series_weights  <code>instance-attribute</code>","text":"<pre><code>series_weights = series_weights\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.series_weights_","title":"series_weights_  <code>instance-attribute</code>","text":"<pre><code>series_weights_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.differentiation","title":"differentiation  <code>instance-attribute</code>","text":"<pre><code>differentiation = differentiation\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.differentiation_max","title":"differentiation_max  <code>instance-attribute</code>","text":"<pre><code>differentiation_max = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.differentiator","title":"differentiator  <code>instance-attribute</code>","text":"<pre><code>differentiator = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.differentiator_","title":"differentiator_  <code>instance-attribute</code>","text":"<pre><code>differentiator_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.dropna_from_series","title":"dropna_from_series  <code>instance-attribute</code>","text":"<pre><code>dropna_from_series = dropna_from_series\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.last_window_","title":"last_window_  <code>instance-attribute</code>","text":"<pre><code>last_window_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.index_type_","title":"index_type_  <code>instance-attribute</code>","text":"<pre><code>index_type_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.index_freq_","title":"index_freq_  <code>instance-attribute</code>","text":"<pre><code>index_freq_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.training_range_","title":"training_range_  <code>instance-attribute</code>","text":"<pre><code>training_range_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.series_names_in_","title":"series_names_in_  <code>instance-attribute</code>","text":"<pre><code>series_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.exog_in_","title":"exog_in_  <code>instance-attribute</code>","text":"<pre><code>exog_in_ = False\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.exog_names_in_","title":"exog_names_in_  <code>instance-attribute</code>","text":"<pre><code>exog_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.exog_type_in_","title":"exog_type_in_  <code>instance-attribute</code>","text":"<pre><code>exog_type_in_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.exog_dtypes_in_","title":"exog_dtypes_in_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_in_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.exog_dtypes_out_","title":"exog_dtypes_out_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_out_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.X_train_series_names_in_","title":"X_train_series_names_in_  <code>instance-attribute</code>","text":"<pre><code>X_train_series_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.X_train_window_features_names_out_","title":"X_train_window_features_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_window_features_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.X_train_exog_names_out_","title":"X_train_exog_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_exog_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.X_train_features_names_out_","title":"X_train_features_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_features_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.in_sample_residuals_","title":"in_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.in_sample_residuals_by_bin_","title":"in_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.out_sample_residuals_","title":"out_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.out_sample_residuals_by_bin_","title":"out_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.creation_date","title":"creation_date  <code>instance-attribute</code>","text":"<pre><code>creation_date = strftime('%Y-%m-%d %H:%M:%S')\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.is_fitted","title":"is_fitted  <code>instance-attribute</code>","text":"<pre><code>is_fitted = False\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.fit_date","title":"fit_date  <code>instance-attribute</code>","text":"<pre><code>fit_date = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.skforecast_version","title":"skforecast_version  <code>instance-attribute</code>","text":"<pre><code>skforecast_version = __version__\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.python_version","title":"python_version  <code>instance-attribute</code>","text":"<pre><code>python_version = split(' ')[0]\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.forecaster_id","title":"forecaster_id  <code>instance-attribute</code>","text":"<pre><code>forecaster_id = forecaster_id\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._probabilistic_mode","title":"_probabilistic_mode  <code>instance-attribute</code>","text":"<pre><code>_probabilistic_mode = 'binned'\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = max(\n    [\n        ws\n        for ws in [max_lag, max_size_window_features]\n        if ws is not None\n    ]\n)\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.window_features_class_names","title":"window_features_class_names  <code>instance-attribute</code>","text":"<pre><code>window_features_class_names = None\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.fit_kwargs","title":"fit_kwargs  <code>instance-attribute</code>","text":"<pre><code>fit_kwargs = check_select_fit_kwargs(\n    regressor=regressor, fit_kwargs=fit_kwargs\n)\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.binner","title":"binner  <code>instance-attribute</code>","text":"<pre><code>binner = {}\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.binner_intervals_","title":"binner_intervals_  <code>instance-attribute</code>","text":"<pre><code>binner_intervals_ = {}\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.binner_kwargs","title":"binner_kwargs  <code>instance-attribute</code>","text":"<pre><code>binner_kwargs = binner_kwargs\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _repr_html_(self):\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    (\n        params,\n        training_range_,\n        series_names_in_,\n        exog_names_in_,\n        transformer_series,\n    ) = self._preprocess_repr(\n            regressor          = self.regressor,\n            training_range_    = self.training_range_,\n            series_names_in_   = self.series_names_in_,\n            exog_names_in_     = self.exog_names_in_,\n            transformer_series = self.transformer_series,\n        )\n\n    style, unique_id = get_style_repr_html(self.is_fitted)\n\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Regressor:&lt;/strong&gt; {type(self.regressor).__name__}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window features:&lt;/strong&gt; {self.window_features_names}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Series encoding:&lt;/strong&gt; {self.encoding}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Weight function included:&lt;/strong&gt; {self.weight_func is not None}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Series weights:&lt;/strong&gt; {self.series_weights}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Differentiation order:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skforecast version:&lt;/strong&gt; {self.skforecast_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n            &lt;ul&gt;\n                {exog_names_in_}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Data Transformations&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for series:&lt;/strong&gt; {transformer_series}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Training Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Series names (levels):&lt;/strong&gt; {series_names_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {training_range_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Regressor Parameters&lt;/summary&gt;\n            &lt;ul&gt;\n                {params}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n            &lt;ul&gt;\n                {self.fit_kwargs}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursivemultiseries.html\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/independent-multi-time-series-forecasting.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return style + content\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._create_lags","title":"_create_lags","text":"<pre><code>_create_lags(y, X_as_pandas=False, train_index=None)\n</code></pre> <p>Create the lagged values and their target variable from a time series.</p> <p>Note that the returned matrix <code>X_data</code> contains the lag 1 in the first  column, the lag 2 in the in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>Training time series values.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_data</code> is a pandas DataFrame.</p> <code>False</code> <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_data</code> when <code>X_as_pandas</code> is <code>True</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray, pandas DataFrame, None</code> <p>Lagged values (predictors).</p> <code>y_data</code> <code>numpy ndarray</code> <p>Values of the time series related to each row of <code>X_data</code>.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _create_lags(\n    self,\n    y: np.ndarray,\n    X_as_pandas: bool = False,\n    train_index: pd.Index | None = None\n) -&gt; tuple[np.ndarray | pd.DataFrame | None, np.ndarray]:\n    \"\"\"\n    Create the lagged values and their target variable from a time series.\n\n    Note that the returned matrix `X_data` contains the lag 1 in the first \n    column, the lag 2 in the in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        Training time series values.\n    X_as_pandas : bool, default False\n        If `True`, the returned matrix `X_data` is a pandas DataFrame.\n    train_index : pandas Index, default None\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_data` when `X_as_pandas` is `True`.\n\n    Returns\n    -------\n    X_data : numpy ndarray, pandas DataFrame, None\n        Lagged values (predictors).\n    y_data : numpy ndarray\n        Values of the time series related to each row of `X_data`.\n\n    \"\"\"\n\n    X_data = None\n    if self.lags is not None:\n        n_rows = len(y) - self.window_size\n        X_data = np.full(\n            shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n        )\n        for i, lag in enumerate(self.lags):\n            X_data[:, i] = y[self.window_size - lag: -lag]\n\n        if X_as_pandas:\n            X_data = pd.DataFrame(\n                         data    = X_data,\n                         columns = self.lags_names,\n                         index   = train_index\n                     )\n\n    y_data = y[self.window_size:]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._create_window_features","title":"_create_window_features","text":"<pre><code>_create_window_features(y, train_index, X_as_pandas=False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>train_index</code> <code>pandas Index</code> <p>Index of the training data. It is used to create the pandas DataFrame <code>X_train_window_features</code> when <code>X_as_pandas</code> is <code>True</code>.</p> required <code>X_as_pandas</code> <code>bool</code> <p>If <code>True</code>, the returned matrix <code>X_train_window_features</code> is a  pandas DataFrame.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train_window_features</code> <code>list</code> <p>List of numpy ndarrays or pandas DataFrames with the window features.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _create_window_features(\n    self, \n    y: pd.Series,\n    train_index: pd.Index,\n    X_as_pandas: bool = False,\n) -&gt; tuple[list[np.ndarray | pd.DataFrame], list[str]]:\n    \"\"\"\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    train_index : pandas Index\n        Index of the training data. It is used to create the pandas DataFrame\n        `X_train_window_features` when `X_as_pandas` is `True`.\n    X_as_pandas : bool, default False\n        If `True`, the returned matrix `X_train_window_features` is a \n        pandas DataFrame.\n\n    Returns\n    -------\n    X_train_window_features : list\n        List of numpy ndarrays or pandas DataFrames with the window features.\n    X_train_window_features_names_out_ : list\n        Names of the window features.\n\n    \"\"\"\n\n    len_train_index = len(train_index)\n    X_train_window_features = []\n    X_train_window_features_names_out_ = []\n    for wf in self.window_features:\n        X_train_wf = wf.transform_batch(y)\n        if not isinstance(X_train_wf, pd.DataFrame):\n            raise TypeError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a pandas DataFrame.\"\n            )\n        X_train_wf = X_train_wf.iloc[-len_train_index:]\n        if not len(X_train_wf) == len_train_index:\n            raise ValueError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a DataFrame with the same number of rows as \"\n                f\"the input time series - `window_size`: {len_train_index}.\"\n            )\n        if not (X_train_wf.index == train_index).all():\n            raise ValueError(\n                f\"The method `transform_batch` of {type(wf).__name__} \"\n                f\"must return a DataFrame with the same index as \"\n                f\"the input time series - `window_size`.\"\n            )\n\n        X_train_window_features_names_out_.extend(X_train_wf.columns)\n        if not X_as_pandas:\n            X_train_wf = X_train_wf.to_numpy()     \n        X_train_window_features.append(X_train_wf)\n\n    return X_train_window_features, X_train_window_features_names_out_\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._create_train_X_y_single_series","title":"_create_train_X_y_single_series","text":"<pre><code>_create_train_X_y_single_series(y, ignore_exog, exog=None)\n</code></pre> <p>Create training matrices from univariate time series and exogenous variables. This method does not transform the exog variables.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>ignore_exog</code> <code>bool</code> <p>If <code>True</code>, <code>exog</code> is ignored.</p> required <code>exog</code> <code>pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train_lags</code> <code>pandas DataFrame</code> <p>Training values of lags. Shape: (len(y) - self.max_lag, len(self.lags))</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features.</p> <code>X_train_exog</code> <code>pandas DataFrame</code> <p>Training values of exogenous variables. Shape: (len(y) - self.max_lag, len(exog.columns))</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. Shape: (len(y) - self.max_lag, )</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _create_train_X_y_single_series(\n    self,\n    y: pd.Series,\n    ignore_exog: bool,\n    exog: pd.DataFrame | None = None\n) -&gt; tuple[pd.DataFrame, list[str], pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create training matrices from univariate time series and exogenous\n    variables. This method does not transform the exog variables.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    ignore_exog : bool\n        If `True`, `exog` is ignored.\n    exog : pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    X_train_lags : pandas DataFrame\n        Training values of lags.\n        Shape: (len(y) - self.max_lag, len(self.lags))\n    X_train_window_features_names_out_ : list\n        Names of the window features.\n    X_train_exog : pandas DataFrame\n        Training values of exogenous variables.\n        Shape: (len(y) - self.max_lag, len(exog.columns))\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n        Shape: (len(y) - self.max_lag, )\n\n    \"\"\"\n\n    series_name = y.name\n    if len(y) &lt;= self.window_size:\n        raise ValueError(\n            f\"Length of '{series_name}' must be greater than the maximum window size \"\n            f\"needed by the forecaster.\\n\"\n            f\"    Length '{series_name}': {len(y)}.\\n\"\n            f\"    Max window size: {self.window_size}.\\n\"\n            f\"    Lags window size: {self.max_lag}.\\n\"\n            f\"    Window features window size: {self.max_size_window_features}.\"\n        )\n\n    if self.encoding is None:\n        fit_transformer = False\n        transformer_series = self.transformer_series_['_unknown_level']\n    else:\n        fit_transformer = False if self.is_fitted else True\n        transformer_series = self.transformer_series_[series_name]\n\n    y_values = y.to_numpy()\n    y_index = y.index\n\n    y_values = transform_numpy(\n                   array             = y_values,\n                   transformer       = transformer_series,\n                   fit               = fit_transformer,\n                   inverse_transform = False\n               )\n\n    if self.differentiator_[series_name] is not None:\n        if not self.is_fitted:\n            y_values = self.differentiator_[series_name].fit_transform(y_values)\n        else:\n            differentiator = copy(self.differentiator_[series_name])\n            y_values = differentiator.fit_transform(y_values)\n\n    X_train_autoreg = []\n    train_index = y_index[self.window_size:]\n\n    X_train_lags, y_train = self._create_lags(\n        y=y_values, X_as_pandas=True, train_index=train_index\n    )\n    if X_train_lags is not None:\n        X_train_autoreg.append(X_train_lags)\n\n    X_train_window_features_names_out_ = None\n    if self.window_features is not None:\n        # NOTE: The first `self.differentiation_max` positions of `y_values`\n        # must be removed to match the length of `y_train` after creating\n        # the window features. This is because `y_train` is created using the \n        # global window size of the Forecaster, which includes the maximum \n        # differentiation (self.differentiation_max).\n        n_diff = 0 if self.differentiation is None else self.differentiation_max\n        y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n        X_train_window_features, X_train_window_features_names_out_ = (\n            self._create_window_features(\n                y=y_window_features, X_as_pandas=True, train_index=train_index\n            )\n        )\n        X_train_autoreg.extend(X_train_window_features)\n\n    if len(X_train_autoreg) == 1:\n        X_train_autoreg = X_train_autoreg[0]\n    else:\n        X_train_autoreg = pd.concat(X_train_autoreg, axis=1)\n\n    X_train_autoreg['_level_skforecast'] = series_name\n\n    if ignore_exog:\n        X_train_exog = None\n    else:\n        if exog is not None:\n            # The first `self.window_size` positions have to be removed from exog\n            # since they are not in X_train_autoreg.\n            X_train_exog = exog.iloc[self.window_size:, ]\n        else:\n            # NOTE: This is faster than creating a pandas Series without values.\n            X_train_exog = pd.Series(\n                               data  = np.nan,\n                               index = train_index,\n                               name  = '_dummy_exog_col_to_keep_shape'\n                           )\n\n    y_train = pd.Series(\n                  data  = y_train,\n                  index = train_index,\n                  name  = 'y'\n              )\n\n    return X_train_autoreg, X_train_window_features_names_out_, X_train_exog, y_train\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._create_train_X_y","title":"_create_train_X_y","text":"<pre><code>_create_train_X_y(\n    series, exog=None, store_last_window=True\n)\n</code></pre> <p>Create training matrices from multiple time series and exogenous variables. See Notes section for more details depending on the type of <code>series</code> and <code>exog</code>.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>store_last_window</code> <code>(bool, list)</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <ul> <li>If <code>True</code>, last window is stored for all series. </li> <li>If <code>list</code>, last window is stored for the series present in the list.</li> <li>If <code>False</code>, last window is not stored.</li> </ul> <code>True</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values of the time series related to each row of <code>X_train</code>.</p> <code>series_indexes</code> <code>dict</code> <p>Dictionary with the index of each series.</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) provided by the user during training.</p> <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series (levels) included in the matrix <code>X_train</code> created internally for training. It can be different from <code>series_names_in_</code> if some series are dropped during the training process because of NaNs or because they are not present in the training period.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>X_train_window_features_names_out_</code> <code>list</code> <p>Names of the window features included in the matrix <code>X_train</code> created internally for training.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> <code>last_window_</code> <code>dict</code> <p>Last window of training data for each series. It stores the values  needed to predict the next <code>step</code> immediately after the training data.</p> Notes <ul> <li>If <code>series</code> is a wide-format pandas DataFrame, each column represents a different time series, and the index must be either a <code>DatetimeIndex</code> or  a <code>RangeIndex</code> with frequency or step size, as appropriate</li> <li>If <code>series</code> is a long-format pandas DataFrame with a MultiIndex, the  first level of the index must contain the series IDs, and the second  level must be a <code>DatetimeIndex</code> with the same frequency across all series.</li> <li>If series is a dictionary, each key must be a series ID, and each value  must be a named pandas Series. All series must have the same index, which  must be either a <code>DatetimeIndex</code> or a <code>RangeIndex</code>, and they must share the  same frequency or step size, as appropriate.</li> <li>If <code>exog</code> is a wide-format pandas DataFrame, it must share the same  index type as series. Each column represents a different exogenous variable,  and the same values are applied to all time series.</li> <li>If <code>exog</code> is a long-format pandas Series or DataFrame with a MultiIndex,  the first level contains the series IDs to which it belongs, and the second  level must be a pandas <code>DatetimeIndex</code>. Each exogenous variable must be  represented as a separate column.</li> <li>If <code>exog</code> is a dictionary, each key must correspond to a series ID, and  each value must be either a named pandas <code>Series</code> or <code>DataFrame</code> with the  same index type as <code>series</code>, or <code>None</code>. It is not required for all series  to contain all exogenous variables, but data types must be consistent  across series for each variable.</li> </ul> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _create_train_X_y(\n    self,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    store_last_window: bool | list[str] = True,\n) -&gt; tuple[\n    pd.DataFrame,\n    pd.Series,\n    dict[str, pd.Index],\n    list[str],\n    list[str],\n    list[str],\n    list[str],\n    list[str],\n    dict[str, type],\n    dict[str, type],\n    dict[str, pd.Series],\n]:\n    \"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. See Notes section for more details depending on the type of\n    `series` and `exog`.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    store_last_window : bool, list, default True\n        Whether or not to store the last window (`last_window_`) of training data.\n\n        - If `True`, last window is stored for all series. \n        - If `list`, last window is stored for the series present in the list.\n        - If `False`, last window is not stored.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values of the time series related to each row of `X_train`.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    series_names_in_ : list\n        Names of the series (levels) provided by the user during training.\n    X_train_series_names_in_ : list\n        Names of the series (levels) included in the matrix `X_train` created\n        internally for training. It can be different from `series_names_in_` if\n        some series are dropped during the training process because of NaNs or\n        because they are not present in the training period.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training before the transformation\n        applied by `transformer_exog`. If `transformer_exog` is not used, it\n        is equal to `exog_dtypes_out_`.\n    exog_dtypes_out_ : dict\n        Type of each exogenous variable/s used in training after the transformation \n        applied by `transformer_exog`. If `transformer_exog` is not used, it \n        is equal to `exog_dtypes_in_`.\n    last_window_ : dict\n        Last window of training data for each series. It stores the values \n        needed to predict the next `step` immediately after the training data.\n\n    Notes\n    -----\n    - If `series` is a wide-format pandas DataFrame, each column represents a\n    different time series, and the index must be either a `DatetimeIndex` or \n    a `RangeIndex` with frequency or step size, as appropriate\n    - If `series` is a long-format pandas DataFrame with a MultiIndex, the \n    first level of the index must contain the series IDs, and the second \n    level must be a `DatetimeIndex` with the same frequency across all series.\n    - If series is a dictionary, each key must be a series ID, and each value \n    must be a named pandas Series. All series must have the same index, which \n    must be either a `DatetimeIndex` or a `RangeIndex`, and they must share the \n    same frequency or step size, as appropriate.\n    - If `exog` is a wide-format pandas DataFrame, it must share the same \n    index type as series. Each column represents a different exogenous variable, \n    and the same values are applied to all time series.\n    - If `exog` is a long-format pandas Series or DataFrame with a MultiIndex, \n    the first level contains the series IDs to which it belongs, and the second \n    level must be a pandas `DatetimeIndex`. Each exogenous variable must be \n    represented as a separate column.\n    - If `exog` is a dictionary, each key must correspond to a series ID, and \n    each value must be either a named pandas `Series` or `DataFrame` with the \n    same index type as `series`, or `None`. It is not required for all series \n    to contain all exogenous variables, but data types must be consistent \n    across series for each variable.\n\n    \"\"\"\n\n    series_dict, series_indexes = check_preprocess_series(series=series)\n    series_names_in_ = list(series_dict.keys())\n\n    if self.is_fitted and not set(series_names_in_).issubset(set(self.series_names_in_)):\n        raise ValueError(\n            f\"Once the Forecaster has been trained, `series` must contain \"\n            f\"the same series names as those used during training:\\n\"\n            f\" Got      : {series_names_in_}\\n\"\n            f\" Expected : {self.series_names_in_}\"\n        )\n\n    exog_dict = {serie: None for serie in series_names_in_}\n    exog_names_in_ = None\n    if exog is not None:\n        exog_dict, exog_names_in_ = check_preprocess_exog_multiseries(\n                                        series_names_in_  = series_names_in_,\n                                        series_index_type = type(series_indexes[series_names_in_[0]]),\n                                        exog              = exog,\n                                        exog_dict         = exog_dict\n                                    )\n\n        if self.is_fitted:\n            if self.exog_names_in_ is None:\n                raise ValueError(\n                    \"Once the Forecaster has been trained, `exog` must be `None` \"\n                    \"because no exogenous variables were added during training.\"\n                )\n            else:\n                if not set(exog_names_in_) == set(self.exog_names_in_):\n                    raise ValueError(\n                        f\"Once the Forecaster has been trained, `exog` must contain \"\n                        f\"the same exogenous variables as those used during training:\\n\"\n                        f\" Got      : {exog_names_in_}\\n\"\n                        f\" Expected : {self.exog_names_in_}\"\n                    )\n\n    if not self.is_fitted:\n        self.transformer_series_ = initialize_transformer_series(\n                                       forecaster_name    = type(self).__name__,\n                                       series_names_in_   = series_names_in_,\n                                       encoding           = self.encoding,\n                                       transformer_series = self.transformer_series\n                                   )\n\n        self.differentiator_ = initialize_differentiator_multiseries(\n                                   series_names_in_ = series_names_in_,\n                                   differentiator   = self.differentiator\n                               )\n\n    series_dict, exog_dict = align_series_and_exog_multiseries(\n                                 series_dict = series_dict,\n                                 exog_dict   = exog_dict\n                             )\n\n    if not self.is_fitted and self.transformer_series_['_unknown_level'] is not None:\n        self.transformer_series_['_unknown_level'].fit(\n            np.concatenate(list(series_dict.values())).reshape(-1, 1)\n        )\n\n    ignore_exog = True if exog is None else False\n    input_matrices = [\n        [series_dict[k], exog_dict[k], ignore_exog]\n         for k in series_dict.keys()\n    ]\n\n    X_train_autoreg_buffer = []\n    X_train_exog_buffer = []\n    y_train_buffer = []\n    for matrices in input_matrices:\n\n        (\n            X_train_autoreg,\n            X_train_window_features_names_out_,\n            X_train_exog,\n            y_train\n        ) = self._create_train_X_y_single_series(\n            y           = matrices[0],\n            exog        = matrices[1],\n            ignore_exog = matrices[2],\n        )\n\n        X_train_autoreg_buffer.append(X_train_autoreg)\n        X_train_exog_buffer.append(X_train_exog)\n        y_train_buffer.append(y_train)\n\n    X_train = pd.concat(X_train_autoreg_buffer, axis=0, copy=False)\n    y_train = pd.concat(y_train_buffer, axis=0, copy=False)\n\n    if self.is_fitted:\n        encoded_values = self.encoder.transform(X_train[['_level_skforecast']])\n    else:\n        encoded_values = self.encoder.fit_transform(X_train[['_level_skforecast']])\n        for i, code in enumerate(self.encoder.categories_[0]):\n            self.encoding_mapping_[code] = i\n\n    if self.encoding == 'onehot': \n        encoded_values.columns = encoded_values.columns.str.replace('_level_skforecast_', '')\n        X_train = pd.concat([\n            X_train.drop(columns='_level_skforecast'), encoded_values\n        ], axis=1, copy=False)\n    else:\n        X_train['_level_skforecast'] = encoded_values\n\n    if self.encoding == 'ordinal_category':\n        X_train['_level_skforecast'] = (\n            X_train['_level_skforecast'].astype('category')\n        )\n\n    del encoded_values\n\n    exog_dtypes_in_ = None\n    exog_dtypes_out_ = None\n    X_train_exog_names_out_ = None\n    if exog is not None:\n\n        X_train_exog = pd.concat(X_train_exog_buffer, axis=0, copy=False)\n\n        if isinstance(X_train_exog, pd.Series):\n            warnings.warn(\n                f\"No exogenous variables were found in `exog` that match the \"\n                f\"series IDs provided in `series`. As a result, no exogenous \"\n                f\"variables are included in the training matrices. Please \"\n                f\"review the series IDs in `exog` and ensure they match the \"\n                f\"following IDs: {series_names_in_}. The forecaster will be \"\n                f\"considered trained without exogenous variables.\",\n                MissingExogWarning\n            )\n        else:\n            if '_dummy_exog_col_to_keep_shape' in X_train_exog.columns:\n                X_train_exog = (\n                    X_train_exog.drop(columns=['_dummy_exog_col_to_keep_shape'])\n                )\n\n            exog_names_in_ = X_train_exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=X_train_exog)\n\n            fit_transformer = False if self.is_fitted else True\n            X_train_exog = transform_dataframe(\n                               df                = X_train_exog,\n                               transformer       = self.transformer_exog,\n                               fit               = fit_transformer,\n                               inverse_transform = False\n                           )\n\n            if not (X_train_exog.index == X_train.index).all():\n                raise ValueError(\n                    \"Different index for `series` and `exog` after transformation. \"\n                    \"They must be equal to ensure the correct alignment of values.\"\n                )\n\n            check_exog_dtypes(X_train_exog, call_check_exog=False)\n            exog_dtypes_out_ = get_exog_dtypes(exog=X_train_exog)\n            X_train_exog_names_out_ = X_train_exog.columns.to_list()\n            X_train = pd.concat([X_train, X_train_exog], axis=1, copy=False)\n\n    # TODO: replace with y_train.hasnans\n    if y_train.isna().to_numpy().any():\n        mask = y_train.notna().to_numpy()\n        y_train = y_train.iloc[mask]\n        X_train = X_train.iloc[mask,]\n        warnings.warn(\n            \"NaNs detected in `y_train`. They have been dropped because the \"\n            \"target variable cannot have NaN values. Same rows have been \"\n            \"dropped from `X_train` to maintain alignment. This is caused by \"\n            \"series with interspersed NaNs.\",\n            MissingValuesWarning\n        )\n\n    if self.dropna_from_series:\n        # TODO: replace with X_train.hasnans()\n        if np.any(X_train.isnull().to_numpy()):\n            mask = X_train.notna().all(axis=1).to_numpy()\n            X_train = X_train.iloc[mask, ]\n            y_train = y_train.iloc[mask]\n            warnings.warn(\n                \"NaNs detected in `X_train`. They have been dropped. If \"\n                \"you want to keep them, set `forecaster.dropna_from_series = False`. \"\n                \"Same rows have been removed from `y_train` to maintain alignment. \"\n                \"This caused by series with interspersed NaNs.\",\n                MissingValuesWarning\n            )\n    else:\n        # TODO: replace with X_train.hasnans()\n        if np.any(X_train.isnull().to_numpy()):\n            warnings.warn(\n                \"NaNs detected in `X_train`. Some regressors do not allow \"\n                \"NaN values during training. If you want to drop them, \"\n                \"set `forecaster.dropna_from_series = True`.\",\n                MissingValuesWarning\n            )\n\n    if X_train.empty:\n        raise ValueError(\n            \"All samples have been removed due to NaNs. Set \"\n            \"`forecaster.dropna_from_series = False` or review `exog` values.\"\n        )\n\n    if self.encoding == 'onehot':\n        X_train_series_names_in_ = [\n            col for col in series_names_in_ if X_train[col].sum() &gt; 0\n        ]\n    else:\n        unique_levels = X_train['_level_skforecast'].unique()\n        X_train_series_names_in_ = [\n            k for k, v in self.encoding_mapping_.items()\n            if v in unique_levels\n        ]\n\n    # The last time window of training data is stored so that lags needed as\n    # predictors in the first iteration of `predict()` can be calculated.\n    last_window_ = None\n    if store_last_window:\n\n        series_to_store = (\n            X_train_series_names_in_ if store_last_window is True else store_last_window\n        )\n\n        series_not_in_series_dict = set(series_to_store) - set(X_train_series_names_in_)\n        if series_not_in_series_dict:\n            warnings.warn(\n                f\"Series {series_not_in_series_dict} are not present in \"\n                f\"`series`. No last window is stored for them.\",\n                IgnoredArgumentWarning\n            )\n            series_to_store = [\n                s for s in series_to_store \n                if s not in series_not_in_series_dict\n            ]\n\n        if series_to_store:\n            last_window_ = {\n                k: v.iloc[-self.window_size:].copy()\n                for k, v in series_dict.items()\n                if k in series_to_store\n            }\n\n    return (\n        X_train,\n        y_train,\n        series_indexes,\n        series_names_in_,\n        X_train_series_names_in_,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_,\n        last_window_\n    )\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.create_train_X_y","title":"create_train_X_y","text":"<pre><code>create_train_X_y(\n    series, exog=None, suppress_warnings=False\n)\n</code></pre> <p>Create training matrices from multiple time series and exogenous variables. See Notes section for more details depending on the type of <code>series</code> and <code>exog</code>.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the creation of the training matrices. See skforecast.exceptions.warn_skforecast_categories  for more information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Training values (predictors).</p> <code>y_train</code> <code>pandas Series</code> <p>Values (target) of the time series related to each row of <code>X_train</code>.</p> Notes <ul> <li>If <code>series</code> is a wide-format pandas DataFrame, each column represents a different time series, and the index must be either a <code>DatetimeIndex</code> or  a <code>RangeIndex</code> with frequency or step size, as appropriate</li> <li>If <code>series</code> is a long-format pandas DataFrame with a MultiIndex, the  first level of the index must contain the series IDs, and the second  level must be a <code>DatetimeIndex</code> with the same frequency across all series.</li> <li>If series is a dictionary, each key must be a series ID, and each value  must be a named pandas Series. All series must have the same index, which  must be either a <code>DatetimeIndex</code> or a <code>RangeIndex</code>, and they must share the  same frequency or step size, as appropriate.</li> <li>If <code>exog</code> is a wide-format pandas DataFrame, it must share the same  index type as series. Each column represents a different exogenous variable,  and the same values are applied to all time series.</li> <li>If <code>exog</code> is a long-format pandas Series or DataFrame with a MultiIndex,  the first level contains the series IDs to which it belongs, and the second  level must be a pandas <code>DatetimeIndex</code>. Each exogenous variable must be  represented as a separate column.</li> <li>If <code>exog</code> is a dictionary, each key must correspond to a series ID, and  each value must be either a named pandas <code>Series</code> or <code>DataFrame</code> with the  same index type as <code>series</code>, or <code>None</code>. It is not required for all series  to contain all exogenous variables, but data types must be consistent  across series for each variable.</li> </ul> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    suppress_warnings: bool = False\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Create training matrices from multiple time series and exogenous\n    variables. See Notes section for more details depending on the type of\n    `series` and `exog`.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the creation\n        of the training matrices. See skforecast.exceptions.warn_skforecast_categories \n        for more information.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training values (predictors).\n    y_train : pandas Series\n        Values (target) of the time series related to each row of `X_train`.\n\n    Notes\n    -----\n    - If `series` is a wide-format pandas DataFrame, each column represents a\n    different time series, and the index must be either a `DatetimeIndex` or \n    a `RangeIndex` with frequency or step size, as appropriate\n    - If `series` is a long-format pandas DataFrame with a MultiIndex, the \n    first level of the index must contain the series IDs, and the second \n    level must be a `DatetimeIndex` with the same frequency across all series.\n    - If series is a dictionary, each key must be a series ID, and each value \n    must be a named pandas Series. All series must have the same index, which \n    must be either a `DatetimeIndex` or a `RangeIndex`, and they must share the \n    same frequency or step size, as appropriate.\n    - If `exog` is a wide-format pandas DataFrame, it must share the same \n    index type as series. Each column represents a different exogenous variable, \n    and the same values are applied to all time series.\n    - If `exog` is a long-format pandas Series or DataFrame with a MultiIndex, \n    the first level contains the series IDs to which it belongs, and the second \n    level must be a pandas `DatetimeIndex`. Each exogenous variable must be \n    represented as a separate column.\n    - If `exog` is a dictionary, each key must correspond to a series ID, and \n    each value must be either a named pandas `Series` or `DataFrame` with the \n    same index type as `series`, or `None`. It is not required for all series \n    to contain all exogenous variables, but data types must be consistent \n    across series for each variable.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    output = self._create_train_X_y(\n                 series            = series, \n                 exog              = exog, \n                 store_last_window = False\n             )\n\n    X_train = output[0]\n    y_train = output[1]\n\n    if self.encoding is None:\n        X_train = X_train.drop(columns='_level_skforecast')\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._train_test_split_one_step_ahead","title":"_train_test_split_one_step_ahead","text":"<pre><code>_train_test_split_one_step_ahead(\n    series, initial_train_size, exog=None\n)\n</code></pre> <p>Create matrices needed to train and test the forecaster for one-step-ahead predictions.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>dict</code> <p>Training time series (already checked and preprocessed as a dict).</p> required <code>initial_train_size</code> <code>int</code> <p>Initial size of the training set. It is the number of observations used to train the forecaster before making the first prediction.</p> required <code>exog</code> <code>dict</code> <p>Exogenous variable/s included as predictor/s (already checked and  preprocessed as a dict).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>pandas DataFrame</code> <p>Predictor values used to train the model.</p> <code>y_train</code> <code>pandas Series</code> <p>Target values related to each row of <code>X_train</code>.</p> <code>X_test</code> <code>pandas DataFrame</code> <p>Predictor values used to test the model.</p> <code>y_test</code> <code>pandas Series</code> <p>Target values related to each row of <code>X_test</code>.</p> <code>X_train_encoding</code> <code>pandas Series</code> <p>Series identifiers for each row of <code>X_train</code>.</p> <code>X_test_encoding</code> <code>pandas Series</code> <p>Series identifiers for each row of <code>X_test</code>.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _train_test_split_one_step_ahead(\n    self,\n    series: dict[str, pd.Series],\n    initial_train_size: int,\n    exog: dict[str, pd.DataFrame | None] | None = None\n) -&gt; tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n    \"\"\"\n    Create matrices needed to train and test the forecaster for one-step-ahead\n    predictions.\n\n    Parameters\n    ----------\n    series : dict\n        Training time series (already checked and preprocessed as a dict).\n    initial_train_size : int\n        Initial size of the training set. It is the number of observations used\n        to train the forecaster before making the first prediction.\n    exog : dict, default None\n        Exogenous variable/s included as predictor/s (already checked and \n        preprocessed as a dict).\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n\n    \"\"\"\n\n    # NOTE: `series` and `exog` are assumed to be an already checked dict as\n    # they have gone through the `check_one_step_ahead_input` function.\n    min_index = []\n    max_index = []\n    for v in series.values():\n        idx = v.index\n        min_index.append(idx[0])\n        max_index.append(idx[-1])\n\n    if isinstance(idx, pd.DatetimeIndex):\n        span_index = pd.date_range(\n            start=min(min_index), end=max(max_index), freq=idx.freqstr\n        )\n    else:\n        span_index = pd.RangeIndex(\n            start=min(min_index), stop=max(max_index) + 1, step=idx.step\n        )\n\n    fold = [\n        [0, initial_train_size],\n        [initial_train_size - self.window_size, initial_train_size],\n        [initial_train_size - self.window_size, len(span_index)],\n        [0, 0],  # Dummy value\n        True\n    ]\n    data_fold = _extract_data_folds_multiseries(\n                    series             = series,\n                    folds              = [fold],\n                    span_index         = span_index,\n                    window_size        = self.window_size,\n                    exog               = exog,\n                    dropna_last_window = self.dropna_from_series,\n                    externally_fitted  = False\n                )\n    series_train, _, levels_last_window, exog_train, exog_test, _ = next(data_fold)\n\n    start_test_date = span_index[initial_train_size - self.window_size]\n    series_test = {\n        k: v.loc[start_test_date:]\n        for k, v in series.items()\n        if k in levels_last_window and not v.empty and not v.isna().to_numpy().all()\n    }\n\n    forecaster_state = (self.is_fitted, self.series_names_in_, self.exog_names_in_)\n\n    self.is_fitted = False\n    X_train, y_train, _, series_names_in_, _, exog_names_in_, *_ = (\n        self._create_train_X_y(\n            series            = series_train,\n            exog              = exog_train,\n            store_last_window = False\n        )\n    )\n\n    self.series_names_in_ = series_names_in_\n    if exog is not None:\n        self.exog_names_in_ = exog_names_in_\n    self.is_fitted = True\n\n    X_test, y_test, *_ = self._create_train_X_y(\n                             series            = series_test,\n                             exog              = exog_test,\n                             store_last_window = False\n                         )\n\n    self.is_fitted, self.series_names_in_, self.exog_names_in_ = forecaster_state\n\n    if self.encoding in [\"ordinal\", \"ordinal_category\"]:\n        X_train_encoding = self.encoder.inverse_transform(\n            X_train[[\"_level_skforecast\"]]\n        ).ravel()\n        X_test_encoding = self.encoder.inverse_transform(\n            X_test[[\"_level_skforecast\"]]\n        ).ravel()\n    elif self.encoding == 'onehot':\n        encoding_keys = self.encoding_mapping_.keys()\n        X_train_encoding = self.encoder.inverse_transform(\n            X_train.loc[:, encoding_keys]\n        ).ravel()\n        X_test_encoding = self.encoder.inverse_transform(\n            X_test.loc[:, encoding_keys]\n        ).ravel()\n    else:\n        X_train_encoding = self.encoder.inverse_transform(\n            X_train[[\"_level_skforecast\"]]\n        ).ravel()\n        X_test_encoding = self.encoder.inverse_transform(\n            X_test[[\"_level_skforecast\"]]\n        ).ravel()\n        X_train = X_train.drop(columns=\"_level_skforecast\")\n        X_test = X_test.drop(columns=\"_level_skforecast\")\n\n    X_train_encoding = pd.Series(\n        data=X_train_encoding, index=X_train.index\n    ).fillna(\"_unknown_level\")\n    X_test_encoding = pd.Series(\n        data=X_test_encoding, index=X_test.index\n    ).fillna(\"_unknown_level\")\n\n    return X_train, y_train, X_test, y_test, X_train_encoding, X_test_encoding\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._weight_func_all_1","title":"_weight_func_all_1","text":"<pre><code>_weight_func_all_1(index)\n</code></pre> <p>Weight function that assigns a weight of 1 to all observations.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>pandas Index</code> <p>Index of the series.</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _weight_func_all_1(\n    self, \n    index: pd.Index\n) -&gt; np.ndarray:\n    \"\"\"\n    Weight function that assigns a weight of 1 to all observations.\n\n    Parameters\n    ----------\n    index : pandas Index\n        Index of the series.\n\n    Returns\n    -------\n    weights : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    weights = np.ones(len(index), dtype=float)\n\n    return weights\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.create_sample_weights","title":"create_sample_weights","text":"<pre><code>create_sample_weights(series_names_in_, X_train)\n</code></pre> <p>Create weights for each observation according to the forecaster's attributes <code>series_weights</code> and <code>weight_func</code>. The resulting weights are product of both types of weights.</p> <p>Parameters:</p> Name Type Description Default <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) used during training.</p> required <code>X_train</code> <code>pandas DataFrame</code> <p>Dataframe created with the <code>create_train_X_y</code> method, first return.</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>numpy ndarray</code> <p>Weights to use in <code>fit</code> method.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def create_sample_weights(\n    self,\n    series_names_in_: list,\n    X_train: pd.DataFrame\n) -&gt; np.ndarray:\n    \"\"\"\n    Create weights for each observation according to the forecaster's attributes\n    `series_weights` and `weight_func`. The resulting weights are product of both\n    types of weights.\n\n    Parameters\n    ----------\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    X_train : pandas DataFrame\n        Dataframe created with the `create_train_X_y` method, first return.\n\n    Returns\n    -------\n    weights : numpy ndarray\n        Weights to use in `fit` method.\n\n    \"\"\"\n\n    weights = None\n    weights_samples = None\n    series_weights = None\n\n    if self.series_weights is not None:\n        # Series not present in series_weights have a weight of 1 in all their samples.\n        # Keys in series_weights not present in series are ignored.\n        series_not_in_series_weights = (\n            set(series_names_in_) - set(self.series_weights.keys())\n        )\n        if series_not_in_series_weights:\n            warnings.warn(\n                f\"{series_not_in_series_weights} not present in `series_weights`. \"\n                f\"A weight of 1 is given to all their samples.\",\n                IgnoredArgumentWarning\n            )\n        self.series_weights_ = {col: 1. for col in series_names_in_}\n        self.series_weights_.update(\n            {\n                k: v\n                for k, v in self.series_weights.items()\n                if k in self.series_weights_\n            }\n        )\n\n        if self.encoding == \"onehot\":\n            series_weights = [\n                np.repeat(self.series_weights_[serie], sum(X_train[serie]))\n                for serie in series_names_in_\n            ]\n        else:\n            series_weights = [\n                np.repeat(\n                    self.series_weights_[serie],\n                    sum(X_train[\"_level_skforecast\"] == self.encoding_mapping_[serie]),\n                )\n                for serie in series_names_in_\n            ]\n\n        series_weights = np.concatenate(series_weights)\n\n    if self.weight_func is not None:\n        if isinstance(self.weight_func, Callable):\n            self.weight_func_ = {\n                col: copy(self.weight_func) for col in series_names_in_\n            }\n        else:\n            # Series not present in weight_func have a weight of 1 in all their samples\n            series_not_in_weight_func = (\n                set(series_names_in_) - set(self.weight_func.keys())\n            )\n            if series_not_in_weight_func:\n                warnings.warn(\n                    f\"{series_not_in_weight_func} not present in `weight_func`. \"\n                    f\"A weight of 1 is given to all their samples.\",\n                    IgnoredArgumentWarning\n                )\n            self.weight_func_ = {\n                col: self._weight_func_all_1 for col in series_names_in_\n            }\n            self.weight_func_.update(\n                {\n                    k: v\n                    for k, v in self.weight_func.items()\n                    if k in self.weight_func_\n                }\n            )\n\n        weights_samples = []\n        for key in self.weight_func_.keys():\n            if self.encoding == \"onehot\":\n                idx = X_train.index[X_train[key] == 1.0]\n            else:\n                idx = X_train.index[\n                    X_train[\"_level_skforecast\"] == self.encoding_mapping_[key]\n                ]\n            weights_samples.append(self.weight_func_[key](idx))\n        weights_samples = np.concatenate(weights_samples)\n\n    if series_weights is not None:\n        weights = series_weights\n        if weights_samples is not None:\n            weights = weights * weights_samples\n    else:\n        if weights_samples is not None:\n            weights = weights_samples\n\n    if weights is not None:\n        if np.isnan(weights).any():\n            raise ValueError(\n                \"The resulting `weights` cannot have NaN values.\"\n            )\n        if np.any(weights &lt; 0):\n            raise ValueError(\n                \"The resulting `weights` cannot have negative values.\"\n            )\n        if np.sum(weights) == 0:\n            raise ValueError(\n                \"The resulting `weights` cannot be normalized because \"\n                \"the sum of the weights is zero.\"\n            )\n\n    return weights\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.fit","title":"fit","text":"<pre><code>fit(\n    series,\n    exog=None,\n    store_last_window=True,\n    store_in_sample_residuals=False,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Training Forecaster. See Notes section for more details depending on  the type of <code>series</code> and <code>exog</code>.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>store_last_window</code> <code>(bool, list)</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <ul> <li>If <code>True</code>, last window is stored for all series. </li> <li>If <code>list</code>, last window is stored for the series present in the list.</li> <li>If <code>False</code>, last window is not stored.</li> </ul> <code>True</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the training  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Notes <ul> <li>If <code>series</code> is a wide-format pandas DataFrame, each column represents a different time series, and the index must be either a <code>DatetimeIndex</code> or  a <code>RangeIndex</code> with frequency or step size, as appropriate</li> <li>If <code>series</code> is a long-format pandas DataFrame with a MultiIndex, the  first level of the index must contain the series IDs, and the second  level must be a <code>DatetimeIndex</code> with the same frequency across all series.</li> <li>If series is a dictionary, each key must be a series ID, and each value  must be a named pandas Series. All series must have the same index, which  must be either a <code>DatetimeIndex</code> or a <code>RangeIndex</code>, and they must share the  same frequency or step size, as appropriate.</li> <li>If <code>exog</code> is a wide-format pandas DataFrame, it must share the same  index type as series. Each column represents a different exogenous variable,  and the same values are applied to all time series.</li> <li>If <code>exog</code> is a long-format pandas Series or DataFrame with a MultiIndex,  the first level contains the series IDs to which it belongs, and the second  level must be a pandas <code>DatetimeIndex</code>. Each exogenous variable must be  represented as a separate column.</li> <li>If <code>exog</code> is a dictionary, each key must correspond to a series ID, and  each value must be either a named pandas <code>Series</code> or <code>DataFrame</code> with the  same index type as <code>series</code>, or <code>None</code>. It is not required for all series  to contain all exogenous variables, but data types must be consistent  across series for each variable.</li> </ul> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    store_last_window: bool | list[str] = True,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Training Forecaster. See Notes section for more details depending on \n    the type of `series` and `exog`.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    store_last_window : bool, list, default True\n        Whether or not to store the last window (`last_window_`) of training data.\n\n        - If `True`, last window is stored for all series. \n        - If `list`, last window is stored for the series present in the list.\n        - If `False`, last window is not stored.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the training \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    - If `series` is a wide-format pandas DataFrame, each column represents a\n    different time series, and the index must be either a `DatetimeIndex` or \n    a `RangeIndex` with frequency or step size, as appropriate\n    - If `series` is a long-format pandas DataFrame with a MultiIndex, the \n    first level of the index must contain the series IDs, and the second \n    level must be a `DatetimeIndex` with the same frequency across all series.\n    - If series is a dictionary, each key must be a series ID, and each value \n    must be a named pandas Series. All series must have the same index, which \n    must be either a `DatetimeIndex` or a `RangeIndex`, and they must share the \n    same frequency or step size, as appropriate.\n    - If `exog` is a wide-format pandas DataFrame, it must share the same \n    index type as series. Each column represents a different exogenous variable, \n    and the same values are applied to all time series.\n    - If `exog` is a long-format pandas Series or DataFrame with a MultiIndex, \n    the first level contains the series IDs to which it belongs, and the second \n    level must be a pandas `DatetimeIndex`. Each exogenous variable must be \n    represented as a separate column.\n    - If `exog` is a dictionary, each key must correspond to a series ID, and \n    each value must be either a named pandas `Series` or `DataFrame` with the \n    same index type as `series`, or `None`. It is not required for all series \n    to contain all exogenous variables, but data types must be consistent \n    across series for each variable.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    # TODO: create a method reset_forecaster() to reset all attributes\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_                       = None\n    self.index_type_                        = None\n    self.index_freq_                        = None\n    self.training_range_                    = None\n    self.series_names_in_                   = None\n    self.exog_in_                           = False\n    self.exog_names_in_                     = None\n    self.exog_type_in_                      = None\n    self.exog_dtypes_in_                    = None\n    self.exog_dtypes_out_                   = None\n    self.X_train_series_names_in_           = None\n    self.X_train_window_features_names_out_ = None\n    self.X_train_exog_names_out_            = None\n    self.X_train_features_names_out_        = None\n    self.in_sample_residuals_               = None\n    self.in_sample_residuals_by_bin_        = None\n    self.binner                             = {}\n    self.binner_intervals_                  = {}\n    self.is_fitted                          = False\n    self.fit_date                           = None\n\n    (\n        X_train,\n        y_train,\n        series_indexes,\n        series_names_in_,\n        X_train_series_names_in_,\n        exog_names_in_,\n        X_train_window_features_names_out_,\n        X_train_exog_names_out_,\n        exog_dtypes_in_,\n        exog_dtypes_out_,\n        last_window_\n    ) = self._create_train_X_y(\n            series=series, exog=exog, store_last_window=store_last_window\n        )\n\n    sample_weight = self.create_sample_weights(\n                        series_names_in_ = series_names_in_,\n                        X_train          = X_train\n                    )\n\n    X_train_regressor = (\n        X_train\n        if self.encoding is not None\n        else X_train.drop(columns=\"_level_skforecast\")\n    )\n    if sample_weight is not None:\n        self.regressor.fit(\n            X             = X_train_regressor,\n            y             = y_train,\n            sample_weight = sample_weight,\n            **self.fit_kwargs\n        )\n    else:\n        self.regressor.fit(X=X_train_regressor, y=y_train, **self.fit_kwargs)\n\n    self.series_names_in_ = series_names_in_\n    self.X_train_series_names_in_ = X_train_series_names_in_\n    self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n    self.X_train_features_names_out_ = X_train_regressor.columns.to_list()\n\n    self.is_fitted = True\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = {k: v[[0, -1]] for k, v in series_indexes.items()}\n    self.index_type_ = type(series_indexes[series_names_in_[0]])\n    if isinstance(series_indexes[series_names_in_[0]], pd.DatetimeIndex):\n        self.index_freq_ = series_indexes[series_names_in_[0]].freqstr\n    else:\n        self.index_freq_ = series_indexes[series_names_in_[0]].step\n\n    # NOTE: When `exog` doesn't match series IDs, exogs are not included in the\n    # training matrices, X_train_exog_names_out_ is None and the forecaster \n    # will be considered trained without exogenous variables.\n    if exog is not None and X_train_exog_names_out_ is not None:\n        self.exog_in_ = True\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog)\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n    self.in_sample_residuals_ = {}\n    self.in_sample_residuals_by_bin_ = {}\n    if self._probabilistic_mode is not False:\n        y_train = y_train.to_numpy()\n        y_pred = self.regressor.predict(X_train_regressor)\n        if self.encoding is not None:\n            for level in X_train_series_names_in_:\n                if self.encoding == 'onehot':\n                    mask = X_train[level].to_numpy() == 1.\n                else:\n                    encoded_value = self.encoding_mapping_[level]\n                    mask = X_train['_level_skforecast'].to_numpy() == encoded_value\n\n                self._binning_in_sample_residuals(\n                    level                     = level,\n                    y_true                    = y_train[mask],\n                    y_pred                    = y_pred[mask],\n                    store_in_sample_residuals = store_in_sample_residuals,\n                    random_state              = random_state\n                )\n\n        # NOTE: the _unknown_level is a random sample of 10_000 residuals of all levels.\n        self._binning_in_sample_residuals(\n            level                     = '_unknown_level',\n            y_true                    = y_train,\n            y_pred                    = y_pred,\n            store_in_sample_residuals = store_in_sample_residuals,\n            random_state              = random_state\n        )\n\n    if not store_in_sample_residuals:\n        # NOTE: create empty dictionaries to avoid errors when calling predict()\n        if self.encoding is not None:\n            for level in X_train_series_names_in_:\n                self.in_sample_residuals_[level] = None\n                self.in_sample_residuals_by_bin_[level] = None\n        self.in_sample_residuals_['_unknown_level'] = None\n        self.in_sample_residuals_by_bin_['_unknown_level'] = None\n\n    if store_last_window:\n        self.last_window_ = last_window_\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._binning_in_sample_residuals","title":"_binning_in_sample_residuals","text":"<pre><code>_binning_in_sample_residuals(\n    level,\n    y_true,\n    y_pred,\n    store_in_sample_residuals=False,\n    random_state=123,\n)\n</code></pre> <p>Bin residuals according to the predicted value each residual is associated with. First a <code>skforecast.preprocessing.QuantileBinner</code> object is fitted to the predicted values. Then, residuals are binned according to the predicted value each residual is associated with. Residuals are stored in the forecaster object as <code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code>.</p> <p><code>y_true</code> and <code>y_pred</code> assumed to be differentiated and/or transformed according to the attributes <code>differentiation</code> and <code>transformer_series</code>. The number of residuals stored per bin is limited to  <code>10_000 // self.binner.n_bins_</code>. The total number of residuals stored is <code>10_000</code>. New in version 0.15.0</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Name of the series (level) to store the residuals.</p> required <code>y_true</code> <code>numpy ndarray</code> <p>True values of the time series.</p> required <code>y_pred</code> <code>numpy ndarray</code> <p>Predicted values of the time series.</p> required <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> and <code>in_sample_residuals_by_bin_</code> attributes). If <code>False</code>, only the intervals of the bins are stored.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _binning_in_sample_residuals(\n    self,\n    level: str,\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Bin residuals according to the predicted value each residual is\n    associated with. First a `skforecast.preprocessing.QuantileBinner` object\n    is fitted to the predicted values. Then, residuals are binned according\n    to the predicted value each residual is associated with. Residuals are\n    stored in the forecaster object as `in_sample_residuals_` and\n    `in_sample_residuals_by_bin_`.\n\n    `y_true` and `y_pred` assumed to be differentiated and/or transformed\n    according to the attributes `differentiation` and `transformer_series`.\n    The number of residuals stored per bin is limited to \n    `10_000 // self.binner.n_bins_`. The total number of residuals stored is\n    `10_000`.\n    **New in version 0.15.0**\n\n    Parameters\n    ----------\n    level : str\n        Name of the series (level) to store the residuals.\n    y_true : numpy ndarray\n        True values of the time series.\n    y_pred : numpy ndarray\n        Predicted values of the time series.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` and `in_sample_residuals_by_bin_`\n        attributes).\n        If `False`, only the intervals of the bins are stored.\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    residuals = y_true - y_pred\n\n    if self._probabilistic_mode == \"binned\":\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        self.binner[level] = QuantileBinner(**self.binner_kwargs)\n        self.binner[level].fit(y_pred)\n        self.binner_intervals_[level] = self.binner[level].intervals_\n\n    if store_in_sample_residuals:\n        rng = np.random.default_rng(seed=random_state)\n        if self._probabilistic_mode == \"binned\":\n            data['bin'] = self.binner[level].transform(y_pred).astype(int)\n            self.in_sample_residuals_by_bin_[level] = (\n                data.groupby('bin')['residuals'].apply(np.array).to_dict()\n            )\n\n            max_sample = 10_000 // self.binner[level].n_bins_\n            for k, v in self.in_sample_residuals_by_bin_[level].items():\n                if len(v) &gt; max_sample:\n                    sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                    self.in_sample_residuals_by_bin_[level][k] = sample\n\n        if len(residuals) &gt; 10_000:\n            residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=10_000)\n            ]\n        self.in_sample_residuals_[level] = residuals\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._create_predict_inputs","title":"_create_predict_inputs","text":"<pre><code>_create_predict_inputs(\n    steps,\n    levels=None,\n    last_window=None,\n    exog=None,\n    predict_probabilistic=False,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    check_inputs=True,\n)\n</code></pre> <p>Create the inputs needed for the first iteration of the prediction  process. As this is a recursive process, the last window is updated at  each iteration of the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>predict_probabilistic</code> <code>bool</code> <p>If <code>True</code>, the necessary checks for probabilistic predictions will be  performed.</p> <code>False</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors needed in the first  iteration of the prediction (t + 1).</p> <code>exog_values_dict</code> <code>(dict, None)</code> <p>Exogenous variable/s included as predictor/s for each series in  each step. The keys are the steps and the values are numpy arrays where each column is an exog and each row a series (level).</p> <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> <code>prediction_index</code> <code>pandas Index</code> <p>Index of the predictions.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: int,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    predict_probabilistic: bool = False,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    check_inputs: bool = True\n) -&gt; tuple[pd.DataFrame, dict[str, np.ndarray] | None, list[str], pd.Index]:\n    \"\"\"\n    Create the inputs needed for the first iteration of the prediction \n    process. As this is a recursive process, the last window is updated at \n    each iteration of the prediction process.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    levels : str, list, default None\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    predict_probabilistic : bool, default False\n        If `True`, the necessary checks for probabilistic predictions will be \n        performed.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    last_window : pandas DataFrame\n        Series values used to create the predictors needed in the first \n        iteration of the prediction (t + 1).\n    exog_values_dict : dict, None\n        Exogenous variable/s included as predictor/s for each series in \n        each step. The keys are the steps and the values are numpy arrays\n        where each column is an exog and each row a series (level).\n    levels : list\n        Names of the series (levels) to be predicted.\n    prediction_index : pandas Index\n        Index of the predictions.\n\n    \"\"\"\n\n    input_levels_is_None = True if levels is None else False\n    levels, input_levels_is_list = prepare_levels_multiseries(\n        X_train_series_names_in_=self.X_train_series_names_in_, levels=levels\n    )\n\n    if self.is_fitted:\n        if last_window is None:\n            levels, last_window = preprocess_levels_self_last_window_multiseries(\n                                      levels               = levels,\n                                      input_levels_is_list = input_levels_is_list,\n                                      last_window_         = self.last_window_\n                                  )\n        else:\n            if input_levels_is_None and isinstance(last_window, pd.DataFrame):\n                levels = last_window.columns.to_list()\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)) and isinstance(exog.index, pd.MultiIndex):\n        if not isinstance(exog.index.levels[1], pd.DatetimeIndex):\n            raise TypeError(\n                f\"When `exog` is a pandas MultiIndex DataFrame, its index \"\n                f\"must be a pandas DatetimeIndex. If you want to use a pandas \"\n                f\"RangeIndex, use a dictionary instead. Found `exog` index \"\n                f\"type: {type(exog.index.levels[1])}.\"\n            )\n\n        exog = exog.copy().to_frame() if isinstance(exog, pd.Series) else exog.copy()\n        exog = {\n            series_id: exog.loc[series_id] \n            for series_id in exog.index.levels[0]\n            if series_id in levels\n        }\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name  = type(self).__name__,\n            steps            = steps,\n            is_fitted        = self.is_fitted,\n            exog_in_         = self.exog_in_,\n            index_type_      = self.index_type_,\n            index_freq_      = self.index_freq_,\n            window_size      = self.window_size,\n            last_window      = last_window,\n            exog             = exog,\n            exog_names_in_   = self.exog_names_in_,\n            interval         = None,\n            levels           = levels,\n            series_names_in_ = self.series_names_in_,\n            encoding         = self.encoding\n        )\n\n        if predict_probabilistic:\n            check_residuals_input(\n                forecaster_name              = type(self).__name__,\n                use_in_sample_residuals      = use_in_sample_residuals,\n                in_sample_residuals_         = self.in_sample_residuals_,\n                out_sample_residuals_        = self.out_sample_residuals_,\n                use_binned_residuals         = use_binned_residuals,\n                in_sample_residuals_by_bin_  = self.in_sample_residuals_by_bin_,\n                out_sample_residuals_by_bin_ = self.out_sample_residuals_by_bin_,\n                levels                       = levels,\n                encoding                     = self.encoding\n            )\n\n    last_window = last_window.iloc[\n        -self.window_size :, last_window.columns.get_indexer(levels)\n    ].copy()\n    prediction_index = expand_index(\n                           index = last_window.index,\n                           steps = steps\n                       )\n\n    if exog is not None:\n        if isinstance(exog, dict):\n            # Empty dataframe to be filled with the exog values of each level\n            empty_exog = pd.DataFrame(\n                data={\n                    col: pd.Series(dtype=dtype)\n                    for col, dtype in self.exog_dtypes_in_.items()\n                },\n                index=prediction_index,\n            )\n        else:\n            exog = input_to_frame(data=exog, input_name='exog')                \n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.iloc[:steps, :]\n    else:\n        exog_values = None\n\n    # NOTE: This needs to be done to ensure that the last window dtype is float.\n    last_window_values = last_window.to_numpy()\n    last_window_matrix = np.full(\n        shape=last_window.shape, fill_value=np.nan, order='F', dtype=float\n    )\n    exog_values_all_levels = []\n    for idx_level, level in enumerate(levels):\n        last_window_level = last_window_values[:, idx_level]\n        last_window_level = transform_numpy(\n            array             = last_window_level,\n            transformer       = self.transformer_series_.get(level, self.transformer_series_['_unknown_level']),\n            fit               = False,\n            inverse_transform = False\n        )\n\n        if self.differentiation is not None:\n            if level not in self.differentiator_.keys():\n                self.differentiator_[level] = copy(self.differentiator_['_unknown_level'])\n            if self.differentiator_[level] is not None:\n                last_window_level = (\n                    self.differentiator_[level].fit_transform(last_window_level)\n                )\n\n        last_window_matrix[:, idx_level] = last_window_level\n\n        if isinstance(exog, dict):\n            # Fill the empty dataframe with the exog values of each level\n            # and transform them if necessary\n            exog_values = exog.get(level, None)\n            if exog_values is not None:\n                if isinstance(exog_values, pd.Series):\n                    exog_values = exog_values.to_frame()\n\n                exog_values = exog_values.reindex_like(empty_exog)\n            else:\n                exog_values = empty_exog.copy()\n\n        exog_values_all_levels.append(exog_values)\n\n    last_window = pd.DataFrame(\n                      data    = last_window_matrix,\n                      columns = levels,\n                      index   = last_window.index\n                  )\n\n    if exog is not None:\n        # Exog is transformed into a dict where each key is a step and each value\n        # is a numpy array where each column is an exog and each row a series\n        exog_values_all_levels = pd.concat(exog_values_all_levels)\n        if isinstance(exog, dict):\n            exog_values_all_levels = transform_dataframe(\n                                         df                = exog_values_all_levels,\n                                         transformer       = self.transformer_exog,\n                                         fit               = False,\n                                         inverse_transform = False\n                                     )\n\n            check_exog_dtypes(exog=exog_values_all_levels)\n\n        exog_values_all_levels = exog_values_all_levels.to_numpy()\n        exog_values_dict = {\n            i + 1: exog_values_all_levels[i::steps, :] \n            for i in range(steps)\n        }\n    else:\n        exog_values_dict = None\n\n    return last_window, exog_values_dict, levels, prediction_index\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._recursive_predict","title":"_recursive_predict","text":"<pre><code>_recursive_predict(\n    steps,\n    levels,\n    last_window,\n    exog_values_dict=None,\n    residuals=None,\n    use_binned_residuals=True,\n)\n</code></pre> <p>Predict n steps for one or multiple levels. It is an iterative process in which, each prediction, is used as a predictor for the next step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>levels</code> <code>list</code> <p>Time series to be predicted.</p> required <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors needed in the first  iteration of the prediction (t + 1).</p> required <code>exog_values_dict</code> <code>dict</code> <p>Exogenous variable/s included as predictor/s for each series in  each step. The keys are the steps and the values are numpy arrays where each column is an exog and each row a series (level).</p> <code>None</code> <code>residuals</code> <code>numpy ndarray</code> <p>Residuals used to generate bootstrapping predictions in the form (steps, levels).</p> <code>None</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly. New in version 0.15.0</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray</code> <p>Predicted values.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _recursive_predict(\n    self,\n    steps: int,\n    levels: list,\n    last_window: pd.DataFrame,\n    exog_values_dict: dict[str, np.ndarray] | None = None,\n    residuals: np.ndarray | None = None,\n    use_binned_residuals: bool = True\n) -&gt; np.ndarray:\n    \"\"\"\n    Predict n steps for one or multiple levels. It is an iterative process\n    in which, each prediction, is used as a predictor for the next step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    levels : list\n        Time series to be predicted.\n    last_window : pandas DataFrame\n        Series values used to create the predictors needed in the first \n        iteration of the prediction (t + 1).\n    exog_values_dict : dict, default None\n        Exogenous variable/s included as predictor/s for each series in \n        each step. The keys are the steps and the values are numpy arrays\n        where each column is an exog and each row a series (level).\n    residuals : numpy ndarray, default None\n        Residuals used to generate bootstrapping predictions in the form\n        (steps, levels).\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n        **New in version 0.15.0**\n\n    Returns\n    -------\n    predictions : numpy ndarray\n        Predicted values.\n\n    \"\"\"\n\n    original_device = set_cpu_gpu_device(regressor=self.regressor, device='cpu')\n\n    n_levels = len(levels)\n    n_lags = len(self.lags) if self.lags is not None else 0\n    n_window_features = (\n        len(self.X_train_window_features_names_out_)\n        if self.window_features is not None\n        else 0\n    )\n    n_autoreg = n_lags + n_window_features\n    n_exog = len(self.X_train_exog_names_out_) if exog_values_dict is not None else 0\n\n    if self.encoding is not None:\n        if self.encoding == \"onehot\":\n            levels_encoded = np.zeros(\n                (n_levels, len(self.X_train_series_names_in_)), dtype=float\n            )\n            for i, level in enumerate(levels):\n                if level in self.X_train_series_names_in_:\n                    levels_encoded[i, self.X_train_series_names_in_.index(level)] = 1.\n        else:\n            levels_encoded = np.array(\n                [self.encoding_mapping_.get(level, np.nan) for level in levels],\n                dtype=\"float64\"\n            ).reshape(-1, 1)\n        levels_encoded_shape = levels_encoded.shape[1]\n    else:\n        levels_encoded_shape = 0\n\n    features_shape = n_autoreg + levels_encoded_shape + n_exog\n    features = np.full(\n        shape=(n_levels, features_shape), fill_value=np.nan, order='F', dtype=float\n    )\n    if self.encoding is not None:\n        features[:, n_autoreg: n_autoreg + levels_encoded_shape] = levels_encoded\n\n    predictions = np.full(\n        shape=(steps, n_levels), fill_value=np.nan, order='C', dtype=float\n    )\n    last_window = np.concatenate((last_window.to_numpy(), predictions), axis=0)\n\n    for i in range(steps):\n\n        if self.lags is not None:\n            features[:, :n_lags] = last_window[\n                -self.lags - (steps - i), :\n            ].transpose()\n        if self.window_features is not None:\n            features[:, n_lags:n_autoreg] = np.concatenate(\n                [\n                    wf.transform(last_window[i:-(steps - i), :]) \n                    for wf in self.window_features\n                ],\n                axis=1\n            )\n        if exog_values_dict is not None:\n            features[:, -n_exog:] = exog_values_dict[i + 1]\n\n        pred = self.regressor.predict(features)\n\n        if residuals is not None:\n\n            if use_binned_residuals:\n                step_residual = np.full(\n                    shape=n_levels, fill_value=np.nan, dtype=float\n                )\n                for j, level in enumerate(levels):\n                    predicted_bin = (\n                        self.binner\n                        .get(level, self.binner['_unknown_level'])\n                        .transform(pred[j])\n                        .item()\n                    )\n                    step_residual[j] = residuals[predicted_bin][i, j]\n            else:\n                step_residual = residuals[i, :]\n\n            pred += step_residual\n\n        predictions[i, :] = pred \n\n        # Update `last_window` values. The first position is discarded and \n        # the new prediction is added at the end.\n        last_window[-(steps - i), :] = pred\n\n    set_cpu_gpu_device(regressor=self.regressor, device=original_device)\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.create_predict_X","title":"create_predict_X","text":"<pre><code>create_predict_X(\n    steps,\n    levels=None,\n    last_window=None,\n    exog=None,\n    suppress_warnings=False,\n    check_inputs=True,\n)\n</code></pre> <p>Create the predictors needed to predict <code>steps</code> ahead. As it is a recursive process, the predictors are created at each iteration of the prediction  process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>X_predict</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the predictors. The columns are <code>level</code> and  one column for each predictor. The index is the same as the prediction  index.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def create_predict_X(\n    self,\n    steps: int,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    suppress_warnings: bool = False,\n    check_inputs: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create the predictors needed to predict `steps` ahead. As it is a recursive\n    process, the predictors are created at each iteration of the prediction \n    process.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    levels : str, list, default None\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    X_predict : pandas DataFrame\n        Long-format DataFrame with the predictors. The columns are `level` and \n        one column for each predictor. The index is the same as the prediction \n        index.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    (\n        last_window,\n        exog_values_dict,\n        levels,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            levels       = levels,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs\n        )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = self._recursive_predict(\n                          steps            = steps,\n                          levels           = levels,\n                          last_window      = last_window,\n                          exog_values_dict = exog_values_dict\n                      )\n\n    if self.lags is not None:\n        idx_lags = np.arange(-steps, 0)[:, None] - self.lags\n    len_X_train_series_names_in_ = len(self.X_train_series_names_in_)\n    exog_shape = len(self.X_train_exog_names_out_) if exog is not None else 0\n\n    X_predict = []\n    for i, level in enumerate(levels):\n\n        X_predict_level = []\n        full_predictors_level = np.concatenate(\n            (last_window[level].to_numpy(), predictions[:, i])\n        )\n\n        if self.lags is not None:\n            X_predict_level.append(\n                full_predictors_level[idx_lags + len(full_predictors_level)]\n            )\n\n        if self.window_features is not None:\n            X_window_features = np.full(\n                shape      = (steps, len(self.X_train_window_features_names_out_)), \n                fill_value = np.nan, \n                order      = 'C',\n                dtype      = float\n            )\n            for j in range(steps):\n                X_window_features[j, :] = np.concatenate(\n                    [\n                        wf.transform(full_predictors_level[j:-(steps - j)]) \n                        for wf in self.window_features\n                    ]\n                )\n            X_predict_level.append(X_window_features)\n\n        if self.encoding is not None:\n            if self.encoding == 'onehot':\n                level_encoded = np.zeros(\n                                    shape = (1, len_X_train_series_names_in_),\n                                    dtype = float\n                                )\n                level_encoded[0][self.X_train_series_names_in_.index(level)] = 1.\n            else:\n                level_encoded = np.array(\n                                    [self.encoding_mapping_.get(level, None)],\n                                    dtype = 'float64'\n                                )\n\n            level_encoded = np.tile(level_encoded, (steps, 1))\n            X_predict_level.append(level_encoded)\n\n        if exog is not None:\n            exog_cols = np.full(\n                shape=(steps, exog_shape), fill_value=np.nan, order='C', dtype=float\n            )\n            for j in range(steps):\n                exog_cols[j, :] = exog_values_dict[j + 1][i, :]\n            X_predict_level.append(exog_cols)\n\n        X_predict.append(np.concatenate(X_predict_level, axis=1))\n\n    X_predict = pd.DataFrame(\n                    data    = np.concatenate(X_predict),\n                    index   = np.tile(prediction_index, len(levels)),\n                    columns = self.X_train_features_names_out_\n                )\n    X_predict.insert(0, 'level', np.repeat(levels, steps))\n\n    # NOTE: Order needed to have the same structure as the output of predict methods.\n    order_dict = {level: i for i, level in enumerate(levels)}\n    X_predict['order'] = X_predict['level'].map(order_dict)\n    X_predict = (\n        X_predict\n        .reset_index()\n        .sort_values(by=['index', 'order'])\n        .set_index('index')\n        .rename_axis(index=None)\n        .drop(columns='order')\n    )\n\n    if self.exog_in_:\n        categorical_features = any(\n            not pd.api.types.is_numeric_dtype(dtype) or pd.api.types.is_bool_dtype(dtype) \n            for dtype in set(self.exog_dtypes_out_)\n        )\n        if categorical_features:\n            X_predict = X_predict.astype(self.exog_dtypes_out_)\n\n    if self.transformer_series is not None or self.differentiation is not None:\n        warnings.warn(\n            \"The output matrix is in the transformed scale due to the \"\n            \"inclusion of transformations or differentiation in the Forecaster. \"\n            \"As a result, any predictions generated using this matrix will also \"\n            \"be in the transformed scale. Please refer to the documentation \"\n            \"for more details: \"\n            \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n            DataTransformationWarning\n        )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_predict\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.predict","title":"predict","text":"<pre><code>predict(\n    steps,\n    levels=None,\n    last_window=None,\n    exog=None,\n    suppress_warnings=False,\n    check_inputs=True,\n)\n</code></pre> <p>Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor for the next step. Only levels whose last window ends at the same datetime index can be predicted together.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the predictions. The columns are <code>level</code> and <code>pred</code>.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    suppress_warnings: bool = False,\n    check_inputs: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead. It is an recursive process in which, each prediction,\n    is used as a predictor for the next step. Only levels whose last window\n    ends at the same datetime index can be predicted together.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    levels : str, list, default None\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Long-format DataFrame with the predictions. The columns are `level`\n        and `pred`.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    (\n        last_window,\n        exog_values_dict,\n        levels,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            levels       = levels,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs\n        )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = self._recursive_predict(\n                          steps            = steps,\n                          levels           = levels,\n                          last_window      = last_window,\n                          exog_values_dict = exog_values_dict\n                      )\n\n    for i, level in enumerate(levels):\n        if self.differentiation is not None and self.differentiator_[level] is not None:\n            predictions[:, i] = (\n                self\n                .differentiator_[level]\n                .inverse_transform_next_window(predictions[:, i])\n            )\n\n        predictions[:, i] = transform_numpy(\n            array             = predictions[:, i],\n            transformer       = self.transformer_series_.get(level, self.transformer_series_['_unknown_level']),\n            fit               = False,\n            inverse_transform = True\n        )\n\n    n_steps, n_levels = predictions.shape\n    predictions = pd.DataFrame(\n        {\"level\": np.tile(levels, n_steps), \"pred\": predictions.ravel()},\n        index = np.repeat(prediction_index, n_levels),\n    )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.predict_bootstrapping","title":"predict_bootstrapping","text":"<pre><code>predict_bootstrapping(\n    steps,\n    levels=None,\n    last_window=None,\n    exog=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Generate multiple forecasting predictions using a bootstrapping process.  By sampling from a collection of past observed errors (the residuals), each iteration of bootstrapping generates a different set of predictions.  Only levels whose last window ends at the same datetime index can be  predicted together. See the References section for more information. </p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly. New in version 0.15.0</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>boot_predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the bootstrapping predictions. The columns are <code>level</code>, <code>pred_boot_0</code>, <code>pred_boot_1</code>, ..., <code>pred_boot_n_boot</code>.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def predict_bootstrapping(\n    self,\n    steps: int,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate multiple forecasting predictions using a bootstrapping process. \n    By sampling from a collection of past observed errors (the residuals),\n    each iteration of bootstrapping generates a different set of predictions. \n    Only levels whose last window ends at the same datetime index can be \n    predicted together. See the References section for more information. \n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    levels : str, list, default None\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n        **New in version 0.15.0**\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    boot_predictions : pandas DataFrame\n        Long-format DataFrame with the bootstrapping predictions. The columns\n        are `level`, `pred_boot_0`, `pred_boot_1`, ..., `pred_boot_n_boot`.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    (\n        last_window,\n        exog_values_dict,\n        levels,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps                   = steps,\n            levels                  = levels,\n            last_window             = last_window,\n            exog                    = exog,\n            predict_probabilistic   = True,\n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    n_levels = len(levels)\n    rng = np.random.default_rng(seed=random_state)\n    sampled_residuals_grid = np.full(\n                                 shape      = (steps, n_boot, n_levels),\n                                 fill_value = np.nan,\n                                 order      = 'F',\n                                 dtype      = float\n                             )\n    if use_binned_residuals:\n        sampled_residuals = {\n            k: sampled_residuals_grid.copy() \n            for k in range(self.binner_kwargs['n_bins'])\n        }\n        for bin in sampled_residuals.keys():\n            for i, level in enumerate(levels):\n                sampled_residuals[bin][:, :, i] = rng.choice(\n                    a       = residuals_by_bin.get(level, residuals_by_bin['_unknown_level'])[bin],\n                    size    = (steps, n_boot),\n                    replace = True\n                )\n    else:\n        for i, level in enumerate(levels):\n            sampled_residuals_grid[:, :, i] = rng.choice(\n                a       = residuals.get(level, residuals['_unknown_level']),\n                size    = (steps, n_boot),\n                replace = True\n            )\n        sampled_residuals = {'all': sampled_residuals_grid}\n\n    boot_columns = []\n    boot_predictions = np.full(\n                           shape      = (steps, n_levels, n_boot),\n                           fill_value = np.nan,\n                           order      = 'F',\n                           dtype      = float\n                       )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        for i in range(n_boot):\n\n            if use_binned_residuals:\n                boot_sampled_residuals = {\n                    k: v[:, i, :]\n                    for k, v in sampled_residuals.items()\n                }\n            else:\n                boot_sampled_residuals = sampled_residuals['all'][:, i, :]\n\n            boot_columns.append(f\"pred_boot_{i}\")\n            boot_predictions[:, :, i] = self._recursive_predict(\n                steps                = steps,\n                levels               = levels,\n                last_window          = last_window,\n                exog_values_dict     = exog_values_dict,\n                residuals            = boot_sampled_residuals,\n                use_binned_residuals = use_binned_residuals,\n            )\n\n    for i, level in enumerate(levels):\n\n        if self.differentiation is not None and self.differentiator_[level] is not None:\n            boot_predictions[:, i, :] = (\n                self.differentiator_[level]\n                .inverse_transform_next_window(boot_predictions[:, i, :])\n            )\n\n        transformer_level = self.transformer_series_.get(\n                                level,\n                                self.transformer_series_['_unknown_level']\n                            )\n        if transformer_level is not None:\n            boot_predictions[:, i, :] = np.apply_along_axis(\n                func1d            = transform_numpy,\n                axis              = 0,\n                arr               = boot_predictions[:, i, :],\n                transformer       = transformer_level,\n                fit               = False,\n                inverse_transform = True\n            )\n\n    boot_predictions = pd.DataFrame(\n                           data    = boot_predictions.reshape(-1, n_boot),\n                           index   = np.repeat(prediction_index, n_levels),\n                           columns = boot_columns\n                       )\n    boot_predictions.insert(0, 'level', np.tile(levels, steps))\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return boot_predictions\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._predict_interval_conformal","title":"_predict_interval_conformal","text":"<pre><code>_predict_interval_conformal(\n    steps,\n    levels=None,\n    last_window=None,\n    exog=None,\n    nominal_coverage=0.95,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n)\n</code></pre> <p>Generate prediction intervals using the conformal prediction  split method [1]_.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int, str, pandas Timestamp</code> <p>Number of steps to predict. </p> <ul> <li>If steps is int, number of steps to predict. </li> <li>If str or pandas Datetime, the prediction will be up to that date.</li> </ul> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>nominal_coverage</code> <code>float</code> <p>Nominal coverage, also known as expected coverage, of the prediction intervals. Must be between 0 and 1.</p> <code>0.95</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _predict_interval_conformal(\n    self,\n    steps: int | str | pd.Timestamp,\n    levels: str | list[str] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    nominal_coverage: float = 0.95,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate prediction intervals using the conformal prediction \n    split method [1]_.\n\n    Parameters\n    ----------\n    steps : int, str, pandas Timestamp\n        Number of steps to predict. \n\n        - If steps is int, number of steps to predict. \n        - If str or pandas Datetime, the prediction will be up to that date.\n    levels : str, list, default None\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    nominal_coverage : float, default 0.95\n        Nominal coverage, also known as expected coverage, of the prediction\n        intervals. Must be between 0 and 1.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    References\n    ----------\n    .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    (\n        last_window,\n        exog_values_dict,\n        levels,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps                   = steps,\n            levels                  = levels,\n            last_window             = last_window,\n            exog                    = exog,\n            predict_probabilistic   = True,\n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n        residuals_by_bin = self.in_sample_residuals_by_bin_\n    else:\n        residuals = self.out_sample_residuals_\n        residuals_by_bin = self.out_sample_residuals_by_bin_\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        predictions = self._recursive_predict(\n                          steps            = steps,\n                          levels           = levels,\n                          last_window      = last_window,\n                          exog_values_dict = exog_values_dict\n                      )\n\n    n_levels = len(levels)\n    correction_factor = np.full(\n        shape=(steps, n_levels), fill_value=np.nan, order='C', dtype=float\n    )\n    if use_binned_residuals:\n        for i, level in enumerate(levels):\n            residuals_level = residuals_by_bin.get(level, residuals_by_bin['_unknown_level'])\n            correction_factor_by_bin = {\n                k: np.quantile(np.abs(v), nominal_coverage)\n                for k, v in residuals_level.items()\n            }\n            replace_func = np.vectorize(lambda x: correction_factor_by_bin[x])\n            predictions_bin = (\n                self.binner\n                .get(level, self.binner['_unknown_level'])\n                .transform(predictions[:, i])\n            )\n            correction_factor[:, i] = replace_func(predictions_bin)\n    else:\n        for i, level in enumerate(levels):\n            correction_factor[:, i] = np.quantile(\n                np.abs(residuals.get(level, residuals['_unknown_level'])), nominal_coverage\n            )\n\n    lower_bound = predictions - correction_factor\n    upper_bound = predictions + correction_factor\n\n    # NOTE: Create a 3D array with shape (n_levels, intervals, steps)\n    predictions = np.array([predictions, lower_bound, upper_bound]).swapaxes(0, 2)\n\n    for i, level in enumerate(levels):\n\n        if self.differentiation is not None and self.differentiator_[level] is not None:\n            predictions[i, :, :] = (\n                self.differentiator_[level]\n                .inverse_transform_next_window(predictions[i, :, :])\n            )\n\n        transformer_level = self.transformer_series_.get(\n                                level,\n                                self.transformer_series_['_unknown_level']\n                            )\n        if transformer_level is not None:\n            predictions[i, :, :] = np.apply_along_axis(\n                func1d            = transform_numpy,\n                axis              = 0,\n                arr               = predictions[i, :, :],\n                transformer       = transformer_level,\n                fit               = False,\n                inverse_transform = True\n            )\n\n    predictions = pd.DataFrame(\n                      data    = predictions.swapaxes(0, 1).reshape(-1, 3),\n                      index   = np.repeat(prediction_index, n_levels),\n                      columns = [\"pred\", \"lower_bound\", \"upper_bound\"]\n                  )\n    predictions.insert(0, 'level', np.tile(levels, steps))\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.predict_interval","title":"predict_interval","text":"<pre><code>predict_interval(\n    steps,\n    levels=None,\n    last_window=None,\n    exog=None,\n    method=\"conformal\",\n    interval=[5, 95],\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Predict n steps ahead and estimate prediction intervals using either  bootstrapping or conformal prediction methods. Refer to the References  section for additional details on these methods.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction  intervals [1]_.</li> <li>'conformal': Employs the conformal prediction split method for  interval estimation [2]_.</li> </ul> <code>'conformal'</code> <code>interval</code> <code>(float, list, tuple)</code> <p>Confidence level of the prediction interval. Interpretation depends  on the method used:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0  and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code>  percentiles.</li> <li>If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which  must be between 0 and 100 inclusive. For example, interval  of 95% should be as <code>interval = [2.5, 97.5]</code>.</li> <li>When using <code>method='conformal'</code>, the interval must be a float or  a list/tuple defining a symmetric interval.</li> </ul> <code>[5, 95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly. New in version 0.15.0</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the predictions and the lower and upper bounds of the estimated interval. The columns are <code>level</code>, <code>pred</code>, <code>lower_bound</code>, <code>upper_bound</code>.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> <p>.. [2] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    method: str = 'conformal',\n    interval: float | list[float] | tuple[float] = [5, 95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using either \n    bootstrapping or conformal prediction methods. Refer to the References \n    section for additional details on these methods.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    levels : str, list, default None\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    method : str, default 'conformal'\n        Technique used to estimate prediction intervals. Available options:\n\n        - 'bootstrapping': Bootstrapping is used to generate prediction \n        intervals [1]_.\n        - 'conformal': Employs the conformal prediction split method for \n        interval estimation [2]_.\n    interval : float, list, tuple, default [5, 95]\n        Confidence level of the prediction interval. Interpretation depends \n        on the method used:\n\n        - If `float`, represents the nominal (expected) coverage (between 0 \n        and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]` \n        percentiles.\n        - If `list` or `tuple`, defines the exact percentiles to compute, which \n        must be between 0 and 100 inclusive. For example, interval \n        of 95% should be as `interval = [2.5, 97.5]`.\n        - When using `method='conformal'`, the interval must be a float or \n        a list/tuple defining a symmetric interval.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n        **New in version 0.15.0**\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Long-format DataFrame with the predictions and the lower and upper\n        bounds of the estimated interval. The columns are `level`, `pred`,\n        `lower_bound`, `upper_bound`.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    if method == \"bootstrapping\":\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=False)\n            interval = np.array(interval) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal='interval')\n            interval = np.array([0.5 - interval / 2, 0.5 + interval / 2])\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               levels                  = levels,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals,\n                               random_state            = random_state,\n                               suppress_warnings       = suppress_warnings\n                           )\n\n        predictions = self.predict(\n                          steps             = steps,\n                          levels            = levels,\n                          last_window       = last_window,\n                          exog              = exog,\n                          suppress_warnings = suppress_warnings,\n                          check_inputs      = False\n                      )\n\n        boot_predictions[['lower_bound', 'upper_bound']] = (\n            boot_predictions.iloc[:, 1:].quantile(q=interval, axis=1).transpose()\n        )\n        predictions = pd.concat([\n            predictions, boot_predictions[['lower_bound', 'upper_bound']]\n        ], axis=1)\n\n    elif method == 'conformal':\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal='interval')\n            nominal_coverage = interval\n\n        predictions = self._predict_interval_conformal(\n                          steps                   = steps,\n                          levels                  = levels,\n                          last_window             = last_window,\n                          exog                    = exog,\n                          nominal_coverage        = nominal_coverage,\n                          use_in_sample_residuals = use_in_sample_residuals,\n                          use_binned_residuals    = use_binned_residuals\n                      )\n    else:\n        raise ValueError(\n            f\"Invalid `method` '{method}'. Choose 'bootstrapping' or 'conformal'.\"\n        )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.predict_quantiles","title":"predict_quantiles","text":"<pre><code>predict_quantiles(\n    steps,\n    levels=None,\n    last_window=None,\n    exog=None,\n    quantiles=[0.05, 0.5, 0.95],\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Calculate the specified quantiles for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  quantile is calculated for each step.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>quantiles</code> <code>(list, tuple)</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>[0.05, 0.5, 0.95]</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating quantiles.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly. New in version 0.15.0</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the quantiles predicted by the forecaster. For example, if <code>quantiles = [0.05, 0.5, 0.95]</code>, the columns are <code>level</code>, <code>q_0.05</code>, <code>q_0.5</code>, <code>q_0.95</code>.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def predict_quantiles(\n    self,\n    steps: int,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    quantiles: list[float] | tuple[float] = [0.05, 0.5, 0.95],\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the specified quantiles for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    quantile is calculated for each step.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    levels : str, list, default None\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    quantiles : list, tuple, default [0.05, 0.5, 0.95]\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating quantiles.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n        **New in version 0.15.0**\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Long-format DataFrame with the quantiles predicted by the forecaster.\n        For example, if `quantiles = [0.05, 0.5, 0.95]`, the columns are\n        `level`, `q_0.05`, `q_0.5`, `q_0.95`.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    check_interval(quantiles=quantiles)\n\n    predictions = self.predict_bootstrapping(\n                      steps                   = steps,\n                      levels                  = levels,\n                      last_window             = last_window,\n                      exog                    = exog,\n                      n_boot                  = n_boot,\n                      use_in_sample_residuals = use_in_sample_residuals,\n                      use_binned_residuals    = use_binned_residuals,\n                      random_state            = random_state,\n                      suppress_warnings       = suppress_warnings\n                  )\n\n    quantiles_cols = [f'q_{q}' for q in quantiles]\n    predictions[quantiles_cols] = (\n        predictions.iloc[:, 1:].quantile(q=quantiles, axis=1).transpose()\n    )\n    predictions = predictions[['level'] + quantiles_cols]\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.predict_dist","title":"predict_dist","text":"<pre><code>predict_dist(\n    steps,\n    distribution,\n    levels=None,\n    last_window=None,\n    exog=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Fit a given probability distribution for each step. After generating  multiple forecasting predictions through a bootstrapping process, each  step is fitted to the given distribution.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>distribution</code> <code>object</code> <p>A distribution object from scipy.stats with methods <code>_pdf</code> and <code>fit</code>.  For example scipy.stats.norm.</p> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels whose last window ends at the same datetime index will be predicted together.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly. New in version 0.15.0</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the parameters of the fitted distribution for each step. The columns are <code>level</code>, <code>param_0</code>, <code>param_1</code>, ...,  <code>param_n</code>, where <code>param_i</code> are the parameters of the distribution.</p> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def predict_dist(\n    self,\n    steps: int,\n    distribution: object,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit a given probability distribution for each step. After generating \n    multiple forecasting predictions through a bootstrapping process, each \n    step is fitted to the given distribution.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    distribution : object\n        A distribution object from scipy.stats with methods `_pdf` and `fit`. \n        For example scipy.stats.norm.\n    levels : str, list, default None\n        Time series to be predicted. If `None` all levels whose last window\n        ends at the same datetime index will be predicted together.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n        **New in version 0.15.0**\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Long-format DataFrame with the parameters of the fitted distribution\n        for each step. The columns are `level`, `param_0`, `param_1`, ..., \n        `param_n`, where `param_i` are the parameters of the distribution.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    \"\"\"\n\n    if not hasattr(distribution, \"_pdf\") or not callable(getattr(distribution, \"fit\", None)):\n        raise TypeError(\n            \"`distribution` must be a valid probability distribution object \"\n            \"from scipy.stats, with methods `_pdf` and `fit`.\"\n        )\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    predictions = self.predict_bootstrapping(\n                      steps                   = steps,\n                      levels                  = levels,\n                      last_window             = last_window,\n                      exog                    = exog,\n                      n_boot                  = n_boot,\n                      use_in_sample_residuals = use_in_sample_residuals,\n                      use_binned_residuals    = use_binned_residuals,\n                      random_state            = random_state,\n                      suppress_warnings       = suppress_warnings\n                  )\n\n    param_names = [\n        p for p in inspect.signature(distribution._pdf).parameters \n        if not p == \"x\"\n    ] + [\"loc\", \"scale\"]\n\n    predictions[param_names] = (\n        predictions.iloc[:, 1:].apply(\n            lambda x: distribution.fit(x), axis=1, result_type='expand'\n        )\n    )\n    predictions = predictions[['level'] + param_names]\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.set_params","title":"set_params","text":"<pre><code>set_params(params)\n</code></pre> <p>Set new values to the parameters of the scikit-learn model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def set_params(\n    self, \n    params: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the scikit-learn model stored in the\n    forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.set_fit_kwargs","title":"set_fit_kwargs","text":"<pre><code>set_fit_kwargs(fit_kwargs)\n</code></pre> <p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.set_lags","title":"set_lags","text":"<pre><code>set_lags(lags=None)\n</code></pre> <p>Set new value to the attribute <code>lags</code>. Attributes <code>lags_names</code>,  <code>max_lag</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. </p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> <li><code>None</code>: no lags are included as predictors.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def set_lags(\n    self, \n    lags: int | list[int] | np.ndarray[int] | range[int] | None = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `lags`. Attributes `lags_names`, \n    `max_lag` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    lags : int, list, numpy ndarray, range, default None\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if self.window_features is None and lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n\n    if self.differentiation is not None:\n        self.window_size += self.differentiation_max\n        if isinstance(self.differentiator, dict):\n            for series in self.differentiator.keys():\n                if self.differentiator[series] is not None:\n                    self.differentiator[series].set_params(window_size=self.window_size)\n        else:\n            self.differentiator.set_params(window_size=self.window_size)\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.set_window_features","title":"set_window_features","text":"<pre><code>set_window_features(window_features=None)\n</code></pre> <p>Set new value to the attribute <code>window_features</code>. Attributes  <code>max_size_window_features</code>, <code>window_features_names</code>,  <code>window_features_class_names</code> and <code>window_size</code> are also updated.</p> <p>Parameters:</p> Name Type Description Default <code>window_features</code> <code>(object, list)</code> <p>Instance or list of instances used to create window features. Window features are created from the original time series and are included as predictors.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def set_window_features(\n    self, \n    window_features: object | list[object] | None = None\n) -&gt; None:\n    \"\"\"\n    Set new value to the attribute `window_features`. Attributes \n    `max_size_window_features`, `window_features_names`, \n    `window_features_class_names` and `window_size` are also updated.\n\n    Parameters\n    ----------\n    window_features : object, list, default None\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if window_features is None and self.lags is None:\n        raise ValueError(\n            \"At least one of the arguments `lags` or `window_features` \"\n            \"must be different from None. This is required to create the \"\n            \"predictors used in training the forecaster.\"\n        )\n\n    self.window_features, self.window_features_names, self.max_size_window_features = (\n        initialize_window_features(window_features)\n    )\n    self.window_features_class_names = None\n    if window_features is not None:\n        self.window_features_class_names = [\n            type(wf).__name__ for wf in self.window_features\n        ] \n    self.window_size = max(\n        [ws for ws in [self.max_lag, self.max_size_window_features] \n         if ws is not None]\n    )\n\n    if self.differentiation is not None:\n        self.window_size += self.differentiation_max\n        if isinstance(self.differentiator, dict):\n            for series in self.differentiator.keys():\n                if self.differentiator[series] is not None:\n                    self.differentiator[series].set_params(window_size=self.window_size)\n        else:\n            self.differentiator.set_params(window_size=self.window_size)\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.set_in_sample_residuals","title":"set_in_sample_residuals","text":"<pre><code>set_in_sample_residuals(\n    series,\n    exog=None,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Set in-sample residuals in case they were not calculated during the training process. </p> <p>In-sample residuals are calculated as the difference between the true  values and the predictions made by the forecaster using the training  data. The following internal attributes are updated:</p> <ul> <li><code>in_sample_residuals_</code>: Dictionary containing a numpy ndarray with the residuals for each series in the form <code>{series: residuals}</code>.</li> <li><code>binner_intervals_</code>: intervals used to bin the residuals are calculated using the quantiles of the predicted values.</li> <li><code>in_sample_residuals_by_bin_</code>: residuals are binned according to the predicted value they are associated with and stored in a dictionary per series, where the keys are the intervals of the predicted values and the  values are the residuals associated with that range. </li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>in_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the sampling  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def set_in_sample_residuals(\n    self,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process. \n\n    In-sample residuals are calculated as the difference between the true \n    values and the predictions made by the forecaster using the training \n    data. The following internal attributes are updated:\n\n    + `in_sample_residuals_`: Dictionary containing a numpy ndarray with the\n    residuals for each series in the form `{series: residuals}`.\n    + `binner_intervals_`: intervals used to bin the residuals are calculated\n    using the quantiles of the predicted values.\n    + `in_sample_residuals_by_bin_`: residuals are binned according to the\n    predicted value they are associated with and stored in a dictionary per\n    series, where the keys are the intervals of the predicted values and the \n    values are the residuals associated with that range. \n\n    A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the sampling \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    (\n        X_train,\n        y_train,\n        series_indexes,\n        _,\n        X_train_series_names_in_,\n        *_\n    ) = self._create_train_X_y(\n            series=series, exog=exog, store_last_window=False\n        )\n\n    # NOTE: Same series names as training is checked in _create_train_X_y.\n    series_index_range = {k: v[[0, -1]] for k, v in series_indexes.items()}\n    for level in self.training_range_.keys():\n        if not series_index_range[level].equals(self.training_range_[level]):\n            raise IndexError(\n                f\"The index range for series '{level}' does not match the range \"\n                f\"used during training. Please ensure the index is aligned \"\n                f\"with the training data.\\n\"\n                f\"    Expected : {self.training_range_[level]}\\n\"\n                f\"    Received : {series_index_range[level]}\"\n            )\n\n    X_train_regressor = (\n        X_train\n        if self.encoding is not None\n        else X_train.drop(columns=\"_level_skforecast\")\n    )\n    X_train_features_names_out_ = X_train_regressor.columns.to_list()\n    if not X_train_features_names_out_ == self.X_train_features_names_out_:\n        raise ValueError(\n            f\"Feature mismatch detected after matrix creation. The features \"\n            f\"generated from the provided data do not match those used during \"\n            f\"the training process. To correctly set in-sample residuals, \"\n            f\"ensure that the same data and preprocessing steps are applied.\\n\"\n            f\"    Expected output : {self.X_train_features_names_out_}\\n\"\n            f\"    Current output  : {X_train_features_names_out_}\"\n        )\n\n    self.in_sample_residuals_ = {}\n    self.in_sample_residuals_by_bin_ = {}\n    y_pred = self.regressor.predict(X_train_regressor)\n    if self.encoding is not None:\n        for level in X_train_series_names_in_:\n            if self.encoding == 'onehot':\n                mask = X_train[level].to_numpy() == 1.\n            else:\n                encoded_value = self.encoding_mapping_[level]\n                mask = X_train['_level_skforecast'].to_numpy() == encoded_value\n\n            self._binning_in_sample_residuals(\n                level                     = level,\n                y_true                    = y_train[mask],\n                y_pred                    = y_pred[mask],\n                store_in_sample_residuals = True,\n                random_state              = random_state\n            )\n\n    # NOTE: the _unknown_level is a random sample of 10_000 residuals of all levels.\n    self._binning_in_sample_residuals(\n        level                     = '_unknown_level',\n        y_true                    = y_train,\n        y_pred                    = y_pred,\n        store_in_sample_residuals = True,\n        random_state              = random_state\n    )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.set_out_sample_residuals","title":"set_out_sample_residuals","text":"<pre><code>set_out_sample_residuals(\n    y_true, y_pred, append=False, random_state=123\n)\n</code></pre> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. <code>y_true</code> and <code>y_pred</code> are expected to be in the original scale of the time series. Residuals are calculated as <code>y_true</code> - <code>y_pred</code>, after applying the necessary transformations and differentiations if the forecaster includes them (<code>self.transformer_series</code> and <code>self.differentiation</code>).</p> <p>A total of 10_000 residuals are stored in the attribute <code>out_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>dict</code> <p>Dictionary of numpy ndarrays or pandas Series with the true values of the time series for each series in the form {series: y_true}.</p> required <code>y_pred</code> <code>dict</code> <p>Dictionary of numpy ndarrays or pandas Series with the predicted values of the time series for each series in the form {series: y_pred}.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals_</code>. If after appending the new residuals, the limit of 10_000 samples is exceeded, a random sample of 10_000 is kept.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>Out-of-sample residuals can only be stored for series seen during  fit. To save residuals for unseen levels use the key '_unknown_level'.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def set_out_sample_residuals(\n    self, \n    y_true: dict[str, np.ndarray | pd.Series],\n    y_pred: dict[str, np.ndarray | pd.Series],\n    append: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. `y_true` and `y_pred` are expected\n    to be in the original scale of the time series. Residuals are calculated\n    as `y_true` - `y_pred`, after applying the necessary transformations and\n    differentiations if the forecaster includes them (`self.transformer_series`\n    and `self.differentiation`).\n\n    A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored.\n\n    Parameters\n    ----------\n    y_true : dict\n        Dictionary of numpy ndarrays or pandas Series with the true values of\n        the time series for each series in the form {series: y_true}.\n    y_pred : dict\n        Dictionary of numpy ndarrays or pandas Series with the predicted values\n        of the time series for each series in the form {series: y_pred}.\n    append : bool, default False\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals_`. If after appending the new residuals,\n        the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n        kept.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    Out-of-sample residuals can only be stored for series seen during \n    fit. To save residuals for unseen levels use the key '_unknown_level'. \n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, dict):\n        raise TypeError(\n            f\"`y_true` must be a dictionary of numpy ndarrays or pandas Series. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, dict):\n        raise TypeError(\n            f\"`y_pred` must be a dictionary of numpy ndarrays or pandas Series. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if not set(y_true.keys()) == set(y_pred.keys()):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same keys. \"\n            f\"Got {set(y_true.keys())} and {set(y_pred.keys())}.\"\n        )\n\n    for k in y_true.keys():\n        if not isinstance(y_true[k], (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"Values of `y_true` must be numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_true[k])} for series '{k}'.\"\n            )\n        if not isinstance(y_pred[k], (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"Values of `y_pred` must be numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_pred[k])} for series '{k}'.\"\n            )\n        if len(y_true[k]) != len(y_pred[k]):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true[k])} and {len(y_pred[k])} for series '{k}'.\"\n            )\n        if isinstance(y_true[k], pd.Series) and isinstance(y_pred[k], pd.Series):\n            if not y_true[k].index.equals(y_pred[k].index):\n                raise ValueError(\n                    f\"When containing pandas Series, elements in `y_true` and \"\n                    f\"`y_pred` must have the same index. Error with series '{k}'.\"\n                )\n\n    # NOTE: Out-of-sample residuals can only be stored for series seen during \n    # fit. To save residuals for unseen levels use the key '_unknown_level'. \n    series_names_in_ = self.series_names_in_ + ['_unknown_level']\n    series_to_update = set(y_pred.keys()).intersection(set(series_names_in_))\n    if not series_to_update:\n        raise ValueError(\n            \"Provided keys in `y_pred` and `y_true` do not match any series \"\n            \"seen during `fit`. Residuals cannot be updated.\"\n        )\n\n    if self.out_sample_residuals_ is None:\n        if self.encoding is not None:\n            self.out_sample_residuals_ = {level: None for level in series_names_in_}\n            self.out_sample_residuals_by_bin_ = {level: {} for level in series_names_in_}\n        else:\n            self.out_sample_residuals_ = {'_unknown_level': None}\n            self.out_sample_residuals_by_bin_ = {'_unknown_level': {}}\n\n    for level in series_to_update:\n        residuals_level, residuals_by_bin_level = (\n            self._binning_out_sample_residuals(\n                level        = level,\n                y_true       = y_true[level],\n                y_pred       = y_pred[level],\n                append       = append,\n                random_state = random_state\n            )\n        )\n        self.out_sample_residuals_[level] = residuals_level\n        self.out_sample_residuals_by_bin_[level] = residuals_by_bin_level\n\n    if self.encoding is None or '_unknown_level' not in series_to_update:\n        if self.encoding is None and list(y_true.keys()) != ['_unknown_level']:\n            warnings.warn(\n                \"As `encoding` is set to `None`, no distinction between levels \"\n                \"is made. All residuals are stored in the '_unknown_level' key.\",\n                UnknownLevelWarning\n            )\n\n        # NOTE: when encoding is None, all levels are combined in '_unknown_level'.\n        if list(self.out_sample_residuals_.keys()) != ['_unknown_level']:\n            # To update completely _unknown_level later\n            self.out_sample_residuals_.pop('_unknown_level', None)\n\n        residuals_all_levels = np.concatenate(\n            [\n                value \n                for value in self.out_sample_residuals_.values()\n                if value is not None\n            ]\n        )\n        rng = np.random.default_rng(seed=random_state)\n        if len(residuals_all_levels) &gt; 10_000:\n            residuals_all_levels = rng.choice(\n                                       a       = residuals_all_levels,\n                                       size    = 10_000,\n                                       replace = False\n                                   )\n\n        all_bins_keys = set(\n            bin_key \n            for dict_level_bins in self.out_sample_residuals_by_bin_.values()\n            for bin_key in dict_level_bins.keys()\n        )\n        residuals_by_bin_all_levels = {\n            bin_key: np.concatenate(\n                [\n                    dict_level_bins.get(bin_key, np.array([]))\n                    for dict_level_bins in self.out_sample_residuals_by_bin_.values()\n                ]\n            )\n            for bin_key in all_bins_keys\n        }\n        for key in residuals_by_bin_all_levels.keys():\n            if len(residuals_by_bin_all_levels[key]) &gt; 10_000:\n                residuals_by_bin_all_levels[key] = rng.choice(\n                    a       = residuals_by_bin_all_levels[key],\n                    size    = 10_000,\n                    replace = False\n                )\n\n        if self.encoding is None:\n            self.out_sample_residuals_ = {'_unknown_level': residuals_all_levels}\n            self.out_sample_residuals_by_bin_ = {'_unknown_level': residuals_by_bin_all_levels}\n        else:\n            self.out_sample_residuals_['_unknown_level'] = residuals_all_levels\n            self.out_sample_residuals_by_bin_['_unknown_level'] = residuals_by_bin_all_levels\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries._binning_out_sample_residuals","title":"_binning_out_sample_residuals","text":"<pre><code>_binning_out_sample_residuals(\n    level, y_true, y_pred, append=False, random_state=123\n)\n</code></pre> <p>Bin out sample residuals using the already fitted binner. <code>y_true</code> and <code>y_pred</code> are expected to be in the original scale of the time series. Residuals are calculated as <code>y_true</code> - <code>y_pred</code>, after  applying the necessary transformations and differentiations if the forecaster includes them (<code>self.transformer_series</code> and <code>self.differentiation</code>).</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Name of the level y_true and y_pred belong to.</p> required <code>y_true</code> <code>numpy ndarray</code> <p>True values of the time series.</p> required <code>y_pred</code> <code>numpy ndarray</code> <p>Predicted values of the time series.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals_</code>. If after appending the new residuals, the limit of 10_000 samples is exceeded, a random sample of 10_000 is kept.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Name Type Description <code>out_sample_residuals</code> <code>numpy ndarray</code> <p>Array with the residual for <code>level</code>.</p> <code>out_sample_residuals_by_bin</code> <code>dict</code> <p>Dictionary with the residuals binned by the fitted binner for <code>level</code>.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def _binning_out_sample_residuals(\n    self,\n    level: str,\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    append: bool = False,\n    random_state: int = 123\n) -&gt; tuple[np.ndarray, dict[int, np.ndarray]]:\n    \"\"\"\n    Bin out sample residuals using the already fitted binner.\n    `y_true` and `y_pred` are expected to be in the original scale of the\n    time series. Residuals are calculated as `y_true` - `y_pred`, after \n    applying the necessary transformations and differentiations if the\n    forecaster includes them (`self.transformer_series` and `self.differentiation`).\n\n    Parameters\n    ----------\n    level : str\n        Name of the level y_true and y_pred belong to.\n    y_true : numpy ndarray\n        True values of the time series.\n    y_pred : numpy ndarray\n        Predicted values of the time series.\n    append : bool, default False\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals_`. If after appending the new residuals,\n        the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n        kept.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    out_sample_residuals : numpy ndarray\n        Array with the residual for `level`.\n    out_sample_residuals_by_bin : dict\n        Dictionary with the residuals binned by the fitted binner for `level`.\n\n    \"\"\"\n\n    # NOTE: if the level is not known or encoding is None, then transformer,\n    # differentiator and binner used are the ones of \"_unknown_level\"\n    transformer = self.transformer_series_.get(level, self.transformer_series_['_unknown_level'])\n    differentiator = copy(\n        self.differentiator_.get(level, self.differentiator_[\"_unknown_level\"])\n    )\n    if differentiator is not None:\n        differentiator.set_params(window_size=None)\n\n    if isinstance(y_true, pd.Series):\n        y_true = y_true.to_numpy()\n    if isinstance(y_pred, pd.Series):\n        y_pred = y_pred.to_numpy()\n\n    if self.transformer_series:\n        y_true = transform_numpy(\n                     array             = y_true,\n                     transformer       = transformer,\n                     fit               = False,\n                     inverse_transform = False\n                 )\n        y_pred = transform_numpy(\n                     array             = y_pred,\n                     transformer       = transformer,\n                     fit               = False,\n                     inverse_transform = False\n                 )\n\n    if differentiator is not None:\n        y_true = differentiator.fit_transform(y_true)[differentiator.order:]\n        y_pred = differentiator.fit_transform(y_pred)[differentiator.order:]\n\n    data = pd.DataFrame(\n        {'prediction': y_pred, 'residuals': y_true - y_pred}\n    ).dropna()\n    y_pred = data['prediction'].to_numpy()\n    residuals = data['residuals'].to_numpy()\n\n    binner = self.binner.get(level, self.binner['_unknown_level'])\n    data['bin'] = binner.transform(y_pred).astype(int)\n    residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n    out_sample_residuals = self.out_sample_residuals_.get(level, np.array([]))\n    out_sample_residuals_by_bin = deepcopy(self.out_sample_residuals_by_bin_.get(level, {}))\n\n    out_sample_residuals = (\n        np.array([]) \n        if out_sample_residuals is None\n        else out_sample_residuals\n    )\n    out_sample_residuals_by_bin = (\n        {} \n        if out_sample_residuals_by_bin is None\n        else out_sample_residuals_by_bin\n    )\n    if append:\n        out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        for k, v in residuals_by_bin.items():\n            if k in out_sample_residuals_by_bin:\n                out_sample_residuals_by_bin[k] = np.concatenate(\n                    (out_sample_residuals_by_bin[k], v)\n                )\n            else:\n                out_sample_residuals_by_bin[k] = v\n    else:\n        out_sample_residuals = residuals\n        out_sample_residuals_by_bin = residuals_by_bin\n\n    max_samples = 10_000 // binner.n_bins_\n    rng = np.random.default_rng(seed=random_state)\n    for k, v in out_sample_residuals_by_bin.items():\n        if len(v) &gt; max_samples:\n            sample = rng.choice(a=v, size=max_samples, replace=False)\n            out_sample_residuals_by_bin[k] = sample\n\n    for k in self.binner_intervals_.get(level, {}).keys():\n        if k not in out_sample_residuals_by_bin:\n            out_sample_residuals_by_bin[k] = np.array([])\n\n    empty_bins = [\n        k for k, v in out_sample_residuals_by_bin.items() \n        if v.size == 0\n    ]\n    if empty_bins:\n        warnings.warn(\n            f\"The following bins of level '{level}' have no out of sample residuals: \"\n            f\"{empty_bins}. No predicted values fall in the interval \"\n            f\"{[self.binner_intervals_[level][bin] for bin in empty_bins]}. \"\n            f\"Empty bins will be filled with a random sample of residuals.\", \n            ResidualsUsageWarning\n        )\n        empty_bin_size = min(max_samples, len(out_sample_residuals))\n        for k in empty_bins:\n            out_sample_residuals_by_bin[k] = rng.choice(\n                a       = out_sample_residuals,\n                size    = empty_bin_size,\n                replace = False\n            )\n\n    if len(out_sample_residuals) &gt; 10_000:\n        out_sample_residuals = rng.choice(\n            a       = out_sample_residuals, \n            size    = 10_000, \n            replace = False\n        )\n\n    return out_sample_residuals, out_sample_residuals_by_bin\n</code></pre>"},{"location":"api/ForecasterRecursiveMultiSeries.html#skforecast.recursive._forecaster_recursive_multiseries.ForecasterRecursiveMultiSeries.get_feature_importances","title":"get_feature_importances","text":"<pre><code>get_feature_importances(sort_importance=True)\n</code></pre> <p>Return feature importances of the regressor stored in the forecaster. Only valid when regressor stores internally the feature importances in the attribute <code>feature_importances_</code> or <code>coef_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\recursive\\_forecaster_recursive_multiseries.py</code> <pre><code>def get_feature_importances(\n    self,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importances of the regressor stored in the\n    forecaster. Only valid when regressor stores internally the feature\n    importances in the attribute `feature_importances_` or `coef_`.\n\n    Parameters\n    ----------\n    sort_importance: bool, default True\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `get_feature_importances()`.\"\n        )\n\n    if isinstance(self.regressor, Pipeline):\n        estimator = self.regressor[-1]\n    else:\n        estimator = self.regressor\n\n    if hasattr(estimator, 'feature_importances_'):\n        feature_importances = estimator.feature_importances_\n    elif hasattr(estimator, 'coef_'):\n        feature_importances = estimator.coef_\n    else:\n        warnings.warn(\n            f\"Impossible to access feature importances for regressor of type \"\n            f\"{type(estimator)}. This method is only valid when the \"\n            f\"regressor stores internally the feature importances in the \"\n            f\"attribute `feature_importances_` or `coef_`.\"\n        )\n        feature_importances = None\n\n    if feature_importances is not None:\n        feature_importances = pd.DataFrame({\n                                  'feature': self.X_train_features_names_out_,\n                                  'importance': feature_importances\n                              })\n        if sort_importance:\n            feature_importances = feature_importances.sort_values(\n                by='importance', ascending=False\n            )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterRnn.html","title":"<code>ForecasterRnn</code>","text":""},{"location":"api/ForecasterRnn.html#skforecast.deep_learning.utils.create_and_compile_model","title":"skforecast.deep_learning.utils.create_and_compile_model","text":"<pre><code>create_and_compile_model(\n    series,\n    lags,\n    steps,\n    levels=None,\n    exog=None,\n    recurrent_layer=\"LSTM\",\n    recurrent_units=100,\n    recurrent_layers_kwargs={\"activation\": \"tanh\"},\n    dense_units=64,\n    dense_layers_kwargs={\"activation\": \"relu\"},\n    output_dense_layer_kwargs={\"activation\": \"linear\"},\n    compile_kwargs={\n        \"optimizer\": Adam(),\n        \"loss\": MeanSquaredError(),\n    },\n    model_name=None,\n)\n</code></pre> <p>Build and compile a RNN-based Keras model for time series prediction,  supporting exogenous variables.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Input time series with shape (n_obs, n_series). Each column is a time series.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Number of lagged time steps to consider in the input, index starts at 1,  so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>levels</code> <code>(str, list)</code> <p>Output level(s) (features) to predict. If None, defaults to the names of  input series.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables to be included as input, should have the same number  of rows as <code>series</code>.</p> <code>None</code> <code>recurrent_layer</code> <code>str</code> <p>Type of recurrent layer to be used, 'LSTM' [1], 'GRU' [2], or 'RNN' [3]_.</p> <code>'LSTM'</code> <code>recurrent_units</code> <code>(int, list)</code> <p>Number of units in the recurrent layer(s). Can be an integer for single  recurrent layer, or a list of integers for multiple recurrent layers.</p> <code>100</code> <code>recurrent_layers_kwargs</code> <code>(dict, list)</code> <p>Additional keyword arguments for the recurrent layers [1], [2], [3]_.  Can be a single dictionary for all layers or a list of dictionaries  specifying different parameters for each recurrent layer.</p> <code>{'activation': 'tanh'}</code> <code>dense_units</code> <code>(int, list, tuple, None)</code> <p>Number of units in the dense layer(s) [4]_. Can be an integer for single dense layer, or a list of integers for multiple dense layers.</p> <code>64</code> <code>dense_layers_kwargs</code> <code>(dict, list)</code> <p>Additional keyword arguments for the dense layers [4]_. Can be a single dictionary for all layers or a list of dictionaries specifying different parameters for each dense layer.</p> <code>{'activation': 'relu'}</code> <code>output_dense_layer_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the output dense layer.</p> <code>{'activation': 'linear'}</code> <code>compile_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the model compilation, such as optimizer  and loss function. [5]_</p> <code>{'optimizer': Adam(), 'loss': MeanSquaredError()}</code> <code>model_name</code> <code>str</code> <p>Name of the model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>model</code> <code>Model</code> <p>Compiled Keras model ready for training.</p> References <p>.. [1] LSTM layer Keras documentation.        https://keras.io/api/layers/recurrent_layers/lstm/</p> <p>.. [2] GRU layer Keras documentation.        https://keras.io/api/layers/recurrent_layers/gru/</p> <p>.. [3] SimpleRNN layer Keras documentation.        https://keras.io/api/layers/recurrent_layers/simple_rnn/</p> <p>.. [4] Dense layer Keras documentation.        https://keras.io/api/layers/core_layers/dense/</p> <p>.. [5] Model training APIs: compile method.        https://keras.io/api/models/model_training_apis/</p> Source code in <code>skforecast\\deep_learning\\utils.py</code> <pre><code>def create_and_compile_model(\n    series: pd.DataFrame,\n    lags: int | list[int] | np.ndarray[int] | range[int],\n    steps: int,\n    levels: str | list[str] | tuple[str] | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    recurrent_layer: str = \"LSTM\",\n    recurrent_units: int | list[int] | tuple[int] = 100,\n    recurrent_layers_kwargs: dict[str, Any] | list[dict[str, Any]] | None = {\"activation\": \"tanh\"},\n    dense_units: int | list[int] | tuple[int] | None = 64,\n    dense_layers_kwargs: dict[str, Any] | list[dict[str, Any]] | None = {\"activation\": \"relu\"},\n    output_dense_layer_kwargs: dict[str, Any] | None = {\"activation\": \"linear\"},\n    compile_kwargs: dict[str, Any] = {\"optimizer\": Adam(), \"loss\": MeanSquaredError()},\n    model_name: str | None = None\n) -&gt; keras.models.Model:\n    \"\"\"\n    Build and compile a RNN-based Keras model for time series prediction, \n    supporting exogenous variables.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Input time series with shape (n_obs, n_series). Each column is a time series.\n    lags : int, list, numpy ndarray, range\n        Number of lagged time steps to consider in the input, index starts at 1, \n        so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n    steps : int\n        Number of steps to predict.\n    levels : str, list, default None\n        Output level(s) (features) to predict. If None, defaults to the names of \n        input series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variables to be included as input, should have the same number \n        of rows as `series`.\n    recurrent_layer : str, default 'LSTM'\n        Type of recurrent layer to be used, 'LSTM' [1]_, 'GRU' [2]_, or 'RNN' [3]_.\n    recurrent_units : int, list, default 100\n        Number of units in the recurrent layer(s). Can be an integer for single \n        recurrent layer, or a list of integers for multiple recurrent layers.\n    recurrent_layers_kwargs : dict, list, default {'activation': 'tanh'}\n        Additional keyword arguments for the recurrent layers [1]_, [2]_, [3]_. \n        Can be a single dictionary for all layers or a list of dictionaries \n        specifying different parameters for each recurrent layer.\n    dense_units : int, list, tuple, None, default 64\n        Number of units in the dense layer(s) [4]_. Can be an integer for single\n        dense layer, or a list of integers for multiple dense layers.\n    dense_layers_kwargs : dict, list, default {'activation': 'relu'}\n        Additional keyword arguments for the dense layers [4]_. Can be a single\n        dictionary for all layers or a list of dictionaries specifying different\n        parameters for each dense layer.\n    output_dense_layer_kwargs : dict, default {'activation': 'linear'}\n        Additional keyword arguments for the output dense layer.\n    compile_kwargs : dict, default {'optimizer': Adam(), 'loss': MeanSquaredError()}\n        Additional keyword arguments for the model compilation, such as optimizer \n        and loss function. [5]_\n    model_name : str, default None\n        Name of the model.\n\n    Returns\n    -------\n    model : keras.models.Model\n        Compiled Keras model ready for training.\n\n    References\n    ----------\n    .. [1] LSTM layer Keras documentation.\n           https://keras.io/api/layers/recurrent_layers/lstm/\n\n    .. [2] GRU layer Keras documentation.\n           https://keras.io/api/layers/recurrent_layers/gru/\n\n    .. [3] SimpleRNN layer Keras documentation.\n           https://keras.io/api/layers/recurrent_layers/simple_rnn/\n\n    .. [4] Dense layer Keras documentation.\n           https://keras.io/api/layers/core_layers/dense/\n\n    .. [5] Model training APIs: compile method.\n           https://keras.io/api/models/model_training_apis/\n\n    \"\"\"\n\n    keras_backend = keras.backend.backend()\n\n    print(f\"keras version: {keras.__version__}\")\n    print(f\"Using backend: {keras_backend}\")\n    if keras_backend == \"tensorflow\":\n        import tensorflow\n        print(f\"tensorflow version: {tensorflow.__version__}\")\n    elif keras_backend == \"torch\":\n        import torch\n        print(f\"torch version: {torch.__version__}\")\n    elif keras_backend == \"jax\":\n        import jax\n        print(f\"jax version: {jax.__version__}\")\n    else:\n        print(\"Backend not recognized\")\n    print(\"\")\n\n    if exog is None:\n        model = _create_and_compile_model_no_exog(\n            series=series,\n            lags=lags,\n            steps=steps,\n            levels=levels,\n            recurrent_layer=recurrent_layer,\n            recurrent_units=recurrent_units,\n            recurrent_layers_kwargs=recurrent_layers_kwargs,\n            dense_units=dense_units,\n            dense_layers_kwargs=dense_layers_kwargs,\n            output_dense_layer_kwargs=output_dense_layer_kwargs,\n            compile_kwargs=compile_kwargs,\n            model_name=model_name\n        )\n    else:\n        model = _create_and_compile_model_exog(\n            series=series,\n            lags=lags,\n            steps=steps,\n            levels=levels,\n            exog=exog,\n            recurrent_layer=recurrent_layer,\n            recurrent_units=recurrent_units,\n            recurrent_layers_kwargs=recurrent_layers_kwargs,\n            dense_units=dense_units,\n            dense_layers_kwargs=dense_layers_kwargs,\n            output_dense_layer_kwargs=output_dense_layer_kwargs,\n            compile_kwargs=compile_kwargs,\n            model_name=model_name\n        )\n\n    return model\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn","title":"skforecast.deep_learning._forecaster_rnn.ForecasterRnn","text":"<pre><code>ForecasterRnn(\n    regressor,\n    levels,\n    lags,\n    transformer_series=MinMaxScaler(feature_range=(0, 1)),\n    transformer_exog=MinMaxScaler(feature_range=(0, 1)),\n    fit_kwargs={},\n    forecaster_id=None,\n)\n</code></pre> <p>               Bases: <code>ForecasterBase</code></p> <p>This class turns any regressor compatible with the Keras API into a Keras RNN multi-series multi-step forecaster. A unique model is created to forecast all time steps and series. Keras enables workflows on top of either JAX, TensorFlow, or PyTorch. See documentation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>regressor or pipeline compatible with the Keras API</code> <p>An instance of a regressor or pipeline compatible with the Keras API.</p> required <code>levels</code> <code>(str, list)</code> <p>Name of one or more time series to be predicted. This determine the series the forecaster will be handling. If <code>None</code>, all series used during training will be available for prediction.</p> required <code>lags</code> <code>int, list, numpy ndarray, range</code> <p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p> <ul> <li><code>int</code>: include lags from 1 to <code>lags</code> (included).</li> <li><code>list</code>, <code>1d numpy ndarray</code> or <code>range</code>: include only lags present in  <code>lags</code>, all elements must be int.</li> </ul> required <code>transformer_series</code> <code>(transformer(preprocessor), dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform. Transformation is applied to each <code>series</code> before training  the forecaster. ColumnTransformers are not allowed since they do not have  inverse_transform method.</p> <ul> <li>If single transformer: it is cloned and applied to all series. </li> <li>If <code>dict</code> of transformers: a different transformer can be used for each series.</li> </ul> <code>None</code> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>`None`</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>`None`</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>regressor or pipeline compatible with the Keras API</code> <p>An instance of a regressor or pipeline compatible with the Keras API. An instance of this regressor is trained for each step. All of them are stored in <code>self.regressors_</code>.</p> <code>levels</code> <code>(str, list)</code> <p>Name of one or more time series to be predicted. This determine the series the forecaster will be handling. If <code>None</code>, all series used during training will be available for prediction.</p> <code>layers_names</code> <code>list</code> <p>Names of the layers in the Keras model used as regressor.</p> <code>steps</code> <code>numpy ndarray</code> <p>Future steps the forecaster will predict when using prediction methods.</p> <code>max_step</code> <code>int</code> <p>Maximum step the forecaster is able to predict. It is the maximum value included in <code>steps</code>.</p> <code>lags</code> <code>numpy ndarray</code> <p>Lags used as predictors.</p> <code>max_lag</code> <code>int</code> <p>Maximum lag included in <code>lags</code>.</p> <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors.</p> <code>transformer_series</code> <code>(object, dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. Transformation is applied to each <code>series</code> before training the forecaster. ColumnTransformers are not allowed since they do not have inverse_transform method.</p> <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> <code>transformer_exog</code> <code>transformer</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>last_window_</code> <code>pandas DataFrame</code> <p>This window represents the most recent data observed by the predictor during its training phase. It contains the values needed to predict the next step immediately after the training data. These values are stored in the original scale of the time series before undergoing any transformations or differentiation.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_names_in_</code> <code>list</code> <p>Names of the series used during training.</p> <code>n_series_in</code> <code>int</code> <p>Number of series used during training.</p> <code>n_levels_out</code> <code>int</code> <p>Number of levels (series) to be predicted by the forecaster.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> <code>X_train_dim_names_</code> <code>dict</code> <p>Labels for the multi-dimensional arrays created internally for training.</p> <code>y_train_dim_names_</code> <code>dict</code> <p>Labels for the multi-dimensional arrays created internally for training.</p> <code>series_val</code> <code>pandas DataFrame</code> <p>Values of the series used for validation during training.</p> <code>exog_val</code> <code>pandas DataFrame</code> <p>Values of the exogenous variables used for validation during training.</p> <code>history</code> <code>dict</code> <p>Dictionary with the history of the training of each step. It is created internally to avoid overwriting.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>in_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting training data. Only stored up  to 10_000 values per step in the form <code>{step: residuals}</code>. If  <code>transformer_series</code> is not <code>None</code>, residuals are stored in the  transformed scale.</p> <code>out_sample_residuals_</code> <code>dict</code> <p>Residuals of the model when predicting non-training data. Only stored up  to 10_000 values per step in the form <code>{step: residuals}</code>. Use  <code>set_out_sample_residuals()</code> method to set values. If <code>transformer_series</code>  is not <code>None</code>, residuals are stored in the transformed scale.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>keras_backend_</code> <code>str</code> <p>Keras backend used to fit the forecaster. It can be 'tensorflow', 'torch'  or 'jax'.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>_probabilistic_mode</code> <code>(str, bool)</code> <p>Private attribute used to indicate whether the forecaster should perform  some calculations during backtesting.</p> <code>dropna_from_series</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>encoding</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiation</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiation_max</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiator</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>differentiator_</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <p>Methods:</p> Name Description <code>create_train_X_y</code> <p>Create training matrices. The resulting multi-dimensional matrices contain</p> <code>fit</code> <p>Training Forecaster.</p> <code>create_predict_X</code> <p>Create the predictors needed to predict <code>steps</code> ahead.</p> <code>predict</code> <p>Predict n steps ahead</p> <code>predict_interval</code> <p>Predict n steps ahead and estimate prediction intervals using conformal </p> <code>plot_history</code> <p>Plots the training and validation loss curves from the given history object stored</p> <code>set_params</code> <p>Set new values to the parameters of the scikit-learn model stored in the</p> <code>set_fit_kwargs</code> <p>Set new values for the additional keyword arguments passed to the <code>fit</code></p> <code>set_lags</code> <p>Not used, present here for API consistency by convention.</p> <code>set_in_sample_residuals</code> <p>Set in-sample residuals in case they were not calculated during the</p> <code>set_out_sample_residuals</code> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    levels: str | list[str],\n    lags: int | list[int] | np.ndarray[int] | range[int],\n    transformer_series: object | dict[str, object] | None = MinMaxScaler(\n        feature_range=(0, 1)\n    ),\n    transformer_exog: object | None = MinMaxScaler(feature_range=(0, 1)),\n    fit_kwargs: dict[str, object] | None = {},\n    forecaster_id: str | int | None = None\n) -&gt; None:\n\n    self.regressor = deepcopy(regressor)\n    self.levels = None\n    self.transformer_series = transformer_series\n    self.transformer_series_ = None\n    self.transformer_exog = transformer_exog\n    self.max_lag = None\n    self.window_size = None\n    self.last_window_ = None\n    self.index_type_ = None\n    self.index_freq_ = None\n    self.training_range_ = None\n    self.series_names_in_ = None\n    self.exog_names_in_ = None\n    self.exog_type_in_ = None\n    self.exog_dtypes_in_ = None\n    self.X_train_dim_names_ = None\n    self.y_train_dim_names_ = None\n    self.history_ = None \n    self.is_fitted = False\n    self.creation_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n    self.fit_date = None\n    self.keras_backend_ = None\n    self.skforecast_version = skforecast.__version__\n    self.python_version = sys.version.split(\" \")[0]\n    self.forecaster_id = forecaster_id\n    self._probabilistic_mode = \"no_binned\"\n    self.weight_func = None  # Ignored in this forecaster\n    self.source_code_weight_func = None  # Ignored in this forecaster\n    self.dropna_from_series = False  # Ignored in this forecaster\n    self.encoding = None  # Ignored in this forecaster\n    self.differentiation = None  # Ignored in this forecaster\n    self.differentiation_max = None  # Ignored in this forecaster\n    self.differentiator = None  # Ignored in this forecaster\n    self.differentiator_ = None  # Ignored in this forecaster\n\n    layer_init = self.regressor.layers[0]\n    layer_end = self.regressor.layers[-1]\n    self.layers_names = [layer.name for layer in self.regressor.layers]\n\n    self.lags, self.lags_names, self.max_lag = initialize_lags(\n        type(self).__name__, lags\n    )\n    n_lags_regressor = layer_init.output.shape[1]\n    if len(self.lags) != n_lags_regressor:\n        raise ValueError(\n            f\"Number of lags ({len(self.lags)}) does not match the number of \"\n            f\"lags expected by the regressor architecture ({n_lags_regressor}).\"\n        )\n\n    self.window_size = self.max_lag\n\n    self.steps = np.arange(layer_end.output.shape[1]) + 1\n    self.max_step = np.max(self.steps)\n\n    if isinstance(levels, str):\n        self.levels = [levels]\n    elif isinstance(levels, list):\n        self.levels = levels\n    else:\n        raise TypeError(\n            f\"`levels` argument must be a string or list. Got {type(levels)}.\"\n        )\n\n    self.n_series_in = self.regressor.get_layer('series_input').output.shape[-1]\n    self.n_levels_out = self.regressor.get_layer('output_dense_td_layer').output.shape[-1]\n    self.exog_in_ = True if \"exog_input\" in self.layers_names else False\n    if self.exog_in_:\n        self.n_exog_in = self.regressor.get_layer('exog_input').output.shape[-1]\n    else:\n        self.n_exog_in = None\n        # NOTE: This is needed because the Reshape layer changes the output \n        # shape in _create_and_compile_model_no_exog\n        self.n_levels_out = int(self.n_levels_out / self.max_step)\n\n    if not len(self.levels) == self.n_levels_out:\n        raise ValueError(\n            f\"Number of levels ({len(self.levels)}) does not match the number of \"\n            f\"levels expected by the regressor architecture ({self.n_levels_out}).\"\n        )\n\n    self.series_val = None\n    self.exog_val = None\n    if \"series_val\" in fit_kwargs:\n        if not isinstance(fit_kwargs[\"series_val\"], pd.DataFrame):\n            raise TypeError(\n                f\"`series_val` must be a pandas DataFrame. \"\n                f\"Got {type(fit_kwargs['series_val'])}.\"\n            )\n        self.series_val = fit_kwargs.pop(\"series_val\")            \n\n        if self.exog_in_:\n            if \"exog_val\" not in fit_kwargs.keys():\n                raise ValueError(\n                    \"If `series_val` is provided, `exog_val` must also be \"\n                    \"provided using the `fit_kwargs` argument when the \"\n                    \"regressor has exogenous variables.\"\n                )\n            else:\n                if not isinstance(fit_kwargs[\"exog_val\"], (pd.Series, pd.DataFrame)):\n                    raise TypeError(\n                        f\"`exog_val` must be a pandas Series or DataFrame. \"\n                        f\"Got {type(fit_kwargs['exog_val'])}.\"\n                    )\n                self.exog_val = input_to_frame(\n                    data=fit_kwargs.pop(\"exog_val\"), input_name='exog_val'\n                )\n\n    self.in_sample_residuals_ = None\n    self.in_sample_residuals_by_bin_ = None  # Ignored in this forecaster\n    self.out_sample_residuals_ = None\n    self.out_sample_residuals_by_bin_ = None  # Ignored in this forecaster\n\n    self.fit_kwargs = check_select_fit_kwargs(\n        regressor=self.regressor, fit_kwargs=fit_kwargs\n    )\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.regressor","title":"regressor  <code>instance-attribute</code>","text":"<pre><code>regressor = deepcopy(regressor)\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.levels","title":"levels  <code>instance-attribute</code>","text":"<pre><code>levels = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.transformer_series","title":"transformer_series  <code>instance-attribute</code>","text":"<pre><code>transformer_series = transformer_series\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.transformer_series_","title":"transformer_series_  <code>instance-attribute</code>","text":"<pre><code>transformer_series_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.transformer_exog","title":"transformer_exog  <code>instance-attribute</code>","text":"<pre><code>transformer_exog = transformer_exog\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.max_lag","title":"max_lag  <code>instance-attribute</code>","text":"<pre><code>max_lag = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.last_window_","title":"last_window_  <code>instance-attribute</code>","text":"<pre><code>last_window_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.index_type_","title":"index_type_  <code>instance-attribute</code>","text":"<pre><code>index_type_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.index_freq_","title":"index_freq_  <code>instance-attribute</code>","text":"<pre><code>index_freq_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.training_range_","title":"training_range_  <code>instance-attribute</code>","text":"<pre><code>training_range_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.series_names_in_","title":"series_names_in_  <code>instance-attribute</code>","text":"<pre><code>series_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.exog_names_in_","title":"exog_names_in_  <code>instance-attribute</code>","text":"<pre><code>exog_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.exog_type_in_","title":"exog_type_in_  <code>instance-attribute</code>","text":"<pre><code>exog_type_in_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.exog_dtypes_in_","title":"exog_dtypes_in_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_in_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.X_train_dim_names_","title":"X_train_dim_names_  <code>instance-attribute</code>","text":"<pre><code>X_train_dim_names_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.y_train_dim_names_","title":"y_train_dim_names_  <code>instance-attribute</code>","text":"<pre><code>y_train_dim_names_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.history_","title":"history_  <code>instance-attribute</code>","text":"<pre><code>history_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.is_fitted","title":"is_fitted  <code>instance-attribute</code>","text":"<pre><code>is_fitted = False\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.creation_date","title":"creation_date  <code>instance-attribute</code>","text":"<pre><code>creation_date = strftime('%Y-%m-%d %H:%M:%S')\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.fit_date","title":"fit_date  <code>instance-attribute</code>","text":"<pre><code>fit_date = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.keras_backend_","title":"keras_backend_  <code>instance-attribute</code>","text":"<pre><code>keras_backend_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.skforecast_version","title":"skforecast_version  <code>instance-attribute</code>","text":"<pre><code>skforecast_version = __version__\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.python_version","title":"python_version  <code>instance-attribute</code>","text":"<pre><code>python_version = split(' ')[0]\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.forecaster_id","title":"forecaster_id  <code>instance-attribute</code>","text":"<pre><code>forecaster_id = forecaster_id\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn._probabilistic_mode","title":"_probabilistic_mode  <code>instance-attribute</code>","text":"<pre><code>_probabilistic_mode = 'no_binned'\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.weight_func","title":"weight_func  <code>instance-attribute</code>","text":"<pre><code>weight_func = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.source_code_weight_func","title":"source_code_weight_func  <code>instance-attribute</code>","text":"<pre><code>source_code_weight_func = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.dropna_from_series","title":"dropna_from_series  <code>instance-attribute</code>","text":"<pre><code>dropna_from_series = False\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.differentiation","title":"differentiation  <code>instance-attribute</code>","text":"<pre><code>differentiation = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.differentiation_max","title":"differentiation_max  <code>instance-attribute</code>","text":"<pre><code>differentiation_max = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.differentiator","title":"differentiator  <code>instance-attribute</code>","text":"<pre><code>differentiator = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.differentiator_","title":"differentiator_  <code>instance-attribute</code>","text":"<pre><code>differentiator_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.layers_names","title":"layers_names  <code>instance-attribute</code>","text":"<pre><code>layers_names = [(name) for layer in (layers)]\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = max_lag\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.steps","title":"steps  <code>instance-attribute</code>","text":"<pre><code>steps = arange(shape[1]) + 1\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.max_step","title":"max_step  <code>instance-attribute</code>","text":"<pre><code>max_step = max(steps)\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.n_series_in","title":"n_series_in  <code>instance-attribute</code>","text":"<pre><code>n_series_in = shape[-1]\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.n_levels_out","title":"n_levels_out  <code>instance-attribute</code>","text":"<pre><code>n_levels_out = shape[-1]\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.exog_in_","title":"exog_in_  <code>instance-attribute</code>","text":"<pre><code>exog_in_ = True if 'exog_input' in layers_names else False\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.n_exog_in","title":"n_exog_in  <code>instance-attribute</code>","text":"<pre><code>n_exog_in = shape[-1]\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.series_val","title":"series_val  <code>instance-attribute</code>","text":"<pre><code>series_val = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.exog_val","title":"exog_val  <code>instance-attribute</code>","text":"<pre><code>exog_val = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.in_sample_residuals_","title":"in_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.in_sample_residuals_by_bin_","title":"in_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>in_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.out_sample_residuals_","title":"out_sample_residuals_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.out_sample_residuals_by_bin_","title":"out_sample_residuals_by_bin_  <code>instance-attribute</code>","text":"<pre><code>out_sample_residuals_by_bin_ = None\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.fit_kwargs","title":"fit_kwargs  <code>instance-attribute</code>","text":"<pre><code>fit_kwargs = check_select_fit_kwargs(\n    regressor=regressor, fit_kwargs=fit_kwargs\n)\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def _repr_html_(self):\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    params = str(self.regressor.get_config())\n    compile_config = str(self.regressor.get_compile_config())\n\n    (\n        _,\n        _,\n        series_names_in_,\n        exog_names_in_,\n        transformer_series,\n    ) = self._preprocess_repr(\n            regressor          = None,\n            series_names_in_   = self.series_names_in_,\n            exog_names_in_     = self.exog_names_in_,\n            transformer_series = self.transformer_series,\n        )\n\n    style, unique_id = get_style_repr_html(self.is_fitted)\n\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Regressor:&lt;/strong&gt; {type(self.regressor).__name__}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Layers names:&lt;/strong&gt; {self.layers_names}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Lags:&lt;/strong&gt; {self.lags}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Maximum steps to predict:&lt;/strong&gt; {self.steps}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Keras backend:&lt;/strong&gt; {self.keras_backend_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skforecast version:&lt;/strong&gt; {self.skforecast_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n            &lt;ul&gt;\n                {exog_names_in_}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Data Transformations&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for series:&lt;/strong&gt; {transformer_series}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Training Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Series names:&lt;/strong&gt; {series_names_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Target series (levels):&lt;/strong&gt; {self.levels}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Regressor Parameters&lt;/summary&gt;\n            &lt;ul&gt;\n                {params}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Compile Parameters&lt;/summary&gt;\n            &lt;ul&gt;\n                {compile_config}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n            &lt;ul&gt;\n                {self.fit_kwargs}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrnn.html\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/forecasting-with-deep-learning-rnn-lstm.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    # Return the combined style and content\n    return style + content\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn._create_lags","title":"_create_lags","text":"<pre><code>_create_lags(y)\n</code></pre> <p>Transforms a 1d array into a 3d array (X) and a 3d array (y). Each row in X is associated with a value of y and it represents the lags that precede it.</p> <p>Notice that, the returned matrix X_data, contains the lag 1 in the first column, the lag 2 in the second column and so on.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray</code> <p>1d numpy ndarray Training time series.</p> required <p>Returns:</p> Name Type Description <code>X_data</code> <code>numpy ndarray</code> <p>3d numpy ndarray with the lagged values (predictors). Shape: (samples - max(lags), len(lags))</p> <code>y_data</code> <code>numpy ndarray</code> <p>3d numpy ndarray with the values of the time series related to each row of <code>X_data</code> for each step. Shape: (len(max_step), samples - max(lags))</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def _create_lags(\n    self, \n    y: np.ndarray\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Transforms a 1d array into a 3d array (X) and a 3d array (y). Each row\n    in X is associated with a value of y and it represents the lags that\n    precede it.\n\n    Notice that, the returned matrix X_data, contains the lag 1 in the first\n    column, the lag 2 in the second column and so on.\n\n    Parameters\n    ----------\n    y : numpy ndarray\n        1d numpy ndarray Training time series.\n\n    Returns\n    -------\n    X_data : numpy ndarray\n        3d numpy ndarray with the lagged values (predictors).\n        Shape: (samples - max(lags), len(lags))\n    y_data : numpy ndarray\n        3d numpy ndarray with the values of the time series related to each\n        row of `X_data` for each step.\n        Shape: (len(max_step), samples - max(lags))\n\n    \"\"\"\n\n    n_rows = len(y) - self.window_size - self.max_step + 1  # rows of y_data\n\n    X_data = np.full(\n        shape=(n_rows, (self.window_size)), fill_value=np.nan, order=\"F\", dtype=float\n    )\n    for i, lag in enumerate(range(self.window_size - 1, -1, -1)):\n        X_data[:, i] = y[self.window_size - lag - 1 : -(lag + self.max_step)]\n\n    # Get lags index\n    X_data = X_data[:, self.lags - 1]\n\n    y_data = np.full(\n        shape=(n_rows, self.max_step), fill_value=np.nan, order=\"F\", dtype=float\n    )\n    for step in range(self.max_step):\n        y_data[:, step] = y[self.window_size + step : self.window_size + step + n_rows]\n\n    # Get steps index\n    y_data = y_data[:, self.steps - 1]\n\n    return X_data, y_data\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn._create_train_X_y","title":"_create_train_X_y","text":"<pre><code>_create_train_X_y(series, exog=None)\n</code></pre> <p>Create training matrices. The resulting multi-dimensional matrices contain the target variable and predictors needed to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>numpy ndarray</code> <p>Training values (predictors) for each step. The resulting array has 3 dimensions: (n_observations, n_lags, n_series)</p> <code>exog_train</code> <code>numpy ndarray</code> <p>Value of exogenous variables aligned with X_train. (n_observations, n_exog)</p> <code>y_train</code> <code>numpy ndarray</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. The resulting array has 3 dimensions: (n_observations, n_steps, n_levels)</p> <code>dimension_names</code> <code>dict</code> <p>Labels for the multi-dimensional arrays created internally for training.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables included in the training matrices.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def _create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[\n    np.ndarray, \n    np.ndarray, \n    np.ndarray, \n    dict[int, list], \n    list[str], \n    dict[str, type], \n    dict[str, type]\n]:\n    \"\"\"\n    Create training matrices. The resulting multi-dimensional matrices contain\n    the target variable and predictors needed to train the model.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n\n    Returns\n    -------\n    X_train : numpy ndarray\n        Training values (predictors) for each step. The resulting array has\n        3 dimensions: (n_observations, n_lags, n_series)\n    exog_train: numpy ndarray\n        Value of exogenous variables aligned with X_train. (n_observations, n_exog)\n    y_train : numpy ndarray\n        Values (target) of the time series related to each row of `X_train`.\n        The resulting array has 3 dimensions: (n_observations, n_steps, n_levels)\n    dimension_names : dict\n        Labels for the multi-dimensional arrays created internally for training.\n    exog_names_in_ : list\n        Names of the exogenous variables included in the training matrices.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training before the transformation\n        applied by `transformer_exog`. If `transformer_exog` is not used, it\n        is equal to `exog_dtypes_out_`.\n    exog_dtypes_out_ : dict\n        Type of each exogenous variable/s used in training after the transformation \n        applied by `transformer_exog`. If `transformer_exog` is not used, it \n        is equal to `exog_dtypes_in_`.\n\n    \"\"\"\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}.\")\n\n    _, series_index = check_extract_values_and_index(\n        data=series, data_label=\"`series`\", return_values=False\n    )\n    series_names_in_ = list(series.columns)\n    if not len(series_names_in_) == self.n_series_in:\n        raise ValueError(\n            f\"Number of series in `series` ({len(series_names_in_)}) \"\n            f\"does not match the number of series expected by the model \"\n            f\"architecture ({self.n_series_in}).\"\n        )\n\n    if not set(self.levels).issubset(set(series_names_in_)):\n        raise ValueError(\n            f\"`levels` defined when initializing the forecaster must be \"\n            f\"included in `series` used for training. \"\n            f\"{set(self.levels) - set(series_names_in_)} not found.\"\n        )\n\n    if len(series) &lt; self.window_size + self.max_step:\n        raise ValueError(\n            f\"Minimum length of `series` for training this forecaster is \"\n            f\"{self.window_size + self.max_step}. Reduce the number of \"\n            f\"predicted steps, {self.max_step}, or the maximum \"\n            f\"lag, {self.max_lag}, if no more data is available.\\n\"\n            f\"    Length `series`: {len(series)}.\\n\"\n            f\"    Max step : {self.max_step}.\\n\"\n            f\"    Lags window size: {self.max_lag}.\"\n        )\n\n    if exog is None and self.exog_in_:\n        raise ValueError(\n            \"The regressor architecture expects exogenous variables during \"\n            \"training. Please provide the `exog` argument. If this is \"\n            \"unexpected, check your regressor architecture or the \"\n            \"initialization parameters of the forecaster.\"\n        )\n    if exog is not None and not self.exog_in_:\n        raise ValueError(\n            \"Exogenous variables (`exog`) were provided, but the model \"\n            \"architecture was not built to expect exogenous variables. Please \"\n            \"remove the `exog` argument or rebuild the model to include \"\n            \"exogenous inputs.\"\n        )\n\n    fit_transformer = False\n    if not self.is_fitted:\n        fit_transformer = True\n        self.transformer_series_ = initialize_transformer_series(\n                                       forecaster_name    = type(self).__name__,\n                                       series_names_in_   = series_names_in_,\n                                       transformer_series = self.transformer_series\n                                   )\n\n    # Step 1: Create lags for all columns\n    X_train = []\n    y_train = []\n\n    # TODO: Add method argument to calculate lags and/or steps\n    for serie in series_names_in_:\n        x = series[serie]\n        check_y(y=x)\n        x = transform_series(\n            series=x,\n            transformer=self.transformer_series_[serie],\n            fit=fit_transformer,\n            inverse_transform=False,\n        )\n        X, _ = self._create_lags(x)\n        X_train.append(X)\n\n    for level in self.levels:\n        y = series[level]\n        check_y(y=y)\n        y = transform_series(\n            series=y,\n            transformer=self.transformer_series_[level],\n            fit=fit_transformer,\n            inverse_transform=False,\n        )\n\n        _, y = self._create_lags(y)\n        y_train.append(y)\n\n    X_train = np.stack(X_train, axis=2)\n    y_train = np.stack(y_train, axis=2)\n\n    train_index = series_index[\n        self.max_lag : (len(series_index) - self.max_step + 1)\n    ]\n    dimension_names = {\n        \"X_train\": {\n            0: train_index,\n            1: self.lags_names[::-1],\n            2: series_names_in_,\n        },\n        \"y_train\": {\n            0: train_index,\n            1: [f\"step_{step}\" for step in self.steps],\n            2: self.levels,\n        },\n    }\n\n    if exog is not None:\n\n        check_exog(exog=exog, allow_nan=False)\n        exog = input_to_frame(data=exog, input_name='exog')\n        _, exog_index = check_extract_values_and_index(\n            data=exog, data_label='`exog`', ignore_freq=True, return_values=False\n        )\n\n        if len(exog.columns) != self.n_exog_in:\n            raise ValueError(\n                f\"Number of columns in `exog` ({len(exog.columns)}) \"\n                f\"does not match the number of exogenous variables expected \"\n                f\"by the model architecture ({self.n_exog_in}).\"\n            )\n\n        series_index_no_ws = series_index[self.window_size:]\n        len_series = len(series)\n        len_series_no_ws = len_series - self.window_size\n        len_exog = len(exog)\n        if not len_exog == len_series and not len_exog == len_series_no_ws:\n            raise ValueError(\n                f\"Length of `exog` must be equal to the length of `series` (if \"\n                f\"index is fully aligned) or length of `series` - `window_size` \"\n                f\"(if `exog` starts after the first `window_size` values).\\n\"\n                f\"    `exog`                   : ({exog_index[0]} -- {exog_index[-1]})  (n={len_exog})\\n\"\n                f\"    `series`                 : ({series.index[0]} -- {series.index[-1]})  (n={len_series})\\n\"\n                f\"    `series` - `window_size` : ({series_index_no_ws[0]} -- {series_index_no_ws[-1]})  (n={len_series_no_ws})\"\n            )\n\n        exog_names_in_ = exog.columns.to_list()\n        if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n            raise ValueError(\n                f\"`exog` cannot contain a column named the same as one of \"\n                f\"the series (column names of series).\\n\"\n                f\"  `series` columns : {series_names_in_}.\\n\"\n                f\"  `exog`   columns : {exog_names_in_}.\"\n            )\n\n        exog_n_dim_in = len(exog_names_in_)\n        exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n        exog = transform_dataframe(\n            df=exog,\n            transformer=self.transformer_exog,\n            fit=fit_transformer,\n            inverse_transform=False,\n        )\n        exog_n_dim_out = len(exog.columns)\n        exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n\n        if exog_n_dim_in != exog_n_dim_out:\n            raise ValueError(\n                f\"Number of columns in `exog` after transformation ({exog_n_dim_out}) \"\n                f\"does not match the number of columns before transformation ({exog_n_dim_in}). \"\n                f\"The ForecasterRnn does not support transformations that \"\n                f\"change the number of columns in `exog`. Preprocess `exog` \"\n                f\"before passing it to the `create_and_compile_model` function.\"\n            )\n\n        if len_exog == len_series:\n            if not (exog_index == series_index).all():\n                raise ValueError(\n                    \"When `exog` has the same length as `series`, the index \"\n                    \"of `exog` must be aligned with the index of `series` \"\n                    \"to ensure the correct alignment of values.\"\n                )\n        else:\n            if not (exog_index == series_index_no_ws).all():\n                raise ValueError(\n                    \"When `exog` doesn't contain the first `window_size` \"\n                    \"observations, the index of `exog` must be aligned with \"\n                    \"the index of `series` minus the first `window_size` \"\n                    \"observations to ensure the correct alignment of values.\"\n                )\n\n        exog_train = []\n        for _, exog_name in enumerate(exog.columns):\n            _, exog_step = self._create_lags(exog[exog_name])\n            exog_train.append(exog_step)\n\n        exog_train = np.stack(exog_train, axis=2)\n\n        dimension_names[\"exog_train\"] = {\n            0: train_index,\n            1: [f\"step_{step}\" for step in self.steps],\n            2: exog.columns.to_list(),\n        }\n    else:\n        exog_train = None\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        exog_dtypes_out_ = None\n        dimension_names[\"exog_train\"] = {\n            0: None,\n            1: None,\n            2: None\n        }\n\n    return (\n        X_train, \n        exog_train, \n        y_train, \n        dimension_names,\n        exog_names_in_,\n        exog_dtypes_in_,\n        exog_dtypes_out_\n    )\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.create_train_X_y","title":"create_train_X_y","text":"<pre><code>create_train_X_y(\n    series, exog=None, suppress_warnings=False\n)\n</code></pre> <p>Create training matrices. The resulting multi-dimensional matrices contain the target variable and predictors needed to train the model.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the creation of the training matrices. See skforecast.exceptions.warn_skforecast_categories  for more information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>numpy ndarray</code> <p>Training values (predictors) for each step. The resulting array has 3 dimensions: (n_observations, n_lags, n_series)</p> <code>exog_train</code> <code>numpy ndarray</code> <p>Value of exogenous variables aligned with X_train. (n_observations, n_exog)</p> <code>y_train</code> <code>numpy ndarray</code> <p>Values (target) of the time series related to each row of <code>X_train</code>. The resulting array has 3 dimensions: (n_observations, n_steps, n_levels)</p> <code>dimension_names</code> <code>dict</code> <p>Labels for the multi-dimensional arrays created internally for training.</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def create_train_X_y(\n    self,\n    series: pd.DataFrame,\n    exog: pd.Series | pd.DataFrame | None = None,\n    suppress_warnings: bool = False\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, dict[int, list]]:\n    \"\"\"\n    Create training matrices. The resulting multi-dimensional matrices contain\n    the target variable and predictors needed to train the model.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the creation\n        of the training matrices. See skforecast.exceptions.warn_skforecast_categories \n        for more information.\n\n    Returns\n    -------\n    X_train : numpy ndarray\n        Training values (predictors) for each step. The resulting array has\n        3 dimensions: (n_observations, n_lags, n_series)\n    exog_train: numpy ndarray\n        Value of exogenous variables aligned with X_train. (n_observations, n_exog)\n    y_train : numpy ndarray\n        Values (target) of the time series related to each row of `X_train`.\n        The resulting array has 3 dimensions: (n_observations, n_steps, n_levels)\n    dimension_names : dict\n        Labels for the multi-dimensional arrays created internally for training.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    output = self._create_train_X_y(series=series, exog=exog)\n\n    X_train = output[0]\n    exog_train = output[1]\n    y_train = output[2]\n    dimension_names = output[3]\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_train, exog_train, y_train, dimension_names\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.fit","title":"fit","text":"<pre><code>fit(\n    series,\n    exog=None,\n    store_last_window=True,\n    store_in_sample_residuals=False,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>series</code> and their indexes must be aligned so that series[i] is regressed on exog[i].</p> <code>None</code> <code>store_last_window</code> <code>bool</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <code>True</code> <code>store_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, in-sample residuals will be stored in the forecaster object after fitting (<code>in_sample_residuals_</code> attribute).</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Set a seed for the random generator so that the stored sample  residuals are always deterministic.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def fit(\n    self,\n    series: pd.DataFrame,\n    exog: pd.Series | pd.DataFrame = None,\n    store_last_window: bool = True,\n    store_in_sample_residuals: bool = False,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor\n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `series` and their indexes must be aligned so\n        that series[i] is regressed on exog[i].\n    store_last_window : bool, default True\n        Whether or not to store the last window (`last_window_`) of training data.\n    store_in_sample_residuals : bool, default False\n        If `True`, in-sample residuals will be stored in the forecaster object\n        after fitting (`in_sample_residuals_` attribute).\n    random_state : int, default 123\n        Set a seed for the random generator so that the stored sample \n        residuals are always deterministic.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction\n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action=\"ignore\")\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_ = None\n    self.index_type_ = None\n    self.index_freq_ = None\n    self.training_range_ = None\n    self.series_names_in_ = None\n    self.exog_names_in_ = None\n    self.exog_type_in_ = None\n    self.exog_dtypes_in_ = None\n    self.exog_dtypes_out_ = None\n    self.X_train_dim_names_ = None\n    self.y_train_dim_names_ = None\n    self.exog_train_dim_names_ = None\n    self.in_sample_residuals_ = None\n    self.is_fitted = False\n    self.fit_date = None\n    self.keras_backend_ = keras.backend.backend()\n\n    (\n        X_train,\n        exog_train,\n        y_train,\n        dimension_names,\n        exog_names_in_,\n        exog_dtypes_in_,\n        exog_dtypes_out_,\n    ) = self._create_train_X_y(series=series, exog=exog)\n\n    # NOTE: Need here to avoid refitting the transformer_series_ with the \n    # validation data.\n    self.is_fitted = True\n    series_names_in_ = dimension_names[\"X_train\"][2]\n\n    if self.keras_backend_ == \"torch\":\n\n        import torch\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using '{self.keras_backend_}' backend with device: {device}\")\n\n        torch_device = torch.device(device)\n        X_train = torch.tensor(X_train).to(torch_device)\n        y_train = torch.tensor(y_train).to(torch_device)\n        if exog_train is not None:\n            exog_train = torch.tensor(exog_train).to(torch_device)\n\n    if self.series_val is not None:\n        series_val = self.series_val[series_names_in_]\n        if exog is not None:\n            exog_val = self.exog_val[exog_names_in_]\n        else:\n            exog_val = None\n\n        X_val, exog_val, y_val, *_ = self._create_train_X_y(\n            series=series_val, exog=exog_val\n        )\n        if self.keras_backend_ == \"torch\":\n            X_val = torch.tensor(X_val).to(torch_device)\n            y_val = torch.tensor(y_val).to(torch_device)\n            if exog_val is not None:\n                exog_val = torch.tensor(exog_val).to(torch_device)\n\n        if self.exog_val is not None:\n            history = self.regressor.fit(\n                x=[X_train, exog_train],\n                y=y_train,\n                validation_data=([X_val, exog_val], y_val),\n                **self.fit_kwargs,\n            )\n        else:\n            history = self.regressor.fit(\n                x=X_train,\n                y=y_train,\n                validation_data=(X_val, y_val),\n                **self.fit_kwargs,\n            )\n\n    else:\n        history = self.regressor.fit(\n            x=X_train if exog_train is None else [X_train, exog_train],\n            y=y_train,\n            **self.fit_kwargs,\n        )\n\n    # TODO: Include binning in the forecaster\n    self.in_sample_residuals_ = {}\n    if store_in_sample_residuals:\n\n        # NOTE: Convert to numpy array if using torch backend\n        if self.keras_backend_ == \"torch\":\n            y_train = y_train.detach().cpu().numpy()\n\n        residuals = y_train - self.regressor.predict(\n            x=X_train if exog_train is None else [X_train, exog_train], verbose=0\n        )\n\n        residuals = np.concatenate(\n            [residuals[:, i, :] for i, step in enumerate(self.steps)]\n        )\n\n        rng = np.random.default_rng(seed=random_state)\n        for i, level in enumerate(self.levels):\n            residuals_level = residuals[:, i]\n            if len(residuals_level) &gt; 10_000:\n                residuals_level = residuals_level[\n                    rng.integers(low=0, high=len(residuals_level), size=10_000)\n                ]\n            self.in_sample_residuals_[level] = residuals_level\n    else:\n        for level in self.levels:\n            self.in_sample_residuals_[level] = None\n\n    self.series_names_in_ = series_names_in_\n    self.X_train_series_names_in_ = series_names_in_\n    self.X_train_dim_names_ = dimension_names[\"X_train\"]\n    self.y_train_dim_names_ = dimension_names[\"y_train\"]\n    self.history_ = history.history\n\n    self.fit_date = pd.Timestamp.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n    self.training_range_ = series.index[[0, -1]]\n    self.index_type_ = type(series.index)\n    if isinstance(series.index, pd.DatetimeIndex):\n        self.index_freq_ = series.index.freqstr\n    else:\n        self.index_freq_ = series.index.step\n\n    if exog is not None:\n        # NOTE: self.exog_in_ is determined by the regressor architecture and\n        # set during initialization.\n        self.exog_names_in_ = exog_names_in_\n        self.exog_type_in_ = type(exog)\n        self.exog_dtypes_in_ = exog_dtypes_in_\n        self.exog_dtypes_out_ = exog_dtypes_out_\n        self.exog_train_dim_names_ = dimension_names[\"exog_train\"]\n        self.X_train_exog_names_out_ = dimension_names[\"exog_train\"][2]\n        self.X_train_features_names_out_ = dimension_names[\"X_train\"][1] + dimension_names[\"exog_train\"][2]\n    else:\n        self.X_train_features_names_out_ = dimension_names[\"X_train\"][1]\n\n    if store_last_window:\n        self.last_window_ = series.iloc[-self.max_lag :, :].copy()\n\n    set_skforecast_warnings(suppress_warnings, action=\"default\")\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn._create_predict_inputs","title":"_create_predict_inputs","text":"<pre><code>_create_predict_inputs(\n    steps=None,\n    levels=None,\n    last_window=None,\n    exog=None,\n    predict_probabilistic=False,\n    use_in_sample_residuals=True,\n    check_inputs=True,\n)\n</code></pre> <p>Create the inputs needed for the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined in the regressor architecture.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as defined in the regressor architecture.</li> </ul> <code>None</code> <code>levels</code> <code>(str, list)</code> <p>Name(s) of the time series to be predicted. It must be included in <code>levels</code>, defined when initializing the forecaster. If <code>None</code>, all all series used during training will be available for prediction.</p> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>predict_probabilistic</code> <code>bool</code> <p>If <code>True</code>, the necessary checks for probabilistic predictions will be  performed.</p> <code>False</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>X</code> <code>list</code> <p>List of numpy arrays needed for prediction. The first element is the matrix of lags and the second element is the matrix of exogenous variables.</p> <code>X_predict_dimension_names</code> <code>dict</code> <p>Labels for the multi-dimensional arrays created internally for prediction.</p> <code>steps</code> <code>list</code> <p>Steps to predict.</p> <code>levels</code> <code>list</code> <p>Levels (series) to predict.</p> <code>prediction_index</code> <code>pandas Index</code> <p>Index of the predictions.</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: int | list[int] | None = None,\n    levels: str | list[str] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    predict_probabilistic: bool = False,\n    use_in_sample_residuals: bool = True,\n    check_inputs: bool = True\n) -&gt; tuple[list[np.ndarray], dict[str, dict], list[int], list[str], pd.Index]:\n    \"\"\"\n    Create the inputs needed for the prediction process.\n\n    Parameters\n    ----------\n    steps : int, list, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined in the regressor architecture.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as defined in the regressor\n        architecture.\n    levels : str, list, default None\n        Name(s) of the time series to be predicted. It must be included\n        in `levels`, defined when initializing the forecaster. If `None`, all\n        all series used during training will be available for prediction.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    predict_probabilistic : bool, default False\n        If `True`, the necessary checks for probabilistic predictions will be \n        performed.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    X : list\n        List of numpy arrays needed for prediction. The first element\n        is the matrix of lags and the second element is the matrix of\n        exogenous variables.\n    X_predict_dimension_names : dict\n        Labels for the multi-dimensional arrays created internally for prediction.\n    steps : list\n        Steps to predict.\n    levels : list\n        Levels (series) to predict.\n    prediction_index : pandas Index\n        Index of the predictions.\n\n    \"\"\"\n\n    levels, _ = prepare_levels_multiseries(\n        X_train_series_names_in_=self.levels, levels=levels\n    )\n\n    steps = prepare_steps_direct(\n                max_step = self.steps,\n                steps    = steps\n            )\n\n    if last_window is None:\n        last_window = self.last_window_\n\n    if check_inputs:\n        check_predict_input(\n            forecaster_name=type(self).__name__,\n            steps=steps,\n            is_fitted=self.is_fitted,\n            exog_in_=self.exog_in_,\n            index_type_=self.index_type_,\n            index_freq_=self.index_freq_,\n            window_size=self.window_size,\n            last_window=last_window,\n            exog=exog,\n            exog_names_in_=self.exog_names_in_,\n            interval=None,\n            max_step=self.max_step,\n            levels=levels,\n            levels_forecaster=self.levels,\n            series_names_in_=self.series_names_in_,\n        )\n\n        if predict_probabilistic:\n            check_residuals_input(\n                forecaster_name              = type(self).__name__,\n                use_in_sample_residuals      = use_in_sample_residuals,\n                in_sample_residuals_         = self.in_sample_residuals_,\n                out_sample_residuals_        = self.out_sample_residuals_,\n                use_binned_residuals         = False,\n                in_sample_residuals_by_bin_  = None,\n                out_sample_residuals_by_bin_ = None,\n                levels                       = self.levels\n            )\n\n    last_window = last_window.iloc[\n        -self.window_size :, last_window.columns.get_indexer(self.series_names_in_)\n    ].copy()\n\n    last_window_values = last_window.to_numpy()\n    last_window_matrix = np.full(\n        shape=last_window.shape, fill_value=np.nan, order='F', dtype=float\n    )\n    for idx_series, series in enumerate(self.series_names_in_):\n        last_window_series = last_window_values[:, idx_series]\n        last_window_series = transform_numpy(\n            array=last_window_series,\n            transformer=self.transformer_series_[series],\n            fit=False,\n            inverse_transform=False,\n        )\n        last_window_matrix[:, idx_series] = last_window_series\n\n    X = [np.reshape(last_window_matrix, (1, self.max_lag, last_window.shape[1]))]\n    X_predict_dimension_names = {\n        \"X_autoreg\": {\n            0: \"batch\",\n            1: self.lags_names[::-1],\n            2: self.X_train_series_names_in_\n        }\n    }\n\n    if exog is not None:\n\n        exog = input_to_frame(data=exog, input_name='exog')\n        exog = transform_dataframe(\n            df=exog,\n            transformer=self.transformer_exog,\n            fit=False,\n            inverse_transform=False,\n        )\n\n        exog_pred = exog.to_numpy()[:self.max_step]\n\n        # NOTE: This is done to ensure that the exogenous variables\n        # have the same number of rows as the maximum step to predict \n        # during backtesting when the last fold is incomplete \n        if len(exog_pred) &lt; self.max_step:\n            exog_pred = np.concatenate(\n                [\n                    exog_pred,\n                    np.full(\n                        shape=(self.max_step - len(exog_pred), exog_pred.shape[1]),\n                        fill_value=0.,\n                        dtype=float\n                    )\n                ],\n                axis=0\n            )\n\n        exog_pred = np.expand_dims(exog_pred, axis=0)\n        X.append(exog_pred)\n\n        X_predict_dimension_names[\"exog_pred\"] = {\n            0: \"batch\",\n            1: [f\"step_{step}\" for step in self.steps],\n            2: self.X_train_exog_names_out_\n        }\n\n    prediction_index = expand_index(\n                           index = last_window.index,\n                           steps = max(steps)\n                       )[np.array(steps) - 1]\n    if isinstance(last_window.index, pd.DatetimeIndex) and np.array_equal(\n        steps, np.arange(min(steps), max(steps) + 1)\n    ):\n        prediction_index.freq = last_window.index.freq\n\n    return X, X_predict_dimension_names, steps, levels, prediction_index\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.create_predict_X","title":"create_predict_X","text":"<pre><code>create_predict_X(\n    steps=None,\n    levels=None,\n    last_window=None,\n    exog=None,\n    suppress_warnings=False,\n    check_inputs=True,\n)\n</code></pre> <p>Create the predictors needed to predict <code>steps</code> ahead.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined in the regressor architecture.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as defined in the regressor architecture.</li> </ul> <code>None</code> <code>levels</code> <code>(str, list)</code> <p>Name(s) of the time series to be predicted. It must be included in <code>levels</code>, defined when initializing the forecaster. If <code>None</code>, all all series used during training will be available for prediction.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed to  predict <code>steps</code>. If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>X_predict</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the predictors for each step.</p> <code>exog_predict</code> <code>pandas DataFrame</code> <p>Pandas DataFrame with the exogenous variables for each step.</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def create_predict_X(\n    self,\n    steps: int | list[int] | None = None,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    suppress_warnings: bool = False,\n    check_inputs: bool = True\n) -&gt; tuple[pd.DataFrame, pd.DataFrame | None]:\n    \"\"\"\n    Create the predictors needed to predict `steps` ahead.\n\n    Parameters\n    ----------\n    steps : int, list, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined in the regressor architecture.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as defined in the regressor\n        architecture.\n    levels : str, list, default None\n        Name(s) of the time series to be predicted. It must be included\n        in `levels`, defined when initializing the forecaster. If `None`, all\n        all series used during training will be available for prediction.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed to \n        predict `steps`.\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    X_predict : pandas DataFrame\n        Pandas DataFrame with the predictors for each step.\n    exog_predict : pandas DataFrame\n        Pandas DataFrame with the exogenous variables for each step.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    (\n        X,\n        X_predict_dimension_names,\n        *_\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            levels       = levels,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs\n        )\n\n    X_predict = pd.DataFrame(\n                    data    = X[0][0], \n                    columns = X_predict_dimension_names['X_autoreg'][2],\n                    index   = X_predict_dimension_names['X_autoreg'][1] \n                )\n\n    exog_predict = None\n    if self.exog_in_:\n        exog_predict = pd.DataFrame(\n            data    = X[1][0], \n            columns = X_predict_dimension_names['exog_pred'][2],\n            index   = X_predict_dimension_names['exog_pred'][1]\n        )\n        # NOTE: not needed in this forecaster\n        # categorical_features = any(\n        #     not pd.api.types.is_numeric_dtype(dtype) or pd.api.types.is_bool_dtype(dtype) \n        #     for dtype in set(self.exog_dtypes_out_)\n        # )\n        # if categorical_features:\n        #     X_predict = X_predict.astype(self.exog_dtypes_out_)\n\n    if self.transformer_series is not None:\n        warnings.warn(\n            \"The output matrix is in the transformed scale due to the \"\n            \"inclusion of transformations in the Forecaster. \"\n            \"As a result, any predictions generated using this matrix will also \"\n            \"be in the transformed scale. Please refer to the documentation \"\n            \"for more details: \"\n            \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n            DataTransformationWarning\n        )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return X_predict, exog_predict\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.predict","title":"predict","text":"<pre><code>predict(\n    steps=None,\n    levels=None,\n    last_window=None,\n    exog=None,\n    suppress_warnings=False,\n    check_inputs=True,\n)\n</code></pre> <p>Predict n steps ahead</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined in the regressor architecture.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as defined in the regressor architecture.</li> </ul> <code>None</code> <code>levels</code> <code>(str, list)</code> <p>Name(s) of the time series to be predicted. It must be included in <code>levels</code>, defined when initializing the forecaster. If <code>None</code>, all all series used during training will be available for prediction.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>`None`</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the fitting process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>`False`</code> <code>check_inputs</code> <code>bool</code> <p>If <code>True</code>, the input is checked for possible warnings and errors  with the <code>check_predict_input</code> function. This argument is created  for internal use and is not recommended to be changed.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values.</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def predict(\n    self,\n    steps: int | list[int] | None = None,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    suppress_warnings: bool = False,\n    check_inputs: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead\n\n    Parameters\n    ----------\n    steps : int, list, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined in the regressor architecture.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as defined in the regressor\n        architecture.\n    levels : str, list, default None\n        Name(s) of the time series to be predicted. It must be included\n        in `levels`, defined when initializing the forecaster. If `None`, all\n        all series used during training will be available for prediction.\n    last_window : pandas DataFrame, default `None`\n        Series values used to create the predictors (lags) needed in the\n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    suppress_warnings : bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the fitting\n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    check_inputs : bool, default True\n        If `True`, the input is checked for possible warnings and errors \n        with the `check_predict_input` function. This argument is created \n        for internal use and is not recommended to be changed.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Predicted values.\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action=\"ignore\")\n\n    (\n        X,\n        _,\n        steps,\n        levels,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps        = steps,\n            levels       = levels,\n            last_window  = last_window,\n            exog         = exog,\n            check_inputs = check_inputs\n        )\n\n    predictions = self.regressor.predict(\n        X[0] if not self.exog_in_ else X, verbose=0\n    )\n    predictions = np.reshape(\n        predictions, (predictions.shape[1], predictions.shape[2])\n    )[np.array(steps) - 1]\n\n    for i, level in enumerate(self.levels):\n        # NOTE: The inverse transformation is applied only if the level\n        # is included in the levels to predict.\n        if level in levels:\n            predictions[:, i] = transform_numpy(\n                array             = predictions[:, i],\n                transformer       = self.transformer_series_[level],\n                fit               = False,\n                inverse_transform = True\n            )\n\n    n_steps, n_levels = predictions.shape\n    predictions = pd.DataFrame(\n        {\"level\": np.tile(self.levels, n_steps), \"pred\": predictions.ravel()},\n        index = np.repeat(prediction_index, n_levels),\n    )\n    predictions = predictions[predictions['level'].isin(levels)]\n\n    set_skforecast_warnings(suppress_warnings, action=\"default\")\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn._predict_interval_conformal","title":"_predict_interval_conformal","text":"<pre><code>_predict_interval_conformal(\n    steps=None,\n    levels=None,\n    last_window=None,\n    exog=None,\n    nominal_coverage=0.95,\n    use_in_sample_residuals=True,\n)\n</code></pre> <p>Generate prediction intervals using the conformal prediction  split method [1]_.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined in the regressor architecture.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as defined in the regressor architecture.</li> </ul> <code>None</code> <code>levels</code> <code>(str, list)</code> <p>Name(s) of the time series to be predicted. It must be included in <code>levels</code>, defined when initializing the forecaster. If <code>None</code>, all all series used during training will be available for prediction.</p> <code>None</code> <code>last_window</code> <code>pandas Series, pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in<code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>nominal_coverage</code> <code>float</code> <p>Nominal coverage, also known as expected coverage, of the prediction intervals. Must be between 0 and 1.</p> <code>0.95</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def _predict_interval_conformal(\n    self,\n    steps: int | list[int] | None = None,\n    levels: str | list[str] | None = None,\n    last_window: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    nominal_coverage: float = 0.95,\n    use_in_sample_residuals: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate prediction intervals using the conformal prediction \n    split method [1]_.\n\n    Parameters\n    ----------\n    steps : int, list, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined in the regressor architecture.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as defined in the regressor\n        architecture.\n    levels : str, list, default None\n        Name(s) of the time series to be predicted. It must be included\n        in `levels`, defined when initializing the forecaster. If `None`, all\n        all series used during training will be available for prediction.\n    last_window : pandas Series, pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in` self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    nominal_coverage : float, default 0.95\n        Nominal coverage, also known as expected coverage, of the prediction\n        intervals. Must be between 0 and 1.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    References\n    ----------\n    .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    (\n        X,\n        _,\n        steps,\n        levels,\n        prediction_index\n    ) = self._create_predict_inputs(\n            steps                   = steps,\n            levels                  = levels,\n            last_window             = last_window,\n            exog                    = exog,\n            predict_probabilistic   = True,\n            use_in_sample_residuals = use_in_sample_residuals\n        )\n\n    if use_in_sample_residuals:\n        residuals = self.in_sample_residuals_\n    else:\n        residuals = self.out_sample_residuals_\n\n    predictions = self.regressor.predict(\n        X[0] if not self.exog_in_ else X, verbose=0\n    )\n    predictions = np.reshape(\n        predictions, (predictions.shape[1], predictions.shape[2])\n    )[np.array(steps) - 1]\n\n    n_steps = len(steps)\n    n_levels = len(self.levels)\n    correction_factor = np.full(\n        shape=(n_steps, n_levels), fill_value=np.nan, order='C', dtype=float\n    )\n    for i, level in enumerate(self.levels):\n        # NOTE: The correction factor is calculated only for the levels\n        # included in the levels to predict.\n        if level in levels:\n            correction_factor[:, i] = np.quantile(\n                np.abs(residuals[level]), nominal_coverage\n            )\n        else:\n            correction_factor[:, i] = 0.\n\n    lower_bound = predictions - correction_factor\n    upper_bound = predictions + correction_factor\n\n    # NOTE: Create a 3D array with shape (n_levels, intervals, steps)\n    predictions = np.array([predictions, lower_bound, upper_bound]).swapaxes(0, 2)\n\n    for i, level in enumerate(self.levels):\n        # NOTE: The inverse transformation is applied only if the level\n        # is included in the levels to predict.\n        if level in levels:\n            transformer_level = self.transformer_series_[level]\n            if transformer_level is not None:\n                predictions[i, :, :] = np.apply_along_axis(\n                    func1d            = transform_numpy,\n                    axis              = 0,\n                    arr               = predictions[i, :, :],\n                    transformer       = transformer_level,\n                    fit               = False,\n                    inverse_transform = True\n                )\n\n    predictions = pd.DataFrame(\n                      data    = predictions.swapaxes(0, 1).reshape(-1, 3),\n                      index   = np.repeat(prediction_index, len(self.levels)),\n                      columns = [\"pred\", \"lower_bound\", \"upper_bound\"]\n                  )\n    predictions.insert(0, 'level', np.tile(self.levels, n_steps))\n    predictions = predictions[predictions['level'].isin(levels)]\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.predict_interval","title":"predict_interval","text":"<pre><code>predict_interval(\n    steps=None,\n    levels=None,\n    last_window=None,\n    exog=None,\n    method=\"conformal\",\n    interval=[5, 95],\n    use_in_sample_residuals=True,\n    suppress_warnings=False,\n    n_boot=None,\n    use_binned_residuals=None,\n    random_state=None,\n)\n</code></pre> <p>Predict n steps ahead and estimate prediction intervals using conformal  prediction method. Refer to the References section for additional details.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>(int, list)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined in the regressor architecture.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as defined in the regressor architecture.</li> </ul> <code>None</code> <code>levels</code> <code>(str, list)</code> <p>Name(s) of the time series to be predicted. It must be included in <code>levels</code>, defined when initializing the forecaster. If <code>None</code>, all all series used during training will be available for prediction.</p> <code>None</code> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>method</code> <code>str</code> <p>Employs the conformal prediction split method for interval estimation [1]_.</p> <code>'conformal'</code> <code>interval</code> <code>(float, list, tuple)</code> <p>Confidence level of the prediction interval. Interpretation depends  on the method used:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0  and 1). For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code>  percentiles.</li> <li>If <code>list</code> or <code>tuple</code>, defines the exact percentiles to compute, which  must be between 0 and 100 inclusive. For example, interval  of 95% should be as <code>interval = [2.5, 97.5]</code>.</li> <li>When using <code>method='conformal'</code>, the interval must be a float or  a list/tuple defining a symmetric interval.</li> </ul> <code>[5, 95]</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the prediction  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>n_boot</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>use_binned_residuals</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <code>random_state</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame with the predictions and the lower and upper bounds of the estimated interval. The columns are <code>level</code>, <code>pred</code>, <code>lower_bound</code>, <code>upper_bound</code>.</p> References <p>.. [1] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int | list[int] | None = None,\n    levels: str | list[str] | None = None,\n    last_window: pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    method: str = 'conformal',\n    interval: float | list[float] | tuple[float] = [5, 95],\n    use_in_sample_residuals: bool = True,\n    suppress_warnings: bool = False,\n    n_boot: Any = None,\n    use_binned_residuals: Any = None,\n    random_state: Any = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict n steps ahead and estimate prediction intervals using conformal \n    prediction method. Refer to the References section for additional details.\n\n    Parameters\n    ----------\n    steps : int, list, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined in the regressor architecture.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as defined in the regressor\n        architecture.\n    levels : str, list, default None\n        Name(s) of the time series to be predicted. It must be included\n        in `levels`, defined when initializing the forecaster. If `None`, all\n        all series used during training will be available for prediction.\n    last_window : pandas DataFrame, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    method : str, default 'conformal'\n        Employs the conformal prediction split method for interval estimation [1]_.\n    interval : float, list, tuple, default [5, 95]\n        Confidence level of the prediction interval. Interpretation depends \n        on the method used:\n\n        - If `float`, represents the nominal (expected) coverage (between 0 \n        and 1). For instance, `interval=0.95` corresponds to `[2.5, 97.5]` \n        percentiles.\n        - If `list` or `tuple`, defines the exact percentiles to compute, which \n        must be between 0 and 100 inclusive. For example, interval \n        of 95% should be as `interval = [2.5, 97.5]`.\n        - When using `method='conformal'`, the interval must be a float or \n        a list/tuple defining a symmetric interval.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the prediction \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    n_boot : Ignored\n        Not used, present here for API consistency by convention.\n    use_binned_residuals : Ignored\n        Not used, present here for API consistency by convention.\n    random_state : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Long-format DataFrame with the predictions and the lower and upper\n        bounds of the estimated interval. The columns are `level`, `pred`,\n        `lower_bound`, `upper_bound`.\n\n    References\n    ----------        \n    .. [1] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    if method == \"conformal\":\n\n        if isinstance(interval, (list, tuple)):\n            check_interval(interval=interval, ensure_symmetric_intervals=True)\n            nominal_coverage = (interval[1] - interval[0]) / 100\n        else:\n            check_interval(alpha=interval, alpha_literal='interval')\n            nominal_coverage = interval\n\n        predictions = self._predict_interval_conformal(\n                          steps                   = steps,\n                          levels                  = levels,\n                          last_window             = last_window,\n                          exog                    = exog,\n                          nominal_coverage        = nominal_coverage,\n                          use_in_sample_residuals = use_in_sample_residuals\n                      )\n    else:\n        raise ValueError(\n            f\"Invalid `method` '{method}'. Only 'conformal' is available.\"\n        )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.plot_history","title":"plot_history","text":"<pre><code>plot_history(\n    ax=None, exclude_first_iteration=False, **fig_kw\n)\n</code></pre> <p>Plots the training and validation loss curves from the given history object stored in the ForecasterRnn.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots() internally.</p> <code>`None`</code> <code>exclude_first_iteration</code> <code>bool</code> <p>Whether to exclude the first epoch from the plot.</p> <code>`False`</code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.subplots().</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def plot_history(\n    self,\n    ax: matplotlib.axes.Axes = None,\n    exclude_first_iteration: bool = False,\n    **fig_kw,\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Plots the training and validation loss curves from the given history object stored\n    in the ForecasterRnn.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes, default `None`\n        Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots()\n        internally.\n    exclude_first_iteration : bool, default `False`\n        Whether to exclude the first epoch from the plot.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.subplots().\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, **fig_kw)\n    else:\n        fig = ax.get_figure()\n\n    # Setting up the plot style\n    if self.history_ is None:\n        raise ValueError(\"ForecasterRnn has not been fitted yet.\")\n\n    # Determine the range of epochs to plot, excluding the first one if specified\n    epoch_range = range(1, len(self.history_[\"loss\"]) + 1)\n    if exclude_first_iteration:\n        epoch_range = range(2, len(self.history_[\"loss\"]) + 1)\n\n    # Plotting training loss\n    ax.plot(\n        epoch_range,\n        self.history_[\"loss\"][\n            exclude_first_iteration:\n        ],  # Skip first element if specified\n        color=\"b\",\n        label=\"Training Loss\",\n    )\n\n    # Plotting validation loss\n    if \"val_loss\" in self.history_:\n        ax.plot(\n            epoch_range,\n            self.history_[\"val_loss\"][\n                exclude_first_iteration:\n            ],  # Skip first element if specified\n            color=\"r\",\n            label=\"Validation Loss\",\n        )\n\n    # Labeling the axes and adding a title\n    ax.set_xlabel(\"Epochs\")\n    ax.set_ylabel(\"Loss\")\n    ax.set_title(\"Training and Validation Loss\")\n\n    # Adding a legend\n    ax.legend()\n\n    # Displaying grid for better readability\n    ax.grid(True, linestyle=\"--\", alpha=0.7)\n\n    # Setting x-axis ticks to integers only\n    ax.set_xticks(epoch_range)\n\n    return fig\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.set_params","title":"set_params","text":"<pre><code>set_params(params)\n</code></pre> <p>Set new values to the parameters of the scikit-learn model stored in the forecaster. It is important to note that all models share the same configuration of parameters and hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def set_params(self, params: dict) -&gt; None:  \n    \"\"\"\n    Set new values to the parameters of the scikit-learn model stored in the\n    forecaster. It is important to note that all models share the same\n    configuration of parameters and hyperparameters.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.reset_states()\n    self.regressor.compile(**params)\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.set_fit_kwargs","title":"set_fit_kwargs","text":"<pre><code>set_fit_kwargs(fit_kwargs)\n</code></pre> <p>Set new values for the additional keyword arguments passed to the <code>fit</code> method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def set_fit_kwargs(self, fit_kwargs: dict) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit`\n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.set_lags","title":"set_lags","text":"<pre><code>set_lags(lags)\n</code></pre> <p>Not used, present here for API consistency by convention.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def set_lags(self, lags: Any) -&gt; None:  # pragma: no cover\n    \"\"\"\n    Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.set_in_sample_residuals","title":"set_in_sample_residuals","text":"<pre><code>set_in_sample_residuals(\n    series,\n    exog=None,\n    random_state=123,\n    suppress_warnings=False,\n)\n</code></pre> <p>Set in-sample residuals in case they were not calculated during the training process. </p> <p>In-sample residuals are calculated as the difference between the true  values and the predictions made by the forecaster using the training  data. The following internal attributes are updated:</p> <ul> <li><code>in_sample_residuals_</code>: Dictionary containing a numpy ndarray with the residuals for each series in the form <code>{series: residuals}</code>.</li> </ul> <p>A total of 10_000 residuals are stored in the attribute <code>in_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored. The number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the sampling  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def set_in_sample_residuals(\n    self,\n    series: pd.DataFrame,\n    exog: pd.Series | pd.DataFrame = None,\n    random_state: int = 123,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Set in-sample residuals in case they were not calculated during the\n    training process. \n\n    In-sample residuals are calculated as the difference between the true \n    values and the predictions made by the forecaster using the training \n    data. The following internal attributes are updated:\n\n    + `in_sample_residuals_`: Dictionary containing a numpy ndarray with the\n    residuals for each series in the form `{series: residuals}`.\n\n    A total of 10_000 residuals are stored in the attribute `in_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored. The number of residuals stored per bin is\n    limited to `10_000 // self.binner.n_bins_`.\n\n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n    suppress_warnings : bool, default False\n        If `True`, skforecast warnings will be suppressed during the sampling \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_in_sample_residuals()`.\"\n        )\n\n    if not isinstance(series, pd.DataFrame):\n        raise TypeError(\n            f\"`series` must be a pandas DataFrame. Got {type(series)}.\"\n        )\n\n    series_index_range = check_extract_values_and_index(\n        data=series, data_label='`series`', return_values=False\n    )[1][[0, -1]]\n    if not series_index_range.equals(self.training_range_):\n        raise IndexError(\n            f\"The index range of `series` does not match the range \"\n            f\"used during training. Please ensure the index is aligned \"\n            f\"with the training data.\\n\"\n            f\"    Expected : {self.training_range_}\\n\"\n            f\"    Received : {series_index_range}\"\n        )\n\n    (\n        X_train,\n        exog_train,\n        y_train,\n        dimension_names,\n        *_\n    ) = self._create_train_X_y(series=series, exog=exog)\n\n    if exog is not None:\n        X_train_features_names_out_ = dimension_names[\"X_train\"][1] + dimension_names[\"exog_train\"][2]\n    else:\n        X_train_features_names_out_ = dimension_names[\"X_train\"][1]\n\n    if not X_train_features_names_out_ == self.X_train_features_names_out_:\n        raise ValueError(\n            f\"Feature mismatch detected after matrix creation. The features \"\n            f\"generated from the provided data do not match those used during \"\n            f\"the training process. To correctly set in-sample residuals, \"\n            f\"ensure that the same data and preprocessing steps are applied.\\n\"\n            f\"    Expected output : {self.X_train_features_names_out_}\\n\"\n            f\"    Current output  : {X_train_features_names_out_}\"\n        )\n\n    # TODO: Include binning in the forecaster\n    self.in_sample_residuals_ = {}\n    residuals = y_train - self.regressor.predict(\n        x=X_train if exog_train is None else [X_train, exog_train], verbose=0\n    )\n    residuals = np.concatenate(\n        [residuals[:, i, :] for i, step in enumerate(self.steps)]\n    )\n\n    rng = np.random.default_rng(seed=random_state)\n    for i, level in enumerate(self.levels):\n        residuals_level = residuals[:, i]\n        if len(residuals_level) &gt; 10_000:\n            residuals_level = residuals_level[\n                rng.integers(low=0, high=len(residuals_level), size=10_000)\n            ]\n        self.in_sample_residuals_[level] = residuals_level\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n</code></pre>"},{"location":"api/ForecasterRnn.html#skforecast.deep_learning._forecaster_rnn.ForecasterRnn.set_out_sample_residuals","title":"set_out_sample_residuals","text":"<pre><code>set_out_sample_residuals(\n    y_true, y_pred, append=False, random_state=123\n)\n</code></pre> <p>Set new values to the attribute <code>out_sample_residuals_</code>. Out of sample residuals are meant to be calculated using observations that did not participate in the training process. <code>y_true</code> and <code>y_pred</code> are expected to be in the original scale of the time series. Residuals are calculated as <code>y_true</code> - <code>y_pred</code>, after applying the necessary transformations and differentiations if the forecaster includes them (<code>self.transformer_series</code> and <code>self.differentiation</code>).</p> <p>A total of 10_000 residuals are stored in the attribute <code>out_sample_residuals_</code>. If the number of residuals is greater than 10_000, a random sample of 10_000 residuals is stored.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>dict</code> <p>Dictionary of numpy ndarrays or pandas Series with the true values of the time series for each series in the form {series: y_true}.</p> required <code>y_pred</code> <code>dict</code> <p>Dictionary of numpy ndarrays or pandas Series with the predicted values of the time series for each series in the form {series: y_pred}.</p> required <code>append</code> <code>bool</code> <p>If <code>True</code>, new residuals are added to the once already stored in the attribute <code>out_sample_residuals_</code>. If after appending the new residuals, the limit of 10_000 samples is exceeded, a random sample of 10_000 is kept.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\deep_learning\\_forecaster_rnn.py</code> <pre><code>def set_out_sample_residuals(\n    self,\n    y_true: dict[str, np.ndarray | pd.Series],\n    y_pred: dict[str, np.ndarray | pd.Series],\n    append: bool = False,\n    random_state: int = 123\n) -&gt; None:\n    \"\"\"\n    Set new values to the attribute `out_sample_residuals_`. Out of sample\n    residuals are meant to be calculated using observations that did not\n    participate in the training process. `y_true` and `y_pred` are expected\n    to be in the original scale of the time series. Residuals are calculated\n    as `y_true` - `y_pred`, after applying the necessary transformations and\n    differentiations if the forecaster includes them (`self.transformer_series`\n    and `self.differentiation`).\n\n    A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n    If the number of residuals is greater than 10_000, a random sample of\n    10_000 residuals is stored.\n\n    Parameters\n    ----------\n    y_true : dict\n        Dictionary of numpy ndarrays or pandas Series with the true values of\n        the time series for each series in the form {series: y_true}.\n    y_pred : dict\n        Dictionary of numpy ndarrays or pandas Series with the predicted values\n        of the time series for each series in the form {series: y_pred}.\n    append : bool, default False\n        If `True`, new residuals are added to the once already stored in the\n        attribute `out_sample_residuals_`. If after appending the new residuals,\n        the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n        kept.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `set_out_sample_residuals()`.\"\n        )\n\n    if not isinstance(y_true, dict):\n        raise TypeError(\n            f\"`y_true` must be a dictionary of numpy ndarrays or pandas Series. \"\n            f\"Got {type(y_true)}.\"\n        )\n\n    if not isinstance(y_pred, dict):\n        raise TypeError(\n            f\"`y_pred` must be a dictionary of numpy ndarrays or pandas Series. \"\n            f\"Got {type(y_pred)}.\"\n        )\n\n    if not set(y_true.keys()) == set(y_pred.keys()):\n        raise ValueError(\n            f\"`y_true` and `y_pred` must have the same keys. \"\n            f\"Got {set(y_true.keys())} and {set(y_pred.keys())}.\"\n        )\n\n    for k in y_true.keys():\n        if not isinstance(y_true[k], (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"Values of `y_true` must be numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_true[k])} for series {k}.\"\n            )\n        if not isinstance(y_pred[k], (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"Values of `y_pred` must be numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_pred[k])} for series {k}.\"\n            )\n        if len(y_true[k]) != len(y_pred[k]):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true[k])} and {len(y_pred[k])} for series {k}.\"\n            )\n        if isinstance(y_true[k], pd.Series) and isinstance(y_pred[k], pd.Series):\n            if not y_true[k].index.equals(y_pred[k].index):\n                raise ValueError(\n                    f\"When containing pandas Series, elements in `y_true` and \"\n                    f\"`y_pred` must have the same index. Error in series {k}.\"\n                )\n\n    series_to_update = set(y_pred.keys()).intersection(set(self.levels))\n    if not series_to_update:\n        raise ValueError(\n            f\"Provided keys in `y_pred` and `y_true` do not match any of the \"\n            f\"target time series in the forecaster, {self.levels}. Residuals \"\n            f\"cannot be updated.\"\n        )\n\n    if self.out_sample_residuals_ is None:\n        self.out_sample_residuals_ = {level: None for level in self.levels}\n\n    rng = np.random.default_rng(seed=random_state)\n    for level in series_to_update:\n\n        y_true_level = deepcopy(y_true[level])\n        y_pred_level = deepcopy(y_pred[level])\n        if not isinstance(y_true_level, np.ndarray):\n            y_true_level = y_true_level.to_numpy()\n        if not isinstance(y_pred_level, np.ndarray):\n            y_pred_level = y_pred_level.to_numpy()\n\n        if self.transformer_series:\n            y_true_level = transform_numpy(\n                               array             = y_true_level,\n                               transformer       = self.transformer_series_[level],\n                               fit               = False,\n                               inverse_transform = False\n                           )\n            y_pred_level = transform_numpy(\n                               array             = y_pred_level,\n                               transformer       = self.transformer_series_[level],\n                               fit               = False,\n                               inverse_transform = False\n                           )\n\n        data = pd.DataFrame(\n            {'prediction': y_pred_level, 'residuals': y_true_level - y_pred_level}\n        ).dropna()\n        residuals = data['residuals'].to_numpy()\n\n        out_sample_residuals = self.out_sample_residuals_.get(level, np.array([]))\n        out_sample_residuals = (\n            np.array([]) \n            if out_sample_residuals is None\n            else out_sample_residuals\n        )\n        if append:\n            out_sample_residuals = np.concatenate([out_sample_residuals, residuals])\n        else:\n            out_sample_residuals = residuals\n\n        if len(out_sample_residuals) &gt; 10_000:\n            out_sample_residuals = rng.choice(\n                a       = out_sample_residuals, \n                size    = 10_000, \n                replace = False\n            )\n\n        self.out_sample_residuals_[level] = out_sample_residuals\n</code></pre>"},{"location":"api/ForecasterSarimax.html","title":"<code>ForecasterSarimax</code>","text":""},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax","title":"skforecast.recursive._forecaster_sarimax.ForecasterSarimax","text":"<pre><code>ForecasterSarimax(\n    regressor,\n    transformer_y=None,\n    transformer_exog=None,\n    fit_kwargs=None,\n    forecaster_id=None,\n)\n</code></pre> <p>This class turns Sarimax model from the skforecast library into a Forecaster  compatible with the skforecast API. New in version 0.10.0</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>Sarimax</code> <p>A Sarimax model instance from skforecast.</p> required <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>None</code> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>None</code> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.  When using the skforecast Sarimax model, the fit kwargs should be passed  using the model parameter <code>sm_fit_kwargs</code> and not this one.</p> <code>None</code> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>regressor</code> <code>Sarimax</code> <p>A Sarimax model instance from skforecast.</p> <code>params</code> <code>dict</code> <p>Parameters of the sarimax model.</p> <code>transformer_y</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and inverse_transform. ColumnTransformers are not allowed since they do not have inverse_transform method. The transformation is applied to <code>y</code> before training the forecaster.</p> <code>transformer_exog</code> <code>object transformer (preprocessor)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. The transformation is applied to <code>exog</code> before training the forecaster. <code>inverse_transform</code> is not available when using ColumnTransformers.</p> <code>window_size</code> <code>int</code> <p>Not used, present here for API consistency by convention.</p> <code>last_window_</code> <code>pandas Series</code> <p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p> <code>extended_index_</code> <code>pandas Index</code> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index_. Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> <code>training_range_</code> <code>pandas Index</code> <p>First and last values of index of the data used during training.</p> <code>series_name_in_</code> <code>str</code> <p>Names of the series provided by the user during training.</p> <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>exog_type_in_</code> <code>type</code> <p>Type of exogenous variable/s used in training.</p> <code>exog_dtypes_in_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training before the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_out_</code>.</p> <code>exog_dtypes_out_</code> <code>dict</code> <p>Type of each exogenous variable/s used in training after the transformation  applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it  is equal to <code>exog_dtypes_in_</code>.</p> <code>X_train_exog_names_out_</code> <code>list</code> <p>Names of the exogenous variables included in the matrix <code>X_train</code> created internally for training. It can be different from <code>exog_names_in_</code> if some exogenous variables are transformed during the training process.</p> <code>fit_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor.</p> <code>creation_date</code> <code>str</code> <p>Date of creation.</p> <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>fit_date</code> <code>str</code> <p>Date of last fit.</p> <code>skforecast_version</code> <code>str</code> <p>Version of skforecast library used to create the forecaster.</p> <code>python_version</code> <code>str</code> <p>Version of python used to create the forecaster.</p> <code>forecaster_id</code> <code>(str, int)</code> <p>Name used as an identifier of the forecaster.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Training Forecaster.</p> <code>predict</code> <p>Forecast future values.</p> <code>predict_interval</code> <p>Forecast future values and their confidence intervals.</p> <code>set_params</code> <p>Set new values to the parameters of the model stored in the forecaster.</p> <code>set_fit_kwargs</code> <p>Set new values for the additional keyword arguments passed to the <code>fit</code> </p> <code>get_feature_importances</code> <p>Return feature importances of the regressor stored in the forecaster.</p> <code>get_info_criteria</code> <p>Get the selected information criteria.</p> <code>summary</code> <p>Show forecaster information.</p> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def __init__(\n    self,\n    regressor: object,\n    transformer_y: object | None = None,\n    transformer_exog: object | None = None,\n    fit_kwargs: dict[str, object] | None = None,\n    forecaster_id: str | int | None = None\n) -&gt; None:\n\n    self.regressor               = copy(regressor)\n    self.transformer_y           = transformer_y\n    self.transformer_exog        = transformer_exog\n    self.window_size             = 1\n    self.last_window_            = None\n    self.extended_index_         = None\n    self.index_type_             = None\n    self.index_freq_             = None\n    self.training_range_         = None\n    self.series_name_in_         = None\n    self.exog_in_                = False\n    self.exog_names_in_          = None\n    self.exog_type_in_           = None\n    self.exog_dtypes_in_         = None\n    self.exog_dtypes_out_        = None\n    self.X_train_exog_names_out_ = None\n    self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.is_fitted               = False\n    self.fit_date                = None\n    self.skforecast_version      = skforecast.__version__\n    self.python_version          = sys.version.split(\" \")[0]\n    self.forecaster_id           = forecaster_id\n\n    if not isinstance(self.regressor, skforecast.sarimax.Sarimax):\n        raise TypeError(\n            f\"`regressor` must be an instance of type \"\n            f\"`skforecast.sarimax.Sarimax`. Got '{type(regressor)}'.\"\n        )\n\n    self.params = self.regressor.get_params(deep=True)\n\n    if fit_kwargs:\n        warnings.warn(\n            \"When using the skforecast Sarimax model, the fit kwargs should \"\n            \"be passed using the model parameter `sm_fit_kwargs`.\",\n            IgnoredArgumentWarning\n        )\n    self.fit_kwargs = {}\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.regressor","title":"regressor  <code>instance-attribute</code>","text":"<pre><code>regressor = copy(regressor)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.transformer_y","title":"transformer_y  <code>instance-attribute</code>","text":"<pre><code>transformer_y = transformer_y\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.transformer_exog","title":"transformer_exog  <code>instance-attribute</code>","text":"<pre><code>transformer_exog = transformer_exog\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = 1\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.last_window_","title":"last_window_  <code>instance-attribute</code>","text":"<pre><code>last_window_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.extended_index_","title":"extended_index_  <code>instance-attribute</code>","text":"<pre><code>extended_index_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.index_type_","title":"index_type_  <code>instance-attribute</code>","text":"<pre><code>index_type_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.index_freq_","title":"index_freq_  <code>instance-attribute</code>","text":"<pre><code>index_freq_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.training_range_","title":"training_range_  <code>instance-attribute</code>","text":"<pre><code>training_range_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.series_name_in_","title":"series_name_in_  <code>instance-attribute</code>","text":"<pre><code>series_name_in_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.exog_in_","title":"exog_in_  <code>instance-attribute</code>","text":"<pre><code>exog_in_ = False\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.exog_names_in_","title":"exog_names_in_  <code>instance-attribute</code>","text":"<pre><code>exog_names_in_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.exog_type_in_","title":"exog_type_in_  <code>instance-attribute</code>","text":"<pre><code>exog_type_in_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.exog_dtypes_in_","title":"exog_dtypes_in_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_in_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.exog_dtypes_out_","title":"exog_dtypes_out_  <code>instance-attribute</code>","text":"<pre><code>exog_dtypes_out_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.X_train_exog_names_out_","title":"X_train_exog_names_out_  <code>instance-attribute</code>","text":"<pre><code>X_train_exog_names_out_ = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.creation_date","title":"creation_date  <code>instance-attribute</code>","text":"<pre><code>creation_date = strftime('%Y-%m-%d %H:%M:%S')\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.is_fitted","title":"is_fitted  <code>instance-attribute</code>","text":"<pre><code>is_fitted = False\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.fit_date","title":"fit_date  <code>instance-attribute</code>","text":"<pre><code>fit_date = None\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.skforecast_version","title":"skforecast_version  <code>instance-attribute</code>","text":"<pre><code>skforecast_version = __version__\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.python_version","title":"python_version  <code>instance-attribute</code>","text":"<pre><code>python_version = split(' ')[0]\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.forecaster_id","title":"forecaster_id  <code>instance-attribute</code>","text":"<pre><code>forecaster_id = forecaster_id\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params = get_params(deep=True)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.fit_kwargs","title":"fit_kwargs  <code>instance-attribute</code>","text":"<pre><code>fit_kwargs = {}\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax._preprocess_repr","title":"_preprocess_repr","text":"<pre><code>_preprocess_repr()\n</code></pre> <p>Format text for repr method.</p> <p>Returns:</p> Name Type Description <code>text</code> <code>str</code> <p>Formatted text.</p> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def _preprocess_repr(self) -&gt; tuple[str, str]:\n    \"\"\"\n    Format text for __repr__ method.\n\n    Returns\n    -------\n    text : str\n        Formatted text.\n\n    \"\"\"\n    params = str(self.params)\n    if len(params) &gt; 58:\n        params = \"\\n    \" + textwrap.fill(\n            params, width=80, subsequent_indent=\"    \"\n        )\n\n    exog_names_in_ = None\n    if self.exog_names_in_ is not None:\n        exog_names_in_ = copy(self.exog_names_in_)\n        if len(exog_names_in_) &gt; 50:\n            exog_names_in_ = exog_names_in_[:50] + [\"...\"]\n        exog_names_in_ = \", \".join(exog_names_in_)\n        if len(exog_names_in_) &gt; 58:\n            exog_names_in_ = \"\\n    \" + textwrap.fill(\n                exog_names_in_, width=80, subsequent_indent=\"    \"\n            )\n\n    return params, exog_names_in_\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def _repr_html_(\n    self\n) -&gt; str:\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    params, exog_names_in_ = self._preprocess_repr()\n    style, unique_id = get_style_repr_html(self.is_fitted)\n\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Regressor:&lt;/strong&gt; {type(self.regressor).__name__}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Order:&lt;/strong&gt; {self.regressor.order}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Seasonal order:&lt;/strong&gt; {self.regressor.seasonal_order}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Trend:&lt;/strong&gt; {self.regressor.trend}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Series name:&lt;/strong&gt; {self.series_name_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Exogenous included:&lt;/strong&gt; {self.exog_in_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Creation date:&lt;/strong&gt; {self.creation_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Last fit date:&lt;/strong&gt; {self.fit_date}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skforecast version:&lt;/strong&gt; {self.skforecast_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Python version:&lt;/strong&gt; {self.python_version}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Forecaster id:&lt;/strong&gt; {self.forecaster_id}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Exogenous Variables&lt;/summary&gt;\n            &lt;ul&gt;\n                {exog_names_in_}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Data Transformations&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for y:&lt;/strong&gt; {self.transformer_y}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Transformer for exog:&lt;/strong&gt; {self.transformer_exog}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Training Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Training range:&lt;/strong&gt; {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index type:&lt;/strong&gt; {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Training index frequency:&lt;/strong&gt; {self.index_freq_ if self.is_fitted else 'Not fitted'}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Regressor Parameters&lt;/summary&gt;\n            &lt;ul&gt;\n                {params}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;details&gt;\n            &lt;summary&gt;Fit Kwargs&lt;/summary&gt;\n            &lt;ul&gt;\n                {self.fit_kwargs}\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/forecastersarimax.html\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/forecasting-sarimax-arima.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return style + content\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.fit","title":"fit","text":"<pre><code>fit(\n    y,\n    exog=None,\n    store_last_window=True,\n    suppress_warnings=False,\n)\n</code></pre> <p>Training Forecaster.</p> <p>Additional arguments to be passed to the <code>fit</code> method of the regressor  can be added with the <code>fit_kwargs</code> argument when initializing the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>store_last_window</code> <code>bool</code> <p>Whether or not to store the last window (<code>last_window_</code>) of training data.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def fit(\n    self,\n    y: pd.Series,\n    exog: pd.Series | pd.DataFrame | None = None,\n    store_last_window: bool = True,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    Training Forecaster.\n\n    Additional arguments to be passed to the `fit` method of the regressor \n    can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n    Parameters\n    ----------\n    y : pandas Series\n        Training time series.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n    store_last_window : bool, default True\n        Whether or not to store the last window (`last_window_`) of training data.\n    suppress_warnings : bool, default False\n        If `True`, warnings generated during fitting will be ignored.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    check_y(y=y)\n    if exog is not None:\n        if len(exog) != len(y):\n            raise ValueError(\n                f\"`exog` must have same number of samples as `y`. \"\n                f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\"\n            )\n        check_exog(exog=exog)\n\n    # Reset values in case the forecaster has already been fitted.\n    self.last_window_            = None\n    self.extended_index_         = None\n    self.index_type_             = None\n    self.index_freq_             = None\n    self.training_range_         = None\n    self.series_name_in_         = None\n    self.exog_in_                = False\n    self.exog_names_in_          = None\n    self.exog_type_in_           = None\n    self.exog_dtypes_in_         = None\n    self.exog_dtypes_out_        = None\n    self.X_train_exog_names_out_ = None\n    self.in_sample_residuals_    = None\n    self.is_fitted               = False\n    self.fit_date                = None\n\n    if exog is not None:\n        self.exog_in_ = True\n        self.exog_type_in_ = type(exog)\n        self.exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n        self.exog_names_in_ = \\\n            exog.columns.to_list() if isinstance(exog, pd.DataFrame) else [exog.name]\n\n    y = transform_series(\n            series            = y,\n            transformer       = self.transformer_y,\n            fit               = True,\n            inverse_transform = False\n        )\n\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = True,\n                   inverse_transform = False\n               )\n\n        check_exog_dtypes(exog, call_check_exog=True)\n        self.exog_dtypes_out_ = get_exog_dtypes(exog=exog)\n        self.X_train_exog_names_out_ = exog.columns.to_list()\n\n    if suppress_warnings:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            self.regressor.fit(y=y, exog=exog)\n    else:\n        self.regressor.fit(y=y, exog=exog)\n\n    self.is_fitted = True\n    self.series_name_in_ = y.name if y.name is not None else 'y'\n    self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n    self.training_range_ = y.index[[0, -1]]\n    self.index_type_ = type(y.index)\n    if isinstance(y.index, pd.DatetimeIndex):\n        self.index_freq_ = y.index.freqstr\n    else: \n        self.index_freq_ = y.index.step\n\n    if store_last_window:\n        self.last_window_ = y.copy()\n\n    self.extended_index_ = self.regressor.sarimax_res.fittedvalues.index.copy()\n    self.params = self.regressor.get_params(deep=True)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax._create_predict_inputs","title":"_create_predict_inputs","text":"<pre><code>_create_predict_inputs(\n    steps,\n    last_window=None,\n    last_window_exog=None,\n    exog=None,\n)\n</code></pre> <p>Create inputs needed for the first iteration of the prediction process.  Since it is a recursive process, last window is updated at each  iteration of the prediction process.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1). If <code>last_window = None</code>, the values stored in <code>self.last_window_</code> are used to calculate the initial predictors, and the predictions start right after training data.</p> <code>None</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only needed when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables. Used to make predictions  unrelated to the original data. Values have to start at the end  of the training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>last_window</code> <code>pandas Series</code> <p>Series predictors.</p> <code>last_window_exog</code> <code>pandas DataFrame, None</code> <p>Values of the exogenous variables aligned with <code>last_window</code>.</p> <code>exog</code> <code>pandas DataFrame, None</code> <p>Exogenous variable/s included as predictor/s.</p> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def _create_predict_inputs(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    last_window_exog: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None\n) -&gt; tuple[pd.Series, pd.DataFrame | None, pd.DataFrame | None]:\n    \"\"\"\n    Create inputs needed for the first iteration of the prediction process. \n    Since it is a recursive process, last window is updated at each \n    iteration of the prediction process.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    last_window : pandas Series, default None\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n        If `last_window = None`, the values stored in `self.last_window_` are\n        used to calculate the initial predictors, and the predictions start\n        right after training data.\n    last_window_exog : pandas Series, pandas DataFrame, default None\n        Values of the exogenous variables aligned with `last_window`. Only\n        needed when `last_window` is not None and the forecaster has been\n        trained including exogenous variables. Used to make predictions \n        unrelated to the original data. Values have to start at the end \n        of the training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    last_window : pandas Series\n        Series predictors.\n    last_window_exog : pandas DataFrame, None\n        Values of the exogenous variables aligned with `last_window`.\n    exog : pandas DataFrame, None\n        Exogenous variable/s included as predictor/s.\n\n    \"\"\"\n\n    # Needs to be a new variable to avoid arima_res_.append when using \n    # self.last_window. It already has it stored.\n    last_window_check = last_window if last_window is not None else self.last_window_\n\n    check_predict_input(\n        forecaster_name  = type(self).__name__,\n        steps            = steps,\n        is_fitted        = self.is_fitted,\n        exog_in_         = self.exog_in_,\n        index_type_      = self.index_type_,\n        index_freq_      = self.index_freq_,\n        window_size      = self.window_size,\n        last_window      = last_window_check,\n        last_window_exog = last_window_exog,\n        exog             = exog,\n        exog_names_in_   = self.exog_names_in_,\n        interval         = None,\n        alpha            = None\n    )\n\n    # If not last_window is provided, last_window needs to be None\n    if last_window is not None:\n        last_window = last_window.copy()\n\n    # When last_window_exog is provided but no last_window\n    if last_window is None and last_window_exog is not None:\n        raise ValueError(\n            \"To make predictions unrelated to the original data, both \"\n            \"`last_window` and `last_window_exog` must be provided.\"\n        )\n\n    # Check if forecaster needs exog\n    if last_window is not None and last_window_exog is None and self.exog_in_:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. To make predictions \"\n            \"unrelated to the original data, same variable/s must be provided \"\n            \"using `last_window_exog`.\"\n        )\n\n    if last_window is not None:\n        # If predictions do not follow directly from the end of the training \n        # data. The internal statsmodels SARIMAX model needs to be updated \n        # using its append method. The data needs to start at the end of the \n        # training series.\n\n        # Check index append values\n        expected_index = expand_index(index=self.extended_index_, steps=1)[0]\n        if expected_index != last_window.index[0]:\n            raise ValueError(\n                f\"To make predictions unrelated to the original data, `last_window` \"\n                f\"has to start at the end of the index seen by the forecaster.\\n\"\n                f\"    Series last index         : {self.extended_index_[-1]}.\\n\"\n                f\"    Expected index            : {expected_index}.\\n\"\n                f\"    `last_window` index start : {last_window.index[0]}.\"\n            )\n\n        last_window = transform_series(\n                          series            = last_window,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = False\n                      )\n\n        # Transform last_window_exog\n        if last_window_exog is not None:\n            # check index last_window_exog\n            if expected_index != last_window_exog.index[0]:\n                raise ValueError(\n                    f\"To make predictions unrelated to the original data, `last_window_exog` \"\n                    f\"has to start at the end of the index seen by the forecaster.\\n\"\n                    f\"    Series last index              : {self.extended_index_[-1]}.\\n\"\n                    f\"    Expected index                 : {expected_index}.\\n\"\n                    f\"    `last_window_exog` index start : {last_window_exog.index[0]}.\"\n                )\n\n            if isinstance(last_window_exog, pd.Series):\n                last_window_exog = last_window_exog.to_frame()\n\n            last_window_exog = transform_dataframe(\n                                   df                = last_window_exog,\n                                   transformer       = self.transformer_exog,\n                                   fit               = False,\n                                   inverse_transform = False\n                               )\n\n    # Exog\n    if exog is not None:\n        if isinstance(exog, pd.Series):\n            exog = exog.to_frame()\n\n        exog = transform_dataframe(\n                   df                = exog,\n                   transformer       = self.transformer_exog,\n                   fit               = False,\n                   inverse_transform = False\n               )  \n        exog = exog.iloc[:steps, ]\n\n    return last_window, last_window_exog, exog\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.predict","title":"predict","text":"<pre><code>predict(\n    steps,\n    last_window=None,\n    last_window_exog=None,\n    exog=None,\n)\n</code></pre> <p>Forecast future values.</p> <p>Generate predictions (forecasts) n steps in the future. Note that if  exogenous variables were used in the model fit, they will be expected  for the predict procedure and will fail otherwise.</p> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index_.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors needed in the  predictions. Used to make predictions unrelated to the original data.  Values have to start at the end of the training data.</p> <code>None</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only needed when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables. Used to make predictions  unrelated to the original data. Values have to start at the end  of the training data.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas Series</code> <p>Predicted values.</p> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def predict(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    last_window_exog: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None\n) -&gt; pd.Series:\n    \"\"\"\n    Forecast future values.\n\n    Generate predictions (forecasts) n steps in the future. Note that if \n    exogenous variables were used in the model fit, they will be expected \n    for the predict procedure and will fail otherwise.\n\n    When predicting using `last_window` and `last_window_exog`, the internal\n    statsmodels SARIMAX will be updated using its append method. To do this,\n    `last_window` data must start at the end of the index seen by the \n    forecaster, this is stored in forecaster.extended_index_.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html\n    to know more about statsmodels append method.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    last_window : pandas Series, default None\n        Series values used to create the predictors needed in the \n        predictions. Used to make predictions unrelated to the original data. \n        Values have to start at the end of the training data.\n    last_window_exog : pandas Series, pandas DataFrame, default None\n        Values of the exogenous variables aligned with `last_window`. Only\n        needed when `last_window` is not None and the forecaster has been\n        trained including exogenous variables. Used to make predictions \n        unrelated to the original data. Values have to start at the end \n        of the training data.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    predictions : pandas Series\n        Predicted values.\n\n    \"\"\"\n\n    last_window, last_window_exog, exog = self._create_predict_inputs(\n                                              steps            = steps,\n                                              last_window      = last_window,\n                                              last_window_exog = last_window_exog,\n                                              exog             = exog,\n                                          )\n\n    if last_window is not None:\n        self.regressor.append(\n            y     = last_window,\n            exog  = last_window_exog,\n            refit = False\n        )\n        self.extended_index_ = self.regressor.sarimax_res.fittedvalues.index\n\n    # Get following n steps predictions\n    predictions = self.regressor.predict(\n                      steps = steps,\n                      exog  = exog\n                  ).iloc[:, 0]\n\n    predictions = transform_series(\n                      series            = predictions,\n                      transformer       = self.transformer_y,\n                      fit               = False,\n                      inverse_transform = True\n                  )\n    predictions.name = 'pred'\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.predict_interval","title":"predict_interval","text":"<pre><code>predict_interval(\n    steps,\n    last_window=None,\n    last_window_exog=None,\n    exog=None,\n    alpha=0.05,\n    interval=None,\n)\n</code></pre> <p>Forecast future values and their confidence intervals.</p> <p>Generate predictions (forecasts) n steps in the future with confidence intervals. Note that if exogenous variables were used in the model fit,  they will be expected for the predict procedure and will fail otherwise.</p> <p>When predicting using <code>last_window</code> and <code>last_window_exog</code>, the internal statsmodels SARIMAX will be updated using its append method. To do this, <code>last_window</code> data must start at the end of the index seen by the  forecaster, this is stored in forecaster.extended_index_.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html to know more about statsmodels append method.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>last_window</code> <code>pandas Series</code> <p>Series values used to create the predictors needed in the  predictions. Used to make predictions unrelated to the original data.  Values have to start at the end of the training data.</p> <code>None</code> <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code>. Only need when <code>last_window</code> is not None and the forecaster has been trained including exogenous variables.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %. If both, <code>alpha</code> and <code>interval</code> are provided, <code>alpha</code> will be used.</p> <code>0.05</code> <code>interval</code> <code>(list, tuple)</code> <p>Confidence of the prediction interval estimated. The values must be symmetric. Sequence of percentiles to compute, which must be between  0 and 100 inclusive. For example, interval of 95% should be as  <code>interval = [2.5, 97.5]</code>. If both, <code>alpha</code> and <code>interval</code> are  provided, <code>alpha</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval.</li> <li>upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def predict_interval(\n    self,\n    steps: int,\n    last_window: pd.Series | None = None,\n    last_window_exog: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | None = None,\n    alpha: float = 0.05,\n    interval: list[float] | tuple[float] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Forecast future values and their confidence intervals.\n\n    Generate predictions (forecasts) n steps in the future with confidence\n    intervals. Note that if exogenous variables were used in the model fit, \n    they will be expected for the predict procedure and will fail otherwise.\n\n    When predicting using `last_window` and `last_window_exog`, the internal\n    statsmodels SARIMAX will be updated using its append method. To do this,\n    `last_window` data must start at the end of the index seen by the \n    forecaster, this is stored in forecaster.extended_index_.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html\n    to know more about statsmodels append method.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    last_window : pandas Series, default None\n        Series values used to create the predictors needed in the \n        predictions. Used to make predictions unrelated to the original data. \n        Values have to start at the end of the training data.\n    last_window_exog : pandas Series, pandas DataFrame, default None\n        Values of the exogenous variables aligned with `last_window`. Only\n        need when `last_window` is not None and the forecaster has been\n        trained including exogenous variables.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s.\n    alpha : float, default 0.05\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, tuple, default None\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n\n    Returns\n    -------\n    predictions : pandas DataFrame\n        Values predicted by the forecaster and their estimated interval.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval.\n        - upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    # If interval and alpha take alpha, if interval transform to alpha\n    if alpha is None:\n        if 100 - interval[1] != interval[0]:\n            raise ValueError(\n                f\"When using `interval` in ForecasterSarimax, it must be symmetrical. \"\n                f\"For example, interval of 95% should be as `interval = [2.5, 97.5]`. \"\n                f\"Got {interval}.\"\n            )\n        alpha = 2 * (100 - interval[1]) / 100\n\n    last_window, last_window_exog, exog = self._create_predict_inputs(\n                                              steps            = steps,\n                                              last_window      = last_window,\n                                              last_window_exog = last_window_exog,\n                                              exog             = exog,\n                                          )\n\n    if last_window is not None:\n        self.regressor.append(\n            y     = last_window,\n            exog  = last_window_exog,\n            refit = False\n        )\n        self.extended_index_ = self.regressor.sarimax_res.fittedvalues.index\n\n    # Get following n steps predictions with intervals\n    predictions = self.regressor.predict(\n                      steps = steps,\n                      exog  = exog,\n                      return_conf_int = True,\n                      alpha = alpha\n                  )\n\n    if self.transformer_y:\n        predictions = predictions.apply(lambda col: transform_series(\n                          series            = col,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = True\n                      ))\n\n    return predictions\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.set_params","title":"set_params","text":"<pre><code>set_params(params)\n</code></pre> <p>Set new values to the parameters of the model stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def set_params(\n    self, \n    params: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the model stored in the forecaster.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.regressor = clone(self.regressor)\n    self.regressor.set_params(**params)\n    self.params = self.regressor.get_params(deep=True)\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.set_fit_kwargs","title":"set_fit_kwargs","text":"<pre><code>set_fit_kwargs(fit_kwargs)\n</code></pre> <p>Set new values for the additional keyword arguments passed to the <code>fit</code>  method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>fit_kwargs</code> <code>dict</code> <p>Dict of the form {\"argument\": new_value}.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def set_fit_kwargs(\n    self, \n    fit_kwargs: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values for the additional keyword arguments passed to the `fit` \n    method of the regressor.\n\n    Parameters\n    ----------\n    fit_kwargs : dict\n        Dict of the form {\"argument\": new_value}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    warnings.warn(\n        \"When using the skforecast Sarimax model, the fit kwargs should \"\n        \"be passed using the model parameter `sm_fit_kwargs`.\",\n        IgnoredArgumentWarning\n    )\n    self.fit_kwargs = {}\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.get_feature_importances","title":"get_feature_importances","text":"<pre><code>get_feature_importances(sort_importance=True)\n</code></pre> <p>Return feature importances of the regressor stored in the forecaster.</p> <p>Parameters:</p> Name Type Description Default <code>sort_importance</code> <code>bool</code> <p>If <code>True</code>, sorts the feature importances in descending order.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feature_importances</code> <code>pandas DataFrame</code> <p>Feature importances associated with each predictor.</p> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def get_feature_importances(\n    self,\n    sort_importance: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return feature importances of the regressor stored in the forecaster.\n\n    Parameters\n    ----------\n    sort_importance: bool, default True\n        If `True`, sorts the feature importances in descending order.\n\n    Returns\n    -------\n    feature_importances : pandas DataFrame\n        Feature importances associated with each predictor.\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n            \"arguments before using `get_feature_importances()`.\"\n        )\n\n    feature_importances = self.regressor.params().to_frame().reset_index()\n    feature_importances.columns = ['feature', 'importance']\n\n    if sort_importance:\n        feature_importances = feature_importances.sort_values(\n                                  by='importance', ascending=False\n                              )\n\n    return feature_importances\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.get_info_criteria","title":"get_info_criteria","text":"<pre><code>get_info_criteria(criteria='aic', method='standard')\n</code></pre> <p>Get the selected information criteria.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html to know more about statsmodels info_criteria method.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>str</code> <p>The information criteria to compute. Valid options are {'aic', 'bic', 'hqic'}.</p> <code>'aic'</code> <code>method</code> <code>str</code> <p>The method for information criteria computation. Default is 'standard' method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl (2007).</p> <code>'standard'</code> <p>Returns:</p> Name Type Description <code>metric</code> <code>float</code> <p>The value of the selected information criteria.</p> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def get_info_criteria(\n    self, \n    criteria: str = 'aic', \n    method: str = 'standard'\n) -&gt; float:\n    \"\"\"\n    Get the selected information criteria.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html\n    to know more about statsmodels info_criteria method.\n\n    Parameters\n    ----------\n    criteria : str, default 'aic'\n        The information criteria to compute. Valid options are {'aic', 'bic',\n        'hqic'}.\n    method : str, default 'standard'\n        The method for information criteria computation. Default is 'standard'\n        method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl\n        (2007).\n\n    Returns\n    -------\n    metric : float\n        The value of the selected information criteria.\n\n    \"\"\"\n\n    if criteria not in ['aic', 'bic', 'hqic']:\n        raise ValueError(\n            \"Invalid value for `criteria`. Valid options are 'aic', 'bic', \"\n            \"and 'hqic'.\"\n        )\n\n    if method not in ['standard', 'lutkepohl']:\n        raise ValueError(\n            \"Invalid value for `method`. Valid options are 'standard' and \"\n            \"'lutkepohl'.\"\n        )\n\n    metric = self.regressor.get_info_criteria(criteria=criteria, method=method)\n\n    return metric\n</code></pre>"},{"location":"api/ForecasterSarimax.html#skforecast.recursive._forecaster_sarimax.ForecasterSarimax.summary","title":"summary","text":"<pre><code>summary()\n</code></pre> <p>Show forecaster information.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\recursive\\_forecaster_sarimax.py</code> <pre><code>def summary(self) -&gt; None:\n    \"\"\"\n    Show forecaster information.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    print(self)\n</code></pre>"},{"location":"api/Sarimax.html","title":"<code>Sarimax</code>","text":""},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax","title":"skforecast.sarimax._sarimax.Sarimax","text":"<pre><code>Sarimax(\n    order=(1, 0, 0),\n    seasonal_order=(0, 0, 0, 0),\n    trend=None,\n    measurement_error=False,\n    time_varying_regression=False,\n    mle_regression=True,\n    simple_differencing=False,\n    enforce_stationarity=True,\n    enforce_invertibility=True,\n    hamilton_representation=False,\n    concentrate_scale=False,\n    trend_offset=1,\n    use_exact_diffuse=False,\n    dates=None,\n    freq=None,\n    missing=\"none\",\n    validate_specification=True,\n    method=\"lbfgs\",\n    maxiter=50,\n    start_params=None,\n    disp=False,\n    sm_init_kwargs={},\n    sm_fit_kwargs={},\n    sm_predict_kwargs={},\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A universal scikit-learn style wrapper for statsmodels SARIMAX.</p> <p>This class wraps the statsmodels.tsa.statespace.sarimax.SARIMAX model [1]_ [2]_  to follow the scikit-learn style. The following docstring is based on the  statsmodels documentation and it is highly recommended to visit their site  for the best level of detail.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>tuple</code> <p>The (p,d,q) order of the model for the number of AR parameters, differences,  and MA parameters. </p> <ul> <li><code>d</code> must be an integer indicating the integration order of the process.</li> <li><code>p</code> and <code>q</code> may either be an integers indicating the AR and MA orders  (so that all lags up to those orders are included) or else iterables  giving specific AR and / or MA lags to include.</li> </ul> <code>(1, 0, 0)</code> <code>seasonal_order</code> <code>tuple</code> <p>The (P,D,Q,s) order of the seasonal component of the model for the AR  parameters, differences, MA parameters, and periodicity. </p> <ul> <li><code>D</code> must be an integer indicating the integration order of the process.</li> <li><code>P</code> and <code>Q</code> may either be an integers indicating the AR and MA orders  (so that all lags up to those orders are included) or else iterables  giving specific AR and / or MA lags to include. </li> <li><code>s</code> is an integer giving the periodicity (number of periods in season), often it is 4 for quarterly data or 12 for monthly data.</li> </ul> <code>(0, 0, 0, 0)</code> <code>trend</code> <code>str</code> <p>Parameter controlling the deterministic trend polynomial <code>A(t)</code>.</p> <ul> <li><code>'c'</code> indicates a constant (i.e. a degree zero component of the  trend polynomial).</li> <li><code>'t'</code> indicates a linear trend with time.</li> <li><code>'ct'</code> indicates both, <code>'c'</code> and <code>'t'</code>. </li> <li>Can also be specified as an iterable defining the non-zero polynomial  exponents to include, in increasing order. For example, <code>[1,1,0,1]</code>  denotes <code>a + b*t + ct^3</code>.</li> </ul> <code>None</code> <code>measurement_error</code> <code>bool</code> <p>Whether or not to assume the endogenous observations <code>y</code> were measured  with error.</p> <code>False</code> <code>time_varying_regression</code> <code>bool</code> <p>Used when an explanatory variables, <code>exog</code>, are provided to select whether  or not coefficients on the exogenous regressors are allowed to vary over time.</p> <code>False</code> <code>mle_regression</code> <code>bool</code> <p>Whether or not to use estimate the regression coefficients for the exogenous variables as part of maximum likelihood estimation or through the Kalman filter (i.e. recursive least squares). If  <code>time_varying_regression</code> is <code>True</code>, this must be set to <code>False</code>.</p> <code>True</code> <code>simple_differencing</code> <code>bool</code> <p>Whether or not to use partially conditional maximum likelihood estimation. </p> <ul> <li>If <code>True</code>, differencing is performed prior to estimation, which  discards the first <code>s*D + d</code> initial rows but results in a smaller  state-space formulation. </li> <li>If <code>False</code>, the full SARIMAX model is put in state-space form so  that all data points can be used in estimation.</li> </ul> <code>False</code> <code>enforce_stationarity</code> <code>bool</code> <p>Whether or not to transform the AR parameters to enforce stationarity in the autoregressive component of the model.</p> <code>True</code> <code>enforce_invertibility</code> <code>bool</code> <p>Whether or not to transform the MA parameters to enforce invertibility in the moving average component of the model.</p> <code>True</code> <code>hamilton_representation</code> <code>bool</code> <p>Whether or not to use the Hamilton representation of an ARMA process (if <code>True</code>) or the Harvey representation (if <code>False</code>).</p> <code>False</code> <code>concentrate_scale</code> <code>bool</code> <p>Whether or not to concentrate the scale (variance of the error term) out of the likelihood. This reduces the number of parameters estimated by maximum likelihood by one, but standard errors will then not be available for the scale parameter.</p> <code>False</code> <code>trend_offset</code> <code>int</code> <p>The offset at which to start time trend values. Default is 1, so that if <code>trend='t'</code> the trend is equal to 1, 2, ..., nobs. Typically is only set when the model created by extending a previous dataset.</p> <code>1</code> <code>use_exact_diffuse</code> <code>bool</code> <p>Whether or not to use exact diffuse initialization for non-stationary states. Default is <code>False</code> (in which case approximate diffuse initialization is used).</p> <code>False</code> <code>method</code> <code>str</code> <p>The method determines which solver from scipy.optimize is used, and it  can be chosen from among the following strings:</p> <ul> <li><code>'newton'</code> for Newton-Raphson</li> <li><code>'nm'</code> for Nelder-Mead</li> <li><code>'bfgs'</code> for Broyden-Fletcher-Goldfarb-Shanno (BFGS)</li> <li><code>'lbfgs'</code> for limited-memory BFGS with optional box constraints</li> <li><code>'powell'</code> for modified Powell`s method</li> <li><code>'cg'</code> for conjugate gradient</li> <li><code>'ncg'</code> for Newton-conjugate gradient</li> <li><code>'basinhopping'</code> for global basin-hopping solver</li> </ul> <code>'lbfgs'</code> <code>maxiter</code> <code>int</code> <p>The maximum number of iterations to perform.</p> <code>50</code> <code>start_params</code> <code>numpy ndarray</code> <p>Initial guess of the solution for the loglikelihood maximization.  If <code>None</code>, the default is given by regressor.start_params.</p> <code>None</code> <code>disp</code> <code>bool</code> <p>Set to <code>True</code> to print convergence messages.</p> <code>False</code> <code>sm_init_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the statsmodels SARIMAX model  when it is initialized.</p> <code>{}</code> <code>sm_fit_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>fit</code> method of the statsmodels SARIMAX model. The statsmodels SARIMAX.fit parameters  <code>method</code>, <code>max_iter</code>, <code>start_params</code> and <code>disp</code> have been moved to the  initialization of this model and will have priority over those provided  by the user using via <code>sm_fit_kwargs</code>.</p> <code>{} </code> <code>sm_predict_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>get_forecast</code> method of the statsmodels SARIMAXResults object.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>order</code> <code>tuple</code> <p>The (p,d,q) order of the model for the number of AR parameters, differences,  and MA parameters.</p> <code>seasonal_order</code> <code>tuple</code> <p>The (P,D,Q,s) order of the seasonal component of the model for the AR  parameters, differences, MA parameters, and periodicity.</p> <code>trend</code> <code>str</code> <p>Deterministic trend polynomial <code>A(t)</code>.</p> <code>measurement_error</code> <code>bool</code> <p>Whether or not to assume the endogenous observations <code>y</code> were measured  with error.</p> <code>time_varying_regression</code> <code>bool</code> <p>Used when an explanatory variables, <code>exog</code>, are provided to select whether  or not coefficients on the exogenous regressors are allowed to vary over time.</p> <code>mle_regression</code> <code>bool</code> <p>Whether or not to use estimate the regression coefficients for the exogenous variables as part of maximum likelihood estimation or through the Kalman filter (i.e. recursive least squares). If  <code>time_varying_regression</code> is <code>True</code>, this must be set to <code>False</code>.</p> <code>simple_differencing</code> <code>bool</code> <p>Whether or not to use partially conditional maximum likelihood estimation.</p> <code>enforce_stationarity</code> <code>bool</code> <p>Whether or not to transform the AR parameters to enforce stationarity in the autoregressive component of the model.</p> <code>enforce_invertibility</code> <code>bool</code> <p>Whether or not to transform the MA parameters to enforce invertibility in the moving average component of the model.</p> <code>hamilton_representation</code> <code>bool</code> <p>Whether or not to use the Hamilton representation of an ARMA process (if <code>True</code>) or the Harvey representation (if <code>False</code>).</p> <code>concentrate_scale</code> <code>bool</code> <p>Whether or not to concentrate the scale (variance of the error term) out of the likelihood. This reduces the number of parameters estimated by maximum likelihood by one, but standard errors will then not be available for the scale parameter.</p> <code>trend_offset</code> <code>int</code> <p>The offset at which to start time trend values.</p> <code>use_exact_diffuse</code> <code>bool</code> <p>Whether or not to use exact diffuse initialization for non-stationary states.</p> <code>method</code> <code>str</code> <p>The method determines which solver from scipy.optimize is used.</p> <code>maxiter</code> <code>int</code> <p>The maximum number of iterations to perform.</p> <code>start_params</code> <code>numpy ndarray</code> <p>Initial guess of the solution for the loglikelihood maximization.</p> <code>disp</code> <code>bool</code> <p>Set to <code>True</code> to print convergence messages.</p> <code>sm_init_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the statsmodels SARIMAX model  when it is initialized.</p> <code>sm_fit_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>fit</code> method of the statsmodels SARIMAX model.</p> <code>sm_predict_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>get_forecast</code> method of the statsmodels SARIMAXResults object.</p> <code>_sarimax_params</code> <code>dict</code> <p>Parameters of this model that can be set with the <code>set_params</code> method.</p> <code>output_type</code> <code>str</code> <p>Format of the object returned by the predict method. This is set  automatically according to the type of <code>y</code> used in the fit method to  train the model, <code>'numpy'</code> or <code>'pandas'</code>.</p> <code>sarimax</code> <code>object</code> <p>The statsmodels.tsa.statespace.sarimax.SARIMAX object created.</p> <code>fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> <code>sarimax_res</code> <code>object</code> <p>The resulting statsmodels.tsa.statespace.sarimax.SARIMAXResults object  created by statsmodels after fitting the SARIMAX model.</p> <code>training_index</code> <code>pandas Index</code> <p>Index of the training series as long as it is a pandas Series or Dataframe.</p> References <p>.. [1] Statsmodels SARIMAX API Reference.        https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html</p> <p>.. [2] Statsmodels SARIMAXResults API Reference.        https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.html</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fit the model to the data.</p> <code>predict</code> <p>Forecast future values and, if desired, their confidence intervals.</p> <code>append</code> <p>Recreate the results object with new data appended to the original data.</p> <code>apply</code> <p>Apply the fitted parameters to new data unrelated to the original data.</p> <code>extend</code> <p>Recreate the results object for new data that extends the original data.</p> <code>set_params</code> <p>Set new values to the parameters of the regressor.</p> <code>params</code> <p>Get the parameters of the model. The order of variables is the trend</p> <code>summary</code> <p>Get a summary of the SARIMAXResults object.</p> <code>get_info_criteria</code> <p>Get the selected information criteria.</p> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>def __init__(\n    self,\n    order: tuple = (1, 0, 0),\n    seasonal_order: tuple = (0, 0, 0, 0),\n    trend: str = None,\n    measurement_error: bool = False,\n    time_varying_regression: bool = False,\n    mle_regression: bool = True,\n    simple_differencing: bool = False,\n    enforce_stationarity: bool = True,\n    enforce_invertibility: bool = True,\n    hamilton_representation: bool = False,\n    concentrate_scale: bool = False,\n    trend_offset: int = 1,\n    use_exact_diffuse: bool = False,\n    dates = None,\n    freq = None,\n    missing = 'none',\n    validate_specification: bool = True,\n    method: str = 'lbfgs',\n    maxiter: int = 50,\n    start_params: np.ndarray = None,\n    disp: bool = False,\n    sm_init_kwargs: dict[str, object] = {},\n    sm_fit_kwargs: dict[str, object] = {},\n    sm_predict_kwargs: dict[str, object] = {}\n) -&gt; None:\n\n    self.order                   = order\n    self.seasonal_order          = seasonal_order\n    self.trend                   = trend\n    self.measurement_error       = measurement_error\n    self.time_varying_regression = time_varying_regression\n    self.mle_regression          = mle_regression\n    self.simple_differencing     = simple_differencing\n    self.enforce_stationarity    = enforce_stationarity\n    self.enforce_invertibility   = enforce_invertibility\n    self.hamilton_representation = hamilton_representation\n    self.concentrate_scale       = concentrate_scale\n    self.trend_offset            = trend_offset\n    self.use_exact_diffuse       = use_exact_diffuse\n    self.dates                   = dates\n    self.freq                    = freq\n    self.missing                 = missing\n    self.validate_specification  = validate_specification\n    self.method                  = method\n    self.maxiter                 = maxiter\n    self.start_params            = start_params\n    self.disp                    = disp\n\n    # Create the dictionaries with the additional statsmodels parameters to be  \n    # used during the init, fit and predict methods. Note that the statsmodels \n    # SARIMAX.fit parameters `method`, `max_iter`, `start_params` and `disp` \n    # have been moved to the initialization of this model and will have \n    # priority over those provided by the user using via `sm_fit_kwargs`.\n    self.sm_init_kwargs    = sm_init_kwargs\n    self.sm_fit_kwargs     = sm_fit_kwargs\n    self.sm_predict_kwargs = sm_predict_kwargs\n\n    # Params that can be set with the `set_params` method\n    _, _, _, _sarimax_params = inspect.getargvalues(inspect.currentframe())\n    self._sarimax_params = {\n        k: v for k, v in _sarimax_params.items() \n        if k not in ['self', '_', '_sarimax_params']\n    }\n\n    self._consolidate_kwargs()\n\n    # Create Results Attributes \n    self.output_type    = None\n    self.sarimax        = None\n    self.fitted         = False\n    self.sarimax_res    = None\n    self.training_index = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.order","title":"order  <code>instance-attribute</code>","text":"<pre><code>order = order\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.seasonal_order","title":"seasonal_order  <code>instance-attribute</code>","text":"<pre><code>seasonal_order = seasonal_order\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.trend","title":"trend  <code>instance-attribute</code>","text":"<pre><code>trend = trend\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.measurement_error","title":"measurement_error  <code>instance-attribute</code>","text":"<pre><code>measurement_error = measurement_error\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.time_varying_regression","title":"time_varying_regression  <code>instance-attribute</code>","text":"<pre><code>time_varying_regression = time_varying_regression\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.mle_regression","title":"mle_regression  <code>instance-attribute</code>","text":"<pre><code>mle_regression = mle_regression\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.simple_differencing","title":"simple_differencing  <code>instance-attribute</code>","text":"<pre><code>simple_differencing = simple_differencing\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.enforce_stationarity","title":"enforce_stationarity  <code>instance-attribute</code>","text":"<pre><code>enforce_stationarity = enforce_stationarity\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.enforce_invertibility","title":"enforce_invertibility  <code>instance-attribute</code>","text":"<pre><code>enforce_invertibility = enforce_invertibility\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.hamilton_representation","title":"hamilton_representation  <code>instance-attribute</code>","text":"<pre><code>hamilton_representation = hamilton_representation\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.concentrate_scale","title":"concentrate_scale  <code>instance-attribute</code>","text":"<pre><code>concentrate_scale = concentrate_scale\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.trend_offset","title":"trend_offset  <code>instance-attribute</code>","text":"<pre><code>trend_offset = trend_offset\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.use_exact_diffuse","title":"use_exact_diffuse  <code>instance-attribute</code>","text":"<pre><code>use_exact_diffuse = use_exact_diffuse\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.dates","title":"dates  <code>instance-attribute</code>","text":"<pre><code>dates = dates\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.freq","title":"freq  <code>instance-attribute</code>","text":"<pre><code>freq = freq\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.missing","title":"missing  <code>instance-attribute</code>","text":"<pre><code>missing = missing\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.validate_specification","title":"validate_specification  <code>instance-attribute</code>","text":"<pre><code>validate_specification = validate_specification\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method = method\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.maxiter","title":"maxiter  <code>instance-attribute</code>","text":"<pre><code>maxiter = maxiter\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.start_params","title":"start_params  <code>instance-attribute</code>","text":"<pre><code>start_params = start_params\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.disp","title":"disp  <code>instance-attribute</code>","text":"<pre><code>disp = disp\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.sm_init_kwargs","title":"sm_init_kwargs  <code>instance-attribute</code>","text":"<pre><code>sm_init_kwargs = sm_init_kwargs\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.sm_fit_kwargs","title":"sm_fit_kwargs  <code>instance-attribute</code>","text":"<pre><code>sm_fit_kwargs = sm_fit_kwargs\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.sm_predict_kwargs","title":"sm_predict_kwargs  <code>instance-attribute</code>","text":"<pre><code>sm_predict_kwargs = sm_predict_kwargs\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax._sarimax_params","title":"_sarimax_params  <code>instance-attribute</code>","text":"<pre><code>_sarimax_params = {k: vfor (k, v) in (items()) if k not in ['self', '_', '_sarimax_params']}\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.output_type","title":"output_type  <code>instance-attribute</code>","text":"<pre><code>output_type = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.sarimax","title":"sarimax  <code>instance-attribute</code>","text":"<pre><code>sarimax = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.fitted","title":"fitted  <code>instance-attribute</code>","text":"<pre><code>fitted = False\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.sarimax_res","title":"sarimax_res  <code>instance-attribute</code>","text":"<pre><code>sarimax_res = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.training_index","title":"training_index  <code>instance-attribute</code>","text":"<pre><code>training_index = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax._consolidate_kwargs","title":"_consolidate_kwargs","text":"<pre><code>_consolidate_kwargs()\n</code></pre> <p>Create the dictionaries to be used during the init, fit, and predict methods. Note that the parameters in this model's initialization take precedence  over those provided by the user using via the statsmodels kwargs dicts.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>def _consolidate_kwargs(\n    self\n) -&gt; None:\n    \"\"\"\n    Create the dictionaries to be used during the init, fit, and predict methods.\n    Note that the parameters in this model's initialization take precedence \n    over those provided by the user using via the statsmodels kwargs dicts.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # statsmodels.tsa.statespace.SARIMAX parameters\n    _init_kwargs = self.sm_init_kwargs.copy()\n    _init_kwargs.update({\n       'order': self.order,\n       'seasonal_order': self.seasonal_order,\n       'trend': self.trend,\n       'measurement_error': self.measurement_error,\n       'time_varying_regression': self.time_varying_regression,\n       'mle_regression': self.mle_regression,\n       'simple_differencing': self.simple_differencing,\n       'enforce_stationarity': self.enforce_stationarity,\n       'enforce_invertibility': self.enforce_invertibility,\n       'hamilton_representation': self.hamilton_representation,\n       'concentrate_scale': self.concentrate_scale,\n       'trend_offset': self.trend_offset,\n       'use_exact_diffuse': self.use_exact_diffuse,\n       'dates': self.dates,\n       'freq': self.freq,\n       'missing': self.missing,\n       'validate_specification': self.validate_specification\n    })\n    self._init_kwargs = _init_kwargs\n\n    # statsmodels.tsa.statespace.SARIMAX.fit parameters\n    _fit_kwargs = self.sm_fit_kwargs.copy()\n    _fit_kwargs.update({\n       'method': self.method,\n       'maxiter': self.maxiter,\n       'start_params': self.start_params,\n       'disp': self.disp,\n    })        \n    self._fit_kwargs = _fit_kwargs\n\n    # statsmodels.tsa.statespace.SARIMAXResults.get_forecast parameters\n    self._predict_kwargs = self.sm_predict_kwargs.copy()\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax._create_sarimax","title":"_create_sarimax","text":"<pre><code>_create_sarimax(endog, exog=None)\n</code></pre> <p>A helper method to create a new statsmodel SARIMAX model.</p> <p>Additional keyword arguments to pass to the statsmodels SARIMAX model  when it is initialized can be added with the <code>init_kwargs</code> argument  when initializing the model.</p> <p>Parameters:</p> Name Type Description Default <code>endog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>The endogenous variable.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>The exogenous variables.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>def _create_sarimax(\n    self,\n    endog: np.ndarray | pd.Series | pd.DataFrame,\n    exog: np.ndarray | pd.Series | pd.DataFrame | None = None\n) -&gt; None:\n    \"\"\"\n    A helper method to create a new statsmodel SARIMAX model.\n\n    Additional keyword arguments to pass to the statsmodels SARIMAX model \n    when it is initialized can be added with the `init_kwargs` argument \n    when initializing the model.\n\n    Parameters\n    ----------\n    endog : numpy ndarray, pandas Series, pandas DataFrame\n        The endogenous variable.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default None\n        The exogenous variables.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.sarimax = SARIMAX(endog=endog, exog=exog, **self._init_kwargs)\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.fit","title":"fit","text":"<pre><code>fit(y, exog=None)\n</code></pre> <p>Fit the model to the data.</p> <p>Additional keyword arguments to pass to the <code>fit</code> method of the statsmodels SARIMAX model can be added with the <code>fit_kwargs</code> argument  when initializing the model.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Training time series.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and their indexes must be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>def fit(\n    self,\n    y: np.ndarray | pd.Series | pd.DataFrame,\n    exog: np.ndarray | pd.Series | pd.DataFrame | None = None\n) -&gt; None:\n    \"\"\"\n    Fit the model to the data.\n\n    Additional keyword arguments to pass to the `fit` method of the\n    statsmodels SARIMAX model can be added with the `fit_kwargs` argument \n    when initializing the model.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        Training time series.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and their indexes must be aligned so\n        that y[i] is regressed on exog[i].\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # Reset values in case the model has already been fitted.\n    self.output_type    = None\n    self.sarimax_res    = None\n    self.fitted         = False\n    self.training_index = None\n\n    self.output_type = 'numpy' if isinstance(y, np.ndarray) else 'pandas'\n\n    self._create_sarimax(endog=y, exog=exog)\n    self.sarimax_res = self.sarimax.fit(**self._fit_kwargs)\n    self.fitted = True\n\n    if self.output_type == 'pandas':\n        self.training_index = y.index\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.predict","title":"predict","text":"<pre><code>predict(\n    steps, exog=None, return_conf_int=False, alpha=0.05\n)\n</code></pre> <p>Forecast future values and, if desired, their confidence intervals.</p> <p>Generate predictions (forecasts) n steps in the future with confidence intervals. Note that if exogenous variables were used in the model fit,  they will be expected for the predict procedure and will fail otherwise.</p> <p>Additional keyword arguments to pass to the <code>get_forecast</code> method of the statsmodels SARIMAX model can be added with the <code>predict_kwargs</code> argument  when initializing the model.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to predict.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Value of the exogenous variable/s for the next steps. The number of  observations needed is the number of steps to predict.</p> <code>None</code> <code>return_conf_int</code> <code>bool</code> <p>Whether to get the confidence intervals of the forecasts.</p> <code>False</code> <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %.</p> <code>0.05</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>numpy ndarray, pandas DataFrame</code> <p>Values predicted by the forecaster and their estimated interval. The  output type is the same as the type of <code>y</code> used in the fit method.</p> <ul> <li>pred: predictions.</li> <li>lower_bound: lower bound of the interval. (if <code>return_conf_int</code>)</li> <li>upper_bound: upper bound of the interval. (if <code>return_conf_int</code>)</li> </ul> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>@_check_fitted\ndef predict(\n    self,\n    steps: int,\n    exog: np.ndarray | pd.Series | pd.DataFrame | None = None, \n    return_conf_int: bool = False,\n    alpha: float = 0.05\n) -&gt; np.ndarray | pd.DataFrame:\n    \"\"\"\n    Forecast future values and, if desired, their confidence intervals.\n\n    Generate predictions (forecasts) n steps in the future with confidence\n    intervals. Note that if exogenous variables were used in the model fit, \n    they will be expected for the predict procedure and will fail otherwise.\n\n    Additional keyword arguments to pass to the `get_forecast` method of the\n    statsmodels SARIMAX model can be added with the `predict_kwargs` argument \n    when initializing the model.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to predict. \n    exog : numpy ndarray, pandas Series, pandas DataFrame, default None\n        Value of the exogenous variable/s for the next steps. The number of \n        observations needed is the number of steps to predict. \n    return_conf_int : bool, default False\n        Whether to get the confidence intervals of the forecasts.\n    alpha : float, default 0.05\n        The confidence intervals for the forecasts are (1 - alpha) %.\n\n    Returns\n    -------\n    predictions : numpy ndarray, pandas DataFrame\n        Values predicted by the forecaster and their estimated interval. The \n        output type is the same as the type of `y` used in the fit method.\n\n        - pred: predictions.\n        - lower_bound: lower bound of the interval. (if `return_conf_int`)\n        - upper_bound: upper bound of the interval. (if `return_conf_int`)\n\n    \"\"\"\n\n    # This is done because statsmodels doesn't allow `exog` length greater than\n    # the number of steps\n    if exog is not None and len(exog) &gt; steps:\n        warnings.warn(\n            f\"When predicting using exogenous variables, the `exog` parameter \"\n            f\"must have the same length as the number of predicted steps. Since \"\n            f\"len(exog) &gt; steps, only the first {steps} observations are used.\"\n        )\n        exog = exog[:steps]\n\n    predictions = self.sarimax_res.get_forecast(\n                      steps = steps,\n                      exog  = exog,\n                      **self._predict_kwargs\n                  )\n\n    if not return_conf_int:\n        predictions = predictions.predicted_mean\n        if self.output_type == 'pandas':\n            predictions = predictions.rename(\"pred\").to_frame()\n    else:\n        if self.output_type == 'numpy':\n            predictions = np.column_stack(\n                              [predictions.predicted_mean,\n                               predictions.conf_int(alpha=alpha)]\n                          )\n        else:\n            predictions = pd.concat((\n                              predictions.predicted_mean,\n                              predictions.conf_int(alpha=alpha)),\n                              axis = 1\n                          )\n            predictions.columns = ['pred', 'lower_bound', 'upper_bound']\n\n    return predictions\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.append","title":"append","text":"<pre><code>append(\n    y,\n    exog=None,\n    refit=False,\n    copy_initialization=False,\n    **kwargs\n)\n</code></pre> <p>Recreate the results object with new data appended to the original data.</p> <p>Creates a new result object applied to a dataset that is created by  appending new data to the end of the model's original data [1]_. The new  results can then be used for analysis or forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations from the modeled time-series process.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations of exogenous regressors, if applicable. Must have  the same number of observations as <code>y</code> and their indexes must be  aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>refit</code> <code>bool</code> <p>Whether to re-fit the parameters, based on the combined dataset.</p> <code>False</code> <code>copy_initialization</code> <code>bool</code> <p>Whether or not to copy the initialization from the current results  set to the new model.</p> <code>False</code> <code>**kwargs</code> <p>Keyword arguments may be used to modify model specification arguments  when created the new model object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>The <code>y</code> and <code>exog</code> arguments to this method must be formatted in the same  way (e.g. Pandas Series versus Numpy array) as were the <code>y</code> and <code>exog</code>  arrays passed to the original model.</p> <p>The <code>y</code> argument to this method should consist of new observations that  occurred directly after the last element of <code>y</code>. For any other kind of  dataset, see the apply method.</p> <p>This method will apply filtering to all of the original data as well as  to the new data. To apply filtering only to the new data (which can be  much faster if the original dataset is large), see the extend method.</p> References <p>.. [1] Statsmodels MLEResults append API Reference.        https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.append.html#statsmodels.tsa.statespace.mlemodel.MLEResults.append</p> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>@_check_fitted\ndef append(\n    self,\n    y: np.ndarray | pd.Series | pd.DataFrame,\n    exog: np.ndarray | pd.Series | pd.DataFrame | None = None,\n    refit: bool = False,\n    copy_initialization: bool = False,\n    **kwargs\n) -&gt; None:\n    \"\"\"\n    Recreate the results object with new data appended to the original data.\n\n    Creates a new result object applied to a dataset that is created by \n    appending new data to the end of the model's original data [1]_. The new \n    results can then be used for analysis or forecasting.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        New observations from the modeled time-series process.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default None\n        New observations of exogenous regressors, if applicable. Must have \n        the same number of observations as `y` and their indexes must be \n        aligned so that y[i] is regressed on exog[i].\n    refit : bool, default False\n        Whether to re-fit the parameters, based on the combined dataset.\n    copy_initialization : bool, default False\n        Whether or not to copy the initialization from the current results \n        set to the new model. \n    **kwargs\n        Keyword arguments may be used to modify model specification arguments \n        when created the new model object.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The `y` and `exog` arguments to this method must be formatted in the same \n    way (e.g. Pandas Series versus Numpy array) as were the `y` and `exog` \n    arrays passed to the original model.\n\n    The `y` argument to this method should consist of new observations that \n    occurred directly after the last element of `y`. For any other kind of \n    dataset, see the apply method.\n\n    This method will apply filtering to all of the original data as well as \n    to the new data. To apply filtering only to the new data (which can be \n    much faster if the original dataset is large), see the extend method.\n\n    References\n    ----------\n    .. [1] Statsmodels MLEResults append API Reference.\n           https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.append.html#statsmodels.tsa.statespace.mlemodel.MLEResults.append\n\n    \"\"\"\n\n    fit_kwargs = self._fit_kwargs if refit else None\n\n    self.sarimax_res = self.sarimax_res.append(\n                           endog               = y,\n                           exog                = exog,\n                           refit               = refit,\n                           copy_initialization = copy_initialization,\n                           fit_kwargs          = fit_kwargs,\n                           **kwargs\n                       )\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.apply","title":"apply","text":"<pre><code>apply(\n    y,\n    exog=None,\n    refit=False,\n    copy_initialization=False,\n    **kwargs\n)\n</code></pre> <p>Apply the fitted parameters to new data unrelated to the original data.</p> <p>Creates a new result object using the current fitted parameters, applied  to a completely new dataset that is assumed to be unrelated to the model's original data [1]_. The new results can then be used for analysis or forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations from the modeled time-series process.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations of exogenous regressors, if applicable. Must have  the same number of observations as <code>y</code> and their indexes must be  aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>refit</code> <code>bool</code> <p>Whether to re-fit the parameters, using the new dataset.</p> <code>False</code> <code>copy_initialization</code> <code>bool</code> <p>Whether or not to copy the initialization from the current results  set to the new model.</p> <code>False</code> <code>**kwargs</code> <p>Keyword arguments may be used to modify model specification arguments  when created the new model object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>The <code>y</code> argument to this method should consist of new observations that  are not necessarily related to the original model's <code>y</code> dataset. For  observations that continue that original dataset by follow directly after  its last element, see the append and extend methods.</p> References <p>.. [1] Statsmodels MLEResults apply API Reference.        https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.apply.html#statsmodels.tsa.statespace.mlemodel.MLEResults.apply</p> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>@_check_fitted\ndef apply(\n    self,\n    y: np.ndarray | pd.Series | pd.DataFrame,\n    exog: np.ndarray | pd.Series | pd.DataFrame | None = None,\n    refit: bool = False,\n    copy_initialization: bool = False,\n    **kwargs\n) -&gt; None:\n    \"\"\"\n    Apply the fitted parameters to new data unrelated to the original data.\n\n    Creates a new result object using the current fitted parameters, applied \n    to a completely new dataset that is assumed to be unrelated to the model's\n    original data [1]_. The new results can then be used for analysis or forecasting.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        New observations from the modeled time-series process.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default None\n        New observations of exogenous regressors, if applicable. Must have \n        the same number of observations as `y` and their indexes must be \n        aligned so that y[i] is regressed on exog[i].\n    refit : bool, default False\n        Whether to re-fit the parameters, using the new dataset.\n    copy_initialization : bool, default False\n        Whether or not to copy the initialization from the current results \n        set to the new model. \n    **kwargs\n        Keyword arguments may be used to modify model specification arguments \n        when created the new model object.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The `y` argument to this method should consist of new observations that \n    are not necessarily related to the original model's `y` dataset. For \n    observations that continue that original dataset by follow directly after \n    its last element, see the append and extend methods.\n\n    References\n    ----------\n    .. [1] Statsmodels MLEResults apply API Reference.\n           https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.apply.html#statsmodels.tsa.statespace.mlemodel.MLEResults.apply\n\n    \"\"\"\n\n    fit_kwargs = self._fit_kwargs if refit else None\n\n    self.sarimax_res = self.sarimax_res.apply(\n                           endog               = y,\n                           exog                = exog,\n                           refit               = refit,\n                           copy_initialization = copy_initialization,\n                           fit_kwargs          = fit_kwargs,\n                           **kwargs\n                       )\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.extend","title":"extend","text":"<pre><code>extend(y, exog=None, **kwargs)\n</code></pre> <p>Recreate the results object for new data that extends the original data.</p> <p>Creates a new result object applied to a new dataset that is assumed to  follow directly from the end of the model's original data [1]_. The new  results can then be used for analysis or forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations from the modeled time-series process.</p> required <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>New observations of exogenous regressors, if applicable. Must have  the same number of observations as <code>y</code> and their indexes must be  aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>**kwargs</code> <p>Keyword arguments may be used to modify model specification arguments  when created the new model object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>The <code>y</code> argument to this method should consist of new observations that  occurred directly after the last element of the model's original <code>y</code>  array. For any other kind of dataset, see the apply method.</p> <p>This method will apply filtering only to the new data provided by the <code>y</code>  argument, which can be much faster than re-filtering the entire dataset.  However, the returned results object will only have results for the new  data. To retrieve results for both the new data and the original data,  see the append method.</p> References <p>.. [1] Statsmodels MLEResults extend API Reference.        https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.extend.html#statsmodels.tsa.statespace.mlemodel.MLEResults.extend</p> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>@_check_fitted\ndef extend(\n    self,\n    y: np.ndarray | pd.Series | pd.DataFrame,\n    exog: np.ndarray | pd.Series | pd.DataFrame | None = None,\n    **kwargs\n) -&gt; None:\n    \"\"\"\n    Recreate the results object for new data that extends the original data.\n\n    Creates a new result object applied to a new dataset that is assumed to \n    follow directly from the end of the model's original data [1]_. The new \n    results can then be used for analysis or forecasting.\n\n    Parameters\n    ----------\n    y : numpy ndarray, pandas Series, pandas DataFrame\n        New observations from the modeled time-series process.\n    exog : numpy ndarray, pandas Series, pandas DataFrame, default None\n        New observations of exogenous regressors, if applicable. Must have \n        the same number of observations as `y` and their indexes must be \n        aligned so that y[i] is regressed on exog[i].\n    **kwargs\n        Keyword arguments may be used to modify model specification arguments \n        when created the new model object.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The `y` argument to this method should consist of new observations that \n    occurred directly after the last element of the model's original `y` \n    array. For any other kind of dataset, see the apply method.\n\n    This method will apply filtering only to the new data provided by the `y` \n    argument, which can be much faster than re-filtering the entire dataset. \n    However, the returned results object will only have results for the new \n    data. To retrieve results for both the new data and the original data, \n    see the append method.\n\n    References\n    ----------\n    .. [1] Statsmodels MLEResults extend API Reference.\n           https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.mlemodel.MLEResults.extend.html#statsmodels.tsa.statespace.mlemodel.MLEResults.extend\n\n    \"\"\"\n\n    self.sarimax_res = self.sarimax_res.extend(\n                           endog = y,\n                           exog  = exog,\n                           **kwargs\n                       )\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.set_params","title":"set_params","text":"<pre><code>set_params(**params)\n</code></pre> <p>Set new values to the parameters of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters values.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>def set_params(\n    self, \n    **params: dict[str, object]\n) -&gt; None:\n    \"\"\"\n    Set new values to the parameters of the regressor.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters values.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    params = {k: v for k, v in params.items() if k in self._sarimax_params}\n    for key, value in params.items():\n        setattr(self, key, value)\n\n    self._consolidate_kwargs()\n\n    # Reset values in case the model has already been fitted.\n    self.output_type    = None\n    self.sarimax_res    = None\n    self.fitted         = False\n    self.training_index = None\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.params","title":"params","text":"<pre><code>params()\n</code></pre> <p>Get the parameters of the model. The order of variables is the trend coefficients, the <code>k_exog</code> exogenous coefficients, the <code>k_ar</code> AR  coefficients, and finally the <code>k_ma</code> MA coefficients.</p> <p>Returns:</p> Name Type Description <code>params</code> <code>numpy ndarray, pandas Series</code> <p>The parameters of the model.</p> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>@_check_fitted\ndef params(\n    self\n) -&gt; np.ndarray | pd.Series:\n    \"\"\"\n    Get the parameters of the model. The order of variables is the trend\n    coefficients, the `k_exog` exogenous coefficients, the `k_ar` AR \n    coefficients, and finally the `k_ma` MA coefficients.\n\n    Returns\n    -------\n    params : numpy ndarray, pandas Series\n        The parameters of the model.\n\n    \"\"\"\n\n    return self.sarimax_res.params\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.summary","title":"summary","text":"<pre><code>summary(alpha=0.05, start=None)\n</code></pre> <p>Get a summary of the SARIMAXResults object.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %.</p> <code>0.05</code> <code>start</code> <code>int</code> <p>Integer of the start observation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>summary</code> <code>Summary instance</code> <p>This holds the summary table and text, which can be printed or  converted to various output formats.</p> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>@_check_fitted\ndef summary(\n    self,\n    alpha: float = 0.05,\n    start: int = None\n) -&gt; object:\n    \"\"\"\n    Get a summary of the SARIMAXResults object.\n\n    Parameters\n    ----------\n    alpha : float, default 0.05\n        The confidence intervals for the forecasts are (1 - alpha) %.\n    start : int, default None\n        Integer of the start observation.\n\n    Returns\n    -------\n    summary : Summary instance\n        This holds the summary table and text, which can be printed or \n        converted to various output formats.\n\n    \"\"\"\n\n    return self.sarimax_res.summary(alpha=alpha, start=start)\n</code></pre>"},{"location":"api/Sarimax.html#skforecast.sarimax._sarimax.Sarimax.get_info_criteria","title":"get_info_criteria","text":"<pre><code>get_info_criteria(criteria='aic', method='standard')\n</code></pre> <p>Get the selected information criteria.</p> <p>Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html to know more about statsmodels info_criteria method.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>str</code> <p>The information criteria to compute. Valid options are {'aic', 'bic', 'hqic'}.</p> <code>'aic'</code> <code>method</code> <code>str</code> <p>The method for information criteria computation. Default is 'standard' method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl (2007).</p> <code>'standard'</code> <p>Returns:</p> Name Type Description <code>metric</code> <code>float</code> <p>The value of the selected information criteria.</p> Source code in <code>skforecast\\sarimax\\_sarimax.py</code> <pre><code>@_check_fitted\ndef get_info_criteria(\n    self,\n    criteria: str = 'aic',\n    method: str = 'standard'\n) -&gt; float:\n    \"\"\"\n    Get the selected information criteria.\n\n    Check https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.info_criteria.html\n    to know more about statsmodels info_criteria method.\n\n    Parameters\n    ----------\n    criteria : str, default 'aic'\n        The information criteria to compute. Valid options are {'aic', 'bic',\n        'hqic'}.\n    method : str, default 'standard'\n        The method for information criteria computation. Default is 'standard'\n        method; 'lutkepohl' computes the information criteria as in L\u00fctkepohl\n        (2007).\n\n    Returns\n    -------\n    metric : float\n        The value of the selected information criteria.\n\n    \"\"\"\n\n    if criteria not in ['aic', 'bic', 'hqic']:\n        raise ValueError(\n            \"Invalid value for `criteria`. Valid options are 'aic', 'bic', \"\n            \"and 'hqic'.\"\n        )\n\n    if method not in ['standard', 'lutkepohl']:\n        raise ValueError(\n            \"Invalid value for `method`. Valid options are 'standard' and \"\n            \"'lutkepohl'.\"\n        )\n\n    metric = self.sarimax_res.info_criteria(criteria=criteria, method=method)\n\n    return metric\n</code></pre>"},{"location":"api/datasets.html","title":"<code>Datasets</code>","text":""},{"location":"api/datasets.html#skforecast.datasets.fetch_dataset","title":"skforecast.datasets.fetch_dataset","text":"<pre><code>fetch_dataset(\n    name,\n    version=\"latest\",\n    raw=False,\n    kwargs_read_csv={},\n    verbose=True,\n)\n</code></pre> <p>Fetch a dataset from the skforecast-datasets repository.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the dataset to fetch.</p> required <code>version</code> <code>str</code> <p>Version of the dataset to fetch. If 'latest', the latest version will be  fetched (the one in the main branch). For a list of available versions,  see the repository branches.</p> <code>'latest'</code> <code>raw</code> <code>bool</code> <p>If True, the raw dataset is fetched. If False, the preprocessed dataset  is fetched. The preprocessing consists of setting the column with the  date/time as index and converting the index to datetime. A frequency is  also set to the index.</p> <code>False</code> <code>kwargs_read_csv</code> <code>dict</code> <p>Kwargs to pass to pandas <code>read_csv</code> function.</p> <code>{}</code> <code>verbose</code> <code>bool</code> <p>If True, print information about the dataset.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas DataFrame</code> <p>Dataset.</p> Source code in <code>skforecast\\datasets\\datasets.py</code> <pre><code>def fetch_dataset(\n    name: str,\n    version: str = 'latest',\n    raw: bool = False,\n    kwargs_read_csv: dict = {},\n    verbose: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch a dataset from the skforecast-datasets repository.\n\n    Parameters\n    ----------\n    name: str\n        Name of the dataset to fetch.\n    version: str, int, default `'latest'`\n        Version of the dataset to fetch. If 'latest', the latest version will be \n        fetched (the one in the main branch). For a list of available versions, \n        see the repository branches.\n    raw: bool, default `False`\n        If True, the raw dataset is fetched. If False, the preprocessed dataset \n        is fetched. The preprocessing consists of setting the column with the \n        date/time as index and converting the index to datetime. A frequency is \n        also set to the index.\n    kwargs_read_csv: dict, default `{}`\n        Kwargs to pass to pandas `read_csv` function.\n    verbose: bool, default `True`\n        If True, print information about the dataset.\n\n    Returns\n    -------\n    df: pandas DataFrame\n        Dataset.\n\n    \"\"\"\n\n    version = 'main' if version == 'latest' else f'{version}'\n\n    datasets = {\n        'h2o': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/h2o.csv'\n            ),\n            'sep': ',',\n            'index_col': 'fecha',\n            'date_format': '%Y-%m-%d',\n            'freq': 'MS',\n            'file_type': 'csv',\n            'description': (\n                'Monthly expenditure ($AUD) on corticosteroid drugs that the '\n                'Australian health system had between 1991 and 2008. '\n            ),\n            'source': (\n                'Hyndman R (2023). fpp3: Data for Forecasting: Principles and Practice'\n                '(3rd Edition). http://pkg.robjhyndman.com/fpp3package/,'\n                'https://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.'\n            )\n        },\n        'h2o_exog': {\n            'url': (\n                f\"https://raw.githubusercontent.com/skforecast/\"\n                f\"skforecast-datasets/{version}/data/h2o_exog.csv\"\n            ),\n            'sep': ',',\n            'index_col': 'fecha',\n            'date_format': '%Y-%m-%d',\n            'freq': 'MS',\n            'file_type': 'csv',\n            'description': (\n                'Monthly expenditure ($AUD) on corticosteroid drugs that the '\n                'Australian health system had between 1991 and 2008. Two additional '\n                'variables (exog_1, exog_2) are simulated.'\n            ),\n            'source': (\n                \"Hyndman R (2023). fpp3: Data for Forecasting: Principles and Practice \"\n                \"(3rd Edition). http://pkg.robjhyndman.com/fpp3package/, \"\n                \"https://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\"\n            )\n        },\n        'fuel_consumption': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/consumos-combustibles-mensual.csv'\n            ),\n            'sep': ',',\n            'index_col': 'Fecha',\n            'date_format': '%Y-%m-%d',\n            'freq': 'MS',\n            'file_type': 'csv',\n            'description': (\n                'Monthly fuel consumption in Spain from 1969-01-01 to 2022-08-01.'\n            ),\n            'source': (\n                'Obtained from Corporaci\u00f3n de Reservas Estrat\u00e9gicas de Productos '\n                'Petrol\u00edferos and Corporaci\u00f3n de Derecho P\u00fablico tutelada por el '\n                'Ministerio para la Transici\u00f3n Ecol\u00f3gica y el Reto Demogr\u00e1fico. '\n                'https://www.cores.es/es/estadisticas'\n            )\n        },\n        'items_sales': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/simulated_items_sales.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'file_type': 'csv',\n            'description': 'Simulated time series for the sales of 3 different items.',\n            'source': 'Simulated data.'\n        },\n        'air_quality_valencia': {\n            'url': (\n                f\"https://raw.githubusercontent.com/skforecast/\"\n                f\"skforecast-datasets/{version}/data/air_quality_valencia.csv\"\n            ),\n            'sep': ',',\n            'index_col': 'datetime',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'file_type': 'csv',\n            'description': (\n                'Hourly measures of several air chemical pollutant at Valencia city '\n                '(Avd. Francia) from 2019-01-01 to 20213-12-31. Including the following '\n                'variables: pm2.5 (\u00b5g/m\u00b3), CO (mg/m\u00b3), NO (\u00b5g/m\u00b3), NO2 (\u00b5g/m\u00b3), '\n                'PM10 (\u00b5g/m\u00b3), NOx (\u00b5g/m\u00b3), O3 (\u00b5g/m\u00b3), Veloc. (m/s), Direc. (degrees), '\n                'SO2 (\u00b5g/m\u00b3).'\n            ),\n            'source': (\n                \"Red de Vigilancia y Control de la Contaminaci\u00f3n Atmosf\u00e9rica, \"\n                \"46250047-Val\u00e8ncia - Av. Fran\u00e7a, \"\n                \"https://mediambient.gva.es/es/web/calidad-ambiental/datos-historicos.\"\n            )\n        },\n        'air_quality_valencia_no_missing': {\n            'url': (\n                f\"https://raw.githubusercontent.com/skforecast/\"\n                f\"skforecast-datasets/{version}/data/air_quality_valencia_no_missing.csv\"\n            ),\n            'sep': ',',\n            'index_col': 'datetime',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'file_type': 'csv',\n            'description': (\n                'Hourly measures of several air chemical pollutant at Valencia city '\n                '(Avd. Francia) from 2019-01-01 to 20213-12-31. Including the following '\n                'variables: pm2.5 (\u00b5g/m\u00b3), CO (mg/m\u00b3), NO (\u00b5g/m\u00b3), NO2 (\u00b5g/m\u00b3), '\n                'PM10 (\u00b5g/m\u00b3), NOx (\u00b5g/m\u00b3), O3 (\u00b5g/m\u00b3), Veloc. (m/s), Direc. (degrees), '\n                'SO2 (\u00b5g/m\u00b3). Missing values have been imputed using linear interpolation.'\n            ),\n            'source': (\n                \"Red de Vigilancia y Control de la Contaminaci\u00f3n Atmosf\u00e9rica, \"\n                \"46250047-Val\u00e8ncia - Av. Fran\u00e7a, \"\n                \"https://mediambient.gva.es/es/web/calidad-ambiental/datos-historicos.\"\n            )\n        },\n        'website_visits': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/visitas_por_dia_web_cienciadedatos.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': '1D',\n            'file_type': 'csv',\n            'description': (\n                'Daily visits to the cienciadedatos.net website registered with the '\n                'google analytics service.'\n            ),\n            'source': (\n                \"Amat Rodrigo, J. (2021). cienciadedatos.net (1.0.0). Zenodo. \"\n                \"https://doi.org/10.5281/zenodo.10006330\"\n            )\n        },\n        'bike_sharing': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/bike_sharing_dataset_clean.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date_time',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'file_type': 'csv',\n            'description': (\n                'Hourly usage of the bike share system in the city of Washington D.C. '\n                'during the years 2011 and 2012. In addition to the number of users per '\n                'hour, information about weather conditions and holidays is available.'\n            ),\n            'source': (\n                \"Fanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning \"\n                \"Repository. https://doi.org/10.24432/C5W894.\"\n            )\n        },\n        'bike_sharing_extended_features': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/bike_sharing_extended_features.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date_time',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'file_type': 'csv',\n            'description': (\n                'Hourly usage of the bike share system in the city of Washington D.C. '\n                'during the years 2011 and 2012. In addition to the number of users per '\n                'hour, the dataset was enriched by introducing supplementary features. '\n                'Addition includes calendar-based variables (day of the week, hour of '\n                'the day, month, etc.), indicators for sunlight, incorporation of '\n                'rolling temperature averages, and the creation of polynomial features '\n                'generated from variable pairs. All cyclic variables are encoded using '\n                'sine and cosine functions to ensure accurate representation.'\n            ),\n            'source': (\n                \"Fanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning \"\n                \"Repository. https://doi.org/10.24432/C5W894.\"\n            )\n        },\n        'australia_tourism': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/australia_tourism.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date_time',\n            'date_format': '%Y-%m-%d',\n            'freq': 'Q',\n            'file_type': 'csv',\n            'description': (\n                \"Quarterly overnight trips (in thousands) from 1998 Q1 to 2016 Q4 \"\n                \"across Australia. The tourism regions are formed through the \"\n                \"aggregation of Statistical Local Areas (SLAs) which are defined by \"\n                \"the various State and Territory tourism authorities according to \"\n                \"their research and marketing needs.\"\n            ),\n            'source': (\n                \"Wang, E, D Cook, and RJ Hyndman (2020). A new tidy data structure to \"\n                \"support exploration and modeling of temporal data, Journal of \"\n                \"Computational and Graphical Statistics, 29:3, 466-478, \"\n                \"doi:10.1080/10618600.2019.1695624.\"\n            )\n        },\n        'uk_daily_flights': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/uk_daily_flights.csv'\n            ),\n            'sep': ',',\n            'index_col': 'Date',\n            'date_format': '%d/%m/%Y',\n            'freq': 'D',\n            'file_type': 'csv',\n            'description': 'Daily number of flights in UK from 02/01/2019 to 23/01/2022.',\n            'source': (\n                'Experimental statistics published as part of the Economic activity and '\n                'social change in the UK, real-time indicators release, Published 27 '\n                'January 2022. Daily flight numbers are available in the dashboard '\n                'provided by the European Organisation for the Safety of Air Navigation '\n                '(EUROCONTROL). '\n                'https://www.ons.gov.uk/economy/economicoutputandproductivity/output/'\n                'bulletins/economicactivityandsocialchangeintheukrealtimeindicators/latest'\n            )\n        },\n        'wikipedia_visits': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/wikipedia_visits.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'file_type': 'csv',\n            'description': (\n                'Log daily page views for the Wikipedia page for Peyton Manning. '\n                'Scraped data using the Wikipediatrend package in R.'\n            ),\n            'source': (\n                'https://github.com/facebook/prophet/blob/main/examples/'\n                'example_wp_log_peyton_manning.csv'\n            )\n        },\n        'vic_electricity': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/vic_electricity.csv'\n            ),\n            'sep': ',',\n            'index_col': 'Time',\n            'date_format': '%Y-%m-%dT%H:%M:%SZ',\n            'freq': '30min',\n            'file_type': 'csv',\n            'description': 'Half-hourly electricity demand for Victoria, Australia',\n            'source': (\n                \"O'Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022).tsibbledata: Diverse \"\n                \"Datasets for 'tsibble'. https://tsibbledata.tidyverts.org/, \"\n                \"https://github.com/tidyverts/tsibbledata/. \"\n                \"https://tsibbledata.tidyverts.org/reference/vic_elec.html\"\n            )\n        },\n        'store_sales': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/store_sales.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'file_type': 'csv',\n            'description': (\n                'This dataset contains 913,000 sales transactions from 2013-01-01 to '\n                '2017-12-31 for 50 products (SKU) in 10 stores.'\n            ),\n            'source': (\n                'The original data was obtained from: inversion. (2018). Store Item '\n                'Demand Forecasting Challenge. Kaggle. '\n                'https://kaggle.com/competitions/demand-forecasting-kernels-only'\n            )\n        },\n        'bicimad': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/bicimad_users.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'file_type': 'csv',\n            'description': (\n                'This dataset contains the daily users of the bicycle rental '\n                'service (BiciMad) in the city of Madrid (Spain) from 2014-06-23 '\n                'to 2022-09-30.'\n            ),\n            'source': (\n                'The original data was obtained from: Portal de datos abiertos '\n                'del Ayuntamiento de Madrid https://datos.madrid.es/portal/site/egob'\n            )\n        },\n        'm4_daily': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/m4_daily.parquet'\n            ),\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'D',\n            'file_type': 'parquet',\n            'description': \"Time series with daily frequency from the M4 competition.\",\n            'source': (\n                \"Monash Time Series Forecasting Repository  \"\n                \"(https://zenodo.org/communities/forecasting) Godahewa, R., Bergmeir, \"\n                \"C., Webb, G. I., Hyndman, R. J., &amp; Montero-Manso, P. (2021). Monash \"\n                \"Time Series Forecasting Archive. In Neural Information Processing \"\n                \"Systems Track on Datasets and Benchmarks. \\n\"\n                \"Raw data, available in .tsf format, has been converted to Pandas \"\n                \"format using the code provided by the authors in \"\n                \"https://github.com/rakshitha123/TSForecasting/blob/master/utils/data_loader.py \\n\"\n                \"The category of each time series has been included in the dataset. This \"\n                \"information has been obtained from the Kaggle competition page: \"\n                \"https://www.kaggle.com/datasets/yogesh94/m4-forecasting-competition-dataset\"\n            )\n        },\n        'm4_hourly': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/m4_hourly.parquet'\n            ),\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'file_type': 'parquet',\n            'description': \"Time series with hourly frequency from the M4 competition.\",\n            'source': (\n                \"Monash Time Series Forecasting Repository  \"\n                \"(https://zenodo.org/communities/forecasting) Godahewa, R., Bergmeir, \"\n                \"C., Webb, G. I., Hyndman, R. J., &amp; Montero-Manso, P. (2021). Monash \"\n                \"Time Series Forecasting Archive. In Neural Information Processing \"\n                \"Systems Track on Datasets and Benchmarks. \\n\"\n                \"Raw data, available in .tsf format, has been converted to Pandas \"\n                \"format using the code provided by the authors in \"\n                \"https://github.com/rakshitha123/TSForecasting/blob/master/utils/data_loader.py \\n\"\n                \"The category of each time series has been included in the dataset. This \"\n                \"information has been obtained from the Kaggle competition page: \"\n                \"https://www.kaggle.com/datasets/yogesh94/m4-forecasting-competition-dataset\"\n            )\n        },\n        'ashrae_daily': {\n            'url': 'https://drive.google.com/file/d/1fMsYjfhrFLmeFjKG3jenXjDa5s984ThC/view?usp=sharing',\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'file_type': 'parquet',\n            'description': (\n                \"Daily energy consumption data from the ASHRAE competition with \"\n                \"building metadata and weather data.\"\n            ),\n            'source': (\n                \"Kaggle competition Addison Howard, Chris Balbach, Clayton Miller, \"\n                \"Jeff Haberl, Krishnan Gowri, Sohier Dane. (2019). ASHRAE - Great Energy \"\n                \"Predictor III. Kaggle. https://www.kaggle.com/c/ashrae-energy-prediction/overview\"\n            )\n        },\n        'bdg2_daily': {\n            'url': 'https://drive.google.com/file/d/1KHYopzclKvS1F6Gt6GoJWKnxiuZ2aqen/view?usp=sharing',\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'file_type': 'parquet',\n            'description': (\n                \"Daily energy consumption data from the The Building Data Genome Project 2 \"\n                \"with building metadata and weather data. \"\n                \"https://github.com/buds-lab/building-data-genome-project-2\"\n            ),\n            'source': (\n                \"Miller, C., Kathirgamanathan, A., Picchetti, B. et al. The Building Data \"\n                \"Genome Project 2, energy meter data from the ASHRAE Great Energy \"\n                \"Predictor III competition. Sci Data 7, 368 (2020). \"\n                \"https://doi.org/10.1038/s41597-020-00712-x\"\n            )\n        },\n        'bdg2_daily_sample': {\n            'url': 'https://raw.githubusercontent.com/skforecast/skforecast-datasets/refs/heads/main/data/bdg2_daily_sample.csv',\n            'sep': ',',\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'file_type': 'csv',\n            'description': (\n                \"Daily energy consumption data of two buildings sampled from the \"\n                \"The Building Data Genome Project 2. \"\n                \"https://github.com/buds-lab/building-data-genome-project-2\"\n            ),\n            'source': (\n                \"Miller, C., Kathirgamanathan, A., Picchetti, B. et al. The Building Data \"\n                \"Genome Project 2, energy meter data from the ASHRAE Great Energy \"\n                \"Predictor III competition. Sci Data 7, 368 (2020). \"\n                \"https://doi.org/10.1038/s41597-020-00712-x\"\n            )\n        },\n        'bdg2_hourly': {\n            'url': 'https://drive.google.com/file/d/1I2i5mZJ82Cl_SHPTaWJmLoaXnntdCgh7/view?usp=sharing',\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'file_type': 'parquet',\n            'description': (\n                \"Hourly energy consumption data from the The Building Data Genome Project 2 \"\n                \"with building metadata and weather data. \"\n                \"https://github.com/buds-lab/building-data-genome-project-2\"\n            ),\n            'source': (\n                \"Miller, C., Kathirgamanathan, A., Picchetti, B. et al. The Building Data \"\n                \"Genome Project 2, energy meter data from the ASHRAE Great Energy \"\n                \"Predictor III competition. Sci Data 7, 368 (2020). \"\n                \"https://doi.org/10.1038/s41597-020-00712-x\"\n            )\n        },\n        'bdg2_hourly_sample': {\n            'url': 'https://raw.githubusercontent.com/skforecast/skforecast-datasets/refs/heads/main/data/bdg2_hourly_sample.csv',\n            'sep': ',',\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'H',\n            'file_type': 'csv',\n            'description': (\n                \"Daily energy consumption data of two buildings sampled from the \"\n                \"The Building Data Genome Project 2. \"\n                \"https://github.com/buds-lab/building-data-genome-project-2\"\n            ),\n            'source': (\n                \"Miller, C., Kathirgamanathan, A., Picchetti, B. et al. The Building Data \"\n                \"Genome Project 2, energy meter data from the ASHRAE Great Energy \"\n                \"Predictor III competition. Sci Data 7, 368 (2020). \"\n                \"https://doi.org/10.1038/s41597-020-00712-x\"\n            )\n        },\n        'm5': {\n            'url': [\n                'https://drive.google.com/file/d/1JOqBsSHegly6iSJFgmkugAko734c6ZW5/view?usp=sharing',\n                'https://drive.google.com/file/d/1BhO1BUvs-d7ipXrm7caC3Wd_d0C_6PZ8/view?usp=sharing',\n                'https://drive.google.com/file/d/1oHwkQ_QycJVTZMb6bH8C2klQB971gXXA/view?usp=sharing',\n                'https://drive.google.com/file/d/1OvYzFlDG04YgTvju2k02vHEOj0nIuwei/view?usp=sharing'\n            ],\n            'sep': None,\n            'index_col': 'timestamp',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'file_type': 'parquet',\n            'description': (\n                \"Daily sales data from the M5 competition with product metadata and calendar data.\"\n            ),\n            'source': (\n                \"Addison Howard, inversion, Spyros Makridakis, and vangelis. \"\n                \"M5 Forecasting - Accuracy. https://kaggle.com/competitions/m5-forecasting-accuracy, 2020. Kaggle.\"\n            )\n        },\n        'ett_m1': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/refs/heads/{version}/data/ETTm1.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': '15min',\n            'file_type': 'csv',\n            'description': (\n                'Data from an electricity transformer station was collected between July '\n                '2016 and July 2018 (2 years x 365 days x 24 hours x 4 intervals per '\n                'hour = 70,080 data points). Each data point consists of 8 features, '\n                'including the date of the point, the predictive value \"Oil Temperature (OT)\", '\n                'and 6 different types of external power load features: High UseFul Load (HUFL), '\n                'High UseLess Load (HULL), Middle UseFul Load (MUFL), Middle UseLess Load (MULL), '\n                'Low UseFul Load (LUFL), Low UseLess Load (LULL).'\n            ),\n            'source': (\n                'Zhou, Haoyi &amp; Zhang, Shanghang &amp; Peng, Jieqi &amp; Zhang, Shuai &amp; Li, '\n                'Jianxin &amp; Xiong, Hui &amp; Zhang, Wancai. (2020). Informer: Beyond Efficient '\n                'Transformer for Long Sequence Time-Series Forecasting. '\n                '[10.48550/arXiv.2012.07436](https://arxiv.org/abs/2012.07436). '\n                'https://github.com/zhouhaoyi/ETDataset'\n            )\n        },\n        'ett_m2': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/ETTm2.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': '15min',\n            'file_type': 'csv',\n            'description': (\n                'Data from an electricity transformer station was collected between July '\n                '2016 and July 2018 (2 years x 365 days x 24 hours x 4 intervals per '\n                'hour = 70,080 data points). Each data point consists of 8 features, '\n                'including the date of the point, the predictive value \"Oil Temperature (OT)\", '\n                'and 6 different types of external power load features: High UseFul Load (HUFL), '\n                'High UseLess Load (HULL), Middle UseFul Load (MUFL), Middle UseLess Load (MULL), '\n                'Low UseFul Load (LUFL), Low UseLess Load (LULL).'\n            ),\n            'source': (\n                'Zhou, Haoyi &amp; Zhang, Shanghang &amp; Peng, Jieqi &amp; Zhang, Shuai &amp; Li, '\n                'Jianxin &amp; Xiong, Hui &amp; Zhang, Wancai. (2020). Informer: Beyond Efficient '\n                'Transformer for Long Sequence Time-Series Forecasting. '\n                '[10.48550/arXiv.2012.07436](https://arxiv.org/abs/2012.07436). '\n                'https://github.com/zhouhaoyi/ETDataset'\n            )\n        },\n        'ett_m2_extended': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/'\n                f'skforecast-datasets/{version}/data/ETTm2_extended.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': '15min',\n            'file_type': 'csv',\n            'description': (\n                'Data from an electricity transformer station was collected between July '\n                '2016 and July 2018 (2 years x 365 days x 24 hours x 4 intervals per '\n                'hour = 70,080 data points). Each data point consists of 8 features, '\n                'including the date of the point, the predictive value \"Oil Temperature (OT)\", '\n                'and 6 different types of external power load features: High UseFul Load (HUFL), '\n                'High UseLess Load (HULL), Middle UseFul Load (MUFL), Middle UseLess Load (MULL), '\n                'Low UseFul Load (LUFL), Low UseLess Load (LULL). Additional variables are '\n                'created based on calendar information (year, month, week, day of the week, '\n                'and hour). These variables have been encoded using the cyclical encoding '\n                'technique (sin and cos transformations) to preserve the cyclical nature '\n                'of the data.'\n            ),\n            'source': (\n                'Zhou, Haoyi &amp; Zhang, Shanghang &amp; Peng, Jieqi &amp; Zhang, Shuai &amp; Li, '\n                'Jianxin &amp; Xiong, Hui &amp; Zhang, Wancai. (2020). Informer: Beyond Efficient '\n                'Transformer for Long Sequence Time-Series Forecasting. '\n                '[10.48550/arXiv.2012.07436](https://arxiv.org/abs/2012.07436). '\n                'https://github.com/zhouhaoyi/ETDataset'\n            )\n        },\n        'expenditures_australia': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/skforecast-datasets/refs/heads/'\n                f'{version}/data/expenditures_australia.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'MS',\n            'file_type': 'csv',\n            'description': (\n                'Monthly expenditure on cafes, restaurants and takeaway food services '\n                'in Victoria (Australia) from April 1982 up to April 2024.'\n            ),\n            'source': (\n                'Australian Bureau of Statistics. Catalogue No. 8501.0 '\n                'https://www.abs.gov.au/statistics/industry/retail-and-wholesale-trade/'\n                'retail-trade-australia/apr-2024/8501011.xlsx'\n            )\n        },\n        'public_transport_madrid': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/skforecast-datasets/refs/heads/'\n                f'{version}/data/public-transport-madrid.csv'\n            ),\n            'sep': ',',\n            'index_col': 'date',\n            'date_format': '%Y-%m-%d',\n            'freq': 'D',\n            'file_type': 'csv',\n            'description': (\n                'Daily users of public transport in Madrid (Spain) from 2023-01-01 to 2024-12-15.'\n            ),\n            'source': (\n                'Consorcio Regional de Transportes de Madrid CRTM, CRTM Evolucion demanda diaria '\n                'https://datos.crtm.es/documents/a7210254c4514a19a51b1617cfd61f75/about'\n            )\n        },\n        'turbine_emission': {\n            'url': (\n                f'https://raw.githubusercontent.com/skforecast/skforecast-datasets/refs/heads/'\n                f'{version}/data/turbine_emission.csv'\n            ),\n            'sep': ',',\n            'index_col': 'datetime',\n            'date_format': '%Y-%m-%d %H:%M:%S',\n            'freq': 'h',\n            'file_type': 'csv',\n            'description': (\n                'The dataset contains 36733 instances of 11 sensor measures aggregated over one hour, '\n                'from a gas turbine located in Turkey for the purpose of studying flue gas emissions, '\n                'namely CO and NOx. Available variables include: Ambient temperature (AT), '\n                'Ambient pressure (AP), Ambient humidity (AH), Air filter difference pressure (AFDP), '\n                'Gas turbine exhaust pressure (GTEP), Turbine inlet temperature (TIT), '\n                'Turbine after temperature (TAT), Compressor discharge pressure (CDP), Turbine energy yield (TEY), '\n                'Carbon monoxide (CO), and Nitrogen oxides (NOx). Covered period from 2011-01-01 00:00:00 '\n                'to 2015-03-11 12:00:00.'\n            ),\n            'source': (\n                'https://archive.ics.uci.edu/dataset/551/gas+turbine+co+and+nox+emission+data+set'\n            )\n        },\n    }\n\n    if name not in datasets.keys():\n        raise ValueError(\n            f\"Dataset '{name}' not found. Available datasets are: {list(datasets.keys())}\"\n        )\n\n    url = datasets[name]['url']\n    file_type = datasets[name]['file_type']\n\n    if not isinstance(url, list):\n        parsed = urlparse(url)\n        if parsed.scheme == \"https\" and parsed.netloc == \"drive.google.com\":\n            file_id = url.split('/')[-2]\n            url = 'https://drive.google.com/uc?id=' + file_id\n        if file_type == 'csv':\n            try:\n                sep = datasets[name]['sep']\n                df = pd.read_csv(url, sep=sep, **kwargs_read_csv)\n            except Exception as e:\n                raise ValueError(\n                    f\"Error reading dataset '{name}' from {url}: {str(e)}.\"\n                )\n        if file_type == 'parquet':\n            try:\n                df = pd.read_parquet(url)\n            except Exception as e:\n                raise ValueError(\n                    f\"Error reading dataset '{name}' from {url}: {str(e)}.\"\n                )\n    else:\n        try: \n            df = []\n            for url_partition in url:\n                path = 'https://drive.google.com/uc?export=download&amp;id=' + url_partition.split('/')[-2]\n                df.append(pd.read_parquet(path))\n        except Exception as e:\n            raise ValueError(\n                f\"Error reading dataset '{name}' from {url}: {str(e)}.\"\n            )\n        df = pd.concat(df, axis=0).reset_index(drop=True)\n\n    if not raw:\n        try:\n            index_col = datasets[name]['index_col']\n            freq = datasets[name]['freq']\n            if freq == 'H' and pd.__version__ &gt;= '2.2.0':\n                freq = \"h\"\n            date_format = datasets[name]['date_format']\n            df = df.set_index(index_col)\n            df.index = pd.to_datetime(df.index, format=date_format)\n            df = df.asfreq(freq)\n            df = df.sort_index()\n        except:\n            pass\n\n    if verbose:\n        print(name)\n        print('-' * len(name))\n        description = textwrap.fill(datasets[name]['description'], width=80)\n        source = textwrap.fill(datasets[name]['source'], width=80)\n        print(description)\n        print(source)\n        print(f\"Shape of the dataset: {df.shape}\")\n\n    return df\n</code></pre>"},{"location":"api/datasets.html#skforecast.datasets.load_demo_dataset","title":"skforecast.datasets.load_demo_dataset","text":"<pre><code>load_demo_dataset(version='latest')\n</code></pre> <p>Load demo data set with monthly expenditure ($AUD) on corticosteroid drugs that the Australian health system had between 1991 and 2008. Obtained from the book: Forecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos. Index is set to datetime with monthly frequency and sorted.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>Version of the dataset to fetch. If 'latest', the latest version will be fetched (the one in the main branch). For a list of available versions, see the repository branches.</p> <code>'latest'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas Series</code> <p>Dataset.</p> Source code in <code>skforecast\\datasets\\datasets.py</code> <pre><code>def load_demo_dataset(version: str = 'latest') -&gt; pd.Series:\n    \"\"\"\n    Load demo data set with monthly expenditure ($AUD) on corticosteroid drugs that\n    the Australian health system had between 1991 and 2008. Obtained from the book:\n    Forecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos.\n    Index is set to datetime with monthly frequency and sorted.\n\n    Parameters\n    ----------\n    version: str, default `'latest'`\n        Version of the dataset to fetch. If 'latest', the latest version will be\n        fetched (the one in the main branch). For a list of available versions,\n        see the repository branches.\n\n    Returns\n    -------\n    df: pandas Series\n        Dataset.\n\n    \"\"\"\n\n    version = 'main' if version == 'latest' else f'{version}'\n\n    url = (\n        f'https://raw.githubusercontent.com/skforecast/skforecast-datasets/{version}/'\n        'data/h2o.csv'\n    )\n\n    df = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n    df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d')\n    df = df.set_index('datetime')\n    df = df.asfreq('MS')\n    df = df['y']\n    df = df.sort_index()\n\n    return df\n</code></pre>"},{"location":"api/exceptions.html","title":"<code>exceptions</code>","text":""},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.set_warnings_style","title":"skforecast.exceptions.exceptions.set_warnings_style","text":"<pre><code>set_warnings_style(style='skforecast')\n</code></pre> <p>Set the warning handler based on the provided style.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>str</code> <p>The style of the warning handler. Either 'skforecast' or 'default'.</p> <code>'skforecast'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def set_warnings_style(style: str = 'skforecast') -&gt; None:\n    \"\"\"\n    Set the warning handler based on the provided style.\n\n    Parameters\n    ----------\n    style : str, default='skforecast'\n        The style of the warning handler. Either 'skforecast' or 'default'.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if style == \"skforecast\":\n        if not hasattr(warnings, \"_original_showwarning\"):\n            warnings._original_showwarning = warnings.showwarning\n        warnings.showwarning = rich_warning_handler\n    else:\n        warnings.showwarning = warnings._original_showwarning\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.DataTypeWarning","title":"skforecast.exceptions.exceptions.DataTypeWarning","text":"<pre><code>DataTypeWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify there are dtypes in the exogenous data that are not 'int', 'float', 'bool' or 'category'. Most machine learning models do not accept other data types, therefore the forecaster <code>fit</code> and <code>predict</code> may fail.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.DataTypeWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.DataTransformationWarning","title":"skforecast.exceptions.exceptions.DataTransformationWarning","text":"<pre><code>DataTransformationWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that the output data is in the transformed space.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.DataTransformationWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.IgnoredArgumentWarning","title":"skforecast.exceptions.exceptions.IgnoredArgumentWarning","text":"<pre><code>IgnoredArgumentWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that an argument is ignored when using a method  or a function.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.IgnoredArgumentWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.IndexWarning","title":"skforecast.exceptions.exceptions.IndexWarning","text":"<pre><code>IndexWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that the index of the input data is not a expected type.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.IndexWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.LongTrainingWarning","title":"skforecast.exceptions.exceptions.LongTrainingWarning","text":"<pre><code>LongTrainingWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that a large number of models will be trained and the the process may take a while to run.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.LongTrainingWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.MissingExogWarning","title":"skforecast.exceptions.exceptions.MissingExogWarning","text":"<pre><code>MissingExogWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to indicate that there are missing exogenous variables in the data. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.MissingExogWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.MissingValuesWarning","title":"skforecast.exceptions.exceptions.MissingValuesWarning","text":"<pre><code>MissingValuesWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to indicate that there are missing values in the data. This  warning occurs when the input data contains missing values, or the training matrix generates missing values. Most machine learning models do not accept missing values, so the Forecaster's <code>fit' and</code>predict' methods may fail.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.MissingValuesWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.OneStepAheadValidationWarning","title":"skforecast.exceptions.exceptions.OneStepAheadValidationWarning","text":"<pre><code>OneStepAheadValidationWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that the one-step-ahead validation is being used.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.OneStepAheadValidationWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.ResidualsUsageWarning","title":"skforecast.exceptions.exceptions.ResidualsUsageWarning","text":"<pre><code>ResidualsUsageWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that a residual are not correctly used in the probabilistic forecasting process.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.ResidualsUsageWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.UnknownLevelWarning","title":"skforecast.exceptions.exceptions.UnknownLevelWarning","text":"<pre><code>UnknownLevelWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that a level being predicted was not part of the training data.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.UnknownLevelWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.SaveLoadSkforecastWarning","title":"skforecast.exceptions.exceptions.SaveLoadSkforecastWarning","text":"<pre><code>SaveLoadSkforecastWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify any issues that may arise when saving or loading a forecaster.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.SaveLoadSkforecastWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.SkforecastVersionWarning","title":"skforecast.exceptions.exceptions.SkforecastVersionWarning","text":"<pre><code>SkforecastVersionWarning(message)\n</code></pre> <p>               Bases: <code>UserWarning</code></p> <p>Warning used to notify that the skforecast version installed in the  environment differs from the version used to initialize the forecaster.</p> <p>Attributes:</p> Name Type Description <code>message</code> Source code in <code>skforecast\\exceptions\\exceptions.py</code> <pre><code>def __init__(self, message):\n    self.message = message\n</code></pre>"},{"location":"api/exceptions.html#skforecast.exceptions.exceptions.SkforecastVersionWarning.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message = message\n</code></pre>"},{"location":"api/experimental.html","title":"<code>experimental</code>","text":""},{"location":"api/experimental.html#skforecast.experimental._experimental.calculate_distance_from_holiday","title":"skforecast.experimental._experimental.calculate_distance_from_holiday","text":"<pre><code>calculate_distance_from_holiday(\n    df,\n    holiday_column=\"is_holiday\",\n    date_column=\"date\",\n    fill_na=0.0,\n)\n</code></pre> <p>Calculate the number of days to the next holiday and the number of days since  the last holiday.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame containing the holiday data.</p> required <code>holiday_column</code> <code>str</code> <p>The name of the column indicating holidays (True/False), by default 'is_holiday'.</p> <code>'is_holiday'</code> <code>date_column</code> <code>str</code> <p>The name of the column containing the dates, by default 'date'.</p> <code>'date'</code> <code>fill_na</code> <code>(int, float)</code> <p>Value to fill for NaN values in the output columns, by default 0.</p> <code>0.</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>DataFrame with additional columns for days to the next holiday ('days_to_holiday')  and days since the last holiday ('days_since_holiday').</p> Notes <p>The function assumes that the input <code>df</code> contains a boolean column indicating holidays and a date column. It calculates the number of days to the next holiday and the number of days since the last holiday for each date in the date column.</p> Source code in <code>skforecast\\experimental\\_experimental.py</code> <pre><code>def calculate_distance_from_holiday(\n    df: pd.DataFrame, \n    holiday_column: str = 'is_holiday',\n    date_column: str = 'date',\n    fill_na: int | float = 0.\n) -&gt; pd.DataFrame:  # pragma: no cover\n    \"\"\"\n    Calculate the number of days to the next holiday and the number of days since \n    the last holiday.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame containing the holiday data.\n    holiday_column : str, default 'is_holiday'\n        The name of the column indicating holidays (True/False), by default 'is_holiday'.\n    date_column : str, default 'date'\n        The name of the column containing the dates, by default 'date'.\n    fill_na : int, float, default 0.\n        Value to fill for NaN values in the output columns, by default 0.\n\n    Returns\n    -------\n    df : pd.DataFrame\n        DataFrame with additional columns for days to the next holiday ('days_to_holiday') \n        and days since the last holiday ('days_since_holiday').\n\n    Notes\n    -----\n    The function assumes that the input `df` contains a boolean column indicating holidays\n    and a date column. It calculates the number of days to the next holiday and the number of\n    days since the last holiday for each date in the date column.\n\n    \"\"\"\n\n    df = df.reset_index(drop=True)\n    df[date_column] = pd.to_datetime(df[date_column])\n\n    dates = df[date_column].to_numpy()\n    holiday_dates = df.loc[df[holiday_column], date_column].to_numpy()\n    holiday_dates_sorted = np.sort(holiday_dates)\n\n    # For next holiday (right side)\n    next_idx = np.searchsorted(holiday_dates_sorted, dates, side='left')\n    has_next = next_idx &lt; len(holiday_dates_sorted)\n    days_to_holiday = np.full(len(dates), np.nan)\n    days_to_holiday[has_next] = (\n        holiday_dates_sorted[next_idx[has_next]] - dates[has_next]\n    ).astype('timedelta64[D]').astype(int)\n\n    # For previous holiday (left side)\n    prev_idx = np.searchsorted(holiday_dates_sorted, dates, side='right') - 1\n    has_prev = prev_idx &gt;= 0\n    days_since_holiday = np.full(len(dates), np.nan)\n    days_since_holiday[has_prev] = (\n        dates[has_prev] - holiday_dates_sorted[prev_idx[has_prev]]\n    ).astype('timedelta64[D]').astype(int)\n\n    df[\"days_to_holiday\"] = pd.Series(days_to_holiday, dtype=\"Int64\").fillna(fill_na)\n    df[\"days_since_holiday\"] = pd.Series(days_since_holiday, dtype=\"Int64\").fillna(fill_na)\n\n    return df\n</code></pre>"},{"location":"api/feature_selection.html","title":"<code>feature_selection</code>","text":""},{"location":"api/feature_selection.html#skforecast.feature_selection.feature_selection.select_features","title":"skforecast.feature_selection.feature_selection.select_features","text":"<pre><code>select_features(\n    forecaster,\n    selector,\n    y,\n    exog=None,\n    select_only=None,\n    force_inclusion=None,\n    subsample=0.5,\n    random_state=123,\n    verbose=True,\n)\n</code></pre> <p>Feature selection using any of the sklearn.feature_selection module selectors  (such as <code>RFECV</code>, <code>SelectFromModel</code>, etc.). Two groups of features are evaluated: autoregressive features (lags and window features) and exogenous features. By default, the selection process is performed on both sets of features at the same time, so that the most relevant autoregressive and exogenous features are selected. However, using the <code>select_only</code> argument, the selection process can focus only on the autoregressive or exogenous features without taking into account the other features. Therefore, all other features will remain in the model.  It is also possible to force the inclusion of certain features in the final list of selected features using the <code>force_inclusion</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursive, ForecasterDirect)</code> <p>Forecaster model. If forecaster is a ForecasterDirect, the selector will only be applied to the features of the first step.</p> required <code>selector</code> <code>object</code> <p>A feature selector from sklearn.feature_selection.</p> required <code>y</code> <code>pandas Series, pandas DataFrame</code> <p>Target time series to which the feature selection will be applied.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>select_only</code> <code>str</code> <p>Decide what type of features to include in the selection process. </p> <ul> <li>If <code>'autoreg'</code>, only autoregressive features (lags and window features) are evaluated by the selector. All exogenous features are included in the output <code>selected_exog</code>.</li> <li>If <code>'exog'</code>, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included  in the outputs <code>selected_lags</code> and <code>selected_window_features</code>.</li> <li>If <code>None</code>, all features are evaluated by the selector.</li> </ul> <code>None</code> <code>force_inclusion</code> <code>(list, str)</code> <p>Features to force include in the final list of selected features.</p> <ul> <li>If <code>list</code>, list of feature names to force include.</li> <li>If <code>str</code>, regular expression to identify features to force include.  For example, if <code>force_inclusion=\"^sun_\"</code>, all features that begin  with \"sun_\" will be included in the final list of selected features.</li> </ul> <code>None</code> <code>subsample</code> <code>(int, float)</code> <p>Proportion of records to use for feature selection.</p> <code>0.5</code> <code>random_state</code> <code>int</code> <p>Sets a seed for the random subsample so that the subsampling process  is always deterministic.</p> <code>123</code> <code>verbose</code> <code>bool</code> <p>Print information about feature selection process.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>selected_lags</code> <code>list</code> <p>List of selected lags.</p> <code>selected_window_features</code> <code>list</code> <p>List of selected window features.</p> <code>selected_exog</code> <code>list</code> <p>List of selected exogenous features.</p> Source code in <code>skforecast\\feature_selection\\feature_selection.py</code> <pre><code>def select_features(\n    forecaster: object,\n    selector: object,\n    y: pd.Series | pd.DataFrame,\n    exog: pd.Series | pd.DataFrame | None = None,\n    select_only: str | None = None,\n    force_inclusion: list[str] | str | None = None,\n    subsample: int | float = 0.5,\n    random_state: int = 123,\n    verbose: bool = True\n) -&gt; tuple[list[int], list[str], list[str]]:\n    \"\"\"\n    Feature selection using any of the sklearn.feature_selection module selectors \n    (such as `RFECV`, `SelectFromModel`, etc.). Two groups of features are\n    evaluated: autoregressive features (lags and window features) and exogenous\n    features. By default, the selection process is performed on both sets of features\n    at the same time, so that the most relevant autoregressive and exogenous features\n    are selected. However, using the `select_only` argument, the selection process\n    can focus only on the autoregressive or exogenous features without taking into\n    account the other features. Therefore, all other features will remain in the model. \n    It is also possible to force the inclusion of certain features in the final list\n    of selected features using the `force_inclusion` parameter.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursive, ForecasterDirect\n        Forecaster model. If forecaster is a ForecasterDirect, the\n        selector will only be applied to the features of the first step.\n    selector : object\n        A feature selector from sklearn.feature_selection.\n    y : pandas Series, pandas DataFrame\n        Target time series to which the feature selection will be applied.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    select_only : str, default None\n        Decide what type of features to include in the selection process. \n\n        - If `'autoreg'`, only autoregressive features (lags and window features)\n        are evaluated by the selector. All exogenous features are included in the\n        output `selected_exog`.\n        - If `'exog'`, only exogenous features are evaluated without the presence\n        of autoregressive features. All autoregressive features are included \n        in the outputs `selected_lags` and `selected_window_features`.\n        - If `None`, all features are evaluated by the selector.\n    force_inclusion : list, str, default None\n        Features to force include in the final list of selected features.\n\n        - If `list`, list of feature names to force include.\n        - If `str`, regular expression to identify features to force include. \n        For example, if `force_inclusion=\"^sun_\"`, all features that begin \n        with \"sun_\" will be included in the final list of selected features.\n    subsample : int, float, default 0.5\n        Proportion of records to use for feature selection.\n    random_state : int, default 123\n        Sets a seed for the random subsample so that the subsampling process \n        is always deterministic.\n    verbose : bool, default True\n        Print information about feature selection process.\n\n    Returns\n    -------\n    selected_lags : list\n        List of selected lags.\n    selected_window_features : list\n        List of selected window features.\n    selected_exog : list\n        List of selected exogenous features.\n\n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    valid_forecasters = ['ForecasterRecursive', 'ForecasterDirect']\n\n    if forecaster_name not in valid_forecasters:\n        raise TypeError(\n            f\"`forecaster` must be one of the following classes: {valid_forecasters}.\"\n        )\n\n    if select_only not in ['autoreg', 'exog', None]:\n        raise ValueError(\n            \"`select_only` must be one of the following values: 'autoreg', 'exog', None.\"\n        )\n\n    if subsample &lt;= 0 or subsample &gt; 1:\n        raise ValueError(\n            \"`subsample` must be a number greater than 0 and less than or equal to 1.\"\n        )\n\n    forecaster = deepcopy(forecaster)\n    forecaster.is_fitted = False\n    X_train, y_train = forecaster.create_train_X_y(y=y, exog=exog)\n    if forecaster_name == 'ForecasterDirect':\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step          = 1,\n                               X_train       = X_train,\n                               y_train       = y_train,\n                               remove_suffix = True\n                           )\n\n    lags_cols = []\n    window_features_cols = []\n    autoreg_cols = []\n    if forecaster.lags is not None:\n        lags_cols = forecaster.lags_names\n        autoreg_cols.extend(lags_cols)\n    if forecaster.window_features is not None:\n        window_features_cols = forecaster.window_features_names\n        autoreg_cols.extend(window_features_cols)\n\n    exog_cols = [col for col in X_train.columns if col not in autoreg_cols]\n\n    forced_autoreg = []\n    forced_exog = []\n    if force_inclusion is not None:\n        if isinstance(force_inclusion, list):\n            forced_autoreg = [col for col in force_inclusion if col in autoreg_cols]\n            forced_exog = [col for col in force_inclusion if col in exog_cols]\n        elif isinstance(force_inclusion, str):\n            forced_autoreg = [col for col in autoreg_cols if re.match(force_inclusion, col)]\n            forced_exog = [col for col in exog_cols if re.match(force_inclusion, col)]\n\n    if select_only == 'autoreg':\n        X_train = X_train.drop(columns=exog_cols)\n    elif select_only == 'exog':\n        X_train = X_train.drop(columns=autoreg_cols)\n\n    if isinstance(subsample, float):\n        subsample = int(len(X_train) * subsample)\n\n    rng = np.random.default_rng(seed=random_state)\n    sample = rng.integers(low=0, high=len(X_train), size=subsample)\n    X_train_sample = X_train.iloc[sample, :]\n    y_train_sample = y_train.iloc[sample]\n    selector.fit(X_train_sample, y_train_sample)\n    selected_features = selector.get_feature_names_out()\n\n    if select_only == 'exog':\n        selected_autoreg = autoreg_cols\n    else:\n        selected_autoreg = [\n            feature\n            for feature in selected_features\n            if feature in autoreg_cols\n        ]\n\n    if select_only == 'autoreg':\n        selected_exog = exog_cols\n    else:\n        selected_exog = [\n            feature\n            for feature in selected_features\n            if feature in exog_cols\n        ]\n\n    if force_inclusion is not None: \n        if select_only != 'autoreg':\n            forced_exog_not_selected = set(forced_exog) - set(selected_features)\n            selected_exog.extend(forced_exog_not_selected)\n            selected_exog.sort(key=exog_cols.index)\n        if select_only != 'exog':\n            forced_autoreg_not_selected = set(forced_autoreg) - set(selected_features)\n            selected_autoreg.extend(forced_autoreg_not_selected)\n            selected_autoreg.sort(key=autoreg_cols.index)\n\n    if len(selected_autoreg) == 0:\n        warnings.warn(\n            \"No autoregressive features have been selected. Since a Forecaster \"\n            \"cannot be created without them, be sure to include at least one \"\n            \"using the `force_inclusion` parameter.\"\n        )\n        selected_lags = []\n        selected_window_features = []\n    else:\n        selected_lags = [\n            int(feature.replace('lag_', '')) \n            for feature in selected_autoreg if feature in lags_cols\n        ]\n        selected_window_features = [\n            feature for feature in selected_autoreg if feature in window_features_cols\n        ]\n\n    if verbose:\n        print(f\"Recursive feature elimination ({selector.__class__.__name__})\")\n        print(\"--------------------------------\" + \"-\" * len(selector.__class__.__name__))\n        print(f\"Total number of records available: {X_train.shape[0]}\")\n        print(f\"Total number of records used for feature selection: {X_train_sample.shape[0]}\")\n        print(f\"Number of features available: {len(autoreg_cols) + len(exog_cols)}\") \n        print(f\"    Lags            (n={len(lags_cols)})\")\n        print(f\"    Window features (n={len(window_features_cols)})\")\n        print(f\"    Exog            (n={len(exog_cols)})\")\n        print(f\"Number of features selected: {len(selected_features)}\")\n        print(f\"    Lags            (n={len(selected_lags)}) : {selected_lags}\")\n        print(f\"    Window features (n={len(selected_window_features)}) : {selected_window_features}\")\n        print(f\"    Exog            (n={len(selected_exog)}) : {selected_exog}\")\n\n    return selected_lags, selected_window_features, selected_exog\n</code></pre>"},{"location":"api/feature_selection.html#skforecast.feature_selection.feature_selection.select_features_multiseries","title":"skforecast.feature_selection.feature_selection.select_features_multiseries","text":"<pre><code>select_features_multiseries(\n    forecaster,\n    selector,\n    series,\n    exog=None,\n    select_only=None,\n    force_inclusion=None,\n    subsample=0.5,\n    random_state=123,\n    verbose=True,\n)\n</code></pre> <p>Feature selection using any of the sklearn.feature_selection module selectors  (such as <code>RFECV</code>, <code>SelectFromModel</code>, etc.). Two groups of features are evaluated: autoregressive features and exogenous features. By default, the  selection process is performed on both sets of features at the same time,  so that the most relevant autoregressive and exogenous features are selected.  However, using the <code>select_only</code> argument, the selection process can focus  only on the autoregressive or exogenous features without taking into account  the other features. Therefore, all other features will remain in the model.  It is also possible to force the inclusion of certain features in the final  list of selected features using the <code>force_inclusion</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate)</code> <p>Forecaster model. If forecaster is a ForecasterDirectMultiVariate, the selector will only be applied to the features of the first step.</p> required <code>selector</code> <code>object</code> <p>A feature selector from sklearn.feature_selection.</p> required <code>series</code> <code>pandas DataFrame, dict</code> <p>Target time series to which the feature selection will be applied.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>None</code> <code>select_only</code> <code>str</code> <p>Decide what type of features to include in the selection process. </p> <ul> <li>If <code>'autoreg'</code>, only autoregressive features (lags and window features)  are evaluated by the selector. All exogenous features are  included in the output <code>selected_exog</code>.</li> <li>If <code>'exog'</code>, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included  in the outputs <code>selected_lags</code> and <code>selected_window_features</code>.</li> <li>If <code>None</code>, all features are evaluated by the selector.</li> </ul> <code>None</code> <code>force_inclusion</code> <code>(list, str)</code> <p>Features to force include in the final list of selected features.</p> <ul> <li>If <code>list</code>, list of feature names to force include.</li> <li>If <code>str</code>, regular expression to identify features to force include.  For example, if <code>force_inclusion=\"^sun_\"</code>, all features that begin  with \"sun_\" will be included in the final list of selected features.</li> </ul> <code>None</code> <code>subsample</code> <code>(int, float)</code> <p>Proportion of records to use for feature selection.</p> <code>0.5</code> <code>random_state</code> <code>int</code> <p>Sets a seed for the random subsample so that the subsampling process  is always deterministic.</p> <code>123</code> <code>verbose</code> <code>bool</code> <p>Print information about feature selection process.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>selected_lags</code> <code>(list, dict)</code> <p>List of selected lags. If the forecaster is a ForecasterDirectMultiVariate, the output is a dict with the selected lags for each series, {series_name: lags}, as the lags can be different for each series.</p> <code>selected_window_features</code> <code>list</code> <p>List of selected window features.</p> <code>selected_exog</code> <code>list</code> <p>List of selected exogenous features.</p> Source code in <code>skforecast\\feature_selection\\feature_selection.py</code> <pre><code>def select_features_multiseries(\n    forecaster: object,\n    selector: object,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    select_only: str | None = None,\n    force_inclusion: list[str] | str | None = None,\n    subsample: int | float = 0.5,\n    random_state: int = 123,\n    verbose: bool = True,\n) -&gt; tuple[list[int] | dict[str, int], list[str], list[str]]:\n    \"\"\"\n    Feature selection using any of the sklearn.feature_selection module selectors \n    (such as `RFECV`, `SelectFromModel`, etc.). Two groups of features are\n    evaluated: autoregressive features and exogenous features. By default, the \n    selection process is performed on both sets of features at the same time, \n    so that the most relevant autoregressive and exogenous features are selected. \n    However, using the `select_only` argument, the selection process can focus \n    only on the autoregressive or exogenous features without taking into account \n    the other features. Therefore, all other features will remain in the model. \n    It is also possible to force the inclusion of certain features in the final \n    list of selected features using the `force_inclusion` parameter.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate\n        Forecaster model. If forecaster is a ForecasterDirectMultiVariate, the\n        selector will only be applied to the features of the first step.\n    selector : object\n        A feature selector from sklearn.feature_selection.\n    series : pandas DataFrame, dict\n        Target time series to which the feature selection will be applied.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variables.\n    select_only : str, default None\n        Decide what type of features to include in the selection process. \n\n        - If `'autoreg'`, only autoregressive features (lags and window features) \n        are evaluated by the selector. All exogenous features are \n        included in the output `selected_exog`.\n        - If `'exog'`, only exogenous features are evaluated without the presence\n        of autoregressive features. All autoregressive features are included \n        in the outputs `selected_lags` and `selected_window_features`.\n        - If `None`, all features are evaluated by the selector.\n    force_inclusion : list, str, default None\n        Features to force include in the final list of selected features.\n\n        - If `list`, list of feature names to force include.\n        - If `str`, regular expression to identify features to force include. \n        For example, if `force_inclusion=\"^sun_\"`, all features that begin \n        with \"sun_\" will be included in the final list of selected features.\n    subsample : int, float, default 0.5\n        Proportion of records to use for feature selection.\n    random_state : int, default 123\n        Sets a seed for the random subsample so that the subsampling process \n        is always deterministic.\n    verbose : bool, default True\n        Print information about feature selection process.\n\n    Returns\n    -------\n    selected_lags : list, dict\n        List of selected lags. If the forecaster is a ForecasterDirectMultiVariate,\n        the output is a dict with the selected lags for each series, {series_name: lags},\n        as the lags can be different for each series.\n    selected_window_features : list\n        List of selected window features.\n    selected_exog : list\n        List of selected exogenous features.\n\n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    valid_forecasters = [\n        'ForecasterRecursiveMultiSeries',\n        'ForecasterDirectMultiVariate'\n    ]\n\n    if forecaster_name not in valid_forecasters:\n        raise TypeError(\n            f\"`forecaster` must be one of the following classes: {valid_forecasters}.\"\n        )\n\n    if select_only not in ['autoreg', 'exog', None]:\n        raise ValueError(\n            \"`select_only` must be one of the following values: 'autoreg', 'exog', None.\"\n        )\n\n    if subsample &lt;= 0 or subsample &gt; 1:\n        raise ValueError(\n            \"`subsample` must be a number greater than 0 and less than or equal to 1.\"\n        )\n\n    forecaster = deepcopy(forecaster)\n    forecaster.is_fitted = False\n    output = forecaster._create_train_X_y(series=series, exog=exog)\n    X_train = output[0]\n    y_train = output[1]\n    if forecaster_name == 'ForecasterDirectMultiVariate':\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step          = 1,\n                               X_train       = X_train,\n                               y_train       = y_train,\n                               remove_suffix = True\n                           )\n        lags_cols = list(\n            chain(*[v for v in forecaster.lags_names.values() if v is not None])\n        )\n        window_features_cols = forecaster.X_train_window_features_names_out_\n        encoding_cols = []\n    else:\n        lags_cols = forecaster.lags_names\n        window_features_cols = output[6]  # X_train_window_features_names_out_ output\n        if forecaster.encoding == 'onehot':\n            encoding_cols = output[4]  # X_train_series_names_in_ output\n        else:\n            encoding_cols = ['_level_skforecast']\n\n    lags_cols = [] if lags_cols is None else lags_cols\n    window_features_cols = [] if window_features_cols is None else window_features_cols\n    autoreg_cols = []\n    if forecaster.lags is not None:\n        autoreg_cols.extend(lags_cols)\n    if forecaster.window_features is not None:\n        autoreg_cols.extend(window_features_cols)\n\n    exog_cols = [\n        col\n        for col in X_train.columns\n        if col not in autoreg_cols and col not in encoding_cols\n    ]\n\n    forced_autoreg = []\n    forced_exog = []\n    if force_inclusion is not None:\n        if isinstance(force_inclusion, list):\n            forced_autoreg = [col for col in force_inclusion if col in autoreg_cols]\n            forced_exog = [col for col in force_inclusion if col in exog_cols]\n        elif isinstance(force_inclusion, str):\n            forced_autoreg = [col for col in autoreg_cols if re.match(force_inclusion, col)]\n            forced_exog = [col for col in exog_cols if re.match(force_inclusion, col)]\n\n    if select_only == 'autoreg':\n        X_train = X_train.drop(columns=exog_cols + encoding_cols)\n    elif select_only == 'exog':\n        X_train = X_train.drop(columns=autoreg_cols + encoding_cols)\n    else:\n        X_train = X_train.drop(columns=encoding_cols)\n\n    if isinstance(subsample, float):\n        subsample = int(len(X_train) * subsample)\n\n    rng = np.random.default_rng(seed=random_state)\n    sample = rng.integers(low=0, high=len(X_train), size=subsample)\n    X_train_sample = X_train.iloc[sample, :]\n    y_train_sample = y_train.iloc[sample]\n    selector.fit(X_train_sample, y_train_sample)\n    selected_features = selector.get_feature_names_out()\n\n    if select_only == 'exog':\n        selected_autoreg = autoreg_cols\n    else:\n        selected_autoreg = [\n            feature\n            for feature in selected_features\n            if feature in autoreg_cols\n        ]\n\n    if select_only == 'autoreg':\n        selected_exog = exog_cols\n    else:\n        selected_exog = [\n            feature\n            for feature in selected_features\n            if feature in exog_cols\n        ]\n\n    if force_inclusion is not None: \n        if select_only != 'autoreg':\n            forced_exog_not_selected = set(forced_exog) - set(selected_features)\n            selected_exog.extend(forced_exog_not_selected)\n            selected_exog.sort(key=exog_cols.index)\n        if select_only != 'exog':\n            forced_autoreg_not_selected = set(forced_autoreg) - set(selected_features)\n            selected_autoreg.extend(forced_autoreg_not_selected)\n            selected_autoreg.sort(key=autoreg_cols.index)\n\n    if len(selected_autoreg) == 0:\n        warnings.warn(\n            \"No autoregressive features have been selected. Since a Forecaster \"\n            \"cannot be created without them, be sure to include at least one \"\n            \"using the `force_inclusion` parameter.\"\n        )\n        selected_lags = []\n        selected_window_features = []\n        verbose_selected_lags = []\n    else:\n        if forecaster_name == 'ForecasterDirectMultiVariate':\n            selected_lags = {\n                series_name: (\n                    [\n                        int(feature.replace(f\"{series_name}_lag_\", \"\"))\n                        for feature in selected_autoreg\n                        if feature in lags_names\n                    ]\n                    if lags_names is not None\n                    else []\n                )\n                for series_name, lags_names in forecaster.lags_names.items()\n            }\n            verbose_selected_lags = [\n                feature for feature in selected_autoreg if feature in lags_cols\n            ]\n        else:\n            selected_lags = [\n                int(feature.replace('lag_', '')) \n                for feature in selected_autoreg \n                if feature in lags_cols\n            ]\n            verbose_selected_lags = selected_lags\n\n        selected_window_features = [\n            feature for feature in selected_autoreg \n            if feature in window_features_cols\n        ]\n\n    if verbose:\n        print(f\"Recursive feature elimination ({selector.__class__.__name__})\")\n        print(\"--------------------------------\" + \"-\" * len(selector.__class__.__name__))\n        print(f\"Total number of records available: {X_train.shape[0]}\")\n        print(f\"Total number of records used for feature selection: {X_train_sample.shape[0]}\")\n        print(f\"Number of features available: {len(autoreg_cols) + len(exog_cols)}\") \n        print(f\"    Lags            (n={len(lags_cols)})\")\n        print(f\"    Window features (n={len(window_features_cols)})\")\n        print(f\"    Exog            (n={len(exog_cols)})\")\n        print(f\"Number of features selected: {len(selected_features)}\")\n        print(f\"    Lags            (n={len(verbose_selected_lags)}) : {verbose_selected_lags}\")\n        print(f\"    Window features (n={len(selected_window_features)}) : {selected_window_features}\")\n        print(f\"    Exog            (n={len(selected_exog)}) : {selected_exog}\")\n\n    return selected_lags, selected_window_features, selected_exog\n</code></pre>"},{"location":"api/metrics.html","title":"<code>Metrics</code>","text":""},{"location":"api/metrics.html#skforecast.metrics.mean_absolute_scaled_error","title":"skforecast.metrics.mean_absolute_scaled_error","text":"<pre><code>mean_absolute_scaled_error(y_true, y_pred, y_train)\n</code></pre> <p>Mean Absolute Scaled Error (MASE)</p> <p>MASE is a scale-independent error metric that measures the accuracy of a forecast. It is the mean absolute error of the forecast divided by the mean absolute error of a naive forecast in the training set. The naive forecast is the one obtained by shifting the time series by one period. If y_train is a list of numpy arrays or pandas Series, it is considered that each element is the true value of the target variable in the training set for each time series. In this case, the naive forecast is calculated for each time series separately.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>pandas Series, numpy ndarray</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>pandas Series, numpy ndarray</code> <p>Predicted values of the target variable.</p> required <code>y_train</code> <code>list, pandas Series, numpy ndarray</code> <p>True values of the target variable in the training set. If <code>list</code>, it is consider that each element is the true value of the target variable in the training set for each time series.</p> required <p>Returns:</p> Name Type Description <code>mase</code> <code>float</code> <p>MASE value.</p> Source code in <code>skforecast\\metrics\\metrics.py</code> <pre><code>def mean_absolute_scaled_error(\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    y_train: list[float] | np.ndarray | pd.Series,\n) -&gt; float:\n    \"\"\"\n    Mean Absolute Scaled Error (MASE)\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If `list`, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    mase : float\n        MASE value.\n\n    \"\"\"\n\n    # NOTE: When using this metric in validation, `y_train` doesn't include\n    # the first window_size observations used to create the predictors and/or\n    # rolling features.\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    \"When `y_train` is a list, each element must be a pandas Series \"\n                    \"or numpy ndarray.\"\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n\n    return mase\n</code></pre>"},{"location":"api/metrics.html#skforecast.metrics.root_mean_squared_scaled_error","title":"skforecast.metrics.root_mean_squared_scaled_error","text":"<pre><code>root_mean_squared_scaled_error(y_true, y_pred, y_train)\n</code></pre> <p>Root Mean Squared Scaled Error (RMSSE)</p> <p>RMSSE is a scale-independent error metric that measures the accuracy of a forecast. It is the root mean squared error of the forecast divided by the root mean squared error of a naive forecast in the training set. The naive forecast is the one obtained by shifting the time series by one period. If y_train is a list of numpy arrays or pandas Series, it is considered that each element is the true value of the target variable in the training set for each time series. In this case, the naive forecast is calculated for each time series separately.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>pandas Series, numpy ndarray</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>pandas Series, numpy ndarray</code> <p>Predicted values of the target variable.</p> required <code>y_train</code> <code>list, pandas Series, numpy ndarray</code> <p>True values of the target variable in the training set. If list, it is consider that each element is the true value of the target variable in the training set for each time series.</p> required <p>Returns:</p> Name Type Description <code>rmsse</code> <code>float</code> <p>RMSSE value.</p> Source code in <code>skforecast\\metrics\\metrics.py</code> <pre><code>def root_mean_squared_scaled_error(\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series,\n    y_train: list[float] | np.ndarray | pd.Series,\n) -&gt; float:\n    \"\"\"\n    Root Mean Squared Scaled Error (RMSSE)\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If list, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    rmsse : float\n        RMSSE value.\n\n    \"\"\"\n\n    # NOTE: When using this metric in validation, `y_train` doesn't include\n    # the first window_size observations used to create the predictors and/or\n    # rolling features.\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n\n    return rmsse\n</code></pre>"},{"location":"api/metrics.html#skforecast.metrics.symmetric_mean_absolute_percentage_error","title":"skforecast.metrics.symmetric_mean_absolute_percentage_error","text":"<pre><code>symmetric_mean_absolute_percentage_error(y_true, y_pred)\n</code></pre> <p>Compute the Symmetric Mean Absolute Percentage Error (SMAPE).</p> <p>SMAPE is a relative error metric used to measure the accuracy  of forecasts. Unlike MAPE, it is symmetric and prevents division  by zero by averaging the absolute values of actual and predicted values.</p> <p>The result is expressed as a percentage and ranges from 0%  (perfect prediction) to 200% (maximum error).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>numpy ndarray, pandas Series</code> <p>True values of the target variable.</p> required <code>y_pred</code> <code>numpy ndarray, pandas Series</code> <p>Predicted values of the target variable.</p> required <p>Returns:</p> Name Type Description <code>smape</code> <code>float</code> <p>SMAPE value as a percentage.</p> Notes <p>When both <code>y_true</code> and <code>y_pred</code> are zero, the corresponding term is treated as zero to avoid division by zero.</p> <p>Examples:</p> <pre><code>import numpy as np\nfrom skforecast.metrics import symmetric_mean_absolute_percentage_error\n\ny_true = np.array([100, 200, 0])\ny_pred = np.array([110, 180, 10])\nresult = symmetric_mean_absolute_percentage_error(y_true, y_pred)\nprint(f\"SMAPE: {result:.2f}%\")\n\n# SMAPE: 73.35%\n</code></pre> Source code in <code>skforecast\\metrics\\metrics.py</code> <pre><code>def symmetric_mean_absolute_percentage_error(\n    y_true: np.ndarray | pd.Series,\n    y_pred: np.ndarray | pd.Series\n) -&gt; float:\n    \"\"\"\n    Compute the Symmetric Mean Absolute Percentage Error (SMAPE).\n\n    SMAPE is a relative error metric used to measure the accuracy \n    of forecasts. Unlike MAPE, it is symmetric and prevents division \n    by zero by averaging the absolute values of actual and predicted values.\n\n    The result is expressed as a percentage and ranges from 0% \n    (perfect prediction) to 200% (maximum error).\n\n    Parameters\n    ----------\n    y_true : numpy ndarray, pandas Series\n        True values of the target variable.\n    y_pred : numpy ndarray, pandas Series\n        Predicted values of the target variable.\n\n    Returns\n    -------\n    smape : float\n        SMAPE value as a percentage.\n\n    Notes\n    -----\n    When both `y_true` and `y_pred` are zero, the corresponding term is treated as zero\n    to avoid division by zero.\n\n    Examples\n    --------\n    ```python\n    import numpy as np\n    from skforecast.metrics import symmetric_mean_absolute_percentage_error\n\n    y_true = np.array([100, 200, 0])\n    y_pred = np.array([110, 180, 10])\n    result = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n    print(f\"SMAPE: {result:.2f}%\")\n\n    # SMAPE: 73.35%\n    ```\n\n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    numerator = np.abs(y_true - y_pred)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n\n    # NOTE: Avoid division by zero\n    mask = denominator != 0\n    smape_values = np.zeros_like(denominator)\n    smape_values[mask] = numerator[mask] / denominator[mask]\n\n    smape = 100 * np.mean(smape_values)\n\n    return smape\n</code></pre>"},{"location":"api/metrics.html#skforecast.metrics.add_y_train_argument","title":"skforecast.metrics.add_y_train_argument","text":"<pre><code>add_y_train_argument(func)\n</code></pre> <p>Add <code>y_train</code> argument to a function if it is not already present.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>Function to which the argument is added.</p> required <p>Returns:</p> Name Type Description <code>wrapper</code> <code>callable</code> <p>Function with <code>y_train</code> argument added.</p> Source code in <code>skforecast\\metrics\\metrics.py</code> <pre><code>def add_y_train_argument(func: Callable) -&gt; Callable:\n    \"\"\"\n    Add `y_train` argument to a function if it is not already present.\n\n    Parameters\n    ----------\n    func : callable\n        Function to which the argument is added.\n\n    Returns\n    -------\n    wrapper : callable\n        Function with `y_train` argument added.\n\n    \"\"\"\n\n    sig = inspect.signature(func)\n\n    if \"y_train\" in sig.parameters:\n        return func\n\n    new_params = list(sig.parameters.values()) + [\n        inspect.Parameter(\"y_train\", inspect.Parameter.KEYWORD_ONLY, default=None)\n    ]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        return func(*args, **kwargs)\n\n    wrapper.__signature__ = new_sig\n\n    return wrapper\n</code></pre>"},{"location":"api/model_selection.html","title":"<code>model_selection</code>","text":""},{"location":"api/model_selection.html#skforecast.model_selection._validation.backtesting_forecaster","title":"skforecast.model_selection._validation.backtesting_forecaster","text":"<pre><code>backtesting_forecaster(\n    forecaster,\n    y,\n    cv,\n    metric,\n    exog=None,\n    interval=None,\n    interval_method=\"bootstrapping\",\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    return_predictors=False,\n    n_jobs=\"auto\",\n    verbose=False,\n    show_progress=True,\n)\n</code></pre> <p>Backtesting of forecaster model following the folds generated by the TimeSeriesFold class and using the metric(s) provided.</p> <p>If <code>forecaster</code> is already trained and <code>initial_train_size</code> is set to <code>None</code> in the TimeSeriesFold class, no initial train will be done and all data will be used to evaluate the model. However, the first <code>len(forecaster.last_window)</code> observations are needed to create the initial predictors, so no predictions are calculated for them.</p> <p>A copy of the original forecaster is created so that it is not modified during  the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursive, ForecasterDirect, ForecasterEquivalentDate)</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>interval</code> <code>(float, list, tuple, str, object)</code> <p>Specifies whether probabilistic predictions should be estimated and the  method to use. The following options are supported:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1).  For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles.</li> <li>If <code>list</code> or <code>tuple</code>: Sequence of percentiles to compute, each value must  be between 0 and 100 inclusive. For example, a 95% confidence interval can  be specified as <code>interval = [2.5, 97.5]</code> or multiple percentiles (e.g. 10,  50 and 90) as <code>interval = [10, 50, 90]</code>.</li> <li>If 'bootstrapping' (str): <code>n_boot</code> bootstrapping predictions will be generated.</li> <li>If scipy.stats distribution object, the distribution parameters will be estimated for each prediction.</li> <li>If None, no probabilistic predictions are estimated.</li> </ul> <code>None</code> <code>interval_method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction  intervals [1]_.</li> <li>'conformal': Employs the conformal prediction split method for  interval estimation [2]_.</li> </ul> <code>'bootstrapping'</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>return_predictors</code> <code>bool</code> <p>If <code>True</code>, the predictors used to make the predictions are also returned.</p> <code>False</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metric_values</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s).</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Value of predictions. The  DataFrame includes the following columns:</p> <ul> <li>pred: Predicted values for the corresponding series and time steps.</li> </ul> <p>If <code>interval</code> is not <code>None</code>, additional columns are included depending on the method:</p> <ul> <li>For <code>float</code>: Columns <code>lower_bound</code> and <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> of 2 elements: Columns <code>lower_bound</code> and <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> with multiple percentiles: One column per percentile  (e.g., <code>p_10</code>, <code>p_50</code>, <code>p_90</code>).</li> <li>For <code>'bootstrapping'</code>: One column per bootstrapping iteration  (e.g., <code>pred_boot_0</code>, <code>pred_boot_1</code>, ..., <code>pred_boot_n</code>).</li> <li>For <code>scipy.stats</code> distribution objects: One column for each estimated  parameter of the distribution (e.g., <code>loc</code>, <code>scale</code>).</li> </ul> <p>If <code>return_predictors</code> is <code>True</code>:</p> <ul> <li>fold: Indicates the fold number where the prediction was made.</li> <li>One column per predictor is created.</li> </ul> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> <p>.. [2] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\model_selection\\_validation.py</code> <pre><code>def backtesting_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    interval: float | list[float] | tuple[float] | str | object | None = None,\n    interval_method: str = 'bootstrapping',\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    return_predictors: bool = False,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n\n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursive, ForecasterDirect, ForecasterEquivalentDate\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    interval : float, list, tuple, str, object, default None\n        Specifies whether probabilistic predictions should be estimated and the \n        method to use. The following options are supported:\n\n        - If `float`, represents the nominal (expected) coverage (between 0 and 1). \n        For instance, `interval=0.95` corresponds to `[2.5, 97.5]` percentiles.\n        - If `list` or `tuple`: Sequence of percentiles to compute, each value must \n        be between 0 and 100 inclusive. For example, a 95% confidence interval can \n        be specified as `interval = [2.5, 97.5]` or multiple percentiles (e.g. 10, \n        50 and 90) as `interval = [10, 50, 90]`.\n        - If 'bootstrapping' (str): `n_boot` bootstrapping predictions will be generated.\n        - If scipy.stats distribution object, the distribution parameters will\n        be estimated for each prediction.\n        - If None, no probabilistic predictions are estimated.\n    interval_method : str, default 'bootstrapping'\n        Technique used to estimate prediction intervals. Available options:\n\n        - 'bootstrapping': Bootstrapping is used to generate prediction \n        intervals [1]_.\n        - 'conformal': Employs the conformal prediction split method for \n        interval estimation [2]_.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction\n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n    return_predictors : bool, default False\n        If `True`, the predictors used to make the predictions are also returned.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions. The  DataFrame includes the following columns:\n\n        - pred: Predicted values for the corresponding series and time steps.\n\n        If `interval` is not `None`, additional columns are included depending on the method:\n\n        - For `float`: Columns `lower_bound` and `upper_bound`.\n        - For `list` or `tuple` of 2 elements: Columns `lower_bound` and `upper_bound`.\n        - For `list` or `tuple` with multiple percentiles: One column per percentile \n        (e.g., `p_10`, `p_50`, `p_90`).\n        - For `'bootstrapping'`: One column per bootstrapping iteration \n        (e.g., `pred_boot_0`, `pred_boot_1`, ..., `pred_boot_n`).\n        - For `scipy.stats` distribution objects: One column for each estimated \n        parameter of the distribution (e.g., `loc`, `scale`).\n\n        If `return_predictors` is `True`:\n\n        - fold: Indicates the fold number where the prediction was made.\n        - One column per predictor is created.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    forecaters_allowed = [\n        'ForecasterRecursive', \n        'ForecasterDirect',\n        'ForecasterEquivalentDate'\n    ]\n\n    if type(forecaster).__name__ not in forecaters_allowed:\n        raise TypeError(\n            f\"`forecaster` must be of type {forecaters_allowed}, for all other types of \"\n            f\" forecasters use the functions available in the other `model_selection` \"\n            f\"modules.\"\n        )\n\n    check_backtesting_input(\n        forecaster              = forecaster,\n        cv                      = cv,\n        y                       = y,\n        metric                  = metric,\n        interval                = interval,\n        interval_method         = interval_method,\n        n_boot                  = n_boot,\n        use_in_sample_residuals = use_in_sample_residuals,\n        use_binned_residuals    = use_binned_residuals,\n        random_state            = random_state,\n        return_predictors       = return_predictors,\n        n_jobs                  = n_jobs,\n        show_progress           = show_progress\n    )\n\n    metric_values, backtest_predictions = _backtesting_forecaster(\n        forecaster              = forecaster,\n        y                       = y,\n        cv                      = cv,\n        metric                  = metric,\n        exog                    = exog,\n        interval                = interval,\n        interval_method         = interval_method,\n        n_boot                  = n_boot,\n        use_in_sample_residuals = use_in_sample_residuals,\n        use_binned_residuals    = use_binned_residuals,\n        random_state            = random_state,\n        return_predictors       = return_predictors,\n        n_jobs                  = n_jobs,\n        verbose                 = verbose,\n        show_progress           = show_progress\n    )\n\n    return metric_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.grid_search_forecaster","title":"skforecast.model_selection._search.grid_search_forecaster","text":"<pre><code>grid_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    param_grid,\n    metric,\n    exog=None,\n    lags_grid=None,\n    return_best=True,\n    n_jobs=\"auto\",\n    verbose=False,\n    show_progress=True,\n    output_file=None,\n)\n</code></pre> <p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursive, ForecasterDirect)</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>(TimeSeriesFold, OneStepAheadFold)</code> <p>TimeSeriesFold or OneStepAheadFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>None</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column lags_label: descriptive label or alias for the lags.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection\\_search.py</code> <pre><code>def grid_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    param_grid: dict,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n    return_best: bool = True,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    output_file: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursive, ForecasterDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold, OneStepAheadFold\n        TimeSeriesFold or OneStepAheadFold object with the information needed to split\n        the data into folds.\n        **New in version 0.14.0**\n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    lags_grid : list, dict, default None\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n    return_best : bool, default True\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    output_file : str, default None\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column lags: lags configuration for each iteration.\n        - column lags_label: descriptive label or alias for the lags.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters(\n                  forecaster    = forecaster,\n                  y             = y,\n                  cv            = cv,\n                  param_grid    = param_grid,\n                  metric        = metric,\n                  exog          = exog,\n                  lags_grid     = lags_grid,\n                  return_best   = return_best,\n                  n_jobs        = n_jobs,\n                  verbose       = verbose,\n                  show_progress = show_progress,\n                  output_file   = output_file\n              )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.random_search_forecaster","title":"skforecast.model_selection._search.random_search_forecaster","text":"<pre><code>random_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    param_distributions,\n    metric,\n    exog=None,\n    lags_grid=None,\n    n_iter=10,\n    random_state=123,\n    return_best=True,\n    n_jobs=\"auto\",\n    verbose=False,\n    show_progress=True,\n    output_file=None,\n)\n</code></pre> <p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursive, ForecasterDirect)</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>(TimeSeriesFold, OneStepAheadFold)</code> <p>TimeSeriesFold or OneStepAheadFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and  distributions or lists of parameters to try.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>None</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column lags_label: descriptive label or alias for the lags.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection\\_search.py</code> <pre><code>def random_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    param_distributions: dict,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n    n_iter: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    output_file: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursive, ForecasterDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold, OneStepAheadFold\n        TimeSeriesFold or OneStepAheadFold object with the information needed to split\n        the data into folds.\n        **New in version 0.14.0**\n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and \n        distributions or lists of parameters to try.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i]. \n    lags_grid : list, dict, default None\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n    n_iter : int, default 10\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default True\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    output_file : str, default None\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column lags: lags configuration for each iteration.\n        - column lags_label: descriptive label or alias for the lags.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters(\n                  forecaster    = forecaster,\n                  y             = y,\n                  cv            = cv,\n                  param_grid    = param_grid,\n                  metric        = metric,\n                  exog          = exog,\n                  lags_grid     = lags_grid,\n                  return_best   = return_best,\n                  n_jobs        = n_jobs,\n                  verbose       = verbose,\n                  show_progress = show_progress,\n                  output_file   = output_file\n              )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.bayesian_search_forecaster","title":"skforecast.model_selection._search.bayesian_search_forecaster","text":"<pre><code>bayesian_search_forecaster(\n    forecaster,\n    y,\n    cv,\n    search_space,\n    metric,\n    exog=None,\n    n_trials=10,\n    random_state=123,\n    return_best=True,\n    n_jobs=\"auto\",\n    verbose=False,\n    show_progress=True,\n    output_file=None,\n    kwargs_create_study={},\n    kwargs_study_optimize={},\n)\n</code></pre> <p>Bayesian search for hyperparameters of a Forecaster object.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursive, ForecasterDirect)</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>(TimeSeriesFold, OneStepAheadFold)</code> <p>TimeSeriesFold or OneStepAheadFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>search_space</code> <code>Callable(optuna)</code> <p>Function with argument <code>trial</code> which returns a dictionary with parameters names  (<code>str</code>) as keys and Trial object from optuna (trial.suggest_float,  trial.suggest_int, trial.suggest_categorical) as values.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>n_trials</code> <code>int</code> <p>Number of parameter settings that are sampled in each lag configuration.</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the sampling for reproducible output. When a new sampler  is passed in <code>kwargs_create_study</code>, the seed must be set within the  sampler. For example <code>{'sampler': TPESampler(seed=145)}</code>.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>None</code> <code>kwargs_create_study</code> <code>dict</code> <p>Additional keyword arguments (key, value mappings) to pass to optuna.create_study(). If default, the direction is set to 'minimize' and a TPESampler(seed=123)  sampler is used during optimization.</p> <code>{}</code> <code>kwargs_study_optimize</code> <code>dict</code> <p>Additional keyword arguments (key, value mappings) to pass to study.optimize().</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> <code>best_trial</code> <code>optuna object</code> <p>The best optimization result returned as a FrozenTrial optuna object.</p> Source code in <code>skforecast\\model_selection\\_search.py</code> <pre><code>def bayesian_search_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold | OneStepAheadFold,\n    search_space: Callable,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_trials: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    output_file: str | None = None,\n    kwargs_create_study: dict = {},\n    kwargs_study_optimize: dict = {}\n) -&gt; tuple[pd.DataFrame, object]:\n    \"\"\"\n    Bayesian search for hyperparameters of a Forecaster object.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursive, ForecasterDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold, OneStepAheadFold\n        TimeSeriesFold or OneStepAheadFold object with the information needed to split\n        the data into folds.\n        **New in version 0.14.0**\n    search_space : Callable (optuna)\n        Function with argument `trial` which returns a dictionary with parameters names \n        (`str`) as keys and Trial object from optuna (trial.suggest_float, \n        trial.suggest_int, trial.suggest_categorical) as values.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    n_trials : int, default 10\n        Number of parameter settings that are sampled in each lag configuration.\n    random_state : int, default 123\n        Sets a seed to the sampling for reproducible output. When a new sampler \n        is passed in `kwargs_create_study`, the seed must be set within the \n        sampler. For example `{'sampler': TPESampler(seed=145)}`.\n    return_best : bool, default True\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    output_file : str, default None\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n    kwargs_create_study : dict, default {}\n        Additional keyword arguments (key, value mappings) to pass to optuna.create_study().\n        If default, the direction is set to 'minimize' and a TPESampler(seed=123) \n        sampler is used during optimization.\n    kwargs_study_optimize : dict, default {}\n        Additional keyword arguments (key, value mappings) to pass to study.optimize().\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column lags: lags configuration for each iteration.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n    best_trial : optuna object\n        The best optimization result returned as a FrozenTrial optuna object.\n\n    \"\"\"\n\n    if return_best and exog is not None and (len(exog) != len(y)):\n        raise ValueError(\n            f\"`exog` must have same number of samples as `y`. \"\n            f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\"\n        )\n\n    results, best_trial = _bayesian_search_optuna(\n                              forecaster            = forecaster,\n                              y                     = y,\n                              cv                    = cv,\n                              exog                  = exog,\n                              search_space          = search_space,\n                              metric                = metric,\n                              n_trials              = n_trials,\n                              random_state          = random_state,\n                              return_best           = return_best,\n                              n_jobs                = n_jobs,\n                              verbose               = verbose,\n                              show_progress         = show_progress,\n                              output_file           = output_file,\n                              kwargs_create_study   = kwargs_create_study,\n                              kwargs_study_optimize = kwargs_study_optimize\n                          )\n\n    return results, best_trial\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._validation.backtesting_forecaster_multiseries","title":"skforecast.model_selection._validation.backtesting_forecaster_multiseries","text":"<pre><code>backtesting_forecaster_multiseries(\n    forecaster,\n    series,\n    cv,\n    metric,\n    levels=None,\n    add_aggregated_metric=True,\n    exog=None,\n    interval=None,\n    interval_method=\"conformal\",\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    return_predictors=False,\n    n_jobs=\"auto\",\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n)\n</code></pre> <p>Backtesting of forecaster model following the folds generated by the TimeSeriesFold class and using the metric(s) provided.</p> <p>If <code>forecaster</code> is already trained and <code>initial_train_size</code> is set to <code>None</code> in the TimeSeriesFold class, no initial train will be done and all data will be used to evaluate the model. However, the first <code>len(forecaster.last_window)</code> observations are needed to create the initial predictors, so no predictions are calculated for them.</p> <p>A copy of the original forecaster is created so that it is not modified during  the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn)</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted. If <code>None</code> all levels will be predicted.</p> <code>None</code> <code>add_aggregated_metric</code> <code>bool</code> <p>If <code>True</code>, and multiple series (<code>levels</code>) are predicted, the aggregated metrics (average, weighted average and pooled) are also returned.</p> <ul> <li>'average': the average (arithmetic mean) of all levels.</li> <li>'weighted_average': the average of the metrics weighted by the number of predicted values of each level.</li> <li>'pooling': the values of all levels are pooled and then the metric is calculated.</li> </ul> <code>True</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>None</code> <code>interval</code> <code>(float, list, tuple, str, object)</code> <p>Specifies whether probabilistic predictions should be estimated and the  method to use. The following options are supported:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1).  For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles.</li> <li>If <code>list</code> or <code>tuple</code>: Sequence of percentiles to compute, each value must  be between 0 and 100 inclusive. For example, a 95% confidence interval can  be specified as <code>interval = [2.5, 97.5]</code> or multiple percentiles (e.g. 10,  50 and 90) as <code>interval = [10, 50, 90]</code>.</li> <li>If 'bootstrapping' (str): <code>n_boot</code> bootstrapping predictions will be generated.</li> <li>If scipy.stats distribution object, the distribution parameters will be estimated for each prediction.</li> <li>If None, no probabilistic predictions are estimated.</li> </ul> <code>None</code> <code>interval_method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction  intervals [1]_.</li> <li>'conformal': Employs the conformal prediction split method for  interval estimation [2]_.</li> </ul> <code>'conformal'</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction  intervals.</p> <code>250</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction error to create predictions.  If <code>False</code>, out of sample residuals (calibration) are used.  Out-of-sample residuals must be precomputed using Forecaster's <code>set_out_sample_residuals()</code> method.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>123</code> <code>return_predictors</code> <code>bool</code> <p>If <code>True</code>, the predictors used to make the predictions are also returned.</p> <code>False</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the backtesting  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>metrics_levels</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s). Index are the levels and columns the metrics.</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Long-format DataFrame containing the predicted values for each series. The  DataFrame includes the following columns:</p> <ul> <li><code>level</code>: Identifier for the time series or level being predicted.</li> <li><code>pred</code>: Predicted values for the corresponding series and time steps.</li> </ul> <p>If <code>interval</code> is not <code>None</code>, additional columns are included depending on the method:</p> <ul> <li>For <code>float</code>: Columns <code>lower_bound</code> and <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> of 2 elements: Columns <code>lower_bound</code> and <code>upper_bound</code>.</li> <li>For <code>list</code> or <code>tuple</code> with multiple percentiles: One column per percentile  (e.g., <code>p_10</code>, <code>p_50</code>, <code>p_90</code>).</li> <li>For <code>'bootstrapping'</code>: One column per bootstrapping iteration  (e.g., <code>pred_boot_0</code>, <code>pred_boot_1</code>, ..., <code>pred_boot_n</code>).</li> <li>For <code>scipy.stats</code> distribution objects: One column for each estimated  parameter of the distribution (e.g., <code>loc</code>, <code>scale</code>).</li> </ul> <p>If <code>return_predictors</code> is <code>True</code>:</p> <ul> <li>fold: Indicates the fold number where the prediction was made.</li> <li>One column per predictor is created.</li> </ul> References <p>.. [1] Forecasting: Principles and Practice (3<sup>rd</sup> ed) Rob J Hyndman and George Athanasopoulos.        https://otexts.com/fpp3/prediction-intervals.html</p> <p>.. [2] MAPIE - Model Agnostic Prediction Interval Estimator.        https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method</p> Source code in <code>skforecast\\model_selection\\_validation.py</code> <pre><code>def backtesting_forecaster_multiseries(\n    forecaster: object,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n    cv: TimeSeriesFold,\n    metric: str | Callable | list[str | Callable],\n    levels: str | list[str] | None = None,\n    add_aggregated_metric: bool = True,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    interval: float | list[float] | tuple[float] | str | object | None = None,\n    interval_method: str = 'conformal',\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    return_predictors: bool = False,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n\n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    levels : str, list, default None\n        Time series to be predicted. If `None` all levels will be predicted.\n    add_aggregated_metric : bool, default True\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variables.\n    interval : float, list, tuple, str, object, default None\n        Specifies whether probabilistic predictions should be estimated and the \n        method to use. The following options are supported:\n\n        - If `float`, represents the nominal (expected) coverage (between 0 and 1). \n        For instance, `interval=0.95` corresponds to `[2.5, 97.5]` percentiles.\n        - If `list` or `tuple`: Sequence of percentiles to compute, each value must \n        be between 0 and 100 inclusive. For example, a 95% confidence interval can \n        be specified as `interval = [2.5, 97.5]` or multiple percentiles (e.g. 10, \n        50 and 90) as `interval = [10, 50, 90]`.\n        - If 'bootstrapping' (str): `n_boot` bootstrapping predictions will be generated.\n        - If scipy.stats distribution object, the distribution parameters will\n        be estimated for each prediction.\n        - If None, no probabilistic predictions are estimated.\n    interval_method : str, default 'conformal'\n        Technique used to estimate prediction intervals. Available options:\n\n        - 'bootstrapping': Bootstrapping is used to generate prediction \n        intervals [1]_.\n        - 'conformal': Employs the conformal prediction split method for \n        interval estimation [2]_.\n    n_boot : int, default 250\n        Number of bootstrapping iterations to perform when estimating prediction \n        intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of\n        prediction error to create predictions. \n        If `False`, out of sample residuals (calibration) are used. \n        Out-of-sample residuals must be precomputed using Forecaster's\n        `set_out_sample_residuals()` method.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default 123\n        Seed for the random number generator to ensure reproducibility.\n    return_predictors : bool, default False\n        If `True`, the predictors used to make the predictions are also returned.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    suppress_warnings: bool, default False\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas DataFrame\n        Long-format DataFrame containing the predicted values for each series. The \n        DataFrame includes the following columns:\n\n        - `level`: Identifier for the time series or level being predicted.\n        - `pred`: Predicted values for the corresponding series and time steps.\n\n        If `interval` is not `None`, additional columns are included depending on the method:\n\n        - For `float`: Columns `lower_bound` and `upper_bound`.\n        - For `list` or `tuple` of 2 elements: Columns `lower_bound` and `upper_bound`.\n        - For `list` or `tuple` with multiple percentiles: One column per percentile \n        (e.g., `p_10`, `p_50`, `p_90`).\n        - For `'bootstrapping'`: One column per bootstrapping iteration \n        (e.g., `pred_boot_0`, `pred_boot_1`, ..., `pred_boot_n`).\n        - For `scipy.stats` distribution objects: One column for each estimated \n        parameter of the distribution (e.g., `loc`, `scale`).\n\n        If `return_predictors` is `True`:\n\n        - fold: Indicates the fold number where the prediction was made.\n        - One column per predictor is created.\n\n    References\n    ----------\n    .. [1] Forecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos.\n           https://otexts.com/fpp3/prediction-intervals.html\n\n    .. [2] MAPIE - Model Agnostic Prediction Interval Estimator.\n           https://mapie.readthedocs.io/en/stable/theoretical_description_regression.html#the-split-method\n\n    \"\"\"\n\n    multi_series_forecasters = [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterDirectMultiVariate',\n        'ForecasterRnn'\n    ]\n\n    forecaster_name = type(forecaster).__name__\n\n    if forecaster_name not in multi_series_forecasters:\n        raise TypeError(\n            f\"`forecaster` must be of type {multi_series_forecasters}, \"\n            f\"for all other types of forecasters use the functions available in \"\n            f\"the `model_selection` module. Got {forecaster_name}\"\n        )\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    if forecaster_name == 'ForecasterRecursiveMultiSeries':\n        series, series_indexes = check_preprocess_series(series)\n        if exog is not None:\n            series_names_in_ = list(series.keys())\n            exog_dict = {serie: None for serie in series_names_in_}\n            exog, _ = check_preprocess_exog_multiseries(\n                          series_names_in_  = series_names_in_,\n                          series_index_type = type(series_indexes[series_names_in_[0]]),\n                          exog              = exog,\n                          exog_dict         = exog_dict\n                      )\n\n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    check_backtesting_input(\n        forecaster              = forecaster,\n        cv                      = cv,\n        metric                  = metric,\n        add_aggregated_metric   = add_aggregated_metric,\n        series                  = series,\n        exog                    = exog,\n        interval                = interval,\n        interval_method         = interval_method,\n        n_boot                  = n_boot,\n        use_in_sample_residuals = use_in_sample_residuals,\n        use_binned_residuals    = use_binned_residuals,\n        random_state            = random_state,\n        return_predictors       = return_predictors,\n        n_jobs                  = n_jobs,\n        show_progress           = show_progress,\n        suppress_warnings       = suppress_warnings\n    )\n\n    metrics_levels, backtest_predictions = _backtesting_forecaster_multiseries(\n        forecaster              = forecaster,\n        series                  = series,\n        cv                      = cv,\n        levels                  = levels,\n        metric                  = metric,\n        add_aggregated_metric   = add_aggregated_metric,\n        exog                    = exog,\n        interval                = interval,\n        interval_method         = interval_method,\n        n_boot                  = n_boot,\n        use_in_sample_residuals = use_in_sample_residuals,\n        use_binned_residuals    = use_binned_residuals,\n        random_state            = random_state,\n        return_predictors       = return_predictors,\n        n_jobs                  = n_jobs,\n        verbose                 = verbose,\n        show_progress           = show_progress,\n        suppress_warnings       = suppress_warnings\n    )\n\n    return metrics_levels, backtest_predictions\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.grid_search_forecaster_multiseries","title":"skforecast.model_selection._search.grid_search_forecaster_multiseries","text":"<pre><code>grid_search_forecaster_multiseries(\n    forecaster,\n    series,\n    cv,\n    param_grid,\n    metric,\n    aggregate_metric=[\n        \"weighted_average\",\n        \"average\",\n        \"pooling\",\n    ],\n    levels=None,\n    exog=None,\n    lags_grid=None,\n    return_best=True,\n    n_jobs=\"auto\",\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n)\n</code></pre> <p>Exhaustive search over specified parameter values for a Forecaster object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate)</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>cv</code> <code>(TimeSeriesFold, OneStepAheadFold)</code> <p>TimeSeriesFold or OneStepAheadFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>aggregate_metric</code> <code>(str, list)</code> <p>Aggregation method/s used to combine the metric/s of all levels (series) when multiple levels are predicted. If list, the first aggregation method is used to select the best parameters.</p> <ul> <li>'average': the average (arithmetic mean) of all levels.</li> <li>'weighted_average': the average of the metrics weighted by the number of predicted values of each level.</li> <li>'pooling': the values of all levels are pooled and then the metric is calculated.</li> </ul> <code>`['weighted_average', 'average', 'pooling']`</code> <code>levels</code> <code>(str, list)</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>None</code> <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>None</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the hyperparameter  search. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column lags_label: descriptive label or alias for the lags.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection\\_search.py</code> <pre><code>def grid_search_forecaster_multiseries(\n    forecaster: object,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n    cv: TimeSeriesFold | OneStepAheadFold,\n    param_grid: dict,\n    metric: str | Callable | list[str | Callable],\n    aggregate_metric: str | list[str] = ['weighted_average', 'average', 'pooling'],\n    levels: str | list[str] | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n    return_best: bool = True,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Exhaustive search over specified parameter values for a Forecaster object.\n    Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    cv : TimeSeriesFold, OneStepAheadFold\n        TimeSeriesFold or OneStepAheadFold object with the information needed to split\n        the data into folds.\n        **New in version 0.14.0**\n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    aggregate_metric : str, list, default `['weighted_average', 'average', 'pooling']`\n        Aggregation method/s used to combine the metric/s of all levels (series)\n        when multiple levels are predicted. If list, the first aggregation method\n        is used to select the best parameters.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    levels : str, list, default None\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variables.\n    lags_grid : list, dict, default None\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n    return_best : bool, default True\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    suppress_warnings: bool, default False\n        If `True`, skforecast warnings will be suppressed during the hyperparameter \n        search. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    output_file : str, default None\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column levels: levels configuration for each iteration.\n        - column lags: lags configuration for each iteration.\n        - column lags_label: descriptive label or alias for the lags.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration. The resulting \n        metric will be the average of the optimization of all levels.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters_multiseries(\n                  forecaster        = forecaster,\n                  series            = series,\n                  cv                = cv,\n                  param_grid        = param_grid,\n                  metric            = metric,\n                  aggregate_metric  = aggregate_metric,\n                  levels            = levels,\n                  exog              = exog,\n                  lags_grid         = lags_grid,\n                  n_jobs            = n_jobs,\n                  return_best       = return_best,\n                  verbose           = verbose,\n                  show_progress     = show_progress,\n                  suppress_warnings = suppress_warnings,\n                  output_file       = output_file\n              )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.random_search_forecaster_multiseries","title":"skforecast.model_selection._search.random_search_forecaster_multiseries","text":"<pre><code>random_search_forecaster_multiseries(\n    forecaster,\n    series,\n    cv,\n    param_distributions,\n    metric,\n    aggregate_metric=[\n        \"weighted_average\",\n        \"average\",\n        \"pooling\",\n    ],\n    levels=None,\n    exog=None,\n    lags_grid=None,\n    n_iter=10,\n    random_state=123,\n    return_best=True,\n    n_jobs=\"auto\",\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n)\n</code></pre> <p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using multi-series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate)</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>cv</code> <code>(TimeSeriesFold, OneStepAheadFold)</code> <p>TimeSeriesFold or OneStepAheadFold object with the information needed to split the data into folds.</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and distributions or  lists of parameters to try.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>aggregate_metric</code> <code>(str, list)</code> <p>Aggregation method/s used to combine the metric/s of all levels (series) when multiple levels are predicted. If list, the first aggregation method is used to select the best parameters.</p> <ul> <li>'average': the average (arithmetic mean) of all levels.</li> <li>'weighted_average': the average of the metrics weighted by the number of predicted values of each level.</li> <li>'pooling': the values of all levels are pooled and then the metric is calculated.</li> </ul> <code>`['weighted_average', 'average', 'pooling']`</code> <code>levels</code> <code>(str, list)</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>None</code> <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>None</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled per lags configuration.  n_iter trades off runtime vs quality of the solution.</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the hyperparameter  search. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column lags_label: descriptive label or alias for the lags.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection\\_search.py</code> <pre><code>def random_search_forecaster_multiseries(\n    forecaster: object,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n    cv: TimeSeriesFold | OneStepAheadFold,\n    param_distributions: dict,\n    metric: str | Callable | list[str | Callable],\n    aggregate_metric: str | list[str] = ['weighted_average', 'average', 'pooling'],\n    levels: str | list[str] | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n    n_iter: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using multi-series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    cv : TimeSeriesFold, OneStepAheadFold\n        TimeSeriesFold or OneStepAheadFold object with the information needed to split\n        the data into folds.\n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and distributions or \n        lists of parameters to try.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    aggregate_metric : str, list, default `['weighted_average', 'average', 'pooling']`\n        Aggregation method/s used to combine the metric/s of all levels (series)\n        when multiple levels are predicted. If list, the first aggregation method\n        is used to select the best parameters.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    levels : str, list, default None\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variables.\n    lags_grid : list, dict, default None\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n    n_iter : int, default 10\n        Number of parameter settings that are sampled per lags configuration. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default True\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    suppress_warnings: bool, default False\n        If `True`, skforecast warnings will be suppressed during the hyperparameter \n        search. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    output_file : str, default None\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column levels: levels configuration for each iteration.\n        - column lags: lags configuration for each iteration.\n        - column lags_label: descriptive label or alias for the lags.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration. The resulting \n        metric will be the average of the optimization of all levels.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(\n        ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state)\n    )\n\n    results = _evaluate_grid_hyperparameters_multiseries(\n                  forecaster        = forecaster,\n                  series            = series,\n                  cv                = cv,\n                  param_grid        = param_grid,\n                  metric            = metric,\n                  aggregate_metric  = aggregate_metric,\n                  levels            = levels,\n                  exog              = exog,\n                  lags_grid         = lags_grid,\n                  return_best       = return_best,\n                  n_jobs            = n_jobs,\n                  verbose           = verbose,\n                  show_progress     = show_progress,\n                  suppress_warnings = suppress_warnings,\n                  output_file       = output_file\n              )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.bayesian_search_forecaster_multiseries","title":"skforecast.model_selection._search.bayesian_search_forecaster_multiseries","text":"<pre><code>bayesian_search_forecaster_multiseries(\n    forecaster,\n    series,\n    cv,\n    search_space,\n    metric,\n    aggregate_metric=[\n        \"weighted_average\",\n        \"average\",\n        \"pooling\",\n    ],\n    levels=None,\n    exog=None,\n    n_trials=10,\n    random_state=123,\n    return_best=True,\n    n_jobs=\"auto\",\n    verbose=False,\n    show_progress=True,\n    suppress_warnings=False,\n    output_file=None,\n    kwargs_create_study={},\n    kwargs_study_optimize={},\n)\n</code></pre> <p>Bayesian search for hyperparameters of a Forecaster object using optuna library.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>(ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate)</code> <p>Forecaster model.</p> required <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <code>search_space</code> <code>Callable</code> <p>Function with argument <code>trial</code> which returns a dictionary with parameters names  (<code>str</code>) as keys and Trial object from optuna (trial.suggest_float,  trial.suggest_int, trial.suggest_categorical) as values.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>aggregate_metric</code> <code>(str, list)</code> <p>Aggregation method/s used to combine the metric/s of all levels (series) when multiple levels are predicted. If list, the first aggregation method is used to select the best parameters.</p> <ul> <li>'average': the average (arithmetic mean) of all levels.</li> <li>'weighted_average': the average of the metrics weighted by the number of predicted values of each level.</li> <li>'pooling': the values of all levels are pooled and then the metric is calculated.</li> </ul> <code>`['weighted_average', 'average', 'pooling']`</code> <code>levels</code> <code>(str, list)</code> <p>level (<code>str</code>) or levels (<code>list</code>) at which the forecaster is optimized.  If <code>None</code>, all levels are taken into account.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>None</code> <code>n_trials</code> <code>int</code> <p>Number of parameter settings that are sampled in each lag configuration.</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the sampling for reproducible output.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the hyperparameter search. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>None</code> <code>kwargs_create_study</code> <code>dict</code> <p>Additional keyword arguments (key, value mappings) to pass to optuna.create_study(). If default, the direction is set to 'minimize' and a TPESampler(seed=123)  sampler is used during optimization.</p> <code>{}</code> <code>kwargs_study_optimize</code> <code>dict</code> <p>Additional keyword arguments (key, value mappings) to pass to study.optimize().</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column levels: levels configuration for each iteration.</li> <li>column lags: lags configuration for each iteration.</li> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration. The resulting  metric will be the average of the optimization of all levels.</li> <li>additional n columns with param = value.</li> </ul> <code>best_trial</code> <code>optuna object</code> <p>The best optimization result returned as a FrozenTrial optuna object.</p> Source code in <code>skforecast\\model_selection\\_search.py</code> <pre><code>def bayesian_search_forecaster_multiseries(\n    forecaster: object,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n    cv: TimeSeriesFold | OneStepAheadFold,\n    search_space: Callable,\n    metric: str | Callable | list[str | Callable],\n    aggregate_metric: str | list[str] = ['weighted_average', 'average', 'pooling'],\n    levels: str | list[str] | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    n_trials: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    output_file: str | None = None,\n    kwargs_create_study: dict = {},\n    kwargs_study_optimize: dict = {}\n) -&gt; tuple[pd.DataFrame, object]:\n    \"\"\"\n    Bayesian search for hyperparameters of a Forecaster object using optuna library.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    search_space : Callable\n        Function with argument `trial` which returns a dictionary with parameters names \n        (`str`) as keys and Trial object from optuna (trial.suggest_float, \n        trial.suggest_int, trial.suggest_categorical) as values.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    aggregate_metric : str, list, default `['weighted_average', 'average', 'pooling']`\n        Aggregation method/s used to combine the metric/s of all levels (series)\n        when multiple levels are predicted. If list, the first aggregation method\n        is used to select the best parameters.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    levels : str, list, default None\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variables.\n    n_trials : int, default 10\n        Number of parameter settings that are sampled in each lag configuration.\n    random_state : int, default 123\n        Sets a seed to the sampling for reproducible output.\n    return_best : bool, default True\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds used for cv or backtesting.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    suppress_warnings: bool, default False\n        If `True`, skforecast warnings will be suppressed during the hyperparameter\n        search. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    output_file : str, default None\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n    kwargs_create_study : dict, default {}\n        Additional keyword arguments (key, value mappings) to pass to optuna.create_study().\n        If default, the direction is set to 'minimize' and a TPESampler(seed=123) \n        sampler is used during optimization.\n    kwargs_study_optimize : dict, default {}\n        Additional keyword arguments (key, value mappings) to pass to study.optimize().\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column levels: levels configuration for each iteration.\n        - column lags: lags configuration for each iteration.\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration. The resulting \n        metric will be the average of the optimization of all levels.\n        - additional n columns with param = value.\n    best_trial : optuna object\n        The best optimization result returned as a FrozenTrial optuna object.\n\n    \"\"\"\n\n    results, best_trial = _bayesian_search_optuna_multiseries(\n                              forecaster            = forecaster,\n                              series                = series,\n                              cv                    = cv,\n                              exog                  = exog,\n                              levels                = levels, \n                              search_space          = search_space,\n                              metric                = metric,\n                              aggregate_metric      = aggregate_metric,\n                              n_trials              = n_trials,\n                              random_state          = random_state,\n                              return_best           = return_best,\n                              n_jobs                = n_jobs,\n                              verbose               = verbose,\n                              show_progress         = show_progress,\n                              suppress_warnings     = suppress_warnings,\n                              output_file           = output_file,\n                              kwargs_create_study   = kwargs_create_study,\n                              kwargs_study_optimize = kwargs_study_optimize\n                          )\n\n    return results, best_trial\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._validation.backtesting_sarimax","title":"skforecast.model_selection._validation.backtesting_sarimax","text":"<pre><code>backtesting_sarimax(\n    forecaster,\n    y,\n    cv,\n    metric,\n    exog=None,\n    alpha=None,\n    interval=None,\n    n_jobs=\"auto\",\n    verbose=False,\n    suppress_warnings_fit=False,\n    show_progress=True,\n)\n</code></pre> <p>Backtesting of ForecasterSarimax.</p> <p>A copy of the original forecaster is created so that it is not modified during  the process.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>alpha</code> <code>float</code> <p>The confidence intervals for the forecasts are (1 - alpha) %. If both, <code>alpha</code> and <code>interval</code> are provided, <code>alpha</code> will be used.</p> <code>0.05</code> <code>interval</code> <code>(list, tuple)</code> <p>Confidence of the prediction interval estimated. The values must be symmetric. Sequence of percentiles to compute, which must be between  0 and 100 inclusive. For example, interval of 95% should be as  <code>interval = [2.5, 97.5]</code>. If both, <code>alpha</code> and <code>interval</code> are  provided, <code>alpha</code> will be used.</p> <code>None</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds and index of training and validation sets used  for backtesting.</p> <code>False</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>metric_values</code> <code>pandas DataFrame</code> <p>Value(s) of the metric(s).</p> <code>backtest_predictions</code> <code>pandas DataFrame</code> <p>Predicted values and their estimated interval if <code>interval</code> is not <code>None</code>.</p> <ul> <li>column pred: predictions.</li> <li>column lower_bound: lower bound of the interval.</li> <li>column upper_bound: upper bound of the interval.</li> </ul> Source code in <code>skforecast\\model_selection\\_validation.py</code> <pre><code>def backtesting_sarimax(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    alpha: float | None = None,\n    interval: list[float] | tuple[float] | None = None,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    suppress_warnings_fit: bool = False,\n    show_progress: bool = True\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of ForecasterSarimax.\n\n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    alpha : float, default 0.05\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, tuple, default None\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting. \n    verbose : bool, default False\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    suppress_warnings_fit : bool, default False\n        If `True`, warnings generated during fitting will be ignored.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Predicted values and their estimated interval if `interval` is not `None`.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n\n    \"\"\"\n\n    if type(forecaster).__name__ not in ['ForecasterSarimax']:\n        raise TypeError(\n            \"`forecaster` must be of type `ForecasterSarimax`, for all other \"\n            \"types of forecasters use the functions available in the other \"\n            \"`model_selection` modules.\"\n        )\n\n    check_backtesting_input(\n        forecaster            = forecaster,\n        cv                    = cv,\n        y                     = y,\n        metric                = metric,\n        interval              = interval,\n        alpha                 = alpha,\n        n_jobs                = n_jobs,\n        show_progress         = show_progress,\n        suppress_warnings_fit = suppress_warnings_fit\n    )\n\n    metric_values, backtest_predictions = _backtesting_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        cv                    = cv,\n        metric                = metric,\n        exog                  = exog,\n        alpha                 = alpha,\n        interval              = interval,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress\n    )\n\n    return metric_values, backtest_predictions\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.grid_search_sarimax","title":"skforecast.model_selection._search.grid_search_sarimax","text":"<pre><code>grid_search_sarimax(\n    forecaster,\n    y,\n    cv,\n    param_grid,\n    metric,\n    exog=None,\n    return_best=True,\n    n_jobs=\"auto\",\n    verbose=False,\n    suppress_warnings_fit=False,\n    show_progress=True,\n    output_file=None,\n)\n</code></pre> <p>Exhaustive search over specified parameter values for a ForecasterSarimax object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and lists of parameter settings to try as values.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>False</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection\\_search.py</code> <pre><code>def grid_search_sarimax(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    param_grid: dict,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    return_best: bool = True,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    suppress_warnings_fit: bool = False,\n    show_progress: bool = True,\n    output_file: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Exhaustive search over specified parameter values for a ForecasterSarimax object.\n    Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series. \n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    param_grid : dict\n        Dictionary with parameters names (`str`) as keys and lists of parameter\n        settings to try as values.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    return_best : bool, default True\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds used for cv or backtesting.\n    suppress_warnings_fit : bool, default False\n        If `True`, warnings generated during fitting will be ignored.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    output_file : str, default None\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterGrid(param_grid))\n\n    results = _evaluate_grid_hyperparameters_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        cv                    = cv,\n        param_grid            = param_grid,\n        metric                = metric,\n        exog                  = exog,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress,\n        output_file           = output_file\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._search.random_search_sarimax","title":"skforecast.model_selection._search.random_search_sarimax","text":"<pre><code>random_search_sarimax(\n    forecaster,\n    y,\n    cv,\n    param_distributions,\n    metric,\n    exog=None,\n    n_iter=10,\n    random_state=123,\n    return_best=True,\n    n_jobs=\"auto\",\n    verbose=False,\n    suppress_warnings_fit=False,\n    show_progress=True,\n    output_file=None,\n)\n</code></pre> <p>Random search over specified parameter values or distributions for a Forecaster  object. Validation is done using time series backtesting.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>ForecasterSarimax</code> <p>Forecaster model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds. New in version 0.14.0</p> required <code>param_distributions</code> <code>dict</code> <p>Dictionary with parameters names (<code>str</code>) as keys and  distributions or lists of parameters to try.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> <ul> <li>If <code>string</code>: {'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}</li> <li>If <code>Callable</code>: Function with arguments <code>y_true</code>, <code>y_pred</code> and <code>y_train</code> (Optional) that returns a float.</li> <li>If <code>list</code>: List containing multiple strings and/or Callables.</li> </ul> required <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s. Must have the same number of observations as <code>y</code> and should be aligned so that y[i] is regressed on exog[i].</p> <code>None</code> <code>n_iter</code> <code>int</code> <p>Number of parameter settings that are sampled.  n_iter trades off runtime vs quality of the solution.</p> <code>10</code> <code>random_state</code> <code>int</code> <p>Sets a seed to the random sampling for reproducible output.</p> <code>123</code> <code>return_best</code> <code>bool</code> <p>Refit the <code>forecaster</code> using the best found parameters on the whole data.</p> <code>True</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_backtesting.</p> <code>'auto'</code> <code>verbose</code> <code>bool</code> <p>Print number of folds used for cv or backtesting.</p> <code>False</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>output_file</code> <code>str</code> <p>Specifies the filename or full path where the results should be saved.  The results will be saved in a tab-separated values (TSV) format. If  <code>None</code>, the results will not be saved to a file. New in version 0.12.0</p> <code>None</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Results for each combination of parameters.</p> <ul> <li>column params: parameters configuration for each iteration.</li> <li>column metric: metric value estimated for each iteration.</li> <li>additional n columns with param = value.</li> </ul> Source code in <code>skforecast\\model_selection\\_search.py</code> <pre><code>def random_search_sarimax(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    param_distributions: dict,\n    metric: str | Callable | list[str | Callable],\n    exog: pd.Series | pd.DataFrame | None = None,\n    n_iter: int = 10,\n    random_state: int = 123,\n    return_best: bool = True,\n    n_jobs: int | str = 'auto',\n    verbose: bool = False,\n    suppress_warnings_fit: bool = False,\n    show_progress: bool = True,\n    output_file: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Random search over specified parameter values or distributions for a Forecaster \n    object. Validation is done using time series backtesting.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series. \n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    param_distributions : dict\n        Dictionary with parameters names (`str`) as keys and \n        distributions or lists of parameters to try.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default None\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    n_iter : int, default 10\n        Number of parameter settings that are sampled. \n        n_iter trades off runtime vs quality of the solution.\n    random_state : int, default 123\n        Sets a seed to the random sampling for reproducible output.\n    return_best : bool, default True\n        Refit the `forecaster` using the best found parameters on the whole data.\n    n_jobs : int, 'auto', default 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default False\n        Print number of folds used for cv or backtesting.\n    suppress_warnings_fit : bool, default False\n        If `True`, warnings generated during fitting will be ignored.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    output_file : str, default None\n        Specifies the filename or full path where the results should be saved. \n        The results will be saved in a tab-separated values (TSV) format. If \n        `None`, the results will not be saved to a file.\n        **New in version 0.12.0**\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Results for each combination of parameters.\n\n        - column params: parameters configuration for each iteration.\n        - column metric: metric value estimated for each iteration.\n        - additional n columns with param = value.\n\n    \"\"\"\n\n    param_grid = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state))\n\n    results = _evaluate_grid_hyperparameters_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        cv                    = cv,\n        param_grid            = param_grid,\n        metric                = metric,\n        exog                  = exog,\n        return_best           = return_best,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress,\n        output_file           = output_file\n    )\n\n    return results\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold","title":"skforecast.model_selection._split.BaseFold","text":"<pre><code>BaseFold(\n    steps=None,\n    initial_train_size=None,\n    window_size=None,\n    differentiation=None,\n    refit=False,\n    fixed_train_size=True,\n    gap=0,\n    skip_folds=None,\n    allow_incomplete_fold=True,\n    return_all_indexes=False,\n    verbose=True,\n)\n</code></pre> <p>Base class for all Fold classes in skforecast. All fold classes should specify all the parameters that can be set at the class level in their <code>__init__</code>.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> <code>None</code> <code>initial_train_size</code> <code>int, str, pandas Timestamp</code> <p>Number of observations used for initial training.</p> <ul> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in  the initial training set.</li> </ul> <code>None</code> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>None</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to refit the forecaster in each fold.</p> <ul> <li>If <code>True</code>, the forecaster is refitted in each fold.</li> <li>If <code>False</code>, the forecaster is trained only in the first fold.</li> <li>If an integer, the forecaster is trained in the first fold and then refitted   every <code>refit</code> folds.</li> </ul> <code>False</code> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>True</code> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>0</code> <code>skip_folds</code> <code>(int, list)</code> <p>Number of folds to skip.</p> <ul> <li>If an integer, every 'skip_folds'-th is returned.</li> <li>If a list, the indexes of the folds to skip.</li> </ul> <p>For example, if <code>skip_folds=3</code> and there are 10 folds, the returned folds are 0, 3, 6, and 9. If <code>skip_folds=[1, 2, 3]</code>, the returned folds are 0, 4, 5, 6, 7, 8, and 9.</p> <code>None</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>. If <code>False</code>, the last fold is excluded if it is incomplete.</p> <code>True</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>refit</code> <code>(bool, int)</code> <p>Whether to refit the forecaster in each fold.</p> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>skip_folds</code> <code>(int, list)</code> <p>Number of folds to skip.</p> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> <p>Methods:</p> Name Description <code>set_params</code> <p>Set the parameters of the Fold object. Before overwriting the current </p> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def __init__(\n    self,\n    steps: int | None = None,\n    initial_train_size: int | str | pd.Timestamp | None = None,\n    window_size: int | None = None,\n    differentiation: int | None = None,\n    refit: bool | int = False,\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: int | list[int] | None = None,\n    allow_incomplete_fold: bool = True,\n    return_all_indexes: bool = False,\n    verbose: bool = True\n) -&gt; None:\n\n    self._validate_params(\n        cv_name               = type(self).__name__,\n        steps                 = steps,\n        initial_train_size    = initial_train_size,\n        window_size           = window_size,\n        differentiation       = differentiation,\n        refit                 = refit,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        skip_folds            = skip_folds,\n        allow_incomplete_fold = allow_incomplete_fold,\n        return_all_indexes    = return_all_indexes,\n        verbose               = verbose\n    )\n\n    self.steps                 = steps\n    self.initial_train_size    = initial_train_size\n    self.window_size           = window_size\n    self.differentiation       = differentiation\n    self.refit                 = refit\n    self.fixed_train_size      = fixed_train_size\n    self.gap                   = gap\n    self.skip_folds            = skip_folds\n    self.allow_incomplete_fold = allow_incomplete_fold\n    self.return_all_indexes    = return_all_indexes\n    self.verbose               = verbose\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.steps","title":"steps  <code>instance-attribute</code>","text":"<pre><code>steps = steps\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.initial_train_size","title":"initial_train_size  <code>instance-attribute</code>","text":"<pre><code>initial_train_size = initial_train_size\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = window_size\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.differentiation","title":"differentiation  <code>instance-attribute</code>","text":"<pre><code>differentiation = differentiation\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.refit","title":"refit  <code>instance-attribute</code>","text":"<pre><code>refit = refit\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.fixed_train_size","title":"fixed_train_size  <code>instance-attribute</code>","text":"<pre><code>fixed_train_size = fixed_train_size\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.gap","title":"gap  <code>instance-attribute</code>","text":"<pre><code>gap = gap\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.skip_folds","title":"skip_folds  <code>instance-attribute</code>","text":"<pre><code>skip_folds = skip_folds\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.allow_incomplete_fold","title":"allow_incomplete_fold  <code>instance-attribute</code>","text":"<pre><code>allow_incomplete_fold = allow_incomplete_fold\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.return_all_indexes","title":"return_all_indexes  <code>instance-attribute</code>","text":"<pre><code>return_all_indexes = return_all_indexes\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.verbose","title":"verbose  <code>instance-attribute</code>","text":"<pre><code>verbose = verbose\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold._validate_params","title":"_validate_params","text":"<pre><code>_validate_params(\n    cv_name,\n    steps=None,\n    initial_train_size=None,\n    window_size=None,\n    differentiation=None,\n    refit=False,\n    fixed_train_size=True,\n    gap=0,\n    skip_folds=None,\n    allow_incomplete_fold=True,\n    return_all_indexes=False,\n    verbose=True,\n)\n</code></pre> <p>Validate all input parameters to ensure correctness.</p> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def _validate_params(\n    self,\n    cv_name: str,\n    steps: int | None = None,\n    initial_train_size: int | str | pd.Timestamp | None = None,\n    window_size: int | None = None,\n    differentiation: int | None = None,\n    refit: bool | int = False,\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: int | list[int] | None = None,\n    allow_incomplete_fold: bool = True,\n    return_all_indexes: bool = False,\n    verbose: bool = True\n) -&gt; None: \n    \"\"\"\n    Validate all input parameters to ensure correctness.\n    \"\"\"\n\n    if cv_name == \"TimeSeriesFold\":\n        if not isinstance(steps, (int, np.integer)) or steps &lt; 1:\n            raise ValueError(\n                f\"`steps` must be an integer greater than 0. Got {steps}.\"\n            )\n        if not isinstance(initial_train_size, (int, np.integer, str, pd.Timestamp, type(None))):\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than 0, a date \"\n                f\"string, a pandas Timestamp, or None. Got {initial_train_size}.\"\n            )\n        if isinstance(initial_train_size, (int, np.integer)) and initial_train_size &lt; 1:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than 0, \"\n                f\"a date string, a pandas Timestamp, or None. Got {initial_train_size}.\"\n            )\n        if not isinstance(refit, (bool, int, np.integer)):\n            raise TypeError(\n                f\"`refit` must be a boolean or an integer equal or greater than 0. \"\n                f\"Got {refit}.\"\n            )\n        if isinstance(refit, (int, np.integer)) and not isinstance(refit, bool) and refit &lt; 0:\n            raise TypeError(\n                f\"`refit` must be a boolean or an integer equal or greater than 0. \"\n                f\"Got {refit}.\"\n            )\n        if not isinstance(fixed_train_size, bool):\n            raise TypeError(\n                f\"`fixed_train_size` must be a boolean: `True`, `False`. \"\n                f\"Got {fixed_train_size}.\"\n            )\n        if not isinstance(gap, (int, np.integer)) or gap &lt; 0:\n            raise ValueError(\n                f\"`gap` must be an integer greater than or equal to 0. Got {gap}.\"\n            )\n        if skip_folds is not None:\n            if not isinstance(skip_folds, (int, np.integer, list, type(None))):\n                raise TypeError(\n                    f\"`skip_folds` must be an integer greater than 0, a list of \"\n                    f\"integers or `None`. Got {skip_folds}.\"\n                )\n            if isinstance(skip_folds, (int, np.integer)) and skip_folds &lt; 1:\n                raise ValueError(\n                    f\"`skip_folds` must be an integer greater than 0, a list of \"\n                    f\"integers or `None`. Got {skip_folds}.\"\n                )\n            if isinstance(skip_folds, list) and any([x &lt; 1 for x in skip_folds]):\n                raise ValueError(\n                    f\"`skip_folds` list must contain integers greater than or \"\n                    f\"equal to 1. The first fold is always needed to train the \"\n                    f\"forecaster. Got {skip_folds}.\"\n                ) \n        if not isinstance(allow_incomplete_fold, bool):\n            raise TypeError(\n                f\"`allow_incomplete_fold` must be a boolean: `True`, `False`. \"\n                f\"Got {allow_incomplete_fold}.\"\n            )\n\n    if cv_name == \"OneStepAheadFold\":\n        if not isinstance(initial_train_size, (int, np.integer, str, pd.Timestamp)):\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than 0, a date \"\n                f\"string, or a pandas Timestamp. Got {initial_train_size}.\"\n            )\n        if isinstance(initial_train_size, (int, np.integer)) and initial_train_size &lt; 1:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than 0, \"\n                f\"a date string, or a pandas Timestamp. Got {initial_train_size}.\"\n            )\n\n    if (\n        not isinstance(window_size, (int, np.integer, pd.DateOffset, type(None)))\n        or isinstance(window_size, (int, np.integer))\n        and window_size &lt; 1\n    ):\n        raise ValueError(\n            f\"`window_size` must be an integer greater than 0. Got {window_size}.\"\n        )\n\n    if not isinstance(return_all_indexes, bool):\n        raise TypeError(\n            f\"`return_all_indexes` must be a boolean: `True`, `False`. \"\n            f\"Got {return_all_indexes}.\"\n        )\n    if differentiation is not None:\n        if not isinstance(differentiation, (int, np.integer)) or differentiation &lt; 0:\n            raise ValueError(\n                f\"`differentiation` must be None or an integer greater than or \"\n                f\"equal to 0. Got {differentiation}.\"\n            )\n    if not isinstance(verbose, bool):\n        raise TypeError(\n            f\"`verbose` must be a boolean: `True`, `False`. \"\n            f\"Got {verbose}.\"\n        )\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold._extract_index","title":"_extract_index","text":"<pre><code>_extract_index(X)\n</code></pre> <p>Extracts and returns the index from the input data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series, pandas DataFrame, pandas Index, dict</code> <p>Time series data or index to split.</p> required <p>Returns:</p> Name Type Description <code>idx</code> <code>pandas Index</code> <p>Index extracted from the input data.</p> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def _extract_index(\n    self,\n    X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame]\n) -&gt; pd.Index:\n    \"\"\"\n    Extracts and returns the index from the input data X.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame, pandas Index, dict\n        Time series data or index to split.\n\n    Returns\n    -------\n    idx : pandas Index\n        Index extracted from the input data.\n\n    \"\"\"\n\n    if isinstance(X, (pd.Series, pd.DataFrame)):\n        idx = X.index\n    elif isinstance(X, dict):\n        indexes_freq = set()\n        not_valid_index = []\n        min_index = []\n        max_index = []\n        for k, v in X.items():\n            if v is None:\n                continue\n\n            idx = v.index\n            if isinstance(v.index, pd.DatetimeIndex):\n                indexes_freq.add(v.index.freqstr)\n            elif isinstance(v.index, pd.RangeIndex):\n                indexes_freq.add(v.index.step)\n            else:\n                not_valid_index.append(k)\n\n            min_index.append(idx[0])\n            max_index.append(idx[-1])\n\n        if not_valid_index:\n            raise TypeError(\n                f\"If `X` is a dictionary, all series must have a Pandas \"\n                f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n                f\"Review series: {not_valid_index}\"\n            )\n\n        if None in indexes_freq:\n            raise ValueError(\n                \"If `X` is a dictionary, all series must have a Pandas \"\n                \"RangeIndex or DatetimeIndex with the same step/frequency. \"\n                \"Found series with no frequency or step.\"\n            )\n        if not len(indexes_freq) == 1:\n            raise ValueError(\n                f\"If `X` is a dictionary, all series must have a Pandas \"\n                f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n                f\"Found frequencies: {sorted(indexes_freq)}\"\n            )\n\n        if isinstance(idx, pd.DatetimeIndex):\n            idx = pd.date_range(\n                start=min(min_index), end=max(max_index), freq=indexes_freq.pop()\n            )\n        else:\n            idx = pd.RangeIndex(\n                start=min(min_index), stop=max(max_index) + 1, step=indexes_freq.pop()\n            )\n    else:\n        idx = X\n\n    return idx\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.BaseFold.set_params","title":"set_params","text":"<pre><code>set_params(params)\n</code></pre> <p>Set the parameters of the Fold object. Before overwriting the current  parameters, the input parameters are validated to ensure correctness.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Dictionary with the parameters to set.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def set_params(\n    self, \n    params: dict\n) -&gt; None:\n    \"\"\"\n    Set the parameters of the Fold object. Before overwriting the current \n    parameters, the input parameters are validated to ensure correctness.\n\n    Parameters\n    ----------\n    params : dict\n        Dictionary with the parameters to set.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(params, dict):\n        raise TypeError(\n            f\"`params` must be a dictionary. Got {type(params)}.\"\n        )\n\n    current_params = deepcopy(vars(self))\n    unknown_params = set(params.keys()) - set(current_params.keys())\n    if unknown_params:\n        warnings.warn(\n            f\"Unknown parameters: {unknown_params}. They have been ignored.\",\n            IgnoredArgumentWarning\n        )\n\n    filtered_params = {k: v for k, v in params.items() if k in current_params}\n    updated_params = {'cv_name': type(self).__name__, **current_params, **filtered_params}\n\n    self._validate_params(**updated_params)\n    for key, value in updated_params.items():\n        setattr(self, key, value)\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.TimeSeriesFold","title":"skforecast.model_selection._split.TimeSeriesFold","text":"<pre><code>TimeSeriesFold(\n    steps,\n    initial_train_size=None,\n    window_size=None,\n    differentiation=None,\n    refit=False,\n    fixed_train_size=True,\n    gap=0,\n    skip_folds=None,\n    allow_incomplete_fold=True,\n    return_all_indexes=False,\n    verbose=True,\n)\n</code></pre> <p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds.  When used within a backtesting or hyperparameter search, the arguments 'initial_train_size', 'window_size' and 'differentiation' are not required as they are automatically set by the backtesting or hyperparameter search functions.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> required <code>initial_train_size</code> <code>int, str, pandas Timestamp</code> <p>Number of observations used for initial training. </p> <ul> <li>If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</li> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in  the initial training set.</li> </ul> <code>None</code> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>None</code> <code>refit</code> <code>(bool, int)</code> <p>Whether to refit the forecaster in each fold.</p> <ul> <li>If <code>True</code>, the forecaster is refitted in each fold.</li> <li>If <code>False</code>, the forecaster is trained only in the first fold.</li> <li>If an integer, the forecaster is trained in the first fold and then refitted   every <code>refit</code> folds.</li> </ul> <code>False</code> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>True</code> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>0</code> <code>skip_folds</code> <code>(int, list)</code> <p>Number of folds to skip.</p> <ul> <li>If an integer, every 'skip_folds'-th is returned.</li> <li>If a list, the indexes of the folds to skip.</li> </ul> <p>For example, if <code>skip_folds=3</code> and there are 10 folds, the returned folds are 0, 3, 6, and 9. If <code>skip_folds=[1, 2, 3]</code>, the returned folds are 0, 4, 5, 6, 7, 8, and 9.</p> <code>None</code> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>. If <code>False</code>, the last fold is excluded if it is incomplete.</p> <code>True</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>steps</code> <code>int</code> <p>Number of observations used to be predicted in each fold. This is also commonly referred to as the forecast horizon or test size.</p> <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training. If <code>None</code> or 0, the initial forecaster is not trained in the first fold.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>refit</code> <code>(bool, int)</code> <p>Whether to refit the forecaster in each fold.</p> <code>fixed_train_size</code> <code>bool</code> <p>Whether the training size is fixed or increases in each fold.</p> <code>gap</code> <code>int</code> <p>Number of observations between the end of the training set and the start of the test set.</p> <code>skip_folds</code> <code>(int, list)</code> <p>Number of folds to skip.</p> <code>allow_incomplete_fold</code> <code>bool</code> <p>Whether to allow the last fold to include fewer observations than <code>steps</code>.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> Notes <p>Returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc. For example, if the input series is <code>X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]</code>, the  <code>initial_train_size = 3</code>, <code>window_size = 2</code>, <code>steps = 4</code>, and <code>gap = 1</code>, the output of the first fold will: [[0, 3], [1, 3], [3, 8], [4, 8], True].</p> <p>The first list <code>[0, 3]</code> indicates that the training set goes from the first to the third observation. The second list <code>[1, 3]</code> indicates that the last window seen by the forecaster during training goes from the second to the third observation. The third list <code>[3, 8]</code> indicates that the test set goes from the fourth to the eighth observation. The fourth list <code>[4, 8]</code> indicates that the test set including the gap goes from the fifth to the eighth observation. The boolean <code>False</code> indicates that the forecaster should not be trained in this fold.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series data into train and test folds.</p> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def __init__(\n    self,\n    steps: int,\n    initial_train_size: int | str | pd.Timestamp | None = None,\n    window_size: int | None = None,\n    differentiation: int | None = None,\n    refit: bool | int = False,\n    fixed_train_size: bool = True,\n    gap: int = 0,\n    skip_folds: int | list[int] | None = None,\n    allow_incomplete_fold: bool = True,\n    return_all_indexes: bool = False,\n    verbose: bool = True\n) -&gt; None:\n\n    super().__init__(\n        steps                 = steps,\n        initial_train_size    = initial_train_size,\n        window_size           = window_size,\n        differentiation       = differentiation,\n        refit                 = refit,\n        fixed_train_size      = fixed_train_size,\n        gap                   = gap,\n        skip_folds            = skip_folds,\n        allow_incomplete_fold = allow_incomplete_fold,\n        return_all_indexes    = return_all_indexes,\n        verbose               = verbose\n    )\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.TimeSeriesFold._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def _repr_html_(self) -&gt; str:\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    style, unique_id = get_style_repr_html()\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Initial train size:&lt;/strong&gt; {self.initial_train_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Steps:&lt;/strong&gt; {self.steps}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Differentiation:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Refit:&lt;/strong&gt; {self.refit}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Fixed train size:&lt;/strong&gt; {self.fixed_train_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Gap:&lt;/strong&gt; {self.gap}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Skip folds:&lt;/strong&gt; {self.skip_folds}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Allow incomplete fold:&lt;/strong&gt; {self.allow_incomplete_fold}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Return all indexes:&lt;/strong&gt; {self.return_all_indexes}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/model_selection.html#skforecast.model_selection._split.TimeSeriesFold\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/backtesting.html#timeseriesfold\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return style + content\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.TimeSeriesFold.split","title":"split","text":"<pre><code>split(X, as_pandas=False)\n</code></pre> <p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series, pandas DataFrame, pandas Index, dict</code> <p>Time series data or index to split.</p> required <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>folds</code> <code>list, pandas DataFrame</code> <p>A list of lists containing the indices (position) for each fold. Each list contains 4 lists and a boolean with the following information:</p> <ul> <li>[train_start, train_end]: list with the start and end positions of the training set.</li> <li>[last_window_start, last_window_end]: list with the start and end positions of the last window seen by the forecaster during training. The last window is used to generate the lags use as predictors. If <code>differentiation</code> is included, the interval is extended as many observations as the differentiation order. If the argument <code>window_size</code> is <code>None</code>, this list is empty.</li> <li>[test_start, test_end]: list with the start and end positions of the test set. These are the observations used to evaluate the forecaster.</li> <li>[test_start_with_gap, test_end_with_gap]: list with the start and end positions of the test set including the gap. The gap is the number of observations between the end of the training set and the start of the test set.</li> <li>fit_forecaster: boolean indicating whether the forecaster should be fitted in this fold.</li> </ul> <p>It is important to note that the returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc.</p> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the following columns: 'fold', 'train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster'.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def split(\n    self,\n    X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n    as_pandas: bool = False\n) -&gt; list | pd.DataFrame:\n    \"\"\"\n    Split the time series data into train and test folds.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame, pandas Index, dict\n        Time series data or index to split.\n    as_pandas : bool, default False\n        If True, the folds are returned as a DataFrame. This is useful to visualize\n        the folds in a more interpretable way.\n\n    Returns\n    -------\n    folds : list, pandas DataFrame\n        A list of lists containing the indices (position) for each fold. Each list\n        contains 4 lists and a boolean with the following information:\n\n        - [train_start, train_end]: list with the start and end positions of the\n        training set.\n        - [last_window_start, last_window_end]: list with the start and end positions\n        of the last window seen by the forecaster during training. The last window\n        is used to generate the lags use as predictors. If `differentiation` is\n        included, the interval is extended as many observations as the\n        differentiation order. If the argument `window_size` is `None`, this list is\n        empty.\n        - [test_start, test_end]: list with the start and end positions of the test\n        set. These are the observations used to evaluate the forecaster.\n        - [test_start_with_gap, test_end_with_gap]: list with the start and end\n        positions of the test set including the gap. The gap is the number of\n        observations between the end of the training set and the start of the test\n        set.\n        - fit_forecaster: boolean indicating whether the forecaster should be fitted\n        in this fold.\n\n        It is important to note that the returned values are the positions of the\n        observations and not the actual values of the index, so they can be used to\n        slice the data directly using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n        'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n        'test_end_with_gap', 'fit_forecaster'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n\n    \"\"\"\n\n    if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n        raise TypeError(\n            f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n            f\"Got {type(X)}.\"\n        )\n\n    window_size_as_date_offset = isinstance(self.window_size, pd.tseries.offsets.DateOffset)\n    if window_size_as_date_offset:\n        # Calculate the window_size in steps. This is not a exact calculation\n        # because the offset follows the calendar rules and the distance between\n        # two dates may not be constant.\n        first_valid_index = X.index[-1] - self.window_size\n        try:\n            window_size_idx_start = X.index.get_loc(first_valid_index)\n            window_size_idx_end = X.index.get_loc(X.index[-1])\n            self.window_size = window_size_idx_end - window_size_idx_start\n        except KeyError:\n            raise ValueError(\n                f\"The length of `y` ({len(X)}), must be greater than or equal \"\n                f\"to the window size ({self.window_size}). This is because  \"\n                f\"the offset (forecaster.offset) is larger than the available \"\n                f\"data. Try to decrease the size of the offset (forecaster.offset), \"\n                f\"the number of `n_offsets` (forecaster.n_offsets) or increase the \"\n                f\"size of `y`.\"\n            )\n\n    if self.initial_train_size is None:\n        if self.window_size is None:\n            raise ValueError(\n                \"To use split method when `initial_train_size` is None, \"\n                \"`window_size` must be an integer greater than 0. \"\n                \"Although no initial training is done and all data is used to \"\n                \"evaluate the model, the first `window_size` observations are \"\n                \"needed to create the initial predictors. Got `window_size` = None.\"\n            )\n        if self.refit:\n            raise ValueError(\n                \"`refit` is only allowed when `initial_train_size` is not `None`. \"\n                \"Set `refit` to `False` if you want to use `initial_train_size = None`.\"\n            )\n        externally_fitted = True\n        self.initial_train_size = self.window_size  # Reset to None later\n    else:\n        if self.window_size is None:\n            warnings.warn(\n                \"Last window cannot be calculated because `window_size` is None.\",\n                IgnoredArgumentWarning\n            )\n        externally_fitted = False\n\n    index = self._extract_index(X)\n    idx = range(len(index))\n    folds = []\n    i = 0\n    last_fold_excluded = False\n\n    self.initial_train_size = date_to_index_position(\n                                  index        = index, \n                                  date_input   = self.initial_train_size, \n                                  method       = 'validation',\n                                  date_literal = 'initial_train_size'\n                              )\n\n    if window_size_as_date_offset:\n        if self.initial_train_size is not None:\n            if self.initial_train_size &lt; self.window_size:\n                raise ValueError(\n                    f\"If `initial_train_size` is an integer, it must be greater than \"\n                    f\"the `window_size` of the forecaster ({self.window_size}) \"\n                    f\"and smaller than the length of the series ({len(X)}). If \"\n                    f\"it is a date, it must be within this range of the index.\"\n                )\n\n    if len(index) &lt; self.initial_train_size + self.steps:\n        raise ValueError(\n            f\"The time series must have at least `initial_train_size + steps` \"\n            f\"observations. Got {len(index)} observations.\"\n        )\n\n    while self.initial_train_size + (i * self.steps) + self.gap &lt; len(index):\n\n        if self.refit:\n            # If `fixed_train_size` the train size doesn't increase but moves by \n            # `steps` positions in each iteration. If `False`, the train size\n            # increases by `steps` in each iteration.\n            train_iloc_start = i * (self.steps) if self.fixed_train_size else 0\n            train_iloc_end = self.initial_train_size + i * (self.steps)\n            test_iloc_start = train_iloc_end\n        else:\n            # The train size doesn't increase and doesn't move.\n            train_iloc_start = 0\n            train_iloc_end = self.initial_train_size\n            test_iloc_start = self.initial_train_size + i * (self.steps)\n\n        if self.window_size is not None:\n            last_window_iloc_start = test_iloc_start - self.window_size\n        test_iloc_end = test_iloc_start + self.gap + self.steps\n\n        partitions = [\n            idx[train_iloc_start : train_iloc_end],\n            idx[last_window_iloc_start : test_iloc_start] if self.window_size is not None else [],\n            idx[test_iloc_start : test_iloc_end],\n            idx[test_iloc_start + self.gap : test_iloc_end]\n        ]\n        folds.append(partitions)\n        i += 1\n\n    if not self.allow_incomplete_fold and len(folds[-1][3]) &lt; self.steps:\n        folds = folds[:-1]\n        last_fold_excluded = True\n\n    # Replace partitions inside folds with length 0 with `None`\n    folds = [\n        [partition if len(partition) &gt; 0 else None for partition in fold] \n         for fold in folds\n    ]\n\n    # Create a flag to know whether to train the forecaster\n    if self.refit == 0:\n        self.refit = False\n\n    if isinstance(self.refit, bool):\n        fit_forecaster = [self.refit] * len(folds)\n        fit_forecaster[0] = True\n    else:\n        fit_forecaster = [False] * len(folds)\n        for i in range(0, len(fit_forecaster), self.refit): \n            fit_forecaster[i] = True\n\n    for i in range(len(folds)): \n        folds[i].append(fit_forecaster[i])\n        if fit_forecaster[i] is False:\n            folds[i][0] = folds[i - 1][0]\n\n    index_to_skip = []\n    if self.skip_folds is not None:\n        if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds &gt; 0:\n            index_to_keep = np.arange(0, len(folds), self.skip_folds)\n            index_to_skip = np.setdiff1d(np.arange(0, len(folds)), index_to_keep, assume_unique=True)\n            index_to_skip = [int(x) for x in index_to_skip]  # Required since numpy 2.0\n        if isinstance(self.skip_folds, list):\n            index_to_skip = [i for i in self.skip_folds if i &lt; len(folds)]        \n\n    if self.verbose:\n        self._print_info(\n            index              = index,\n            folds              = folds,\n            externally_fitted  = externally_fitted,\n            last_fold_excluded = last_fold_excluded,\n            index_to_skip      = index_to_skip\n        )\n\n    folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n    if not self.return_all_indexes:\n        # NOTE: +1 to prevent iloc pandas from deleting the last observation\n        folds = [\n            [[fold[0][0], fold[0][-1] + 1], \n             [fold[1][0], fold[1][-1] + 1] if self.window_size is not None else [],\n             [fold[2][0], fold[2][-1] + 1],\n             [fold[3][0], fold[3][-1] + 1],\n             fold[4]] \n            for fold in folds\n        ]\n\n    if externally_fitted:\n        self.initial_train_size = None\n        folds[0][4] = False\n\n    if as_pandas:\n        if self.window_size is None:\n            for fold in folds:\n                fold[1] = [None, None]\n\n        if not self.return_all_indexes:\n            folds = pd.DataFrame(\n                data = [list(itertools.chain(*fold[:-1])) + [fold[-1]] for fold in folds],\n                columns = [\n                    'train_start',\n                    'train_end',\n                    'last_window_start',\n                    'last_window_end',\n                    'test_start',\n                    'test_end',\n                    'test_start_with_gap',\n                    'test_end_with_gap',\n                    'fit_forecaster'\n                ],\n            )\n        else:\n            folds = pd.DataFrame(\n                data = folds,\n                columns = [\n                    'train_index',\n                    'last_window_index',\n                    'test_index',\n                    'test_index_with_gap',\n                    'fit_forecaster'\n                ],\n            )\n        folds.insert(0, 'fold', range(len(folds)))\n\n    return folds\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.TimeSeriesFold._print_info","title":"_print_info","text":"<pre><code>_print_info(\n    index,\n    folds,\n    externally_fitted,\n    last_fold_excluded,\n    index_to_skip,\n)\n</code></pre> <p>Print information about folds.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>pandas Index</code> <p>Index of the time series data.</p> required <code>folds</code> <code>list</code> <p>A list of lists containing the indices (position) for each fold.</p> required <code>externally_fitted</code> <code>bool</code> <p>Whether an already trained forecaster is to be used.</p> required <code>last_fold_excluded</code> <code>bool</code> <p>Whether the last fold has been excluded because it was incomplete.</p> required <code>index_to_skip</code> <code>list</code> <p>Number of folds skipped.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def _print_info(\n    self,\n    index: pd.Index,\n    folds: list[list[int]],\n    externally_fitted: bool,\n    last_fold_excluded: bool,\n    index_to_skip: list[int]\n) -&gt; None:\n    \"\"\"\n    Print information about folds.\n\n    Parameters\n    ----------\n    index : pandas Index\n        Index of the time series data.\n    folds : list\n        A list of lists containing the indices (position) for each fold.\n    externally_fitted : bool\n        Whether an already trained forecaster is to be used.\n    last_fold_excluded : bool\n        Whether the last fold has been excluded because it was incomplete.\n    index_to_skip : list\n        Number of folds skipped.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    print(\"Information of folds\")\n    print(\"--------------------\")\n    if externally_fitted:\n        print(\n            f\"An already trained forecaster is to be used. Window size: \"\n            f\"{self.window_size}\"\n        )\n    else:\n        if self.differentiation is None:\n            print(\n                f\"Number of observations used for initial training: \"\n                f\"{self.initial_train_size}\"\n            )\n        else:\n            print(\n                f\"Number of observations used for initial training: \"\n                f\"{self.initial_train_size - self.differentiation}\"\n            )\n            print(\n                f\"    First {self.differentiation} observation/s in training sets \"\n                f\"are used for differentiation\"\n            )\n    print(\n        f\"Number of observations used for backtesting: \"\n        f\"{len(index) - self.initial_train_size}\"\n    )\n    print(f\"    Number of folds: {len(folds)}\")\n    print(\n        f\"    Number skipped folds: \"\n        f\"{len(index_to_skip)} {index_to_skip if index_to_skip else ''}\"\n    )\n    print(f\"    Number of steps per fold: {self.steps}\")\n    print(\n        f\"    Number of steps to exclude between last observed data \"\n        f\"(last window) and predictions (gap): {self.gap}\"\n    )\n    if last_fold_excluded:\n        print(\"    Last fold has been excluded because it was incomplete.\")\n    if len(folds[-1][3]) &lt; self.steps:\n        print(f\"    Last fold only includes {len(folds[-1][3])} observations.\")\n    print(\"\")\n\n    if self.differentiation is None:\n        differentiation = 0\n    else:\n        differentiation = self.differentiation\n\n    for i, fold in enumerate(folds):\n        is_fold_skipped   = i in index_to_skip\n        has_training      = fold[-1] if i != 0 else True\n        training_start    = (\n            index[fold[0][0] + differentiation] if fold[0] is not None else None\n        )\n        training_end      = index[fold[0][-1]] if fold[0] is not None else None\n        training_length   = (\n            len(fold[0]) - differentiation if fold[0] is not None else 0\n        )\n        validation_start  = index[fold[3][0]]\n        validation_end    = index[fold[3][-1]]\n        validation_length = len(fold[3])\n\n        print(f\"Fold: {i}\")\n        if is_fold_skipped:\n            print(\"    Fold skipped\")\n        elif not externally_fitted and has_training:\n            print(\n                f\"    Training:   {training_start} -- {training_end}  \"\n                f\"(n={training_length})\"\n            )\n            print(\n                f\"    Validation: {validation_start} -- {validation_end}  \"\n                f\"(n={validation_length})\"\n            )\n        else:\n            print(\"    Training:   No training in this fold\")\n            print(\n                f\"    Validation: {validation_start} -- {validation_end}  \"\n                f\"(n={validation_length})\"\n            )\n\n    print(\"\")\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.OneStepAheadFold","title":"skforecast.model_selection._split.OneStepAheadFold","text":"<pre><code>OneStepAheadFold(\n    initial_train_size,\n    window_size=None,\n    differentiation=None,\n    return_all_indexes=False,\n    verbose=True,\n)\n</code></pre> <p>               Bases: <code>BaseFold</code></p> <p>Class to split time series data into train and test folds for one-step-ahead forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>initial_train_size</code> <code>int, str, pandas Timestamp</code> <p>Number of observations used for initial training.</p> <ul> <li>If an integer, the number of observations used for initial training.</li> <li>If a date string or pandas Timestamp, it is the last date included in  the initial training set.</li> </ul> required <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>None</code> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>None</code> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>initial_train_size</code> <code>int</code> <p>Number of observations used for initial training.</p> <code>window_size</code> <code>int</code> <p>Number of observations needed to generate the autoregressive predictors.</p> <code>differentiation</code> <code>int</code> <p>Number of observations to use for differentiation. This is used to extend the <code>last_window</code> as many observations as the differentiation order.</p> <code>return_all_indexes</code> <code>bool</code> <p>Whether to return all indexes or only the start and end indexes of each fold.</p> <code>verbose</code> <code>bool</code> <p>Whether to print information about generated folds.</p> <code>steps</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>fixed_train_size</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>gap</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>skip_folds</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>allow_incomplete_fold</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <code>refit</code> <code>Any</code> <p>This attribute is not used in this class. It is included for API consistency.</p> <p>Methods:</p> Name Description <code>split</code> <p>Split the time series data into train and test folds.</p> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def __init__(\n    self,\n    initial_train_size: int | str | pd.Timestamp,\n    window_size: int | None = None,\n    differentiation: int | None = None,\n    return_all_indexes: bool = False,\n    verbose: bool = True\n) -&gt; None:\n\n    super().__init__(\n        initial_train_size = initial_train_size,\n        window_size        = window_size,\n        differentiation    = differentiation,\n        return_all_indexes = return_all_indexes,\n        verbose            = verbose\n    )\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.OneStepAheadFold._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def _repr_html_(self) -&gt; str:\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    style, unique_id = get_style_repr_html()\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Initial train size:&lt;/strong&gt; {self.initial_train_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Differentiation:&lt;/strong&gt; {self.differentiation}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Return all indexes:&lt;/strong&gt; {self.return_all_indexes}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/model_selection.html#skforecast.model_selection._split.OneStepAheadFold\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/faq/parameters-search-backtesting-vs-one-step-ahead.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return style + content\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.OneStepAheadFold.split","title":"split","text":"<pre><code>split(X, as_pandas=False, externally_fitted=None)\n</code></pre> <p>Split the time series data into train and test folds.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series, DataFrame, Index, or dictionary</code> <p>Time series data or index to split.</p> required <code>as_pandas</code> <code>bool</code> <p>If True, the folds are returned as a DataFrame. This is useful to visualize the folds in a more interpretable way.</p> <code>False</code> <code>externally_fitted</code> <code>Any</code> <p>This argument is not used in this class. It is included for API consistency.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fold</code> <code>list, pandas DataFrame</code> <p>A list of lists containing the indices (position) of the fold. The list contains 2 lists with the following information:</p> <ul> <li>[train_start, train_end]: list with the start and end positions of the training set.</li> <li>[test_start, test_end]: list with the start and end positions of the test set. These are the observations used to evaluate the forecaster.</li> </ul> <p>It is important to note that the returned values are the positions of the observations and not the actual values of the index, so they can be used to slice the data directly using iloc.</p> <p>If <code>as_pandas</code> is <code>True</code>, the folds are returned as a DataFrame with the following columns: 'fold', 'train_start', 'train_end', 'test_start', 'test_end'.</p> <p>Following the python convention, the start index is inclusive and the end index is exclusive. This means that the last index is not included in the slice.</p> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def split(\n    self,\n    X: pd.Series | pd.DataFrame | pd.Index | dict[str, pd.Series | pd.DataFrame],\n    as_pandas: bool = False,\n    externally_fitted: Any = None\n) -&gt; list | pd.DataFrame:\n    \"\"\"\n    Split the time series data into train and test folds.\n\n    Parameters\n    ----------\n    X : pandas Series, DataFrame, Index, or dictionary\n        Time series data or index to split.\n    as_pandas : bool, default False\n        If True, the folds are returned as a DataFrame. This is useful to visualize\n        the folds in a more interpretable way.\n    externally_fitted : Any\n        This argument is not used in this class. It is included for API consistency.\n\n    Returns\n    -------\n    fold : list, pandas DataFrame\n        A list of lists containing the indices (position) of the fold. The list\n        contains 2 lists with the following information:\n\n        - [train_start, train_end]: list with the start and end positions of the\n        training set.\n        - [test_start, test_end]: list with the start and end positions of the test\n        set. These are the observations used to evaluate the forecaster.\n\n        It is important to note that the returned values are the positions of the\n        observations and not the actual values of the index, so they can be used to\n        slice the data directly using iloc.\n\n        If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n        following columns: 'fold', 'train_start', 'train_end', 'test_start', 'test_end'.\n\n        Following the python convention, the start index is inclusive and the end\n        index is exclusive. This means that the last index is not included in the\n        slice.\n\n    \"\"\"\n\n    if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n        raise TypeError(\n            f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n            f\"Got {type(X)}.\"\n        )\n\n    index = self._extract_index(X)\n\n    self.initial_train_size = date_to_index_position(\n                                  index        = index, \n                                  date_input   = self.initial_train_size, \n                                  method       = 'validation',\n                                  date_literal = 'initial_train_size'\n                              )\n\n    fold = [\n        [0, self.initial_train_size - 1],\n        [self.initial_train_size, len(X)],\n        True\n    ]\n\n    if self.verbose:\n        self._print_info(index=index, fold=fold)\n\n    # NOTE: +1 to prevent iloc pandas from deleting the last observation\n    if self.return_all_indexes:\n        fold = [\n            [range(fold[0][0], fold[0][1] + 1)],\n            [range(fold[1][0], fold[1][1])],\n            fold[2]\n        ]\n    else:\n        fold = [\n            [fold[0][0], fold[0][1] + 1],\n            [fold[1][0], fold[1][1]],\n            fold[2]\n        ]\n\n    if as_pandas:\n        if not self.return_all_indexes:\n            fold = pd.DataFrame(\n                data = [list(itertools.chain(*fold[:-1])) + [fold[-1]]],\n                columns = [\n                    'train_start',\n                    'train_end',\n                    'test_start',\n                    'test_end',\n                    'fit_forecaster'\n                ],\n            )\n        else:\n            fold = pd.DataFrame(\n                data = [fold],\n                columns = [\n                    'train_index',\n                    'test_index',\n                    'fit_forecaster'\n                ],\n            )\n        fold.insert(0, 'fold', range(len(fold)))\n\n    return fold\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._split.OneStepAheadFold._print_info","title":"_print_info","text":"<pre><code>_print_info(index, fold)\n</code></pre> <p>Print information about folds.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>pandas Index</code> <p>Index of the time series data.</p> required <code>fold</code> <code>list</code> <p>A list of lists containing the indices (position) of the fold.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\model_selection\\_split.py</code> <pre><code>def _print_info(\n    self,\n    index: pd.Index,\n    fold: list[list[int]]\n) -&gt; None:\n    \"\"\"\n    Print information about folds.\n\n    Parameters\n    ----------\n    index : pandas Index\n        Index of the time series data.\n    fold : list\n        A list of lists containing the indices (position) of the fold.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if self.differentiation is None:\n        differentiation = 0\n    else:\n        differentiation = self.differentiation\n\n    initial_train_size = self.initial_train_size - differentiation\n    test_length = len(index) - (initial_train_size + differentiation)\n\n    print(\"Information of folds\")\n    print(\"--------------------\")\n    print(\n        f\"Number of observations in train: {initial_train_size}\"\n    )\n    if self.differentiation is not None:\n        print(\n            f\"    First {differentiation} observation/s in training set \"\n            f\"are used for differentiation\"\n        )\n    print(\n        f\"Number of observations in test: {test_length}\"\n    )\n\n    training_start = index[fold[0][0] + differentiation]\n    training_end = index[fold[0][-1]]\n    test_start  = index[fold[1][0]]\n    test_end    = index[fold[1][-1] - 1]\n\n    print(\n        f\"Training : {training_start} -- {training_end} (n={initial_train_size})\"\n    )\n    print(\n        f\"Test     : {test_start} -- {test_end} (n={test_length})\"\n    )\n    print(\"\")\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._utils.initialize_lags_grid","title":"skforecast.model_selection._utils.initialize_lags_grid","text":"<pre><code>initialize_lags_grid(forecaster, lags_grid=None)\n</code></pre> <p>Initialize lags grid and lags label for model selection. </p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>Forecaster</code> <p>Forecaster model. ForecasterRecursive, ForecasterDirect,  ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.</p> required <code>lags_grid</code> <code>(list, dict)</code> <p>Lists of lags to try, containing int, lists, numpy ndarray, or range  objects. If <code>dict</code>, the keys are used as labels in the <code>results</code>  DataFrame, and the values are used as the lists of lags to try.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>lags_grid</code> <code>dict</code> <p>Dictionary with lags configuration for each iteration.</p> <code>lags_label</code> <code>str</code> <p>Label for lags representation in the results object.</p> Source code in <code>skforecast\\model_selection\\_utils.py</code> <pre><code>def initialize_lags_grid(\n    forecaster: object, \n    lags_grid: (\n        list[int | list[int] | np.ndarray[int] | range[int]]\n        | dict[str, list[int | list[int] | np.ndarray[int] | range[int]]]\n        | None\n    ) = None,\n) -&gt; tuple[dict[str, int], str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default None\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(\n            f\"`lags_grid` argument must be a list, dict or None. \"\n            f\"Got {type(lags_grid)}.\"\n        )\n\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]  # Required since numpy 2.0\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n\n    return lags_grid, lags_label\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._utils.check_backtesting_input","title":"skforecast.model_selection._utils.check_backtesting_input","text":"<pre><code>check_backtesting_input(\n    forecaster,\n    cv,\n    metric,\n    add_aggregated_metric=True,\n    y=None,\n    series=None,\n    exog=None,\n    interval=None,\n    interval_method=\"bootstrapping\",\n    alpha=None,\n    n_boot=250,\n    use_in_sample_residuals=True,\n    use_binned_residuals=True,\n    random_state=123,\n    return_predictors=False,\n    n_jobs=\"auto\",\n    show_progress=True,\n    suppress_warnings=False,\n    suppress_warnings_fit=False,\n)\n</code></pre> <p>This is a helper function to check most inputs of backtesting functions in  modules <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>Forecaster</code> <p>Forecaster model.</p> required <code>cv</code> <code>TimeSeriesFold</code> <p>TimeSeriesFold object with the information needed to split the data into folds.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> required <code>add_aggregated_metric</code> <code>bool</code> <p>If <code>True</code>, the aggregated metrics (average, weighted average and pooling) over all levels are also returned (only multiseries).</p> <code>True</code> <code>y</code> <code>pandas Series</code> <p>Training time series for uni-series forecasters.</p> <code>None</code> <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series for multi-series forecasters.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>None</code> <code>interval</code> <code>(float, list, tuple, str, object)</code> <p>Specifies whether probabilistic predictions should be estimated and the  method to use. The following options are supported:</p> <ul> <li>If <code>float</code>, represents the nominal (expected) coverage (between 0 and 1).  For instance, <code>interval=0.95</code> corresponds to <code>[2.5, 97.5]</code> percentiles.</li> <li>If <code>list</code> or <code>tuple</code>: Sequence of percentiles to compute, each value must  be between 0 and 100 inclusive. For example, a 95% confidence interval can  be specified as <code>interval = [2.5, 97.5]</code> or multiple percentiles (e.g. 10,  50 and 90) as <code>interval = [10, 50, 90]</code>.</li> <li>If 'bootstrapping' (str): <code>n_boot</code> bootstrapping predictions will be generated.</li> <li>If scipy.stats distribution object, the distribution parameters will be estimated for each prediction.</li> <li>If None, no probabilistic predictions are estimated.</li> </ul> <code>None</code> <code>interval_method</code> <code>str</code> <p>Technique used to estimate prediction intervals. Available options:</p> <ul> <li>'bootstrapping': Bootstrapping is used to generate prediction  intervals.</li> <li>'conformal': Employs the conformal prediction split method for  interval estimation.</li> </ul> <code>'bootstrapping'</code> <code>alpha</code> <code>float</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>None</code> <code>n_boot</code> <code>int</code> <p>Number of bootstrapping iterations to perform when estimating prediction     intervals.</p> <code>`250`</code> <code>use_in_sample_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals from the training data are used as proxy of prediction  error to create prediction intervals.  If <code>False</code>, out_sample_residuals  are used if they are already stored inside the forecaster.</p> <code>True</code> <code>use_binned_residuals</code> <code>bool</code> <p>If <code>True</code>, residuals are selected based on the predicted values  (binned selection). If <code>False</code>, residuals are selected randomly.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generator to ensure reproducibility.</p> <code>`123`</code> <code>return_predictors</code> <code>bool</code> <p>If <code>True</code>, the predictors used to make the predictions are also returned.</p> <code>False</code> <code>n_jobs</code> <code>(int, 'auto')</code> <p>The number of jobs to run in parallel. If <code>-1</code>, then the number of jobs is  set to the number of cores. If 'auto', <code>n_jobs</code> is set using the function skforecast.utils.select_n_jobs_fit_forecaster.</p> <code>`'auto'`</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the backtesting  process. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <code>suppress_warnings_fit</code> <code>bool</code> <p>If <code>True</code>, warnings generated during fitting will be ignored. Only  <code>ForecasterSarimax</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\model_selection\\_utils.py</code> <pre><code>def check_backtesting_input(\n    forecaster: object,\n    cv: object,\n    metric: str | Callable | list[str | Callable],\n    add_aggregated_metric: bool = True,\n    y: pd.Series | None = None,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame] = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    interval: float | list[float] | tuple[float] | str | object | None = None,\n    interval_method: str = 'bootstrapping',    \n    alpha: float | None = None,\n    n_boot: int = 250,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = True,\n    random_state: int = 123,\n    return_predictors: bool = False,\n    n_jobs: int | str = 'auto',\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    suppress_warnings_fit: bool = False\n) -&gt; None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default True\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default None\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default None\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variables.\n    interval : float, list, tuple, str, object, default None\n        Specifies whether probabilistic predictions should be estimated and the \n        method to use. The following options are supported:\n\n        - If `float`, represents the nominal (expected) coverage (between 0 and 1). \n        For instance, `interval=0.95` corresponds to `[2.5, 97.5]` percentiles.\n        - If `list` or `tuple`: Sequence of percentiles to compute, each value must \n        be between 0 and 100 inclusive. For example, a 95% confidence interval can \n        be specified as `interval = [2.5, 97.5]` or multiple percentiles (e.g. 10, \n        50 and 90) as `interval = [10, 50, 90]`.\n        - If 'bootstrapping' (str): `n_boot` bootstrapping predictions will be generated.\n        - If scipy.stats distribution object, the distribution parameters will\n        be estimated for each prediction.\n        - If None, no probabilistic predictions are estimated.\n    interval_method : str, default 'bootstrapping'\n        Technique used to estimate prediction intervals. Available options:\n\n        + 'bootstrapping': Bootstrapping is used to generate prediction \n        intervals.\n        + 'conformal': Employs the conformal prediction split method for \n        interval estimation.\n    alpha : float, default None\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations to perform when estimating prediction\n            intervals.\n    use_in_sample_residuals : bool, default True\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default True\n        If `True`, residuals are selected based on the predicted values \n        (binned selection).\n        If `False`, residuals are selected randomly.\n    random_state : int, default `123`\n        Seed for the random number generator to ensure reproducibility.\n    return_predictors : bool, default False\n        If `True`, the predictors used to make the predictions are also returned.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    suppress_warnings: bool, default False\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default False\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"TimeSeriesFold\":\n        raise TypeError(f\"`cv` must be a 'TimeSeriesFold' object. Got '{cv_name}'.\")\n\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n\n    forecasters_uni = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterSarimax\",\n        \"ForecasterEquivalentDate\",\n    ]\n    forecasters_direct = [\n        \"ForecasterDirect\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\"\n    ]\n    forecasters_multi_no_dict = [\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    ]\n    forecasters_multi_dict = [\n        \"ForecasterRecursiveMultiSeries\"\n    ]\n    # NOTE: ForecasterSarimax has interval but not with bootstrapping or conformal\n    forecasters_boot_conformal = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterEquivalentDate\",\n    ]\n    forecasters_return_predictors = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterRecursiveMultiSeries\",\n        \"ForecasterDirectMultiVariate\"\n    ]\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = 'y'\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi_no_dict:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = 'series'\n        data_length = len(series)\n\n    elif forecaster_name in forecasters_multi_dict:\n\n        # NOTE: Checks are not need as they are done in the function \n        # `check_preprocess_series` that is used before `check_backtesting_input`\n        # in the backtesting function.\n\n        data_name = 'series'\n        data_length = max([len(series[serie]) for serie in series])\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            # NOTE: Checks are not need as they are done in the function \n            # `check_preprocess_exog_multiseries` that is used before \n            # `check_backtesting_input` in the backtesting function.\n            pass\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\"\n                )\n\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation_max != cv.differentiation:\n            if forecaster_name == \"ForecasterRecursiveMultiSeries\" and isinstance(\n                forecaster.differentiation, dict\n            ):\n                raise ValueError(\n                    f\"When using a dict as `differentiation` in ForecasterRecursiveMultiSeries, \"\n                    f\"the `differentiation` included in the cv ({cv.differentiation}) must be \"\n                    f\"the same as the maximum `differentiation` included in the forecaster \"\n                    f\"({forecaster.differentiation_max}). Set the same value \"\n                    f\"for both using the `differentiation` argument.\"\n                )\n            else:\n                raise ValueError(\n                    f\"The differentiation included in the forecaster \"\n                    f\"({forecaster.differentiation_max}) differs from the differentiation \"\n                    f\"included in the cv ({cv.differentiation}). Set the same value \"\n                    f\"for both using the `differentiation` argument.\"\n                )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            f\"`metric` must be a string, a callable function, or a list containing \"\n            f\"multiple strings and/or callables. Got {type(metric)}.\"\n        )\n\n    if forecaster_name == \"ForecasterEquivalentDate\" and isinstance(\n        forecaster.offset, pd.tseries.offsets.DateOffset\n    ):\n        # NOTE: Checks when initial_train_size is not None cannot be done here\n        # because the forecaster is not fitted yet and we don't know the\n        # window_size since pd.DateOffset is not a fixed window size.\n        if initial_train_size is None:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}) or \"\n                f\"a date within this range of the index.\"\n            )\n    elif initial_train_size is not None:\n        if forecaster_name in forecasters_uni:\n            index = cv._extract_index(y)\n        else:\n            index = cv._extract_index(series)\n\n        initial_train_size = date_to_index_position(\n                                 index        = index, \n                                 date_input   = initial_train_size, \n                                 method       = 'validation',\n                                 date_literal = 'initial_train_size'\n                             )\n        if initial_train_size &lt; forecaster.window_size or initial_train_size &gt;= data_length:\n            raise ValueError(\n                f\"If `initial_train_size` is an integer, it must be greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}). If \"\n                f\"it is a date, it must be within this range of the index.\"\n            )\n        if initial_train_size + gap &gt;= data_length:\n            raise ValueError(\n                f\"The total size of `initial_train_size` {initial_train_size} plus \"\n                f\"`gap` {gap} cannot be greater than the length of `{data_name}` \"\n                f\"({data_length}).\"\n            )\n    else:\n        if forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer smaller than the \"\n                f\"length of `{data_name}` ({data_length}).\"\n            )\n        else:\n            if not forecaster.is_fitted:\n                raise NotFittedError(\n                    \"`forecaster` must be already trained if no `initial_train_size` \"\n                    \"is provided.\"\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError(\n            \"`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.\"\n        )\n\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot &lt; 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError(\"`use_in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError(\"`use_binned_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state &lt; 0:\n        raise TypeError(f\"`random_state` must be an integer greater than 0. Got {random_state}.\")\n    if not isinstance(return_predictors, bool):\n        raise TypeError(\"`return_predictors` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError(\"`suppress_warnings_fit` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n\n        if forecaster_name in forecasters_boot_conformal:\n\n            if interval_method == 'conformal':\n                if not isinstance(interval, (float, list, tuple)):\n                    raise TypeError(\n                        f\"When `interval_method` is 'conformal', `interval` must \"\n                        f\"be a float or a list/tuple defining a symmetric interval. \"\n                        f\"Got {type(interval)}.\"\n                    )\n            elif interval_method == 'bootstrapping':\n                if (\n                    not isinstance(interval, (float, list, tuple, str))\n                    and (not hasattr(interval, \"_pdf\") or not callable(getattr(interval, \"fit\", None)))\n                ):                \n                    raise TypeError(\n                        f\"When `interval_method` is 'bootstrapping', `interval` \"\n                        f\"must be a float, a list or tuple of floats, a \"\n                        f\"scipy.stats distribution object (with methods `_pdf` and \"\n                        f\"`fit`) or the string 'bootstrapping'. Got {type(interval)}.\"\n                    )\n                if isinstance(interval, (list, tuple)):\n                    for i in interval:\n                        if not isinstance(i, (int, float)):\n                            raise TypeError(\n                                f\"`interval` must be a list or tuple of floats. \"\n                                f\"Got {type(i)} in {interval}.\"\n                            )\n                    if len(interval) == 2:\n                        check_interval(interval=interval)\n                    else:\n                        for q in interval:\n                            if (q &lt; 0.) or (q &gt; 100.):\n                                raise ValueError(\n                                    \"When `interval` is a list or tuple, all values must be \"\n                                    \"between 0 and 100 inclusive.\"\n                                )\n                elif isinstance(interval, str):\n                    if interval != 'bootstrapping':\n                        raise ValueError(\n                            f\"When `interval` is a string, it must be 'bootstrapping'.\"\n                            f\"Got {interval}.\"\n                        )\n            else:\n                raise ValueError(\n                    f\"`interval_method` must be 'bootstrapping' or 'conformal'. \"\n                    f\"Got {interval_method}.\"\n                )\n        else:\n            check_interval(interval=interval, alpha=alpha)\n\n    if return_predictors and forecaster_name not in forecasters_return_predictors:\n        raise ValueError(\n            f\"`return_predictors` is only allowed for forecasters of type \"\n            f\"{forecasters_return_predictors}. Got {forecaster_name}.\"\n        )\n\n    if (\n        not allow_incomplete_fold\n        and initial_train_size is not None\n        and data_length - (initial_train_size + gap) &lt; steps\n    ):        \n        raise ValueError(\n            f\"There is not enough data to evaluate {steps} steps in a single \"\n            f\"fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n\"\n            f\"    Data available for test : {data_length - (initial_train_size + gap)}\\n\"\n            f\"    Steps                   : {steps}\"\n        )\n\n    if forecaster_name in forecasters_direct and forecaster.max_step &lt; steps + gap:\n        raise ValueError(\n            f\"When using a {forecaster_name}, the combination of steps \"\n            f\"+ gap ({steps + gap}) cannot be greater than the `steps` parameter \"\n            f\"declared when the forecaster is initialized ({forecaster.max_step}).\"\n        )\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._utils.check_one_step_ahead_input","title":"skforecast.model_selection._utils.check_one_step_ahead_input","text":"<pre><code>check_one_step_ahead_input(\n    forecaster,\n    cv,\n    metric,\n    y=None,\n    series=None,\n    exog=None,\n    show_progress=True,\n    suppress_warnings=False,\n)\n</code></pre> <p>This is a helper function to check most inputs of hyperparameter tuning functions in modules <code>model_selection</code> when using a <code>OneStepAheadFold</code>.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>Forecaster</code> <p>Forecaster model.</p> required <code>cv</code> <code>OneStepAheadFold</code> <p>OneStepAheadFold object with the information needed to split the data into folds.</p> required <code>metric</code> <code>(str, Callable, list)</code> <p>Metric used to quantify the goodness of fit of the model.</p> required <code>y</code> <code>pandas Series</code> <p>Training time series for uni-series forecasters.</p> <code>None</code> <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series for multi-series forecasters.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variables.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed during the hyperparameter  search. See skforecast.exceptions.warn_skforecast_categories for more information.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\model_selection\\_utils.py</code> <pre><code>def check_one_step_ahead_input(\n    forecaster: object,\n    cv: object,\n    metric: str | Callable | list[str | Callable],\n    y: pd.Series | None = None,\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame] = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    show_progress: bool = True,\n    suppress_warnings: bool = False\n) -&gt; None:\n    \"\"\"\n    This is a helper function to check most inputs of hyperparameter tuning\n    functions in modules `model_selection` when using a `OneStepAheadFold`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : OneStepAheadFold\n        OneStepAheadFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    y : pandas Series, default None\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default None\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variables.\n    show_progress : bool, default True\n        Whether to show a progress bar.\n    suppress_warnings: bool, default False\n        If `True`, skforecast warnings will be suppressed during the hyperparameter \n        search. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"OneStepAheadFold\":\n        raise TypeError(f\"`cv` must be a 'OneStepAheadFold' object. Got '{cv_name}'.\")\n\n    initial_train_size = cv.initial_train_size\n\n    forecasters_one_step_ahead = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        'ForecasterRecursiveMultiSeries',\n        'ForecasterDirectMultiVariate'\n    ]\n    if forecaster_name not in forecasters_one_step_ahead:\n        raise TypeError(\n            f\"Only forecasters of type {forecasters_one_step_ahead} are allowed \"\n            f\"when using `cv` of type `OneStepAheadFold`. Got {forecaster_name}.\"\n        )\n\n    forecasters_uni = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n    ]\n    forecasters_multi_no_dict = [\n        \"ForecasterDirectMultiVariate\",\n    ]\n    forecasters_multi_dict = [\n        \"ForecasterRecursiveMultiSeries\"\n    ]\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(f\"`y` must be a pandas Series. Got {type(y)}\")\n        data_name = 'y'\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi_no_dict:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(f\"`series` must be a pandas DataFrame. Got {type(series)}\")\n        data_name = 'series'\n        data_length = len(series)\n\n    elif forecaster_name in forecasters_multi_dict:\n\n        # NOTE: Checks are not need as they are done in the function \n        # `check_preprocess_series` that is used before `check_one_step_ahead_input`\n        # in the backtesting function.\n\n        data_name = 'series'\n        data_length = max([len(series[serie]) for serie in series])\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            # NOTE: Checks are not need as they are done in the function \n            # `check_preprocess_exog_multiseries` that is used before \n            # `check_backtesting_input` in the backtesting function.\n            pass\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\"\n                )\n\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation_max != cv.differentiation:\n            if forecaster_name == \"ForecasterRecursiveMultiSeries\" and isinstance(\n                forecaster.differentiation, dict\n            ):\n                raise ValueError(\n                    f\"When using a dict as `differentiation` in ForecasterRecursiveMultiSeries, \"\n                    f\"the `differentiation` included in the cv ({cv.differentiation}) must be \"\n                    f\"the same as the maximum `differentiation` included in the forecaster \"\n                    f\"({forecaster.differentiation_max}). Set the same value \"\n                    f\"for both using the `differentiation` argument.\"\n                )\n            else:\n                raise ValueError(\n                    f\"The differentiation included in the forecaster \"\n                    f\"({forecaster.differentiation_max}) differs from the differentiation \"\n                    f\"included in the cv ({cv.differentiation}). Set the same value \"\n                    f\"for both using the `differentiation` argument.\"\n                )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            f\"`metric` must be a string, a callable function, or a list containing \"\n            f\"multiple strings and/or callables. Got {type(metric)}.\"\n        )\n\n    if forecaster_name in forecasters_uni:\n        index = cv._extract_index(y)\n    else:\n        index = cv._extract_index(series)\n\n    initial_train_size = date_to_index_position(\n                             index        = index, \n                             date_input   = initial_train_size, \n                             method       = 'validation',\n                             date_literal = 'initial_train_size'\n                         )\n    if initial_train_size &lt; forecaster.window_size or initial_train_size &gt;= data_length:\n        raise ValueError(\n            f\"If `initial_train_size` is an integer, it must be greater than \"\n            f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n            f\"and smaller than the length of `{data_name}` ({data_length}). If \"\n            f\"it is a date, it must be within this range of the index.\"\n        )\n\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n\n    if not suppress_warnings:\n        warnings.warn(\n            \"One-step-ahead predictions are used for faster model comparison, but they \"\n            \"may not fully represent multi-step prediction performance. It is recommended \"\n            \"to backtest the final model for a more accurate multi-step performance \"\n            \"estimate.\", OneStepAheadValidationWarning\n        )\n</code></pre>"},{"location":"api/model_selection.html#skforecast.model_selection._utils.select_n_jobs_backtesting","title":"skforecast.model_selection._utils.select_n_jobs_backtesting","text":"<pre><code>select_n_jobs_backtesting(forecaster, refit)\n</code></pre> <p>Select the optimal number of jobs to use in the backtesting process. This selection is based on heuristics and is not guaranteed to be optimal.</p> <p>The number of jobs is chosen as follows:</p> <ul> <li>If <code>refit</code> is an integer, then <code>n_jobs = 1</code>. This is because parallelization doesn't  work with intermittent refit.</li> <li>If forecaster is 'ForecasterRecursive' and regressor is a linear regressor,  then <code>n_jobs = 1</code>.</li> <li>If forecaster is 'ForecasterRecursive' and regressor is not a linear  regressor then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate' and <code>refit = True</code>, then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate' and <code>refit = False</code>, then <code>n_jobs = 1</code>.</li> <li>If forecaster is 'ForecasterRecursiveMultiSeries', then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate',  then <code>n_jobs = 1</code>.</li> <li>If regressor is a <code>LGBMRegressor(n_jobs=1)</code>, then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If regressor is a <code>LGBMRegressor</code> with internal n_jobs != 1, then <code>n_jobs = 1</code>. This is because <code>lightgbm</code> is highly optimized for gradient boosting and parallelizes operations at a very fine-grained level, making additional parallelization unnecessary and potentially harmful due to resource contention.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>Forecaster</code> <p>Forecaster model.</p> required <code>refit</code> <code>(bool, int)</code> <p>If the forecaster is refitted during the backtesting process.</p> required <p>Returns:</p> Name Type Description <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> Source code in <code>skforecast\\model_selection\\_utils.py</code> <pre><code>def select_n_jobs_backtesting(\n    forecaster: object,\n    refit: bool | int\n) -&gt; int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n\n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    else:\n        if forecaster_name in ['ForecasterRecursive']:\n            if regressor_name in linear_regressors:\n                n_jobs = 1\n            elif regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n            # Parallelization is applied during the fitting process.\n            n_jobs = 1\n        elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            n_jobs = 1\n        else:\n            n_jobs = 1\n\n    return n_jobs\n</code></pre>"},{"location":"api/plot.html","title":"<code>plot</code>","text":""},{"location":"api/plot.html#skforecast.plot.plot.set_dark_theme","title":"skforecast.plot.plot.set_dark_theme","text":"<pre><code>set_dark_theme(custom_style=None)\n</code></pre> <p>Set aspects of the visual theme for all matplotlib plots. This function changes the global defaults for all plots using the matplotlib rcParams system. The theme includes specific colors for figure and axes backgrounds, gridlines, text, labels, and ticks. It also sets the font size and line width.</p> <p>Parameters:</p> Name Type Description Default <code>custom_style</code> <code>dict</code> <p>Optional dictionary containing custom styles to be added or override the default dark theme. It is applied after the default theme is set by using the <code>plt.rcParams.update()</code> method.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\plot\\plot.py</code> <pre><code>def set_dark_theme(\n    custom_style: dict | None = None\n) -&gt; None:\n    \"\"\"\n    Set aspects of the visual theme for all matplotlib plots.\n    This function changes the global defaults for all plots using the matplotlib\n    rcParams system. The theme includes specific colors for figure and axes\n    backgrounds, gridlines, text, labels, and ticks. It also sets the font size\n    and line width.\n\n    Parameters\n    ----------\n    custom_style : dict, default None\n        Optional dictionary containing custom styles to be added or override the\n        default dark theme. It is applied after the default theme is set by\n        using the `plt.rcParams.update()` method.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    plt.style.use('fivethirtyeight')\n    dark_style = {\n        'figure.facecolor': '#001633',\n        'axes.facecolor': '#001633',\n        'savefig.facecolor': '#001633',\n        'axes.grid': True,\n        'axes.grid.which': 'both',\n        'axes.spines.left': False,\n        'axes.spines.right': False,\n        'axes.spines.top': False,\n        'axes.spines.bottom': False,\n        'grid.color': '#212946',\n        'grid.linewidth': '1',\n        'text.color': '0.9',\n        'axes.labelcolor': '0.9',\n        'xtick.color': '0.9',\n        'ytick.color': '0.9',\n        'font.size': 10,\n        'lines.linewidth': 1.5\n    }\n\n    if custom_style is not None:\n        dark_style.update(custom_style)\n\n    plt.rcParams.update(dark_style)\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_residuals","title":"skforecast.plot.plot.plot_residuals","text":"<pre><code>plot_residuals(\n    residuals=None,\n    y_true=None,\n    y_pred=None,\n    fig=None,\n    **fig_kw\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>residuals</code> <code>pandas Series, numpy ndarray</code> <p>Values of residuals. If <code>None</code>, residuals are calculated internally using <code>y_true</code> and <code>y_true</code>.</p> <code>None.</code> <code>y_true</code> <code>pandas Series, numpy ndarray</code> <p>Ground truth (correct) values. Ignored if residuals is not <code>None</code>.</p> <code>None.</code> <code>y_pred</code> <code>pandas Series, numpy ndarray</code> <p>Values of predictions. Ignored if residuals is not <code>None</code>.</p> <code>None. </code> <code>fig</code> <code>Figure</code> <p>Pre-existing fig for the plot. Otherwise, call matplotlib.pyplot.figure() internally.</p> <code>None. </code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.figure()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast\\plot\\plot.py</code> <pre><code>def plot_residuals(\n    residuals: np.ndarray | pd.Series | None = None,\n    y_true: np.ndarray | pd.Series | None = None,\n    y_pred: np.ndarray | pd.Series | None = None,\n    fig: matplotlib.figure.Figure | None = None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Parameters\n    ----------\n    residuals : pandas Series, numpy ndarray, default None.\n        Values of residuals. If `None`, residuals are calculated internally using\n        `y_true` and `y_true`.\n    y_true : pandas Series, numpy ndarray, default None.\n        Ground truth (correct) values. Ignored if residuals is not `None`.\n    y_pred : pandas Series, numpy ndarray, default None. \n        Values of predictions. Ignored if residuals is not `None`.\n    fig : matplotlib.figure.Figure, default None. \n        Pre-existing fig for the plot. Otherwise, call matplotlib.pyplot.figure()\n        internally.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.figure()\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if residuals is None and (y_true is None or y_pred is None):\n        raise ValueError(\n            \"If `residuals` argument is None then, `y_true` and `y_pred` must be provided.\"\n        )\n\n    if residuals is None:\n        residuals = y_true - y_pred\n\n    if fig is None:\n        fig = plt.figure(constrained_layout=True, **fig_kw)\n\n    gs  = matplotlib.gridspec.GridSpec(2, 2, figure=fig)\n    ax1 = plt.subplot(gs[0, :])\n    ax2 = plt.subplot(gs[1, 0])\n    ax3 = plt.subplot(gs[1, 1])\n\n    ax1.plot(residuals)\n    sns.histplot(residuals, kde=True, bins=30, ax=ax2)\n    plot_acf(residuals, ax=ax3, lags=60)\n\n    ax1.set_title(\"Residuals\")\n    ax2.set_title(\"Distribution\")\n    ax3.set_title(\"Autocorrelation\")\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.calculate_lag_autocorrelation","title":"skforecast.plot.plot.calculate_lag_autocorrelation","text":"<pre><code>calculate_lag_autocorrelation(\n    data,\n    n_lags=50,\n    last_n_samples=None,\n    sort_by=\"partial_autocorrelation_abs\",\n    acf_kwargs={},\n    pacf_kwargs={},\n)\n</code></pre> <p>Calculate autocorrelation and partial autocorrelation for a time series. This is a wrapper around statsmodels.acf [1]_ and statsmodels.pacf [2]_.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pandas Series, pandas DataFrame</code> <p>Time series to calculate autocorrelation. If a DataFrame is provided, it must have exactly one column.</p> required <code>n_lags</code> <code>int</code> <p>Number of lags to calculate autocorrelation.</p> <code>50</code> <code>last_n_samples</code> <code>int or None</code> <p>Number of most recent samples to use. If None, use the entire series.  Note that partial correlations can only be computed for lags up to  50% of the sample size. For example, if the series has 10 samples,  <code>n_lags</code> must be less than or equal to 5. This parameter is useful to speed up calculations when the series is very long.</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>Sort results by 'lag', 'partial_autocorrelation_abs',  'partial_autocorrelation', 'autocorrelation_abs' or 'autocorrelation'.</p> <code>'partial_autocorrelation_abs'</code> <code>acf_kwargs</code> <code>dict</code> <p>Optional arguments to pass to statsmodels.tsa.stattools.acf.</p> <code>{}</code> <code>pacf_kwargs</code> <code>dict</code> <p>Optional arguments to pass to statsmodels.tsa.stattools.pacf.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>results</code> <code>pandas DataFrame</code> <p>Autocorrelation and partial autocorrelation values.</p> References <p>.. [1] Statsmodels acf API Reference.        https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html</p> <p>.. [2] Statsmodels pacf API Reference.        https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html</p> <p>Examples:</p> <pre><code>import pandas as pd\nfrom skforecast.plot import calculate_lag_autocorrelation\n\ndata = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ncalculate_lag_autocorrelation(data = data, n_lags = 4)\n\n#    lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n# 0    1                     0.777778                 0.777778             0.700000         0.700000\n# 1    4                     0.360707                -0.360707             0.078788        -0.078788\n# 2    3                     0.274510                -0.274510             0.148485         0.148485\n# 3    2                     0.227273                -0.227273             0.412121         0.412121\n</code></pre> Source code in <code>skforecast\\plot\\plot.py</code> <pre><code>def calculate_lag_autocorrelation(\n    data: pd.Series | pd.DataFrame,\n    n_lags: int = 50,\n    last_n_samples: int | None = None,\n    sort_by: str = \"partial_autocorrelation_abs\",\n    acf_kwargs: dict[str, object] = {},\n    pacf_kwargs: dict[str, object] = {},\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate autocorrelation and partial autocorrelation for a time series.\n    This is a wrapper around statsmodels.acf [1]_ and statsmodels.pacf [2]_.\n\n    Parameters\n    ----------\n    data : pandas Series, pandas DataFrame\n        Time series to calculate autocorrelation. If a DataFrame is provided,\n        it must have exactly one column.\n    n_lags : int\n        Number of lags to calculate autocorrelation.\n    last_n_samples : int or None, default None\n        Number of most recent samples to use. If None, use the entire series. \n        Note that partial correlations can only be computed for lags up to \n        50% of the sample size. For example, if the series has 10 samples, \n        `n_lags` must be less than or equal to 5. This parameter is useful\n        to speed up calculations when the series is very long.\n    sort_by : str, default 'partial_autocorrelation_abs'\n        Sort results by 'lag', 'partial_autocorrelation_abs', \n        'partial_autocorrelation', 'autocorrelation_abs' or 'autocorrelation'.\n    acf_kwargs : dict, default {}\n        Optional arguments to pass to statsmodels.tsa.stattools.acf.\n    pacf_kwargs : dict, default {}\n        Optional arguments to pass to statsmodels.tsa.stattools.pacf.\n\n    Returns\n    -------\n    results : pandas DataFrame\n        Autocorrelation and partial autocorrelation values.\n\n    References\n    ----------\n    .. [1] Statsmodels acf API Reference.\n           https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html\n\n    .. [2] Statsmodels pacf API Reference.\n           https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html\n\n    Examples\n    --------\n    ```python\n    import pandas as pd\n    from skforecast.plot import calculate_lag_autocorrelation\n\n    data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    calculate_lag_autocorrelation(data = data, n_lags = 4)\n\n    #    lag  partial_autocorrelation_abs  partial_autocorrelation  autocorrelation_abs  autocorrelation\n    # 0    1                     0.777778                 0.777778             0.700000         0.700000\n    # 1    4                     0.360707                -0.360707             0.078788        -0.078788\n    # 2    3                     0.274510                -0.274510             0.148485         0.148485\n    # 3    2                     0.227273                -0.227273             0.412121         0.412121\n    ```\n\n    \"\"\"\n\n    if not isinstance(data, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"`data` must be a pandas Series or a DataFrame with a single column. \"\n            f\"Got {type(data)}.\"\n        )\n    if isinstance(data, pd.DataFrame) and data.shape[1] != 1:\n        raise ValueError(\n            f\"If `data` is a DataFrame, it must have exactly one column. \"\n            f\"Got {data.shape[1]} columns.\"\n        )\n    if not isinstance(n_lags, int) or n_lags &lt;= 0:\n        raise TypeError(f\"`n_lags` must be a positive integer. Got {n_lags}.\")\n\n    if last_n_samples is not None:\n        if not isinstance(last_n_samples, int) or last_n_samples &lt;= 0:\n            raise TypeError(f\"`last_n_samples` must be a positive integer. Got {last_n_samples}.\")\n        data = data.iloc[-last_n_samples:]\n\n    if sort_by not in [\n        \"lag\", \"partial_autocorrelation_abs\", \"partial_autocorrelation\",\n        \"autocorrelation_abs\", \"autocorrelation\",\n    ]:\n        raise ValueError(\n            \"`sort_by` must be 'lag', 'partial_autocorrelation_abs', 'partial_autocorrelation', \"\n            \"'autocorrelation_abs' or 'autocorrelation'.\"\n        )\n\n    pacf_values = pacf(data, nlags=n_lags, **pacf_kwargs)\n    acf_values = acf(data, nlags=n_lags, **acf_kwargs)\n\n    results = pd.DataFrame(\n        {\n            \"lag\": range(n_lags + 1),\n            \"partial_autocorrelation_abs\": np.abs(pacf_values),\n            \"partial_autocorrelation\": pacf_values,\n            \"autocorrelation_abs\": np.abs(acf_values),\n            \"autocorrelation\": acf_values,\n        }\n    ).iloc[1:]\n\n    if sort_by == \"lag\":\n        results = results.sort_values(by=sort_by, ascending=True).reset_index(drop=True)\n    else:\n        results = results.sort_values(by=sort_by, ascending=False).reset_index(drop=True)\n\n    return results\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_multivariate_time_series_corr","title":"skforecast.plot.plot.plot_multivariate_time_series_corr","text":"<pre><code>plot_multivariate_time_series_corr(corr, ax=None, **fig_kw)\n</code></pre> <p>Heatmap plot of a correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>corr</code> <code>pandas DataFrame</code> <p>correlation matrix</p> required <code>ax</code> <code>Axes</code> <p>Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots()  internally.</p> <code>None</code> <code>fig_kw</code> <code>dict</code> <p>Other keyword arguments are passed to matplotlib.pyplot.subplots()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast\\plot\\plot.py</code> <pre><code>def plot_multivariate_time_series_corr(\n    corr: pd.DataFrame,\n    ax: matplotlib.axes.Axes | None = None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Heatmap plot of a correlation matrix.\n\n    Parameters\n    ----------\n    corr : pandas DataFrame\n        correlation matrix\n    ax : matplotlib.axes.Axes, default None\n        Pre-existing ax for the plot. Otherwise, call matplotlib.pyplot.subplots() \n        internally.\n    fig_kw : dict\n        Other keyword arguments are passed to matplotlib.pyplot.subplots()\n\n    Returns\n    -------\n    fig: matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, **fig_kw)\n\n    sns.heatmap(\n        corr,\n        annot=True,\n        linewidths=.5,\n        ax=ax,\n        cmap=sns.color_palette(\"viridis\", as_cmap=True)\n    )\n\n    ax.set_xlabel('Time series')\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_prediction_distribution","title":"skforecast.plot.plot.plot_prediction_distribution","text":"<pre><code>plot_prediction_distribution(\n    bootstrapping_predictions, bw_method=None, **fig_kw\n)\n</code></pre> <p>Ridge plot of bootstrapping predictions. This plot is very useful to understand  the uncertainty of forecasting predictions.</p> <p>Parameters:</p> Name Type Description Default <code>bootstrapping_predictions</code> <code>pandas DataFrame</code> <p>Bootstrapping predictions created with <code>Forecaster.predict_bootstrapping</code>.</p> required <code>bw_method</code> <code>(str, scalar, Callable)</code> <p>The method used to calculate the estimator bandwidth. This can be 'scott',  'silverman', a scalar constant or a Callable. If None (default), 'scott'  is used. See scipy.stats.gaussian_kde for more information.</p> <code>None</code> <code>fig_kw</code> <code>dict</code> <p>All additional keyword arguments are passed to the <code>pyplot.figure</code> call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib Figure.</p> Source code in <code>skforecast\\plot\\plot.py</code> <pre><code>def plot_prediction_distribution(\n    bootstrapping_predictions: pd.DataFrame,\n    bw_method: Any | None = None,\n    **fig_kw\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Ridge plot of bootstrapping predictions. This plot is very useful to understand \n    the uncertainty of forecasting predictions.\n\n    Parameters\n    ----------\n    bootstrapping_predictions : pandas DataFrame\n        Bootstrapping predictions created with `Forecaster.predict_bootstrapping`.\n    bw_method : str, scalar, Callable, default None\n        The method used to calculate the estimator bandwidth. This can be 'scott', \n        'silverman', a scalar constant or a Callable. If None (default), 'scott' \n        is used. See scipy.stats.gaussian_kde for more information.\n    fig_kw : dict\n        All additional keyword arguments are passed to the `pyplot.figure` call.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Matplotlib Figure.\n\n    \"\"\"\n\n    index = bootstrapping_predictions.index.astype(str).to_list()[::-1]\n    palette = sns.cubehelix_palette(len(index), rot=-.25, light=.7, reverse=False)\n    fig, axs = plt.subplots(len(index), 1, sharex=True, **fig_kw)\n    if not isinstance(axs, np.ndarray):\n        axs = np.array([axs])\n\n    for i, step in enumerate(index):\n        plot = (\n            bootstrapping_predictions.loc[step, :]\n            .plot.kde(ax=axs[i], bw_method=bw_method, lw=0.5)\n        )\n\n        # Fill density area\n        x = plot.get_children()[0]._x\n        y = plot.get_children()[0]._y\n        axs[i].fill_between(x, y, color=palette[i])\n        prediction_mean = bootstrapping_predictions.loc[step, :].mean()\n\n        # Closest point on x to the prediction mean\n        idx = np.abs(x - prediction_mean).argmin()\n        axs[i].vlines(x[idx], ymin=0, ymax=y[idx], linestyle=\"dashed\", color='w')\n\n        axs[i].spines['top'].set_visible(False)\n        axs[i].spines['right'].set_visible(False)\n        axs[i].spines['bottom'].set_visible(False)\n        axs[i].spines['left'].set_visible(False)\n        axs[i].set_yticklabels([])\n        axs[i].set_yticks([])\n        axs[i].set_ylabel(step, rotation='horizontal')\n        axs[i].set_xlabel('prediction')\n\n    fig.subplots_adjust(hspace=-0)\n    fig.suptitle('Forecasting distribution per step')\n\n    return fig\n</code></pre>"},{"location":"api/plot.html#skforecast.plot.plot.plot_prediction_intervals","title":"skforecast.plot.plot.plot_prediction_intervals","text":"<pre><code>plot_prediction_intervals(\n    predictions,\n    y_true,\n    target_variable,\n    initial_x_zoom=None,\n    title=None,\n    xaxis_title=None,\n    yaxis_title=None,\n    ax=None,\n    kwargs_subplots={\"figsize\": (7, 3)},\n    kwargs_fill_between={\n        \"color\": \"#444444\",\n        \"alpha\": 0.3,\n        \"zorder\": 1,\n    },\n)\n</code></pre> <p>Plot predicted intervals vs real values using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>pandas DataFrame</code> <p>Predicted values and intervals. Expected columns are 'pred', 'lower_bound' and 'upper_bound'.</p> required <code>y_true</code> <code>pandas Series, pandas DataFrame</code> <p>Real values of target variable.</p> required <code>target_variable</code> <code>str</code> <p>Name of target variable.</p> required <code>initial_x_zoom</code> <code>list</code> <p>Initial zoom of x-axis, by default None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title of the plot, by default None.</p> <code>None</code> <code>xaxis_title</code> <code>str</code> <p>Title of x-axis, by default None.</p> <code>None</code> <code>yaxis_title</code> <code>str</code> <p>Title of y-axis, by default None.</p> <code>None</code> <code>ax</code> <code>matplotlib axes</code> <p>Axes where to plot, by default None.</p> <code>None</code> <code>kwargs_subplots</code> <code>dict</code> <p>Additional keyword arguments (key, value mappings) to pass to <code>plt.subplots</code>.</p> <code>{'figsize': (7, 3)}</code> <code>kwargs_fill_between</code> <code>dict</code> <p>Additional keyword arguments (key, value mappings) to pass to <code>ax.fill_between</code>.</p> <code>{'color': '#444444', 'alpha': 0.3}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\plot\\plot.py</code> <pre><code>def plot_prediction_intervals(\n    predictions: pd.DataFrame,\n    y_true: pd.Series | pd.DataFrame,\n    target_variable: str,\n    initial_x_zoom: list[str] | None = None,\n    title: str | None = None,\n    xaxis_title: str | None = None,\n    yaxis_title: str | None = None,\n    ax: plt.Axes | None = None,\n    kwargs_subplots: dict[str, object] = {'figsize': (7, 3)},\n    kwargs_fill_between: dict[str, object] = {'color': '#444444', 'alpha': 0.3, 'zorder': 1}\n):\n    \"\"\"\n    Plot predicted intervals vs real values using matplotlib.\n\n    Parameters\n    ----------\n    predictions : pandas DataFrame\n        Predicted values and intervals. Expected columns are 'pred', 'lower_bound'\n        and 'upper_bound'.\n    y_true : pandas Series, pandas DataFrame\n        Real values of target variable.\n    target_variable : str\n        Name of target variable.\n    initial_x_zoom : list, default None\n        Initial zoom of x-axis, by default None.\n    title : str, default None\n        Title of the plot, by default None.\n    xaxis_title : str, default None\n        Title of x-axis, by default None.\n    yaxis_title : str, default None\n        Title of y-axis, by default None.\n    ax : matplotlib axes, default None\n        Axes where to plot, by default None.\n    kwargs_subplots : dict, default {'figsize': (7, 3)}\n        Additional keyword arguments (key, value mappings) to pass to `plt.subplots`.\n    kwargs_fill_between : dict, default {'color': '#444444', 'alpha': 0.3}\n        Additional keyword arguments (key, value mappings) to pass to `ax.fill_between`.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots(**kwargs_subplots)\n\n    if isinstance(y_true, pd.Series):\n        y_true = y_true.to_frame()\n\n    y_true.loc[predictions.index, target_variable].plot(ax=ax, label='real value')\n    predictions['pred'].plot(ax=ax, label='prediction')\n    ax.fill_between(\n        predictions.index,\n        predictions['lower_bound'],\n        predictions['upper_bound'],\n        label='prediction interval',\n        **kwargs_fill_between\n    )\n    ax.set_ylabel(yaxis_title)\n    ax.set_xlabel(xaxis_title)\n    ax.set_title(title)\n    ax.legend()\n\n    if initial_x_zoom is not None:\n        ax.set_xlim(initial_x_zoom)\n</code></pre>"},{"location":"api/preprocessing.html","title":"<code>preprocessing</code>","text":""},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures","title":"skforecast.preprocessing.preprocessing.RollingFeatures","text":"<pre><code>RollingFeatures(\n    stats,\n    window_sizes,\n    min_periods=None,\n    features_names=None,\n    fillna=None,\n    kwargs_stats={\"ewm\": {\"alpha\": 0.3}},\n)\n</code></pre> <p>This class computes rolling features. To avoid data leakage, the last point  in the window is excluded from calculations, ('closed': 'left' and  'center': False).</p> <p>Currently, the following statistics are supported: 'mean', 'std', 'min', 'max', 'sum', 'median', 'ratio_min_max', 'coef_variation', 'ewm'. For 'ewm', the alpha parameter can be set in the kwargs_stats dictionary, default is {'ewm': {'alpha': 0.3}}.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>(str, list)</code> <p>Statistics to compute over the rolling window. Can be a <code>string</code> or a <code>list</code>, and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max', 'sum', 'median', 'ratio_min_max', 'coef_variation', 'ewm'. For 'ewm', the alpha parameter can be set in the kwargs_stats dictionary, default is {'ewm': {'alpha': 0.3}}.</p> required <code>window_sizes</code> <code>(int, list)</code> <p>Size of the rolling window for each statistic. If an <code>int</code>, all stats share  the same window size. If a <code>list</code>, it should have the same length as stats.</p> required <code>min_periods</code> <code>(int, list)</code> <p>Minimum number of observations in window required to have a value.  Same as the <code>min_periods</code> argument of pandas rolling. If <code>None</code>,  defaults to <code>window_sizes</code>.</p> <code>None</code> <code>features_names</code> <code>list</code> <p>Names of the output features. If <code>None</code>, default names will be used in the  format 'roll_stat_window_size', for example 'roll_mean_7'.</p> <code>None</code> <code>fillna</code> <code>(str, float)</code> <p>Fill missing values in <code>transform_batch</code> method. Available  methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.</p> <code>None</code> <code>kwargs_stats</code> <code>dict</code> <p>Dictionary with additional arguments for the statistics. The keys are the statistic names and the values are dictionaries with the arguments for the corresponding statistic. For example, {'ewm': {'alpha': 0.3}}.</p> <code>{'ewm': {'alpha': 0.3}}</code> <p>Attributes:</p> Name Type Description <code>stats</code> <code>list</code> <p>Statistics to compute over the rolling window.</p> <code>n_stats</code> <code>int</code> <p>Number of statistics to compute.</p> <code>window_sizes</code> <code>list</code> <p>Size of the rolling window for each statistic.</p> <code>max_window_size</code> <code>int</code> <p>Maximum window size.</p> <code>min_periods</code> <code>list</code> <p>Minimum number of observations in window required to have a value.</p> <code>features_names</code> <code>list</code> <p>Names of the output features.</p> <code>fillna</code> <code>(str, float)</code> <p>Method to fill missing values in <code>transform_batch</code> method.</p> <code>unique_rolling_windows</code> <code>dict</code> <p>Dictionary containing unique rolling window parameters and the corresponding statistics.</p> <code>kwargs_stats</code> <code>dict</code> <p>Dictionary with additional arguments for the statistics.</p> <p>Methods:</p> Name Description <code>transform_batch</code> <p>Transform an entire pandas Series using rolling windows and compute the </p> <code>transform</code> <p>Transform a numpy array using rolling windows and compute the </p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def __init__(\n    self, \n    stats: str | list[str],\n    window_sizes: int | list[int],\n    min_periods: int | list[int] | None = None,\n    features_names: list[str] | None = None, \n    fillna: str | float | None = None,\n    kwargs_stats: dict[str, dict[str, object]] | None = {'ewm': {'alpha': 0.3}}\n) -&gt; None:\n\n    self._validate_params(\n        stats,\n        window_sizes,\n        min_periods,\n        features_names,\n        fillna,\n        kwargs_stats\n    )\n\n    if isinstance(stats, str):\n        stats = [stats]\n    self.stats = stats\n    self.n_stats = len(stats)\n\n    if isinstance(window_sizes, int):\n        window_sizes = [window_sizes] * self.n_stats\n    self.window_sizes = window_sizes\n    self.max_window_size = max(window_sizes)\n\n    if min_periods is None:\n        min_periods = self.window_sizes\n    elif isinstance(min_periods, int):\n        min_periods = [min_periods] * self.n_stats\n    self.min_periods = min_periods\n\n    if features_names is None:\n        features_names = []\n        for stat, window_size in zip(self.stats, self.window_sizes):\n            if stat not in kwargs_stats:\n                features_names.append(f\"roll_{stat}_{window_size}\")\n            else:\n                kwargs_suffix = \"_\".join([f\"{k}_{v}\" for k, v in kwargs_stats[stat].items()])\n                features_names.append(f\"roll_{stat}_{window_size}_{kwargs_suffix}\")\n    self.features_names = features_names\n\n    self.fillna = fillna\n    self.kwargs_stats = kwargs_stats if kwargs_stats is not None else {}\n\n    window_params_list = []\n    for i in range(len(self.stats)):\n        window_params = (self.window_sizes[i], self.min_periods[i])\n        window_params_list.append(window_params)\n\n    # Find unique window parameter combinations\n    unique_rolling_windows = {}\n    for i, params in enumerate(window_params_list):\n        key = f\"{params[0]}_{params[1]}\"\n        if key not in unique_rolling_windows:\n            unique_rolling_windows[key] = {\n                'params': {\n                    'window': params[0], \n                    'min_periods': params[1], \n                    'center': False,\n                    'closed': 'left'\n                },\n                'stats_idx': [], \n                'stats_names': [], \n                'rolling_obj': None\n            }\n        unique_rolling_windows[key]['stats_idx'].append(i)\n        unique_rolling_windows[key]['stats_names'].append(self.features_names[i])\n\n    self.unique_rolling_windows = unique_rolling_windows\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.stats","title":"stats  <code>instance-attribute</code>","text":"<pre><code>stats = stats\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.n_stats","title":"n_stats  <code>instance-attribute</code>","text":"<pre><code>n_stats = len(stats)\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.window_sizes","title":"window_sizes  <code>instance-attribute</code>","text":"<pre><code>window_sizes = window_sizes\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.max_window_size","title":"max_window_size  <code>instance-attribute</code>","text":"<pre><code>max_window_size = max(window_sizes)\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.min_periods","title":"min_periods  <code>instance-attribute</code>","text":"<pre><code>min_periods = min_periods\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.features_names","title":"features_names  <code>instance-attribute</code>","text":"<pre><code>features_names = features_names\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.fillna","title":"fillna  <code>instance-attribute</code>","text":"<pre><code>fillna = fillna\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.kwargs_stats","title":"kwargs_stats  <code>instance-attribute</code>","text":"<pre><code>kwargs_stats = (\n    kwargs_stats if kwargs_stats is not None else {}\n)\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.unique_rolling_windows","title":"unique_rolling_windows  <code>instance-attribute</code>","text":"<pre><code>unique_rolling_windows = unique_rolling_windows\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def _repr_html_(self) -&gt; str:\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    style, unique_id = get_style_repr_html()\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Stats:&lt;/strong&gt; {self.stats}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Window size:&lt;/strong&gt; {self.window_sizes}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Maximum window size:&lt;/strong&gt; {self.max_window_size}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Minimum periods:&lt;/strong&gt; {self.min_periods}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Features names:&lt;/strong&gt; {self.features_names}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Fill na strategy:&lt;/strong&gt; {self.fillna}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Kwargs stats:&lt;/strong&gt; {self.kwargs_stats}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/window-features-and-custom-features.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return style + content\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures._validate_params","title":"_validate_params","text":"<pre><code>_validate_params(\n    stats,\n    window_sizes,\n    min_periods=None,\n    features_names=None,\n    fillna=None,\n    kwargs_stats=None,\n)\n</code></pre> <p>Validate the parameters of the RollingFeatures class.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>(str, list)</code> <p>Statistics to compute over the rolling window. Can be a <code>string</code> or a <code>list</code>, and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max', 'sum', 'median', 'ratio_min_max', 'coef_variation', 'ewm'.</p> required <code>window_sizes</code> <code>(int, list)</code> <p>Size of the rolling window for each statistic. If an <code>int</code>, all stats share  the same window size. If a <code>list</code>, it should have the same length as stats.</p> required <code>min_periods</code> <code>(int, list)</code> <p>Minimum number of observations in window required to have a value.  Same as the <code>min_periods</code> argument of pandas rolling. If <code>None</code>,  defaults to <code>window_sizes</code>.</p> <code>None</code> <code>features_names</code> <code>list</code> <p>Names of the output features. If <code>None</code>, default names will be used in the  format 'roll_stat_window_size', for example 'roll_mean_7'.</p> <code>None</code> <code>fillna</code> <code>(str, float)</code> <p>Fill missing values in <code>transform_batch</code> method. Available  methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.</p> <code>None</code> <code>kwargs_stats</code> <code>dict</code> <p>Dictionary with additional arguments for the statistics. The keys are the statistic names and the values are dictionaries with the arguments for the corresponding statistic. For example, {'ewm': {'alpha': 0.3}}.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def _validate_params(\n    self, \n    stats: str | list[str], \n    window_sizes: int | list[int],\n    min_periods: int | list[int] | None = None,\n    features_names: list[str] | None = None, \n    fillna: str | float | None = None,\n    kwargs_stats: dict[str, dict[str, object]] | None = None\n) -&gt; None:\n    \"\"\"\n    Validate the parameters of the RollingFeatures class.\n\n    Parameters\n    ----------\n    stats : str, list\n        Statistics to compute over the rolling window. Can be a `string` or a `list`,\n        and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n        'sum', 'median', 'ratio_min_max', 'coef_variation', 'ewm'.\n    window_sizes : int, list\n        Size of the rolling window for each statistic. If an `int`, all stats share \n        the same window size. If a `list`, it should have the same length as stats.\n    min_periods : int, list, default None\n        Minimum number of observations in window required to have a value. \n        Same as the `min_periods` argument of pandas rolling. If `None`, \n        defaults to `window_sizes`.\n    features_names : list, default None\n        Names of the output features. If `None`, default names will be used in the \n        format 'roll_stat_window_size', for example 'roll_mean_7'.\n    fillna : str, float, default None\n        Fill missing values in `transform_batch` method. Available \n        methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n    kwargs_stats : dict, default None\n        Dictionary with additional arguments for the statistics. The keys are the\n        statistic names and the values are dictionaries with the arguments for the\n        corresponding statistic. For example, {'ewm': {'alpha': 0.3}}.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    # stats\n    allowed_stats = [\n        'mean', 'std', 'min', 'max', 'sum', 'median', 'ratio_min_max', \n        'coef_variation', 'ewm'\n    ]\n\n    if not isinstance(stats, (str, list)):\n        raise TypeError(\n            f\"`stats` must be a string or a list of strings. Got {type(stats)}.\"\n        )        \n    if isinstance(stats, str):\n        stats = [stats]\n\n    for stat in set(stats):\n        if stat not in allowed_stats:\n            raise ValueError(\n                f\"Statistic '{stat}' is not allowed. Allowed stats are: {allowed_stats}.\"\n            )\n    n_stats = len(stats)\n\n    # window_sizes\n    if not isinstance(window_sizes, (int, list)):\n        raise TypeError(\n            f\"`window_sizes` must be an int or a list of ints. Got {type(window_sizes)}.\"\n        )\n\n    if isinstance(window_sizes, list):\n        n_window_sizes = len(window_sizes)\n        if n_window_sizes != n_stats:\n            raise ValueError(\n                f\"Length of `window_sizes` list ({n_window_sizes}) \"\n                f\"must match length of `stats` list ({n_stats}).\"\n            )\n\n    # Check duplicates (stats, window_sizes)\n    if isinstance(window_sizes, int):\n        window_sizes = [window_sizes] * n_stats\n    if len(set(zip(stats, window_sizes))) != n_stats:\n        raise ValueError(\n            f\"Duplicate (stat, window_size) pairs are not allowed.\\n\"\n            f\"    `stats`       : {stats}\\n\"\n            f\"    `window_sizes : {window_sizes}\"\n        )\n\n    # min_periods\n    if not isinstance(min_periods, (int, list, type(None))):\n        raise TypeError(\n            f\"`min_periods` must be an int, list of ints, or None. Got {type(min_periods)}.\"\n        )\n\n    if min_periods is not None:\n        if isinstance(min_periods, int):\n            min_periods = [min_periods] * n_stats\n        elif isinstance(min_periods, list):\n            n_min_periods = len(min_periods)\n            if n_min_periods != n_stats:\n                raise ValueError(\n                    f\"Length of `min_periods` list ({n_min_periods}) \"\n                    f\"must match length of `stats` list ({n_stats}).\"\n                )\n\n        for i, min_period in enumerate(min_periods):\n            if min_period &gt; window_sizes[i]:\n                raise ValueError(\n                    \"Each `min_period` must be less than or equal to its \"\n                    \"corresponding `window_size`.\"\n                )\n\n    # features_names\n    if not isinstance(features_names, (list, type(None))):\n        raise TypeError(\n            f\"`features_names` must be a list of strings or None. Got {type(features_names)}.\"\n        )\n\n    if isinstance(features_names, list):\n        n_features_names = len(features_names)\n        if n_features_names != n_stats:\n            raise ValueError(\n                f\"Length of `features_names` list ({n_features_names}) \"\n                f\"must match length of `stats` list ({n_stats}).\"\n            )\n\n    # fillna\n    if fillna is not None:\n        if not isinstance(fillna, (int, float, str)):\n            raise TypeError(\n                f\"`fillna` must be a float, string, or None. Got {type(fillna)}.\"\n            )\n\n        if isinstance(fillna, str):\n            allowed_fill_strategy = ['mean', 'median', 'ffill', 'bfill']\n            if fillna not in allowed_fill_strategy:\n                raise ValueError(\n                    f\"'{fillna}' is not allowed. Allowed `fillna` \"\n                    f\"values are: {allowed_fill_strategy} or a float value.\"\n                )\n\n    # kwargs_stats\n    allowed_kwargs_stats = ['ewm']\n    if kwargs_stats is not None:\n        if not isinstance(kwargs_stats, dict):\n            raise TypeError(\n                f\"`kwargs_stats` must be a dictionary or None. Got {type(kwargs_stats)}.\"\n            )\n\n        for stat in kwargs_stats.keys():\n            if stat not in allowed_kwargs_stats:\n                raise ValueError(\n                    f\"Invalid statistic '{stat}' found in `kwargs_stats`. \"\n                    f\"Allowed statistics with additional arguments are: \"\n                    f\"{allowed_kwargs_stats}. Please ensure all keys in \"\n                    f\"`kwargs_stats` are among the allowed statistics.\"\n                )\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures._apply_stat_pandas","title":"_apply_stat_pandas","text":"<pre><code>_apply_stat_pandas(rolling_obj, stat)\n</code></pre> <p>Apply the specified statistic to a pandas rolling object.</p> <p>Parameters:</p> Name Type Description Default <code>rolling_obj</code> <code>pandas Rolling</code> <p>Rolling object to apply the statistic.</p> required <code>stat</code> <code>str</code> <p>Statistic to compute.</p> required <p>Returns:</p> Name Type Description <code>stat_series</code> <code>pandas Series</code> <p>Series with the computed statistic.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def _apply_stat_pandas(\n    self, \n    rolling_obj: pd.core.window.rolling.Rolling, \n    stat: str\n) -&gt; pd.Series:\n    \"\"\"\n    Apply the specified statistic to a pandas rolling object.\n\n    Parameters\n    ----------\n    rolling_obj : pandas Rolling\n        Rolling object to apply the statistic.\n    stat : str\n        Statistic to compute.\n\n    Returns\n    -------\n    stat_series : pandas Series\n        Series with the computed statistic.\n\n    \"\"\"\n\n    if stat == 'mean':\n        return rolling_obj.mean()\n    elif stat == 'std':\n        return rolling_obj.std()\n    elif stat == 'min':\n        return rolling_obj.min()\n    elif stat == 'max':\n        return rolling_obj.max()\n    elif stat == 'sum':\n        return rolling_obj.sum()\n    elif stat == 'median':\n        return rolling_obj.median()\n    elif stat == 'ratio_min_max':\n        return rolling_obj.min() / rolling_obj.max()\n    elif stat == 'coef_variation':\n        return rolling_obj.std() / rolling_obj.mean()\n    elif stat == 'ewm':\n        kwargs = self.kwargs_stats.get(stat, {})\n        return rolling_obj.apply(lambda x: _ewm_jit(x.to_numpy(), **kwargs))\n    else:\n        raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.transform_batch","title":"transform_batch","text":"<pre><code>transform_batch(X)\n</code></pre> <p>Transform an entire pandas Series using rolling windows and compute the  specified statistics.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas Series</code> <p>The input data series to transform.</p> required <p>Returns:</p> Name Type Description <code>rolling_features</code> <code>pandas DataFrame</code> <p>A DataFrame containing the rolling features.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def transform_batch(\n    self, \n    X: pd.Series\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform an entire pandas Series using rolling windows and compute the \n    specified statistics.\n\n    Parameters\n    ----------\n    X : pandas Series\n        The input data series to transform.\n\n    Returns\n    -------\n    rolling_features : pandas DataFrame\n        A DataFrame containing the rolling features.\n\n    \"\"\"\n\n    for k in self.unique_rolling_windows.keys():\n        rolling_obj = X.rolling(**self.unique_rolling_windows[k]['params'])\n        self.unique_rolling_windows[k]['rolling_obj'] = rolling_obj\n\n    rolling_features = []\n    for i, stat in enumerate(self.stats):\n        window_size = self.window_sizes[i]\n        min_periods = self.min_periods[i]\n\n        key = f\"{window_size}_{min_periods}\"\n        rolling_obj = self.unique_rolling_windows[key]['rolling_obj']\n\n        stat_series = self._apply_stat_pandas(rolling_obj=rolling_obj, stat=stat)            \n        rolling_features.append(stat_series)\n\n    rolling_features = pd.concat(rolling_features, axis=1)\n    rolling_features.columns = self.features_names\n    rolling_features = rolling_features.iloc[self.max_window_size:]\n\n    if self.fillna is not None:\n        if self.fillna == 'mean':\n            rolling_features = rolling_features.fillna(rolling_features.mean())\n        elif self.fillna == 'median':\n            rolling_features = rolling_features.fillna(rolling_features.median())\n        elif self.fillna == 'ffill':\n            rolling_features = rolling_features.ffill()\n        elif self.fillna == 'bfill':\n            rolling_features = rolling_features.bfill()\n        else:\n            rolling_features = rolling_features.fillna(self.fillna)\n\n    return rolling_features\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures._apply_stat_numpy_jit","title":"_apply_stat_numpy_jit","text":"<pre><code>_apply_stat_numpy_jit(X_window, stat)\n</code></pre> <p>Apply the specified statistic to a numpy array using Numba JIT.</p> <p>Parameters:</p> Name Type Description Default <code>X_window</code> <code>numpy array</code> <p>Array with the rolling window.</p> required <code>stat</code> <code>str</code> <p>Statistic to compute.</p> required <p>Returns:</p> Name Type Description <code>stat_value</code> <code>float</code> <p>Value of the computed statistic.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def _apply_stat_numpy_jit(\n    self, \n    X_window: np.ndarray, \n    stat: str\n) -&gt; float:\n    \"\"\"\n    Apply the specified statistic to a numpy array using Numba JIT.\n\n    Parameters\n    ----------\n    X_window : numpy array\n        Array with the rolling window.\n    stat : str\n        Statistic to compute.\n\n    Returns\n    -------\n    stat_value : float\n        Value of the computed statistic.\n\n    \"\"\"\n\n    if stat == 'mean':\n        return _np_mean_jit(X_window)\n    elif stat == 'std':\n        return _np_std_jit(X_window)\n    elif stat == 'min':\n        return _np_min_jit(X_window)\n    elif stat == 'max':\n        return _np_max_jit(X_window)\n    elif stat == 'sum':\n        return _np_sum_jit(X_window)\n    elif stat == 'median':\n        return _np_median_jit(X_window)\n    elif stat == 'ratio_min_max':\n        return _np_min_max_ratio_jit(X_window)\n    elif stat == 'coef_variation':\n        return _np_cv_jit(X_window)\n    elif stat == 'ewm':\n        kwargs = self.kwargs_stats.get(stat, {})\n        return _ewm_jit(X_window, **kwargs)\n    else:\n        raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.RollingFeatures.transform","title":"transform","text":"<pre><code>transform(X)\n</code></pre> <p>Transform a numpy array using rolling windows and compute the  specified statistics. The returned array will have the shape  (X.shape[1] if exists, n_stats). For example, if X is a flat array, the output will have shape (n_stats,). If X is a 2D array, the output will have shape (X.shape[1], n_stats).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>The input data array to transform.</p> required <p>Returns:</p> Name Type Description <code>rolling_features</code> <code>numpy ndarray</code> <p>An array containing the computed statistics.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def transform(\n    self, \n    X: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"\n    Transform a numpy array using rolling windows and compute the \n    specified statistics. The returned array will have the shape \n    (X.shape[1] if exists, n_stats). For example, if X is a flat\n    array, the output will have shape (n_stats,). If X is a 2D array,\n    the output will have shape (X.shape[1], n_stats).\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        The input data array to transform.\n\n    Returns\n    -------\n    rolling_features : numpy ndarray\n        An array containing the computed statistics.\n\n    \"\"\"\n\n    array_ndim = X.ndim\n    if array_ndim == 1:\n        X = X[:, np.newaxis]\n\n    rolling_features = np.full(\n        shape=(X.shape[1], self.n_stats), fill_value=np.nan, dtype=float\n    )\n    for i in range(X.shape[1]):\n        for j, stat in enumerate(self.stats):\n            X_window = X[-self.window_sizes[j]:, i]\n            X_window = X_window[~np.isnan(X_window)]\n            if len(X_window) &gt; 0: \n                rolling_features[i, j] = self._apply_stat_numpy_jit(X_window, stat)\n            else:\n                rolling_features[i, j] = np.nan\n\n    if array_ndim == 1:\n        rolling_features = rolling_features.ravel()\n\n    return rolling_features\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.reshape_series_wide_to_long","title":"skforecast.preprocessing.preprocessing.reshape_series_wide_to_long","text":"<pre><code>reshape_series_wide_to_long(data, return_multi_index=True)\n</code></pre> <p>Convert a pandas DataFrame where each column represents a different time series into a long format DataFrame with a MultiIndex. The index of the input DataFrame must be a pandas DatetimeIndex with a defined frequency. The function reshapes the DataFrame from wide format to long format, where each row corresponds to a specific time point and series ID. The resulting DataFrame will have a MultiIndex with the series IDs as the first level and a pandas DatetimeIndex as the second level. If <code>return_multi_index</code> is set to False, the returned DataFrame have three columns: 'series_id', 'datetime' and 'value', with a regular index.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Wide format series. The index must be a pandas DatetimeIndex with a  defined frequency and each column must represent a different time series.</p> required <code>return_multi_index</code> <code>bool</code> <p>If True, the returned DataFrame will have a MultiIndex with the series IDs as the first level and a pandas DatetimeIndex as the second level. If False, the returned DataFrame will have a regular index.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>data</code> <code>pandas DataFrame</code> <p>Long format series with a MultiIndex. The first level contains the series IDs, and the second level contains a pandas DatetimeIndex with the same frequency for each series.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def reshape_series_wide_to_long(\n    data: pd.DataFrame,\n    return_multi_index: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a pandas DataFrame where each column represents a different time series\n    into a long format DataFrame with a MultiIndex. The index of the input DataFrame\n    must be a pandas DatetimeIndex with a defined frequency. The function reshapes the\n    DataFrame from wide format to long format, where each row corresponds to a\n    specific time point and series ID. The resulting DataFrame will have a MultiIndex\n    with the series IDs as the first level and a pandas DatetimeIndex as the second\n    level. If `return_multi_index` is set to False, the returned DataFrame have three\n    columns: 'series_id', 'datetime' and 'value', with a regular index.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Wide format series. The index must be a pandas DatetimeIndex with a \n        defined frequency and each column must represent a different time series.\n    return_multi_index: bool, default True\n        If True, the returned DataFrame will have a MultiIndex with the series IDs\n        as the first level and a pandas DatetimeIndex as the second level. If False,\n        the returned DataFrame will have a regular index.\n\n    Returns\n    -------\n    data: pandas DataFrame\n        Long format series with a MultiIndex. The first level contains the series IDs,\n        and the second level contains a pandas DatetimeIndex with the same frequency\n        for each series.\n\n    \"\"\"\n\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"`data` must be a pandas DataFrame.\")\n\n    if not isinstance(data.index, pd.DatetimeIndex):\n        raise TypeError(\"`data` index must be a pandas DatetimeIndex.\")\n\n    freq = data.index.freqstr\n    data.index.name = \"datetime\"\n    data = data.reset_index()\n    data = pd.melt(data, id_vars=\"datetime\", var_name=\"series_id\", value_name=\"value\")\n    data = data.groupby(\"series_id\", sort=False).apply(\n        lambda x: x.set_index(\"datetime\").asfreq(freq), include_groups=False\n    )\n\n    if not return_multi_index:\n        data = data.reset_index()\n\n    return data\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.reshape_series_long_to_dict","title":"skforecast.preprocessing.preprocessing.reshape_series_long_to_dict","text":"<pre><code>reshape_series_long_to_dict(\n    data,\n    freq,\n    series_id=None,\n    index=None,\n    values=None,\n    suppress_warnings=False,\n)\n</code></pre> <p>Convert a long-format DataFrame into a dictionary of pandas Series with the  specified frequency. Supports two input formats:</p> <ul> <li>A pandas DataFrame with explicit columns for the series identifier, time  index, and values.</li> <li>A pandas DataFrame with a MultiIndex, where the first level contains the  series IDs, and the second level contains a pandas DatetimeIndex.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Long-format series.</p> required <code>freq</code> <code>str</code> <p>Frequency of the series.</p> required <code>series_id</code> <code>str | None</code> <p>Column name with the series identifier. Not needed if the input data is a pandas DataFrame with MultiIndex.</p> <code>None</code> <code>index</code> <code>str | None</code> <p>Column name with the time index. Not needed if the input data is a pandas DataFrame with MultiIndex.</p> <code>None</code> <code>values</code> <code>str | None</code> <p>Column name with the values. Not needed if the input data is a pandas DataFrame with MultiIndex.</p> <code>None</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress warnings when a series is incomplete after setting the frequency.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>series_dict</code> <code>dict</code> <p>Dictionary with the series.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def reshape_series_long_to_dict(\n    data: pd.DataFrame,\n    freq: str,\n    series_id: str | None = None,\n    index: str | None = None,\n    values: str | None = None,\n    suppress_warnings: bool = False\n) -&gt; dict[str, pd.Series]:\n    \"\"\"\n    Convert a long-format DataFrame into a dictionary of pandas Series with the \n    specified frequency. Supports two input formats:\n\n    - A pandas DataFrame with explicit columns for the series identifier, time \n    index, and values.\n    - A pandas DataFrame with a MultiIndex, where the first level contains the \n    series IDs, and the second level contains a pandas DatetimeIndex.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long-format series.\n    freq: str\n        Frequency of the series.\n    series_id: str, default None\n        Column name with the series identifier. Not needed if the input data\n        is a pandas DataFrame with MultiIndex.\n    index: str, default None\n        Column name with the time index. Not needed if the input data is a pandas\n        DataFrame with MultiIndex.\n    values: str, default None\n        Column name with the values. Not needed if the input data is a pandas\n        DataFrame with MultiIndex.\n    suppress_warnings: bool, default False\n        If True, suppress warnings when a series is incomplete after setting the\n        frequency.\n\n    Returns\n    -------\n    series_dict: dict\n        Dictionary with the series.\n\n    \"\"\"\n\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"`data` must be a pandas DataFrame.\")\n\n    if isinstance(data.index, pd.MultiIndex):\n\n        first_col = data.columns[0]\n        data.index = data.index.set_names([data.index.names[0], None])\n        series_dict = {\n            id: data.loc[id][first_col].rename(id).asfreq(freq)\n            for id in data.index.levels[0]\n        }\n\n    else:\n\n        for col in [series_id, index, values]:\n            if col is None:\n                raise ValueError(\n                    \"Arguments `series_id`, `index`, and `values` must be \"\n                    \"specified when the input DataFrame does not have a MultiIndex. \"\n                    \"Please provide a value for each of these arguments.\"\n                )\n            if col not in data.columns:\n                raise ValueError(f\"Column '{col}' not found in `data`.\")\n\n        data_grouped = data.groupby(series_id, observed=True)   \n        original_sizes = data_grouped.size()\n        series_dict = {}\n        for k, v in data_grouped:\n            series_dict[k] = v.set_index(index)[values].asfreq(freq, fill_value=np.nan).rename(k)\n            series_dict[k].index.name = None\n            if not suppress_warnings and len(series_dict[k]) != original_sizes[k]:\n                warnings.warn(\n                    f\"Series '{k}' is incomplete. NaNs have been introduced after \"\n                    f\"setting the frequency.\",\n                    MissingValuesWarning\n                )\n\n    return series_dict\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.reshape_exog_long_to_dict","title":"skforecast.preprocessing.preprocessing.reshape_exog_long_to_dict","text":"<pre><code>reshape_exog_long_to_dict(\n    data,\n    freq,\n    series_id=None,\n    index=None,\n    drop_all_nan_cols=False,\n    consolidate_dtypes=True,\n    suppress_warnings=False,\n)\n</code></pre> <p>Convert a long-format DataFrame of exogenous variables into a dictionary  of pandas DataFrames with the specified frequency. Supports two input formats:</p> <ul> <li>A pandas DataFrame with explicit columns for the series identifier, time  index, and exogenous variables.</li> <li>A pandas DataFrame with a MultiIndex, where the first level contains the  series IDs, and the second level contains a pandas DatetimeIndex.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Long format exogenous variables.</p> required <code>freq</code> <code>str</code> <p>Frequency of the series.</p> required <code>series_id</code> <code>str | None</code> <p>Column name with the series identifier. Not needed if the input data is a pandas DataFrame with MultiIndex.</p> <code>None</code> <code>index</code> <code>str | None</code> <p>Column name with the time index. Not needed if the input data is a pandas DataFrame with MultiIndex.</p> <code>None</code> <code>drop_all_nan_cols</code> <code>bool</code> <p>If True, drop columns with all values as NaN. This is useful when there are series without some exogenous variables.</p> <code>False</code> <code>consolidate_dtypes</code> <code>bool</code> <p>Consolidate the data types of the exogenous variables if, after setting the frequency, NaNs have been introduced and the data types have changed to float.</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress warnings when exog is incomplete after setting the frequency.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variables.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def reshape_exog_long_to_dict(\n    data: pd.DataFrame,\n    freq: str,\n    series_id: str | None = None,\n    index: str | None = None,\n    drop_all_nan_cols: bool = False,\n    consolidate_dtypes: bool = True,\n    suppress_warnings: bool = False\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Convert a long-format DataFrame of exogenous variables into a dictionary \n    of pandas DataFrames with the specified frequency. Supports two input formats:\n\n    - A pandas DataFrame with explicit columns for the series identifier, time \n    index, and exogenous variables.\n    - A pandas DataFrame with a MultiIndex, where the first level contains the \n    series IDs, and the second level contains a pandas DatetimeIndex.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format exogenous variables.\n    freq: str\n        Frequency of the series.\n    series_id: str, default None\n        Column name with the series identifier. Not needed if the input data\n        is a pandas DataFrame with MultiIndex.\n    index: str, default None\n        Column name with the time index. Not needed if the input data is a pandas\n        DataFrame with MultiIndex.\n    drop_all_nan_cols: bool, default False\n        If True, drop columns with all values as NaN. This is useful when\n        there are series without some exogenous variables.\n    consolidate_dtypes: bool, default True\n        Consolidate the data types of the exogenous variables if, after setting\n        the frequency, NaNs have been introduced and the data types have changed\n        to float.\n    suppress_warnings: bool, default False\n        If True, suppress warnings when exog is incomplete after setting the\n        frequency.\n\n    Returns\n    -------\n    exog_dict: dict\n        Dictionary with the exogenous variables.\n\n    \"\"\"\n\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"`data` must be a pandas DataFrame.\")\n\n    if isinstance(data.index, pd.MultiIndex):\n\n        data.index = data.index.set_names([data.index.names[0], None])\n        exog_dict = {\n            id: data.loc[id].asfreq(freq) for id in data.index.levels[0]\n        }\n\n    else:\n\n        for col in [series_id, index]:\n            if col is None:\n                raise ValueError(\n                    \"Arguments `series_id`, and `index` must be \"\n                    \"specified when the input DataFrame does not have a MultiIndex. \"\n                    \"Please provide a value for each of these arguments.\"\n                )\n            if col not in data.columns:\n                raise ValueError(f\"Column '{col}' not found in `data`.\")\n\n        cols_float_dtype = {\n            col for col in data.columns \n            if pd.api.types.is_float_dtype(data[col])\n        }\n\n        data_grouped = data.groupby(series_id, observed=True) \n        original_sizes = data_grouped.size()\n        exog_dict = dict(tuple(data_grouped))\n        exog_dict = {\n            k: v.set_index(index).asfreq(freq, fill_value=np.nan).drop(columns=series_id)\n            for k, v in exog_dict.items()\n        }\n\n        for k in exog_dict.keys():\n            exog_dict[k].index.name = None\n\n        nans_introduced = False\n        if not suppress_warnings or consolidate_dtypes:\n            for k, v in exog_dict.items():\n                if len(v) != original_sizes[k]:\n                    nans_introduced = True\n                    if not suppress_warnings:\n                        warnings.warn(\n                            f\"Exogenous variables for series '{k}' are incomplete. \"\n                            f\"NaNs have been introduced after setting the frequency.\",\n                            MissingValuesWarning\n                        )\n                    if consolidate_dtypes:\n                        cols_float_dtype.update(\n                            {\n                                col for col in v.columns \n                                if pd.api.types.is_float_dtype(v[col])\n                            }\n                        )\n\n        if consolidate_dtypes and nans_introduced:\n            new_dtypes = {k: float for k in cols_float_dtype}\n            exog_dict = {k: v.astype(new_dtypes) for k, v in exog_dict.items()}\n\n    if drop_all_nan_cols:\n        exog_dict = {k: v.dropna(how=\"all\", axis=1) for k, v in exog_dict.items()}\n\n    return exog_dict\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator","title":"skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator","text":"<pre><code>TimeSeriesDifferentiator(order=1, window_size=None)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transforms a time series into a differentiated time series of a specified order and provides functionality to revert the differentiation. </p> <p>When using a <code>direct</code> module Forecaster, the model in step 1 must be  used if you want to reverse the differentiation of the training time  series with the <code>inverse_transform_training</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>int</code> <p>The order of differentiation to be applied.</p> <code>1</code> <code>window_size</code> <code>int</code> <p>The window size used by the forecaster. This is required to revert the  differentiation for the target variable <code>y</code> or its predicted values.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>order</code> <code>int</code> <p>The order of differentiation.</p> <code>initial_values</code> <code>list</code> <p>List with the first value of the time series before each differentiation. If <code>order = 2</code>, first value correspond with the first value of the original time series and the second value correspond with the first value of the differentiated time series of order 1. These values are necessary to  revert the differentiation and reconstruct the original time series.</p> <code>pre_train_values</code> <code>list</code> <p>List with the first training value of the time series before each differentiation. For <code>order = 1</code>, the value correspond with the last value of the window used to create the predictors. For order &gt; 1, the value correspond with the first value of the differentiated time series prior to the next differentiation. These values are necessary to revert the differentiation and reconstruct the training time series.</p> <code>last_values</code> <code>list</code> <p>List with the last value of the time series before each differentiation,  used to revert differentiation on subsequent data windows. If <code>order = 2</code>,  first value correspond with the last value of the original time series  and the second value correspond with the last value of the differentiated  time series of order 1. This is essential for correctly transforming a  time series that follows immediately after the series used to fit the  transformer.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fits the transformer. Stores the values needed to revert the </p> <code>transform</code> <p>Transforms a time series into a differentiated time series of order n.</p> <code>inverse_transform</code> <p>Reverts the differentiation. To do so, the input array is assumed to be</p> <code>inverse_transform_training</code> <p>Reverts the differentiation. To do so, the input array is assumed to be</p> <code>inverse_transform_next_window</code> <p>Reverts the differentiation. The input array <code>X</code> is assumed to be a </p> <code>set_params</code> <p>Set the parameters of the TimeSeriesDifferentiator.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def __init__(\n    self, \n    order: int = 1,\n    window_size: int | None = None\n) -&gt; None:\n\n    if not isinstance(order, (int, np.integer)):\n        raise TypeError(\n            f\"Parameter `order` must be an integer greater than 0. Found {type(order)}.\"\n        )\n    if order &lt; 1:\n        raise ValueError(\n            f\"Parameter `order` must be an integer greater than 0. Found {order}.\"\n        )\n\n    if window_size is not None:\n        if not isinstance(window_size, (int, np.integer)):\n            raise TypeError(\n                f\"Parameter `window_size` must be an integer greater than 0. \"\n                f\"Found {type(window_size)}.\"\n            )\n        if window_size &lt; 1:\n            raise ValueError(\n                f\"Parameter `window_size` must be an integer greater than 0. \"\n                f\"Found {window_size}.\"\n            )\n\n    self.order = order\n    self.window_size = window_size\n    self.initial_values = []\n    self.pre_train_values = []\n    self.last_values = []\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.order","title":"order  <code>instance-attribute</code>","text":"<pre><code>order = order\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = window_size\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.initial_values","title":"initial_values  <code>instance-attribute</code>","text":"<pre><code>initial_values = []\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.pre_train_values","title":"pre_train_values  <code>instance-attribute</code>","text":"<pre><code>pre_train_values = []\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.last_values","title":"last_values  <code>instance-attribute</code>","text":"<pre><code>last_values = []\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fits the transformer. Stores the values needed to revert the  differentiation of different window of the time series, original  time series, training time series, and a time series that follows immediately after the series used to fit the transformer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Time series to be differentiated.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>TimeSeriesDifferentiator</code> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>@_check_X_numpy_ndarray_1d()\ndef fit(\n    self, \n    X: np.ndarray, \n    y: Any = None\n) -&gt; Self:\n    \"\"\"\n    Fits the transformer. Stores the values needed to revert the \n    differentiation of different window of the time series, original \n    time series, training time series, and a time series that follows\n    immediately after the series used to fit the transformer.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Time series to be differentiated.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    self : TimeSeriesDifferentiator\n\n    \"\"\"\n\n    self.initial_values = []\n    self.pre_train_values = []\n    self.last_values = []\n\n    for i in range(self.order):\n        if i == 0:\n            self.initial_values.append(X[0])\n            if self.window_size is not None:\n                self.pre_train_values.append(X[self.window_size - self.order])\n            self.last_values.append(X[-1])\n            X_diff = np.diff(X, n=1)\n        else:\n            self.initial_values.append(X_diff[0])\n            if self.window_size is not None:\n                self.pre_train_values.append(X_diff[self.window_size - self.order])\n            self.last_values.append(X_diff[-1])\n            X_diff = np.diff(X_diff, n=1)\n\n    return self\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.transform","title":"transform","text":"<pre><code>transform(X, y=None)\n</code></pre> <p>Transforms a time series into a differentiated time series of order n.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Time series to be differentiated.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_diff</code> <code>numpy ndarray</code> <p>Differentiated time series. The length of the array is the same as the original time series but the first n <code>order</code> values are nan.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>@_check_X_numpy_ndarray_1d()\ndef transform(\n    self, \n    X: np.ndarray, \n    y: Any = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Transforms a time series into a differentiated time series of order n.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Time series to be differentiated.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_diff : numpy ndarray\n        Differentiated time series. The length of the array is the same as\n        the original time series but the first n `order` values are nan.\n\n    \"\"\"\n\n    X_diff = np.diff(X, n=self.order)\n    X_diff = np.append((np.full(shape=self.order, fill_value=np.nan)), X_diff)\n\n    return X_diff\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.inverse_transform","title":"inverse_transform","text":"<pre><code>inverse_transform(X, y=None)\n</code></pre> <p>Reverts the differentiation. To do so, the input array is assumed to be the same time series used to fit the transformer but differentiated.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Differentiated time series.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_diff</code> <code>numpy ndarray</code> <p>Reverted differentiated time series.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>@_check_X_numpy_ndarray_1d()\ndef inverse_transform(\n    self, \n    X: np.ndarray, \n    y: Any = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Reverts the differentiation. To do so, the input array is assumed to be\n    the same time series used to fit the transformer but differentiated.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Differentiated time series.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_diff : numpy ndarray\n        Reverted differentiated time series.\n\n    \"\"\"\n\n    # Remove initial nan values if present\n    X = X[np.argmax(~np.isnan(X)):]\n    for i in range(self.order):\n        if i == 0:\n            X_undiff = np.insert(X, 0, self.initial_values[-1])\n            X_undiff = np.cumsum(X_undiff, dtype=float)\n        else:\n            X_undiff = np.insert(X_undiff, 0, self.initial_values[-(i + 1)])\n            X_undiff = np.cumsum(X_undiff, dtype=float)\n\n    return X_undiff\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.inverse_transform_training","title":"inverse_transform_training","text":"<pre><code>inverse_transform_training(X, y=None)\n</code></pre> <p>Reverts the differentiation. To do so, the input array is assumed to be the differentiated training time series generated with the original  time series used to fit the transformer.</p> <p>When using a <code>direct</code> module Forecaster, the model in step 1 must be  used if you want to reverse the differentiation of the training time  series with the <code>inverse_transform_training</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Differentiated time series.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_diff</code> <code>numpy ndarray</code> <p>Reverted differentiated time series.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>@_check_X_numpy_ndarray_1d()\ndef inverse_transform_training(\n    self, \n    X: np.ndarray, \n    y: Any = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Reverts the differentiation. To do so, the input array is assumed to be\n    the differentiated training time series generated with the original \n    time series used to fit the transformer.\n\n    When using a `direct` module Forecaster, the model in step 1 must be \n    used if you want to reverse the differentiation of the training time \n    series with the `inverse_transform_training` method.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Differentiated time series.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_diff : numpy ndarray\n        Reverted differentiated time series.\n\n    \"\"\"\n\n    if not self.pre_train_values:\n        raise ValueError(\n            \"The `window_size` parameter must be set before fitting the \"\n            \"transformer to revert the differentiation of the training \"\n            \"time series.\"\n        )\n\n    # Remove initial nan values if present\n    X = X[np.argmax(~np.isnan(X)):]\n    for i in range(self.order):\n        if i == 0:\n            X_undiff = np.insert(X, 0, self.pre_train_values[-1])\n            X_undiff = np.cumsum(X_undiff, dtype=float)\n        else:\n            X_undiff = np.insert(X_undiff, 0, self.pre_train_values[-(i + 1)])\n            X_undiff = np.cumsum(X_undiff, dtype=float)\n\n    # Remove initial values as they are not part of the training time series\n    X_undiff = X_undiff[self.order:]\n\n    return X_undiff\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.inverse_transform_next_window","title":"inverse_transform_next_window","text":"<pre><code>inverse_transform_next_window(X, y=None)\n</code></pre> <p>Reverts the differentiation. The input array <code>X</code> is assumed to be a  differentiated time series of order n that starts right after the the time series used to fit the transformer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>Differentiated time series. It is assumed o start right after the time series used to fit the transformer.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present here for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_undiff</code> <code>numpy ndarray</code> <p>Reverted differentiated time series.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>@_check_X_numpy_ndarray_1d(ensure_1d=False)\ndef inverse_transform_next_window(\n    self,\n    X: np.ndarray,\n    y: Any = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Reverts the differentiation. The input array `X` is assumed to be a \n    differentiated time series of order n that starts right after the\n    the time series used to fit the transformer.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        Differentiated time series. It is assumed o start right after\n        the time series used to fit the transformer.\n    y : Ignored\n        Not used, present here for API consistency by convention.\n\n    Returns\n    -------\n    X_undiff : numpy ndarray\n        Reverted differentiated time series.\n\n    \"\"\"\n\n    array_ndim = X.ndim\n    if array_ndim == 1:\n        X = X[:, np.newaxis]\n\n    # Remove initial rows with nan values if present\n    X = X[~np.isnan(X).any(axis=1)]\n\n    for i in range(self.order):\n        if i == 0:\n            X_undiff = np.cumsum(X, axis=0, dtype=float) + self.last_values[-1]\n        else:\n            X_undiff = np.cumsum(X_undiff, axis=0, dtype=float) + self.last_values[-(i + 1)]\n\n    if array_ndim == 1:\n        X_undiff = X_undiff.ravel()\n\n    return X_undiff\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.TimeSeriesDifferentiator.set_params","title":"set_params","text":"<pre><code>set_params(**params)\n</code></pre> <p>Set the parameters of the TimeSeriesDifferentiator.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary of the parameters to set.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def set_params(self, **params):\n    \"\"\"\n    Set the parameters of the TimeSeriesDifferentiator.\n\n    Parameters\n    ----------\n    params : dict\n        A dictionary of the parameters to set.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    for param, value in params.items():\n        setattr(self, param, value)\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner","title":"skforecast.preprocessing.preprocessing.QuantileBinner","text":"<pre><code>QuantileBinner(\n    n_bins,\n    method=\"linear\",\n    subsample=200000,\n    dtype=np.float64,\n    random_state=789654,\n)\n</code></pre> <p>QuantileBinner class to bin data into quantile-based bins using <code>numpy.percentile</code>. This class is similar to <code>KBinsDiscretizer</code> but faster for binning data into quantile-based bins. Bin  intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. See more information in <code>numpy.percentile</code> and <code>numpy.digitize</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>The number of quantile-based bins to create.</p> required <code>method</code> <code>str</code> <p>The method used to compute the quantiles. This parameter is passed to  <code>numpy.percentile</code>. Default is 'linear'. Valid values are \"inverse_cdf\", \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\", \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".</p> <code>'linear'</code> <code>subsample</code> <code>int</code> <p>The number of samples to use for computing quantiles. If the dataset  has more samples than <code>subsample</code>, a random subset will be used.</p> <code>200000</code> <code>dtype</code> <code>data type</code> <p>The data type to use for the bin indices. Default is <code>numpy.float64</code>.</p> <code>numpy.float64</code> <code>random_state</code> <code>int</code> <p>The random seed to use for generating a random subset of the data.</p> <code>789654</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>int</code> <p>The number of quantile-based bins to create.</p> <code>method</code> <code>str</code> <p>The method used to compute the quantiles. This parameter is passed to  <code>numpy.percentile</code>. Default is 'linear'. Valid values are 'linear', 'lower', 'higher', 'midpoint', 'nearest'.</p> <code>subsample</code> <code>int</code> <p>The number of samples to use for computing quantiles. If the dataset  has more samples than <code>subsample</code>, a random subset will be used.</p> <code>dtype</code> <code>data type</code> <p>The data type to use for the bin indices. Default is <code>numpy.float64</code>.</p> <code>random_state</code> <code>int</code> <p>The random seed to use for generating a random subset of the data.</p> <code>n_bins_</code> <code>int</code> <p>The number of bins learned during fitting.</p> <code>bin_edges_</code> <code>numpy ndarray</code> <p>The edges of the bins learned during fitting.</p> <code>intervals_</code> <code>dict</code> <p>A dictionary with the bin indices as keys and the corresponding bin intervals as values.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Learn the bin edges based on quantiles from the training data.</p> <code>transform</code> <p>Assign new data to the learned bins.</p> <code>fit_transform</code> <p>Fit the model to the data and return the bin indices for the same data.</p> <code>get_params</code> <p>Get the parameters of the quantile binner.</p> <code>set_params</code> <p>Set the parameters of the QuantileBinner.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def __init__(\n    self,\n    n_bins: int,\n    method: str = \"linear\",\n    subsample: int = 200000,\n    dtype: type = np.float64,\n    random_state: int = 789654\n) -&gt; None:\n\n    self._validate_params(\n        n_bins,\n        method,\n        subsample,\n        dtype,\n        random_state\n    )\n\n    self.n_bins       = n_bins\n    self.method       = method\n    self.subsample    = subsample\n    self.dtype        = dtype\n    self.random_state = random_state\n    self.n_bins_      = None\n    self.bin_edges_   = None\n    self.intervals_   = None\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.n_bins","title":"n_bins  <code>instance-attribute</code>","text":"<pre><code>n_bins = n_bins\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method = method\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.subsample","title":"subsample  <code>instance-attribute</code>","text":"<pre><code>subsample = subsample\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.random_state","title":"random_state  <code>instance-attribute</code>","text":"<pre><code>random_state = random_state\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.n_bins_","title":"n_bins_  <code>instance-attribute</code>","text":"<pre><code>n_bins_ = None\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.bin_edges_","title":"bin_edges_  <code>instance-attribute</code>","text":"<pre><code>bin_edges_ = None\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.intervals_","title":"intervals_  <code>instance-attribute</code>","text":"<pre><code>intervals_ = None\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner._validate_params","title":"_validate_params","text":"<pre><code>_validate_params(\n    n_bins, method, subsample, dtype, random_state\n)\n</code></pre> <p>Validate the parameters passed to the class initializer.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def _validate_params(\n    self,\n    n_bins: int,\n    method: str,\n    subsample: int,\n    dtype: type,\n    random_state: int\n):\n    \"\"\"\n    Validate the parameters passed to the class initializer.\n    \"\"\"\n\n    if not isinstance(n_bins, int) or n_bins &lt; 2:\n        raise ValueError(\n            f\"`n_bins` must be an int greater than 1. Got {n_bins}.\"\n        )\n\n    valid_methods = [\n        \"inverse_cdf\",\n        \"averaged_inverse_cdf\",\n        \"closest_observation\",\n        \"interpolated_inverse_cdf\",\n        \"hazen\",\n        \"weibull\",\n        \"linear\",\n        \"median_unbiased\",\n        \"normal_unbiased\",\n    ]\n    if method not in valid_methods:\n        raise ValueError(\n            f\"`method` must be one of {valid_methods}. Got {method}.\"\n        )\n    if not isinstance(subsample, int) or subsample &lt; 1:\n        raise ValueError(\n            f\"`subsample` must be an integer greater than or equal to 1. \"\n            f\"Got {subsample}.\"\n        )\n    if not isinstance(random_state, int) or random_state &lt; 0:\n        raise ValueError(\n            f\"`random_state` must be an integer greater than or equal to 0. \"\n            f\"Got {random_state}.\"\n        )\n    if not isinstance(dtype, type):\n        raise ValueError(\n            f\"`dtype` must be a valid numpy dtype. Got {dtype}.\"\n        )\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.fit","title":"fit","text":"<pre><code>fit(X)\n</code></pre> <p>Learn the bin edges based on quantiles from the training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>The training data used to compute the quantiles.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def fit(self, X: np.ndarray):\n    \"\"\"\n    Learn the bin edges based on quantiles from the training data.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        The training data used to compute the quantiles.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if X.size == 0:\n        raise ValueError(\"Input data `X` cannot be empty.\")\n    if len(X) &gt; self.subsample:\n        rng = np.random.default_rng(self.random_state)\n        X = X[rng.integers(0, len(X), self.subsample)]\n\n    self.bin_edges_ = np.percentile(\n        a      = X,\n        q      = np.linspace(0, 100, self.n_bins + 1),\n        method = self.method\n    )\n\n    self.n_bins_ = len(self.bin_edges_) - 1\n    self.intervals_ = {\n        int(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n        for i in range(self.n_bins_)\n    }\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.transform","title":"transform","text":"<pre><code>transform(X)\n</code></pre> <p>Assign new data to the learned bins.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray</code> <p>The data to assign to the bins.</p> required <p>Returns:</p> Name Type Description <code>bin_indices</code> <code>numpy ndarray </code> <p>The indices of the bins each value belongs to. Values less than the smallest bin edge are assigned to the first bin, and values greater than the largest bin edge are assigned to the last bin.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def transform(self, X: np.ndarray):\n    \"\"\"\n    Assign new data to the learned bins.\n\n    Parameters\n    ----------\n    X : numpy ndarray\n        The data to assign to the bins.\n\n    Returns\n    -------\n    bin_indices : numpy ndarray \n        The indices of the bins each value belongs to.\n        Values less than the smallest bin edge are assigned to the first bin,\n        and values greater than the largest bin edge are assigned to the last bin.\n\n    \"\"\"\n\n    if self.bin_edges_ is None:\n        raise NotFittedError(\n            \"The model has not been fitted yet. Call 'fit' with training data first.\"\n        )\n\n    bin_indices = np.digitize(X, bins=self.bin_edges_, right=False)\n    bin_indices = np.clip(bin_indices, 1, self.n_bins_).astype(self.dtype) - 1\n\n    return bin_indices\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(X)\n</code></pre> <p>Fit the model to the data and return the bin indices for the same data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The data to fit and transform.</p> required <p>Returns:</p> Name Type Description <code>bin_indices</code> <code>ndarray</code> <p>The indices of the bins each value belongs to. Values less than the smallest bin edge are assigned to the first bin, and values greater than the largest bin edge are assigned to the last bin.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def fit_transform(self, X):\n    \"\"\"\n    Fit the model to the data and return the bin indices for the same data.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        The data to fit and transform.\n\n    Returns\n    -------\n    bin_indices : numpy.ndarray\n        The indices of the bins each value belongs to.\n        Values less than the smallest bin edge are assigned to the first bin,\n        and values greater than the largest bin edge are assigned to the last bin.\n\n    \"\"\"\n\n    self.fit(X)\n\n    return self.transform(X)\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.get_params","title":"get_params","text":"<pre><code>get_params()\n</code></pre> <p>Get the parameters of the quantile binner.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> required <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>A dictionary of the parameters of the quantile binner.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def get_params(self):\n    \"\"\"\n    Get the parameters of the quantile binner.\n\n    Parameters\n    ----------\n    self\n\n    Returns\n    -------\n    params : dict\n        A dictionary of the parameters of the quantile binner.\n\n    \"\"\"\n\n    return {\n        \"n_bins\": self.n_bins,\n        \"method\": self.method,\n        \"subsample\": self.subsample,\n        \"dtype\": self.dtype,\n        \"random_state\": self.random_state,\n    }\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.QuantileBinner.set_params","title":"set_params","text":"<pre><code>set_params(**params)\n</code></pre> <p>Set the parameters of the QuantileBinner.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary of the parameters to set.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def set_params(self, **params):\n    \"\"\"\n    Set the parameters of the QuantileBinner.\n\n    Parameters\n    ----------\n    params : dict\n        A dictionary of the parameters to set.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    for param, value in params.items():\n        setattr(self, param, value)\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator","title":"skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator","text":"<pre><code>ConformalIntervalCalibrator(\n    nominal_coverage=0.8, symmetric_calibration=True\n)\n</code></pre> <p>Transformer that calibrates the prediction interval to achieve the desired  coverage based on conformity scores. It uses the conformal split method.</p> <p>Parameters:</p> Name Type Description Default <code>nominal_coverage</code> <code>float</code> <p>Desired coverage. This is the desired probability that the true value  falls within the calibrated interval.</p> <code>0.8</code> <code>symmetric_calibration</code> <code>bool</code> <p>If True, the calibration factor is the same for the lower and upper bounds. If False, the calibration factor is different for the lower and upper bounds.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>nominal_coverage</code> <code>float</code> <p>Desired coverage. This is the desired probability that the true value  falls within the calibrated interval.</p> <code>symmetric_calibration</code> <code>bool, default True</code> <p>If True, the calibration factor is the same for the lower and upper bounds. If False, the calibration factor is different for the lower and upper bounds.</p> <code>correction_factor_</code> <code>dict</code> <p>Correction factor to achieve the desired coverage. This is the correction factor used when <code>symmetric_calibration</code> is True.</p> <code>correction_factor_lower_</code> <code>dict</code> <p>Correction factor for the lower bound to achieve the desired coverage. It is used when <code>symmetric_calibration</code> is False.</p> <code>correction_factor_upper_</code> <code>dict</code> <p>Correction factor for the upper bound to achieve the desired coverage. It is used when <code>symmetric_calibration</code> is False.</p> <code>fit_coverage_</code> <code>dict</code> <p>Coverage observed in the data used to fit the transformer. This is the empirical coverage from which the correction factor is learned.</p> <code>fit_input_type_</code> <code>str</code> <p>Type of input data used to fit the transformer. Can be 'single' or 'multi'.</p> <code>fit_series_names_</code> <code>list</code> <p>Names of the series used to fit the transformer.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Learn the correction factor needed to achieve the desired coverage.</p> <code>transform</code> <p>Apply the correction factor to the prediction interval to achieve the desired</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def __init__(\n    self,\n    nominal_coverage: float = 0.8,\n    symmetric_calibration: bool = True\n) -&gt; None:\n\n    if nominal_coverage &lt; 0 or nominal_coverage &gt; 1:\n        raise ValueError(\n            f\"`nominal_coverage` must be a float between 0 and 1. Got {nominal_coverage}\"\n        )\n\n    self.nominal_coverage         = nominal_coverage\n    self.symmetric_calibration    = symmetric_calibration\n    self.correction_factor_       = {}\n    self.correction_factor_lower_ = {}\n    self.correction_factor_upper_ = {}\n    self.fit_coverage_            = {}\n    self.fit_input_type_          = None\n    self.fit_series_names_        = None\n    self.is_fitted                = False\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.nominal_coverage","title":"nominal_coverage  <code>instance-attribute</code>","text":"<pre><code>nominal_coverage = nominal_coverage\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.symmetric_calibration","title":"symmetric_calibration  <code>instance-attribute</code>","text":"<pre><code>symmetric_calibration = symmetric_calibration\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.correction_factor_","title":"correction_factor_  <code>instance-attribute</code>","text":"<pre><code>correction_factor_ = {}\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.correction_factor_lower_","title":"correction_factor_lower_  <code>instance-attribute</code>","text":"<pre><code>correction_factor_lower_ = {}\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.correction_factor_upper_","title":"correction_factor_upper_  <code>instance-attribute</code>","text":"<pre><code>correction_factor_upper_ = {}\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.fit_coverage_","title":"fit_coverage_  <code>instance-attribute</code>","text":"<pre><code>fit_coverage_ = {}\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.fit_input_type_","title":"fit_input_type_  <code>instance-attribute</code>","text":"<pre><code>fit_input_type_ = None\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.fit_series_names_","title":"fit_series_names_  <code>instance-attribute</code>","text":"<pre><code>fit_series_names_ = None\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.is_fitted","title":"is_fitted  <code>instance-attribute</code>","text":"<pre><code>is_fitted = False\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator._repr_html_","title":"_repr_html_","text":"<pre><code>_repr_html_()\n</code></pre> <p>HTML representation of the object. The \"General Information\" section is expanded by default.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def _repr_html_(self) -&gt; str:\n    \"\"\"\n    HTML representation of the object.\n    The \"General Information\" section is expanded by default.\n    \"\"\"\n\n    style, unique_id = get_style_repr_html(is_fitted=self.is_fitted)\n\n    content = f\"\"\"\n    &lt;div class=\"container-{unique_id}\"&gt;\n        &lt;h2&gt;{type(self).__name__}&lt;/h2&gt;\n        &lt;details open&gt;\n            &lt;summary&gt;General Information&lt;/summary&gt;\n            &lt;ul&gt;\n                &lt;li&gt;&lt;strong&gt;Nominal coverage:&lt;/strong&gt; {self.nominal_coverage}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Coverage in fit data:&lt;/strong&gt; {self.fit_coverage_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Symmetric interval:&lt;/strong&gt; {self.symmetric_calibration}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Symmetric correction factor:&lt;/strong&gt; {self.correction_factor_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Asymmetric correction factor lower:&lt;/strong&gt; {self.correction_factor_lower_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Asymmetric correction factor upper:&lt;/strong&gt; {self.correction_factor_upper_}&lt;/li&gt;\n                &lt;li&gt;&lt;strong&gt;Fitted series:&lt;/strong&gt; {self.fit_series_names_}&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/details&gt;\n        &lt;p&gt;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/api/preprocessing#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator\"&gt;&amp;#128712 &lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt;\n            &amp;nbsp;&amp;nbsp;\n            &lt;a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/probabilistic-forecasting-conformal-calibration.html\"&gt;&amp;#128462 &lt;strong&gt;User Guide&lt;/strong&gt;&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n    \"\"\"\n\n    return style + content\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.fit","title":"fit","text":"<pre><code>fit(y_true, y_pred_interval)\n</code></pre> <p>Learn the correction factor needed to achieve the desired coverage.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>pandas Series, pandas DataFrame, dict</code> <p>True values of the time series.</p> <ul> <li>If pandas Series, it is assumed that only one series is available.</li> <li>If pandas DataFrame, it is assumed that each column is a different  series which will be calibrated separately. The column names are  used as series names.</li> <li>If dict, it is assumed that each key is a series name and the  corresponding value is a pandas Series with the true values.</li> </ul> required <code>y_pred_interval</code> <code>pandas DataFrame</code> <p>Prediction interval estimated for the time series. </p> <ul> <li>If <code>y_true</code> contains only one series, <code>y_pred_interval</code> must have  two columns, 'lower_bound' and 'upper_bound'.</li> <li>If <code>y_true</code> contains multiple series, <code>y_pred_interval</code> must be a long-format DataFrame with three columns: 'level', 'lower_bound', and 'upper_bound'. The 'level' column identifies the series to which each interval belongs.</li> </ul> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def fit(\n    self,\n    y_true: pd.Series | pd.DataFrame | dict[str, pd.Series],\n    y_pred_interval: pd.DataFrame,\n) -&gt; None:\n    \"\"\"\n    Learn the correction factor needed to achieve the desired coverage.\n\n    Parameters\n    ----------\n    y_true : pandas Series, pandas DataFrame, dict\n        True values of the time series.\n\n        - If pandas Series, it is assumed that only one series is available.\n        - If pandas DataFrame, it is assumed that each column is a different \n        series which will be calibrated separately. The column names are \n        used as series names.\n        - If dict, it is assumed that each key is a series name and the \n        corresponding value is a pandas Series with the true values.\n    y_pred_interval : pandas DataFrame\n        Prediction interval estimated for the time series. \n\n        - If `y_true` contains only one series, `y_pred_interval` must have \n        two columns, 'lower_bound' and 'upper_bound'.\n        - If `y_true` contains multiple series, `y_pred_interval` must be\n        a long-format DataFrame with three columns: 'level', 'lower_bound',\n        and 'upper_bound'. The 'level' column identifies the series to which\n        each interval belongs.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    self.correction_factor_       = {}\n    self.correction_factor_lower_ = {}\n    self.correction_factor_upper_ = {}\n    self.fit_coverage_            = {}\n    self.fit_input_type_          = None\n    self.fit_series_names_        = None\n    self.is_fitted                = False\n\n    if not isinstance(y_true, (pd.Series, pd.DataFrame, dict)):\n        raise TypeError(\n            \"`y_true` must be a pandas Series, pandas DataFrame, or a dictionary.\"\n        )\n\n    if not isinstance(y_pred_interval, (pd.DataFrame)):\n        raise TypeError(\n            \"`y_pred_interval` must be a pandas DataFrame.\"\n        )\n\n    if not set([\"lower_bound\", \"upper_bound\"]).issubset(y_pred_interval.columns):\n        raise ValueError(\n            \"`y_pred_interval` must have columns 'lower_bound' and 'upper_bound'.\"\n        )\n\n    if isinstance(y_true, (pd.DataFrame, dict)) and 'level' not in y_pred_interval.columns:\n        raise ValueError(\n            \"If `y_true` is a pandas DataFrame or a dictionary, `y_pred_interval` \"\n            \"must have an additional column 'level' to identify each series.\"\n        )\n\n    if isinstance(y_true, pd.Series):\n        name = y_true.name if y_true.name is not None else 'y'\n        self.fit_input_type_ = \"single_series\"    \n        y_true = {name: y_true}\n\n        if \"level\" not in y_pred_interval.columns:\n            y_pred_interval = y_pred_interval.copy()\n            y_pred_interval[\"level\"] = name\n        else:\n            if y_pred_interval[\"level\"].nunique() &gt; 1:\n                raise ValueError(\n                    \"If `y_true` is a pandas Series, `y_pred_interval` must have \"\n                    \"only one series. Found multiple values in column 'level'.\"\n                )\n            if y_pred_interval[\"level\"].iat[0] != name:\n                raise ValueError(\n                    f\"Series name in `y_true`, '{name}', does not match the level \"\n                    f\"name in `y_pred_interval`, '{y_pred_interval['level'].iat[0]}'.\"\n                )\n    elif isinstance(y_true, pd.DataFrame):\n        self.fit_input_type_ = \"multiple_series\"\n        y_true = y_true.to_dict(orient='series')\n    else:\n        self.fit_input_type_ = \"multiple_series\"\n        for k, v in y_true.items():\n            if not isinstance(v, pd.Series):\n                raise ValueError(\n                    f\"When `y_true` is a dict, all its values must be pandas \"\n                    f\"Series. Got {type(v)} for series '{k}'.\"\n                )\n\n    y_pred_interval = {\n        k: v[['lower_bound', 'upper_bound']]\n        for k, v in y_pred_interval.groupby('level')\n    }\n\n    if not y_pred_interval.keys() == y_true.keys():\n        raise ValueError(\n            f\"Series names in `y_true` and `y_pred_interval` do not match.\\n\"\n            f\"   `y_true` series names          : {list(y_true.keys())}\\n\"\n            f\"   `y_pred_interval` series names : {list(y_pred_interval.keys())}\"\n        )\n\n    for k in y_true.keys():\n\n        if not y_true[k].index.equals(y_pred_interval[k].index):\n            raise IndexError(\n                f\"Index of `y_true` and `y_pred_interval` must match. Different \"\n                f\"indices found for series '{k}'.\"\n            )\n\n        y_true_ = np.asarray(y_true[k])\n        y_pred_interval_ = np.asarray(y_pred_interval[k])\n\n        lower_bound = y_pred_interval_[:, 0]\n        upper_bound = y_pred_interval_[:, 1]\n        conformity_scores_lower = lower_bound - y_true_\n        conformity_scores_upper = y_true_ - upper_bound\n        conformity_scores = np.max(\n            [\n                conformity_scores_lower,\n                conformity_scores_upper,\n            ],\n            axis=0,\n        )\n\n        self.correction_factor_[k] = float(np.quantile(conformity_scores, self.nominal_coverage))\n        self.correction_factor_lower_[k] = float(\n            -1 * np.quantile(-1 * conformity_scores_lower, (1 - self.nominal_coverage) / 2)\n        )\n        self.correction_factor_upper_[k] = float(\n            np.quantile(conformity_scores_upper,  1 - (1 - self.nominal_coverage) / 2)\n        )\n        coverage_fit_ = calculate_coverage(\n                            y_true      = y_true_,\n                            lower_bound = lower_bound,\n                            upper_bound = upper_bound,\n                        )\n        self.fit_coverage_[k] = float(coverage_fit_)\n\n    self.is_fitted = True\n    self.fit_series_names_ = list(y_true.keys())\n</code></pre>"},{"location":"api/preprocessing.html#skforecast.preprocessing.preprocessing.ConformalIntervalCalibrator.transform","title":"transform","text":"<pre><code>transform(y_pred_interval)\n</code></pre> <p>Apply the correction factor to the prediction interval to achieve the desired coverage.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred_interval</code> <code>pandas DataFrame</code> <p>Prediction interval to be calibrated using conformal method.</p> <ul> <li>If only intervals for one series are available, <code>y_pred_interval</code>  must have two columns, 'lower_bound' and 'upper_bound'.</li> <li>If multiple series are available, <code>y_pred_interval</code> must be a long-format DataFrame with three columns: 'level', 'lower_bound', and 'upper_bound'. The 'level' column identifies the series to which each interval belongs.</li> </ul> required <p>Returns:</p> Name Type Description <code>y_pred_interval_conformal</code> <code>pandas DataFrame</code> <p>Prediction interval with the correction factor applied.</p> Source code in <code>skforecast\\preprocessing\\preprocessing.py</code> <pre><code>def transform(\n    self, \n    y_pred_interval: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply the correction factor to the prediction interval to achieve the desired\n    coverage.\n\n    Parameters\n    ----------\n    y_pred_interval : pandas DataFrame\n        Prediction interval to be calibrated using conformal method.\n\n        - If only intervals for one series are available, `y_pred_interval` \n        must have two columns, 'lower_bound' and 'upper_bound'.\n        - If multiple series are available, `y_pred_interval` must be\n        a long-format DataFrame with three columns: 'level', 'lower_bound',\n        and 'upper_bound'. The 'level' column identifies the series to which\n        each interval belongs.\n\n    Returns\n    -------\n    y_pred_interval_conformal : pandas DataFrame\n        Prediction interval with the correction factor applied.\n\n    \"\"\"\n\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"ConformalIntervalCalibrator not fitted yet. Call 'fit' with \"\n            \"training data first.\"\n        )\n    if not isinstance(y_pred_interval, pd.DataFrame):\n        raise TypeError(\n            \"`y_pred_interval` must be a pandas DataFrame.\"\n        )\n\n    if not set([\"lower_bound\", \"upper_bound\"]).issubset(y_pred_interval.columns):\n        raise ValueError(\n            \"`y_pred_interval` must have columns 'lower_bound' and 'upper_bound'.\"\n        )\n\n    if self.fit_input_type_ == \"single_series\" and 'level' not in y_pred_interval.columns:\n        y_pred_interval = y_pred_interval.copy()\n        y_pred_interval[\"level\"] = self.fit_series_names_[0]\n\n    if self.fit_input_type_ == \"multiple_series\" and 'level' not in y_pred_interval.columns:\n        raise ValueError(\n            \"The transformer was fitted with multiple series. `y_pred_interval` \"\n            \"must be a long-format DataFrame with three columns: 'level', \"\n            \"'lower_bound', and 'upper_bound'. The 'level' column identifies \"\n            \"the series to which each interval belongs.\"\n        )\n\n    conformalized_intervals = []\n    for k, y_pred_interval_ in y_pred_interval.groupby('level')[['lower_bound', 'upper_bound']]:\n\n        if k not in self.fit_series_names_:\n            raise ValueError(\n                f\"Series '{k}' was not seen during fit. Available series are: \"\n                f\"{self.fit_series_names_}.\"\n            )\n\n        correction_factor = self.correction_factor_[k]   \n        correction_factor_lower = self.correction_factor_lower_[k]\n        correction_factor_upper = self.correction_factor_upper_[k]\n\n        index = y_pred_interval_.index\n        y_pred_interval_ = y_pred_interval_.to_numpy()\n        y_pred_interval_conformal = y_pred_interval_.copy()\n\n        if self.symmetric_calibration:\n            y_pred_interval_conformal[:, 0] = (\n                y_pred_interval_conformal[:, 0] - correction_factor\n            )\n            y_pred_interval_conformal[:, 1] = (\n                y_pred_interval_conformal[:, 1] + correction_factor\n            )\n        else:\n            y_pred_interval_conformal[:, 0] = (\n                y_pred_interval_conformal[:, 0] - correction_factor_lower\n            )\n            y_pred_interval_conformal[:, 1] = (\n                y_pred_interval_conformal[:, 1] + correction_factor_upper\n            )\n\n        # If upper bound is less than lower bound, swap them\n        mask = (\n            y_pred_interval_conformal[:, 1]\n            &lt; y_pred_interval_conformal[:, 0]\n        )\n\n        (\n            y_pred_interval_conformal[mask, 0],\n            y_pred_interval_conformal[mask, 1],\n        ) = (\n            y_pred_interval_conformal[mask, 1],\n            y_pred_interval_conformal[mask, 0],\n        )\n\n        y_pred_interval_conformal = pd.DataFrame(\n            data    = y_pred_interval_conformal,\n            columns = ['lower_bound', 'upper_bound'],\n            index   = index\n        )\n        y_pred_interval_conformal.insert(0, 'level', k)\n        conformalized_intervals.append(y_pred_interval_conformal)\n\n    conformalized_intervals = pd.concat(conformalized_intervals)\n\n    return conformalized_intervals\n</code></pre>"},{"location":"api/utils.html","title":"<code>utils</code>","text":""},{"location":"api/utils.html#skforecast.utils.utils.save_forecaster","title":"skforecast.utils.utils.save_forecaster","text":"<pre><code>save_forecaster(\n    forecaster,\n    file_name,\n    save_custom_functions=True,\n    verbose=True,\n)\n</code></pre> <p>Save forecaster model using joblib. If custom functions are used to create weights, they are saved as .py files.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster</code> <code>Forecaster</code> <p>Forecaster created with skforecast library.</p> required <code>file_name</code> <code>str</code> <p>File name given to the object. The save extension will be .joblib.</p> required <code>save_custom_functions</code> <code>bool</code> <p>If True, save custom functions used in the forecaster (weight_func) as  .py files. Custom functions need to be available in the environment  where the forecaster is going to be loaded.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print summary about the forecaster saved.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def save_forecaster(\n    forecaster: object, \n    file_name: str,\n    save_custom_functions: bool = True, \n    verbose: bool = True\n) -&gt; None:\n    \"\"\"\n    Save forecaster model using joblib. If custom functions are used to create\n    weights, they are saved as .py files.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster created with skforecast library.\n    file_name : str\n        File name given to the object. The save extension will be .joblib.\n    save_custom_functions : bool, default True\n        If True, save custom functions used in the forecaster (weight_func) as \n        .py files. Custom functions need to be available in the environment \n        where the forecaster is going to be loaded.\n    verbose : bool, default True\n        Print summary about the forecaster saved.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    file_name = Path(file_name).with_suffix('.joblib')\n\n    # Save forecaster\n    joblib.dump(forecaster, filename=file_name)\n\n    if save_custom_functions:\n        # Save custom functions to create weights\n        if hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n            if isinstance(forecaster.weight_func, dict):\n                for fun in set(forecaster.weight_func.values()):\n                    file_name = fun.__name__ + '.py'\n                    with open(file_name, 'w') as file:\n                        file.write(inspect.getsource(fun))\n            else:\n                file_name = forecaster.weight_func.__name__ + '.py'\n                with open(file_name, 'w') as file:\n                    file.write(inspect.getsource(forecaster.weight_func))\n    else:\n        if hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n            warnings.warn(\n                \"Custom function(s) used to create weights are not saved. To save them, \"\n                \"set `save_custom_functions` to `True`.\",\n                SaveLoadSkforecastWarning\n            )\n\n    if hasattr(forecaster, 'window_features') and forecaster.window_features is not None:\n        skforecast_classes = {'RollingFeatures'}\n        custom_classes = set(forecaster.window_features_class_names) - skforecast_classes\n        if custom_classes:\n            warnings.warn(\n                \"The Forecaster includes custom user-defined classes in the \"\n                \"`window_features` argument. These classes are not saved automatically \"\n                \"when saving the Forecaster. Please ensure you save these classes \"\n                \"manually and import them before loading the Forecaster.\\n\"\n                \"    Custom classes: \" + ', '.join(custom_classes) + \"\\n\"\n                \"Visit the documentation for more information: \"\n                \"https://skforecast.org/latest/user_guides/save-load-forecaster.html#saving-and-loading-a-forecaster-model-with-custom-features\",\n                SaveLoadSkforecastWarning\n            )\n\n    if verbose:\n        forecaster.summary()\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.load_forecaster","title":"skforecast.utils.utils.load_forecaster","text":"<pre><code>load_forecaster(file_name, verbose=True)\n</code></pre> <p>Load forecaster model using joblib. If the forecaster was saved with  custom user-defined classes as as window features or custom functions to create weights, these objects must be available in the environment where the forecaster is going to be loaded.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Object file name.</p> required <code>verbose</code> <code>bool</code> <p>Print summary about the forecaster loaded.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>forecaster</code> <code>Forecaster</code> <p>Forecaster created with skforecast library.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def load_forecaster(\n    file_name: str,\n    verbose: bool = True\n) -&gt; object:\n    \"\"\"\n    Load forecaster model using joblib. If the forecaster was saved with \n    custom user-defined classes as as window features or custom\n    functions to create weights, these objects must be available\n    in the environment where the forecaster is going to be loaded.\n\n    Parameters\n    ----------\n    file_name: str\n        Object file name.\n    verbose: bool, default True\n        Print summary about the forecaster loaded.\n\n    Returns\n    -------\n    forecaster: Forecaster\n        Forecaster created with skforecast library.\n\n    \"\"\"\n\n    forecaster = joblib.load(filename=Path(file_name))\n\n    skforecast_v = skforecast.__version__\n    forecaster_v = forecaster.skforecast_version\n\n    if forecaster_v != skforecast_v:\n        warnings.warn(\n            f\"The skforecast version installed in the environment differs \"\n            f\"from the version used to create the forecaster.\\n\"\n            f\"    Installed Version  : {skforecast_v}\\n\"\n            f\"    Forecaster Version : {forecaster_v}\\n\"\n            f\"This may create incompatibilities when using the library.\",\n             SkforecastVersionWarning\n        )\n\n    if verbose:\n        forecaster.summary()\n\n    return forecaster\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_lags","title":"skforecast.utils.utils.initialize_lags","text":"<pre><code>initialize_lags(forecaster_name, lags)\n</code></pre> <p>Check lags argument input and generate the corresponding numpy ndarray.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>lags</code> <code>Any</code> <p>Lags used as predictors.</p> required <p>Returns:</p> Name Type Description <code>lags</code> <code>numpy ndarray, None</code> <p>Lags used as predictors.</p> <code>lags_names</code> <code>(list, None)</code> <p>Names of the lags used as predictors.</p> <code>max_lag</code> <code>(int, None)</code> <p>Maximum value of the lags.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -&gt; tuple[np.ndarray[int] | None, list[str] | None, int | None]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n\n    \"\"\"\n\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags &lt; 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags &lt; 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name == 'ForecasterDirectMultiVariate':\n                raise TypeError(\n                    f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n            else:\n                raise TypeError(\n                    f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                    f\"tuple or list. Got {type(lags)}.\"\n                )\n\n        lags = np.sort(lags)\n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n\n    return lags, lags_names, max_lag\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_weights","title":"skforecast.utils.utils.initialize_weights","text":"<pre><code>initialize_weights(\n    forecaster_name, regressor, weight_func, series_weights\n)\n</code></pre> <p>Check weights arguments, <code>weight_func</code> and <code>series_weights</code> for the different  forecasters. Create <code>source_code_weight_func</code>, source code of the custom  function(s) used to create weights.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>Regressor of the forecaster.</p> required <code>weight_func</code> <code>(Callable, dict)</code> <p>Argument <code>weight_func</code> of the forecaster.</p> required <code>series_weights</code> <code>dict</code> <p>Argument <code>series_weights</code> of the forecaster.</p> required <p>Returns:</p> Name Type Description <code>weight_func</code> <code>(Callable, dict)</code> <p>Argument <code>weight_func</code> of the forecaster.</p> <code>source_code_weight_func</code> <code>(str, dict)</code> <p>Argument <code>source_code_weight_func</code> of the forecaster.</p> <code>series_weights</code> <code>dict</code> <p>Argument <code>series_weights</code> of the forecaster. Only ForecasterRecursiveMultiSeries.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Callable | dict[str, Callable],\n    series_weights: dict[str, float]\n) -&gt; tuple[Callable | dict[str, Callable] | None, str | dict[str, str] | None, dict[str, float] | None]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster. Only ForecasterRecursiveMultiSeries.\n\n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    f\"Argument `weight_func` must be a Callable or a dict of \"\n                    f\"Callables. Got {type(weight_func)}.\"\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                f\"Argument `series_weights` must be a dict of floats or ints.\"\n                f\"Got {type(series_weights)}.\"\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                f\"does not accept `sample_weight` in its `fit` method.\",\n                IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.initialize_transformer_series","title":"skforecast.utils.utils.initialize_transformer_series","text":"<pre><code>initialize_transformer_series(\n    forecaster_name,\n    series_names_in_,\n    encoding=None,\n    transformer_series=None,\n)\n</code></pre> <p>Initialize <code>transformer_series_</code> attribute for the Forecasters Multiseries.</p> <ul> <li>If <code>transformer_series</code> is <code>None</code>, no transformation is applied.</li> <li>If <code>transformer_series</code> is a scikit-learn transformer (object), the same  transformer is applied to all series (<code>series_names_in_</code>).</li> <li>If <code>transformer_series</code> is a <code>dict</code>, a different transformer can be applied to each series. The keys of the dictionary must be the same as the names of the series in <code>series_names_in_</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) used during training.</p> required <code>encoding</code> <code>str</code> <p>Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <code>transformer_series</code> <code>(object, dict)</code> <p>An instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with methods: fit, transform, fit_transform and  inverse_transform.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>transformer_series_</code> <code>dict</code> <p>Dictionary with the transformer for each series. It is created cloning the  objects in <code>transformer_series</code> and is used internally to avoid overwriting.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def initialize_transformer_series(\n    forecaster_name: str,\n    series_names_in_: list[str],\n    encoding: str | None = None,\n    transformer_series: object | dict[str, object | None] | None = None\n) -&gt; dict[str, object | None]:\n    \"\"\"\n    Initialize `transformer_series_` attribute for the Forecasters Multiseries.\n\n    - If `transformer_series` is `None`, no transformation is applied.\n    - If `transformer_series` is a scikit-learn transformer (object), the same \n    transformer is applied to all series (`series_names_in_`).\n    - If `transformer_series` is a `dict`, a different transformer can be\n    applied to each series. The keys of the dictionary must be the same as the\n    names of the series in `series_names_in_`.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    encoding : str, default None\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n    transformer_series : object, dict, default None\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. \n\n    Returns\n    -------\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n\n    \"\"\"\n\n    multiseries_forecasters = [\n        'ForecasterRecursiveMultiSeries',\n    ]\n\n    if forecaster_name in multiseries_forecasters:\n        if encoding is None:\n            series_names_in_ = ['_unknown_level']\n        else:\n            series_names_in_ = series_names_in_ + ['_unknown_level']\n\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {\n            serie: clone(transformer_series) \n            for serie in series_names_in_\n        }\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        # Only elements already present in transformer_series_ are updated\n        transformer_series_.update(\n            {\n                k: deepcopy(v)\n                for k, v in transformer_series.items()\n                if k in transformer_series_\n            }\n        )\n\n        series_not_in_transformer_series = (\n            set(series_names_in_) - set(transformer_series.keys())\n        ) - {'_unknown_level'}\n        if series_not_in_transformer_series:\n            warnings.warn(\n                f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                f\" No transformation is applied to these series.\",\n                IgnoredArgumentWarning\n            )\n\n    return transformer_series_\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_select_fit_kwargs","title":"skforecast.utils.utils.check_select_fit_kwargs","text":"<pre><code>check_select_fit_kwargs(regressor, fit_kwargs=None)\n</code></pre> <p>Check if <code>fit_kwargs</code> is a dict and select only the keys that are used by the <code>fit</code> method of the regressor.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>object</code> <p>Regressor object.</p> required <code>fit_kwargs</code> <code>dict</code> <p>Dictionary with the arguments to pass to the `fit' method of the forecaster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fit_kwargs</code> <code>dict</code> <p>Dictionary with the arguments to be passed to the <code>fit</code> method of the  regressor after removing the unused keys.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: dict[str, object] | None = None\n) -&gt; dict[str, object]:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default None\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n\n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        fit_params = inspect.signature(regressor.fit).parameters\n\n        # Non used keys\n        non_used_keys = [\n            k for k in fit_kwargs.keys() if k not in fit_params\n        ]\n        if non_used_keys:\n            warnings.warn(\n                f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                f\"regressor's `fit` method.\",\n                IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                \"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                \"a function that defines the individual weights for each sample \"\n                \"based on its index.\",\n                IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {\n            k: v for k, v in fit_kwargs.items() if k in fit_params\n        }\n\n    return fit_kwargs\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_y","title":"skforecast.utils.utils.check_y","text":"<pre><code>check_y(y, series_id='`y`')\n</code></pre> <p>Raise Exception if <code>y</code> is not pandas Series or if it has missing values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>Any</code> <p>Time series values.</p> required <code>series_id</code> <code>str</code> <p>Identifier of the series used in the warning message.</p> <code>'`y`'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_y(\n    y: Any,\n    series_id: str = \"`y`\"\n) -&gt; None:\n    \"\"\"\n    Raise Exception if `y` is not pandas Series or if it has missing values.\n\n    Parameters\n    ----------\n    y : Any\n        Time series values.\n    series_id : str, default '`y`'\n        Identifier of the series used in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(y, pd.Series):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series with a DatetimeIndex or a RangeIndex. \"\n            f\"Found {type(y)}.\"\n        )\n\n    if y.isna().to_numpy().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_exog","title":"skforecast.utils.utils.check_exog","text":"<pre><code>check_exog(exog, allow_nan=True, series_id='`exog`')\n</code></pre> <p>Raise Exception if <code>exog</code> is not pandas Series or pandas DataFrame. If <code>allow_nan = True</code>, issue a warning if <code>exog</code> contains NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>allow_nan</code> <code>bool</code> <p>If True, allows the presence of NaN values in <code>exog</code>. If False (default), issue a warning if <code>exog</code> contains NaN values.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series for which the exogenous variable/s are used in the warning message.</p> <code>'`exog`'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_exog(\n    exog: pd.Series | pd.DataFrame,\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\"\n) -&gt; None:\n    \"\"\"\n    Raise Exception if `exog` is not pandas Series or pandas DataFrame.\n    If `allow_nan = True`, issue a warning if `exog` contains NaN values.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variable/s included as predictor/s.\n    allow_nan : bool, default True\n        If True, allows the presence of NaN values in `exog`. If False (default),\n        issue a warning if `exog` contains NaN values.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isna().to_numpy().any():\n            warnings.warn(\n                f\"{series_id} has missing values. Most machine learning models \"\n                f\"do not allow missing values. Fitting the forecaster may fail.\", \n                MissingValuesWarning\n            )\n\n    return\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.get_exog_dtypes","title":"skforecast.utils.utils.get_exog_dtypes","text":"<pre><code>get_exog_dtypes(exog)\n</code></pre> <p>Store dtypes of <code>exog</code>.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> required <p>Returns:</p> Name Type Description <code>exog_dtypes</code> <code>dict</code> <p>Dictionary with the dtypes in <code>exog</code>.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def get_exog_dtypes(\n    exog: pd.Series | pd.DataFrame, \n) -&gt; dict[str, type]:\n    \"\"\"\n    Store dtypes of `exog`.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    exog_dtypes : dict\n        Dictionary with the dtypes in `exog`.\n\n    \"\"\"\n\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n\n    return exog_dtypes\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_exog_dtypes","title":"skforecast.utils.utils.check_exog_dtypes","text":"<pre><code>check_exog_dtypes(\n    exog, call_check_exog=True, series_id=\"`exog`\"\n)\n</code></pre> <p>Raise Exception if <code>exog</code> has categorical columns with non integer values. This is needed when using machine learning regressors that allow categorical features. Issue a Warning if <code>exog</code> has columns that are not <code>init</code>, <code>float</code>, or <code>category</code>.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variable/s included as predictor/s.</p> required <code>call_check_exog</code> <code>bool</code> <p>If <code>True</code>, call <code>check_exog</code> function.</p> <code>True</code> <code>series_id</code> <code>str</code> <p>Identifier of the series for which the exogenous variable/s are used in the warning message.</p> <code>'`exog`'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_exog_dtypes(\n    exog: pd.Series | pd.DataFrame,\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\"\n) -&gt; None:\n    \"\"\"\n    Raise Exception if `exog` has categorical columns with non integer values.\n    This is needed when using machine learning regressors that allow categorical\n    features.\n    Issue a Warning if `exog` has columns that are not `init`, `float`, or `category`.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variable/s included as predictor/s.\n    call_check_exog : bool, default True\n        If `True`, call `check_exog` function.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    valid_dtypes = (\"int\", \"Int\", \"float\", \"Float\", \"uint\")\n\n    if isinstance(exog, pd.DataFrame):\n\n        for dtype_name in set(exog.dtypes.astype(str)):\n            if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n                warnings.warn(\n                    f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                    f\"Most machine learning models do not allow other types of values. \"\n                    f\"Fitting the forecaster may fail.\", \n                    DataTypeWarning\n                )\n                break\n\n        for col in exog.columns:\n            if isinstance(exog[col].dtype, pd.CategoricalDtype):\n                if not np.issubdtype(exog[col].cat.categories.dtype, np.integer):\n                    raise TypeError(\n                        \"Categorical dtypes in exog must contain only integer values. \"\n                        \"See skforecast docs for more info about how to include \"\n                        \"categorical features https://skforecast.org/\"\n                        \"latest/user_guides/categorical-features.html\"\n                    )\n\n    else:\n\n        dtype_name = str(exog.dtypes)\n        if not (dtype_name.startswith(valid_dtypes) or dtype_name == \"category\"):\n            warnings.warn(\n                f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                f\"machine learning models do not allow other types of values. \"\n                f\"Fitting the forecaster may fail.\", \n                DataTypeWarning\n            )\n\n        if isinstance(exog.dtype, pd.CategoricalDtype):\n            if not np.issubdtype(exog.cat.categories.dtype, np.integer):\n                raise TypeError(\n                    \"Categorical dtypes in exog must contain only integer values. \"\n                    \"See skforecast docs for more info about how to include \"\n                    \"categorical features https://skforecast.org/\"\n                    \"latest/user_guides/categorical-features.html\"\n                )\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_interval","title":"skforecast.utils.utils.check_interval","text":"<pre><code>check_interval(\n    interval=None,\n    ensure_symmetric_intervals=False,\n    quantiles=None,\n    alpha=None,\n    alpha_literal=\"alpha\",\n)\n</code></pre> <p>Check provided confidence interval sequence is valid.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>(list, tuple)</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>ensure_symmetric_intervals</code> <code>bool</code> <p>If True, ensure that the intervals are symmetric.</p> <code>False</code> <code>quantiles</code> <code>(list, tuple)</code> <p>Sequence of quantiles to compute, which must be between 0 and 1  inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as  <code>quantiles = [0.05, 0.5, 0.95]</code>.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>None</code> <code>alpha_literal</code> <code>str</code> <p>Literal used in the exception message when <code>alpha</code> is provided.</p> <code>'alpha'</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_interval(\n    interval: list[float] | tuple[float] | None = None,\n    ensure_symmetric_intervals: bool = False,\n    quantiles: list[float] | tuple[float] | None = None,\n    alpha: float = None,\n    alpha_literal: str | None = 'alpha'\n) -&gt; None:\n    \"\"\"\n    Check provided confidence interval sequence is valid.\n\n    Parameters\n    ----------\n    interval : list, tuple, default None\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    ensure_symmetric_intervals : bool, default False\n        If True, ensure that the intervals are symmetric.\n    quantiles : list, tuple, default None\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    alpha : float, default None\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n    alpha_literal : str, default 'alpha'\n        Literal used in the exception message when `alpha` is provided.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if interval is not None:\n        if not isinstance(interval, (list, tuple)):\n            raise TypeError(\n                \"`interval` must be a `list` or `tuple`. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                \"`interval` must contain exactly 2 values, respectively the \"\n                \"lower and upper interval bounds. For example, interval of 95% \"\n                \"should be as `interval = [2.5, 97.5]`.\"\n            )\n\n        if (interval[0] &lt; 0.) or (interval[0] &gt;= 100.):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be &gt;= 0 and &lt; 100.\"\n            )\n\n        if (interval[1] &lt;= 0.) or (interval[1] &gt; 100.):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be &gt; 0 and &lt;= 100.\"\n            )\n\n        if interval[0] &gt;= interval[1]:\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be less than the \"\n                f\"upper interval bound ({interval[1]}).\"\n            )\n\n        if ensure_symmetric_intervals and interval[0] + interval[1] != 100:\n            raise ValueError(\n                f\"Interval must be symmetric, the sum of the lower, ({interval[0]}), \"\n                f\"and upper, ({interval[1]}), interval bounds must be equal to \"\n                f\"100. Got {interval[0] + interval[1]}.\"\n            )\n\n    if quantiles is not None:\n        if not isinstance(quantiles, (list, tuple)):\n            raise TypeError(\n                \"`quantiles` must be a `list` or `tuple`. For example, quantiles \"\n                \"0.05, 0.5, and 0.95 should be as `quantiles = [0.05, 0.5, 0.95]`.\"\n            )\n\n        for q in quantiles:\n            if (q &lt; 0.) or (q &gt; 1.):\n                raise ValueError(\n                    \"All elements in `quantiles` must be &gt;= 0 and &lt;= 1.\"\n                )\n\n    if alpha is not None:\n        if not isinstance(alpha, float):\n            raise TypeError(\n                f\"`{alpha_literal}` must be a `float`. For example, interval of 95% \"\n                f\"should be as `alpha = 0.05`.\"\n            )\n\n        if (alpha &lt;= 0.) or (alpha &gt;= 1):\n            raise ValueError(\n                f\"`{alpha_literal}` must have a value between 0 and 1. Got {alpha}.\"\n            )\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_predict_input","title":"skforecast.utils.utils.check_predict_input","text":"<pre><code>check_predict_input(\n    forecaster_name,\n    steps,\n    is_fitted,\n    exog_in_,\n    index_type_,\n    index_freq_,\n    window_size,\n    last_window,\n    last_window_exog=None,\n    exog=None,\n    exog_names_in_=None,\n    interval=None,\n    alpha=None,\n    max_step=None,\n    levels=None,\n    levels_forecaster=None,\n    series_names_in_=None,\n    encoding=None,\n)\n</code></pre> <p>Check all inputs of predict method. This is a helper function to validate that inputs used in predict method match attributes of a forecaster already trained.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>steps</code> <code>(int, list)</code> <p>Number of future steps predicted.</p> required <code>is_fitted</code> <code>bool</code> <p>Tag to identify if the regressor has been fitted (trained).</p> required <code>exog_in_</code> <code>bool</code> <p>If the forecaster has been trained using exogenous variable/s.</p> required <code>index_type_</code> <code>type</code> <p>Type of index of the input used in training.</p> required <code>index_freq_</code> <code>str</code> <p>Frequency of Index of the input used in training.</p> required <code>window_size</code> <code>int</code> <p>Size of the window needed to create the predictors. It is equal to  <code>max_lag</code>.</p> required <code>last_window</code> <code>pandas Series, pandas DataFrame, None</code> <p>Values of the series used to create the predictors (lags) need in the  first iteration of prediction (t + 1).</p> required <code>last_window_exog</code> <code>pandas Series, pandas DataFrame</code> <p>Values of the exogenous variables aligned with <code>last_window</code> in  ForecasterSarimax predictions.</p> <code>None</code> <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s included as predictor/s.</p> <code>None</code> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> <code>None</code> <code>interval</code> <code>(list, tuple)</code> <p>Confidence of the prediction interval estimated. Sequence of percentiles to compute, which must be between 0 and 100 inclusive. For example,  interval of 95% should be as <code>interval = [2.5, 97.5]</code>.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>The confidence intervals used in ForecasterSarimax are (1 - alpha) %.</p> <code>None</code> <code>max_step</code> <code>int | None</code> <p>Maximum number of steps allowed (<code>ForecasterDirect</code> and  <code>ForecasterDirectMultiVariate</code>).</p> <code>None</code> <code>levels</code> <code>(str, list)</code> <p>Time series to be predicted (<code>ForecasterRecursiveMultiSeries</code> and `ForecasterRnn).</p> <code>None</code> <code>levels_forecaster</code> <code>(str, list)</code> <p>Time series used as output data of a multiseries problem in a RNN problem (<code>ForecasterRnn</code>).</p> <code>None</code> <code>series_names_in_</code> <code>list</code> <p>Names of the columns used during fit (<code>ForecasterRecursiveMultiSeries</code>,  <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRnn</code>).</p> <code>None</code> <code>encoding</code> <code>str</code> <p>Encoding used to identify the different series (<code>ForecasterRecursiveMultiSeries</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_predict_input(\n    forecaster_name: str,\n    steps: int | list[int],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: pd.Series | pd.DataFrame | None,\n    last_window_exog: pd.Series | pd.DataFrame | None = None,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame] | None = None,\n    exog_names_in_: list[str] | None = None,\n    interval: list[float] | None = None,\n    alpha: float | None = None,\n    max_step: int | None = None,\n    levels: str | list[str] | None = None,\n    levels_forecaster: str | list[str] | None = None,\n    series_names_in_: list[str] | None = None,\n    encoding: str | None = None\n) -&gt; None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    steps : int, list\n        Number of future steps predicted.\n    is_fitted: bool\n        Tag to identify if the regressor has been fitted (trained).\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    window_size: int\n        Size of the window needed to create the predictors. It is equal to \n        `max_lag`.\n    last_window : pandas Series, pandas DataFrame, None\n        Values of the series used to create the predictors (lags) need in the \n        first iteration of prediction (t + 1).\n    last_window_exog : pandas Series, pandas DataFrame, default None\n        Values of the exogenous variables aligned with `last_window` in \n        ForecasterSarimax predictions.\n    exog : pandas Series, pandas DataFrame, dict, default None\n        Exogenous variable/s included as predictor/s.\n    exog_names_in_ : list, default None\n        Names of the exogenous variables used during training.\n    interval : list, tuple, default None\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    alpha : float, default None\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n    max_step: int, default None\n        Maximum number of steps allowed (`ForecasterDirect` and \n        `ForecasterDirectMultiVariate`).\n    levels : str, list, default None\n        Time series to be predicted (`ForecasterRecursiveMultiSeries`\n        and `ForecasterRnn).\n    levels_forecaster : str, list, default None\n        Time series used as output data of a multiseries problem in a RNN problem\n        (`ForecasterRnn`).\n    series_names_in_ : list, default None\n        Names of the columns used during fit (`ForecasterRecursiveMultiSeries`, \n        `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n    encoding : str, default None\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not is_fitted:\n        raise NotFittedError(\n            \"This Forecaster instance is not fitted yet. Call `fit` with \"\n            \"appropriate arguments before using predict.\"\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps &lt; 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) &lt; 1:\n        raise ValueError(\n           f\"The minimum value of `steps` must be equal to or greater than 1. \"\n           f\"Got {min(steps)}.\"\n        )\n\n    if max_step is not None:\n        if max(steps) &gt; max_step:\n            raise ValueError(\n                f\"The maximum value of `steps` must be less than or equal to \"\n                f\"the value of steps defined when initializing the forecaster. \"\n                f\"Got {max(steps)}, but the maximum is {max_step}.\"\n            )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']:\n        if not isinstance(levels, (type(None), str, list)):\n            raise TypeError(\n                \"`levels` must be a `list` of column names, a `str` of a \"\n                \"column name or `None`.\"\n            )\n\n        levels_to_check = (\n            levels_forecaster if forecaster_name == 'ForecasterRnn'\n            else series_names_in_\n        )\n        unknown_levels = set(levels) - set(levels_to_check)\n        if forecaster_name == 'ForecasterRnn':\n            if len(unknown_levels) != 0:\n                raise ValueError(\n                    f\"`levels` names must be included in the series used during fit \"\n                    f\"({levels_to_check}). Got {levels}.\"\n                )\n        else:\n            if len(unknown_levels) != 0 and last_window is not None and encoding is not None:\n                if encoding == 'onehot':\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} were not included in training. The resulting \"\n                        f\"one-hot encoded columns for this feature will be all zeros.\",\n                        UnknownLevelWarning\n                    )\n                else:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} were not included in training. \"\n                        f\"Unknown levels are encoded as NaN, which may cause the \"\n                        f\"prediction to fail if the regressor does not accept NaN values.\",\n                        UnknownLevelWarning\n                    )\n\n    if exog is None and exog_in_:\n        raise ValueError(\n            \"Forecaster trained with exogenous variable/s. \"\n            \"Same variable/s must be provided when predicting.\"\n        )\n\n    if exog is not None and not exog_in_:\n        raise ValueError(\n            \"Forecaster trained without exogenous variable/s. \"\n            \"`exog` must be `None` when predicting.\"\n        )\n\n    # Checks last_window\n    # Check last_window type (pd.Series or pd.DataFrame according to forecaster)\n    if isinstance(last_window, type(None)) and forecaster_name not in [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterRnn'\n    ]:\n        raise ValueError(\n            \"`last_window` was not stored during training. If you don't want \"\n            \"to retrain the Forecaster, provide `last_window` as argument.\"\n        )\n\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', \n                           'ForecasterDirectMultiVariate',\n                           'ForecasterRnn']:\n        if not isinstance(last_window, pd.DataFrame):\n            raise TypeError(\n                f\"`last_window` must be a pandas DataFrame. Got {type(last_window)}.\"\n            )\n\n        last_window_cols = last_window.columns.to_list()\n\n        if (\n            forecaster_name in [\"ForecasterRecursiveMultiSeries\", \"ForecasterRnn\"]\n            and len(set(levels) - set(last_window_cols)) != 0\n        ):\n            missing_levels = set(levels) - set(last_window_cols)\n            raise ValueError(\n                f\"`last_window` must contain a column(s) named as the level(s) to be predicted. \"\n                f\"The following `levels` are missing in `last_window`: {missing_levels}\\n\"\n                f\"Ensure that `last_window` contains all the necessary columns \"\n                f\"corresponding to the `levels` being predicted.\\n\"\n                f\"    Argument `levels`     : {levels}\\n\"\n                f\"    `last_window` columns : {last_window_cols}\\n\"\n                f\"Example: If `levels = ['series_1', 'series_2']`, make sure \"\n                f\"`last_window` includes columns named 'series_1' and 'series_2'.\"\n            )\n\n        if forecaster_name == 'ForecasterDirectMultiVariate':\n            if len(set(series_names_in_) - set(last_window_cols)) &gt; 0:\n                raise ValueError(\n                    f\"`last_window` columns must be the same as the `series` \"\n                    f\"column names used to create the X_train matrix.\\n\"\n                    f\"    `last_window` columns    : {last_window_cols}\\n\"\n                    f\"    `series` columns X train : {series_names_in_}\"\n                )\n    else:\n        if not isinstance(last_window, (pd.Series, pd.DataFrame)):\n            raise TypeError(\n                f\"`last_window` must be a pandas Series or DataFrame. \"\n                f\"Got {type(last_window)}.\"\n            )\n\n    # Check last_window len, nulls and index (type and freq)\n    if len(last_window) &lt; window_size:\n        raise ValueError(\n            f\"`last_window` must have as many values as needed to \"\n            f\"generate the predictors. For this forecaster it is {window_size}.\"\n        )\n    if last_window.isna().to_numpy().any():\n        warnings.warn(\n            \"`last_window` has missing values. Most of machine learning models do \"\n            \"not allow missing values. Prediction method may fail.\", \n            MissingValuesWarning\n        )\n\n    _, last_window_index = check_extract_values_and_index(\n        data=last_window, data_label='`last_window`', ignore_freq=False, return_values=False\n    )\n    if not isinstance(last_window_index, index_type_):\n        raise TypeError(\n            f\"Expected index of type {index_type_} for `last_window`. \"\n            f\"Got {type(last_window_index)}.\"\n        )\n    if isinstance(last_window_index, pd.DatetimeIndex):\n        if not last_window_index.freqstr == index_freq_:\n            raise TypeError(\n                f\"Expected frequency of type {index_freq_} for `last_window`. \"\n                f\"Got {last_window_index.freqstr}.\"\n            )\n\n    # Checks exog\n    if exog is not None:\n\n        # Check type, nulls and expected type\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or dict. Got {type(exog)}.\"\n                )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\"\n                )\n\n        if isinstance(exog, dict):\n            no_exog_levels = set(levels) - set(exog.keys())\n            if no_exog_levels:\n                warnings.warn(\n                    f\"`exog` does not contain keys for levels {no_exog_levels}. \"\n                    f\"Missing levels are filled with NaN. Most of machine learning \"\n                    f\"models do not allow missing values. Prediction method may fail.\",\n                    MissingExogWarning\n                )\n            exogs_to_check = [\n                (f\"`exog` for series '{k}'\", v) \n                for k, v in exog.items() \n                if v is not None and k in levels\n            ]\n        else:\n            exogs_to_check = [('`exog`', exog)]\n\n        last_step = max(steps) if isinstance(steps, list) else steps\n        expected_index = expand_index(last_window_index, 1)[0]\n        for exog_name, exog_to_check in exogs_to_check:\n\n            if not isinstance(exog_to_check, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"{exog_name} must be a pandas Series or DataFrame. Got {type(exog_to_check)}\"\n                )\n\n            if exog_to_check.isna().to_numpy().any():\n                warnings.warn(\n                    f\"{exog_name} has missing values. Most of machine learning models \"\n                    f\"do not allow missing values. Prediction method may fail.\", \n                    MissingValuesWarning\n                )\n\n            # Check exog has many values as distance to max step predicted\n            if len(exog_to_check) &lt; last_step:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(\n                        f\"{exog_name} doesn't have as many values as steps \"\n                        f\"predicted, {last_step}. Missing values are filled \"\n                        f\"with NaN. Most of machine learning models do not \"\n                        f\"allow missing values. Prediction method may fail.\",\n                        MissingValuesWarning\n                    )\n                else: \n                    raise ValueError(\n                        f\"{exog_name} must have at least as many values as \"\n                        f\"steps predicted, {last_step}.\"\n                    )\n\n            # Check name/columns are in exog_names_in_\n            if isinstance(exog_to_check, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(exog_to_check.columns))\n                if col_missing:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(\n                            f\"{col_missing} not present in {exog_name}. All \"\n                            f\"values will be NaN.\",\n                            MissingExogWarning\n                        ) \n                    else:\n                        raise ValueError(\n                            f\"Missing columns in {exog_name}. Expected {exog_names_in_}. \"\n                            f\"Got {exog_to_check.columns.to_list()}.\"\n                        )\n            else:\n                if exog_to_check.name is None:\n                    raise ValueError(\n                        f\"When {exog_name} is a pandas Series, it must have a name. Got None.\"\n                    )\n\n                if exog_to_check.name not in exog_names_in_:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(\n                            f\"'{exog_to_check.name}' was not observed during training. \"\n                            f\"{exog_name} is ignored. Exogenous variables must be one \"\n                            f\"of: {exog_names_in_}.\",\n                            IgnoredArgumentWarning\n                        )\n                    else:\n                        raise ValueError(\n                            f\"'{exog_to_check.name}' was not observed during training. \"\n                            f\"Exogenous variables must be: {exog_names_in_}.\"\n                        )\n\n            # Check index dtype and freq\n            _, exog_index = check_extract_values_and_index(\n                data=exog_to_check, data_label=exog_name, ignore_freq=True, return_values=False\n            )\n            if not isinstance(exog_index, index_type_):\n                raise TypeError(\n                    f\"Expected index of type {index_type_} for {exog_name}. \"\n                    f\"Got {type(exog_index)}.\"\n                )\n\n            # Check exog starts one step ahead of last_window end.\n            if expected_index != exog_index[0]:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(\n                        f\"To make predictions {exog_name} must start one step \"\n                        f\"ahead of `last_window`. Missing values are filled \"\n                        f\"with NaN.\\n\"\n                        f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                        f\"    {exog_name} starts at : {exog_index[0]}.\\n\"\n                        f\"    Expected index : {expected_index}.\",\n                        MissingValuesWarning\n                    )  \n                else:\n                    raise ValueError(\n                        f\"To make predictions {exog_name} must start one step \"\n                        f\"ahead of `last_window`.\\n\"\n                        f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                        f\"    {exog_name} starts at : {exog_index[0]}.\\n\"\n                        f\"    Expected index : {expected_index}.\"\n                    )\n\n    # Checks ForecasterSarimax\n    if forecaster_name == 'ForecasterSarimax':\n        # Check last_window_exog type, len, nulls and index (type and freq)\n        if last_window_exog is not None:\n            if not exog_in_:\n                raise ValueError(\n                    \"Forecaster trained without exogenous variable/s. \"\n                    \"`last_window_exog` must be `None` when predicting.\"\n                )\n\n            if not isinstance(last_window_exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`last_window_exog` must be a pandas Series or a \"\n                    f\"pandas DataFrame. Got {type(last_window_exog)}.\"\n                )\n            if len(last_window_exog) &lt; window_size:\n                raise ValueError(\n                    f\"`last_window_exog` must have as many values as needed to \"\n                    f\"generate the predictors. For this forecaster it is {window_size}.\"\n                )\n            if last_window_exog.isna().to_numpy().any():\n                warnings.warn(\n                    \"`last_window_exog` has missing values. Most of machine learning \"\n                    \"models do not allow missing values. Prediction method may fail.\",\n                    MissingValuesWarning\n            )\n            _, last_window_exog_index = check_extract_values_and_index(\n                data=last_window_exog, data_label='`last_window_exog`', return_values=False\n            )\n            if not isinstance(last_window_exog_index, index_type_):\n                raise TypeError(\n                    f\"Expected index of type {index_type_} for `last_window_exog`. \"\n                    f\"Got {type(last_window_exog_index)}.\"\n                )\n            if isinstance(last_window_exog_index, pd.DatetimeIndex):\n                if not last_window_exog_index.freqstr == index_freq_:\n                    raise TypeError(\n                        f\"Expected frequency of type {index_freq_} for \"\n                        f\"`last_window_exog`. Got {last_window_exog_index.freqstr}.\"\n                    )\n\n            # Check all columns are in the pd.DataFrame, last_window_exog\n            if isinstance(last_window_exog, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(last_window_exog.columns))\n                if col_missing:\n                    raise ValueError(\n                        f\"Missing columns in `last_window_exog`. Expected {exog_names_in_}. \"\n                        f\"Got {last_window_exog.columns.to_list()}.\"\n                    )\n            else:\n                if last_window_exog.name is None:\n                    raise ValueError(\n                        \"When `last_window_exog` is a pandas Series, it must have a \"\n                        \"name. Got None.\"\n                    )\n\n                if last_window_exog.name not in exog_names_in_:\n                    raise ValueError(\n                        f\"'{last_window_exog.name}' was not observed during training. \"\n                        f\"Exogenous variables must be: {exog_names_in_}.\"\n                    )\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_residuals_input","title":"skforecast.utils.utils.check_residuals_input","text":"<pre><code>check_residuals_input(\n    forecaster_name,\n    use_in_sample_residuals,\n    in_sample_residuals_,\n    out_sample_residuals_,\n    use_binned_residuals,\n    in_sample_residuals_by_bin_,\n    out_sample_residuals_by_bin_,\n    levels=None,\n    encoding=None,\n)\n</code></pre> <p>Check residuals input arguments in Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>use_in_sample_residuals</code> <code>bool</code> <p>Indicates if in sample or out sample residuals are used.</p> required <code>in_sample_residuals_</code> <code>numpy ndarray, dict</code> <p>Residuals of the model when predicting training data.</p> required <code>out_sample_residuals_</code> <code>numpy ndarray, dict</code> <p>Residuals of the model when predicting non training data.</p> required <code>use_binned_residuals</code> <code>bool</code> <p>Indicates if residuals are binned.</p> required <code>in_sample_residuals_by_bin_</code> <code>dict</code> <p>In sample residuals binned according to the predicted value each residual is associated with.</p> required <code>out_sample_residuals_by_bin_</code> <code>dict</code> <p>Out of sample residuals binned according to the predicted value each residual is associated with.</p> required <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted (Forecasters multiseries).</p> <code>None</code> <code>encoding</code> <code>str</code> <p>Encoding used to identify the different series (ForecasterRecursiveMultiSeries).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_residuals_input(\n    forecaster_name: str,\n    use_in_sample_residuals: bool,\n    in_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    out_sample_residuals_: np.ndarray | dict[str, np.ndarray] | None,\n    use_binned_residuals: bool,\n    in_sample_residuals_by_bin_: dict[str | int, np.ndarray | dict[int, np.ndarray]] | None,\n    out_sample_residuals_by_bin_: dict[str | int, np.ndarray | dict[int, np.ndarray]] | None,\n    levels: list[str] | None = None,\n    encoding: str | None = None\n) -&gt; None:\n    \"\"\"\n    Check residuals input arguments in Forecasters.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    use_in_sample_residuals : bool\n        Indicates if in sample or out sample residuals are used.\n    in_sample_residuals_ : numpy ndarray, dict\n        Residuals of the model when predicting training data.\n    out_sample_residuals_ : numpy ndarray, dict\n        Residuals of the model when predicting non training data.\n    use_binned_residuals : bool\n        Indicates if residuals are binned.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with.\n    levels : list, default None\n        Names of the series (levels) to be predicted (Forecasters multiseries).\n    encoding : str, default None\n        Encoding used to identify the different series (ForecasterRecursiveMultiSeries).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    forecasters_multiseries = [\n        'ForecasterRecursiveMultiSeries',\n        'ForecasterDirectMultiVariate',\n        'ForecasterRnn'\n    ]\n\n    if use_in_sample_residuals:\n        if use_binned_residuals:\n            residuals = in_sample_residuals_by_bin_\n            literal = \"in_sample_residuals_by_bin_\"\n        else:\n            residuals = in_sample_residuals_\n            literal = \"in_sample_residuals_\"\n\n        if (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        ):\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`store_in_sample_residuals = True` when fitting the forecaster \"\n                f\"or use the `set_in_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`, \"\n                        f\"most likely because they were not present in the training data. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels.\",\n                        UnknownLevelWarning\n                    )\n    else:\n        if use_binned_residuals:\n            residuals = out_sample_residuals_by_bin_\n            literal = \"out_sample_residuals_by_bin_\"\n        else:\n            residuals = out_sample_residuals_\n            literal = \"out_sample_residuals_\"\n\n        if (\n            residuals is None\n            or (isinstance(residuals, dict) and not residuals)\n            or (isinstance(residuals, np.ndarray) and residuals.size == 0)\n        ):\n            raise ValueError(\n                f\"`forecaster.{literal}` is either None or empty. Use \"\n                f\"`use_in_sample_residuals = True` or the \"\n                f\"`set_out_sample_residuals()` method before predicting.\"\n            )\n\n        if forecaster_name in forecasters_multiseries:\n            if encoding is not None:\n                unknown_levels = set(levels) - set(residuals.keys())\n                if unknown_levels:\n                    warnings.warn(\n                        f\"`levels` {unknown_levels} are not present in `forecaster.{literal}`. \"\n                        f\"A random sample of the residuals from other levels will be used. \"\n                        f\"This can lead to inaccurate intervals for the unknown levels. \"\n                        f\"Otherwise, Use the `set_out_sample_residuals()` method before \"\n                        f\"predicting to set the residuals for these levels.\",\n                        UnknownLevelWarning\n                    )\n\n    if forecaster_name in forecasters_multiseries:\n        for level in residuals.keys():\n            if residuals[level] is None or len(residuals[level]) == 0:\n                raise ValueError(\n                    f\"Residuals for level '{level}' are None. Check `forecaster.{literal}`.\"\n                )\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_extract_values_and_index","title":"skforecast.utils.utils.check_extract_values_and_index","text":"<pre><code>check_extract_values_and_index(\n    data,\n    data_label=\"`y`\",\n    ignore_freq=False,\n    return_values=True,\n)\n</code></pre> <p>Return values and index of series separately. Check that index is a pandas <code>DatetimeIndex</code> or <code>RangeIndex</code>. Optionally, check that the index has a frequency.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pandas Series, pandas DataFrame</code> <p>Time series.</p> required <code>data_label</code> <code>str</code> <p>Label of the data to be used in warnings and errors.</p> <code>'`y`'</code> <code>ignore_freq</code> <code>bool</code> <p>If <code>True</code>, ignore the frequency of the index. If <code>False</code>, check that the index is a pandas <code>DatetimeIndex</code> with a frequency.</p> <code>False</code> <code>return_values</code> <code>bool</code> <p>If <code>True</code> return the values of <code>data</code> as numpy ndarray. This option is intended to avoid copying data when it is not necessary.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>data_values</code> <code>numpy ndarray, None</code> <p>Numpy array with values of <code>data</code>.</p> <code>data_index</code> <code>pandas Index</code> <p>Index of <code>data</code>.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_extract_values_and_index(\n    data: pd.Series | pd.DataFrame,\n    data_label: str = '`y`',\n    ignore_freq: bool = False,\n    return_values: bool = True\n) -&gt; tuple[np.ndarray | None, pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Check that index is a pandas\n    `DatetimeIndex` or `RangeIndex`. Optionally, check that the index has a\n    frequency.\n\n    Parameters\n    ----------\n    data : pandas Series, pandas DataFrame\n        Time series.\n    data_label : str, default '`y`'\n        Label of the data to be used in warnings and errors.\n    ignore_freq : bool, default False\n        If `True`, ignore the frequency of the index. If `False`, check that the\n        index is a pandas `DatetimeIndex` with a frequency.\n    return_values : bool, default True\n        If `True` return the values of `data` as numpy ndarray. This option is\n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    data_values : numpy ndarray, None\n        Numpy array with values of `data`.\n    data_index : pandas Index\n        Index of `data`.\n\n    \"\"\"\n\n    if isinstance(data.index, pd.DatetimeIndex):            \n        if not ignore_freq and data.index.freq is None:\n            raise ValueError(\n                f\"{data_label} has a pandas DatetimeIndex without a frequency. \"\n                f\"To avoid this error, set the frequency of the DatetimeIndex.\"\n            )\n        data_index = data.index\n    elif isinstance(data.index, pd.RangeIndex):\n        data_index = data.index\n    else:\n        raise TypeError(\n            f\"{data_label} has an unsupported index type. The index must be a \"\n            f\"pandas DatetimeIndex or a RangeIndex. Got {type(data.index)}.\"\n        )\n\n    data_values = data.to_numpy(copy=True).ravel() if return_values else None\n\n    return data_values, data_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.cast_exog_dtypes","title":"skforecast.utils.utils.cast_exog_dtypes","text":"<pre><code>cast_exog_dtypes(exog, exog_dtypes)\n</code></pre> <p>Cast <code>exog</code> to a specified types. This is done because, for a forecaster to  accept a categorical exog, it must contain only integer values. Due to the  internal modifications of numpy, the values may be casted to <code>float</code>, so  they have to be re-converted to <code>int</code>.</p> <ul> <li>If <code>exog</code> is a pandas Series, <code>exog_dtypes</code> must be a dict with a  single value.</li> <li>If <code>exog_dtypes</code> is <code>category</code> but the current type of <code>exog</code> is <code>float</code>,  then the type is cast to <code>int</code> and then to <code>category</code>. </li> </ul> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>exog_dtypes</code> <code>dict[str, type]</code> <p>Dictionary with name and type of the series or data frame columns.</p> required <p>Returns:</p> Name Type Description <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables casted to the indicated dtypes.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def cast_exog_dtypes(\n    exog: pd.Series | pd.DataFrame,\n    exog_dtypes: dict[str, type],\n) -&gt; pd.Series | pd.DataFrame:  # pragma: no cover\n    \"\"\"\n    Cast `exog` to a specified types. This is done because, for a forecaster to \n    accept a categorical exog, it must contain only integer values. Due to the \n    internal modifications of numpy, the values may be casted to `float`, so \n    they have to be re-converted to `int`.\n\n    - If `exog` is a pandas Series, `exog_dtypes` must be a dict with a \n    single value.\n    - If `exog_dtypes` is `category` but the current type of `exog` is `float`, \n    then the type is cast to `int` and then to `category`. \n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    exog_dtypes: dict\n        Dictionary with name and type of the series or data frame columns.\n\n    Returns\n    -------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables casted to the indicated dtypes.\n\n    \"\"\"\n\n    # Remove keys from exog_dtypes not in exog.columns\n    exog_dtypes = {k: v for k, v in exog_dtypes.items() if k in exog.columns}\n\n    if isinstance(exog, pd.Series) and exog.dtypes != list(exog_dtypes.values())[0]:\n        exog = exog.astype(list(exog_dtypes.values())[0])\n    elif isinstance(exog, pd.DataFrame):\n        for col, initial_dtype in exog_dtypes.items():\n            if exog[col].dtypes != initial_dtype:\n                if initial_dtype == \"category\" and exog[col].dtypes == float:\n                    exog[col] = exog[col].astype(int).astype(\"category\")\n                else:\n                    exog[col] = exog[col].astype(initial_dtype)\n\n    return exog\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.exog_to_direct","title":"skforecast.utils.utils.exog_to_direct","text":"<pre><code>exog_to_direct(exog, steps)\n</code></pre> <p>Transforms <code>exog</code> to a pandas DataFrame with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>pandas Series, pandas DataFrame</code> <p>Exogenous variables.</p> required <code>steps</code> <code>int</code> <p>Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Name Type Description <code>exog_direct</code> <code>pandas DataFrame</code> <p>Exogenous variables transformed.</p> <code>exog_direct_names</code> <code>list</code> <p>Names of the columns of the exogenous variables transformed. Only  created if <code>exog</code> is a pandas Series or DataFrame.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def exog_to_direct(\n    exog: pd.Series | pd.DataFrame,\n    steps: int\n) -&gt; tuple[pd.DataFrame, list[str]]:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : pandas DataFrame\n        Exogenous variables transformed.\n    exog_direct_names : list\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\")\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_cols = exog.columns\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog.iloc[i : n_rows - (steps - 1 - i), ]\n        exog_step.index = pd.RangeIndex(len(exog_step))\n        exog_step.columns = [f\"{col}_step_{i + 1}\" for col in exog_cols]\n        exog_direct.append(exog_step)\n\n    if len(exog_direct) &gt; 1:\n        exog_direct = pd.concat(exog_direct, axis=1, copy=False)\n    else:\n        exog_direct = exog_direct[0]\n\n    exog_direct_names = exog_direct.columns.to_list()\n    exog_direct.index = exog_idx[-len(exog_direct):]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.exog_to_direct_numpy","title":"skforecast.utils.utils.exog_to_direct_numpy","text":"<pre><code>exog_to_direct_numpy(exog, steps)\n</code></pre> <p>Transforms <code>exog</code> to numpy ndarray with the shape needed for Direct forecasting.</p> <p>Parameters:</p> Name Type Description Default <code>exog</code> <code>numpy ndarray, pandas Series, pandas DataFrame</code> <p>Exogenous variables, shape(samples,). If exog is a pandas format, the  direct exog names are created.</p> required <code>steps</code> <code>int</code> <p>Number of steps that will be predicted using exog.</p> required <p>Returns:</p> Name Type Description <code>exog_direct</code> <code>numpy ndarray</code> <p>Exogenous variables transformed.</p> <code>exog_direct_names</code> <code>(list, None)</code> <p>Names of the columns of the exogenous variables transformed. Only  created if <code>exog</code> is a pandas Series or DataFrame.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def exog_to_direct_numpy(\n    exog: np.ndarray | pd.Series | pd.DataFrame,\n    steps: int\n) -&gt; tuple[np.ndarray, list[str] | None]:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n\n    Parameters\n    ----------\n    exog : numpy ndarray, pandas Series, pandas DataFrame\n        Exogenous variables, shape(samples,). If exog is a pandas format, the \n        direct exog names are created.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : numpy ndarray\n        Exogenous variables transformed.\n    exog_direct_names : list, None\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n\n    \"\"\"\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        exog_cols = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n        exog_direct_names = [\n            f\"{col}_step_{i + 1}\" for i in range(steps) for col in exog_cols\n        ]\n        exog = exog.to_numpy()\n    else:\n        exog_direct_names = None\n        if not isinstance(exog, np.ndarray):\n            raise TypeError(\n                f\"`exog` must be a numpy ndarray, pandas Series or DataFrame. \"\n                f\"Got {type(exog)}.\"\n            )\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog[i : n_rows - (steps - 1 - i)]\n        exog_direct.append(exog_step)\n\n    if len(exog_direct) &gt; 1:\n        exog_direct = np.concatenate(exog_direct, axis=1)\n    else:\n        exog_direct = exog_direct[0]\n\n    return exog_direct, exog_direct_names\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.expand_index","title":"skforecast.utils.utils.expand_index","text":"<pre><code>expand_index(index, steps)\n</code></pre> <p>Create a new index of length <code>steps</code> starting at the end of the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>pandas Index, None</code> <p>Original index.</p> required <code>steps</code> <code>int</code> <p>Number of steps to expand.</p> required <p>Returns:</p> Name Type Description <code>new_index</code> <code>pandas Index</code> <p>New index.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def expand_index(\n    index: pd.Index | None, \n    steps: int\n) -&gt; pd.Index:\n    \"\"\"\n    Create a new index of length `steps` starting at the end of the index.\n\n    Parameters\n    ----------\n    index : pandas Index, None\n        Original index.\n    steps : int\n        Number of steps to expand.\n\n    Returns\n    -------\n    new_index : pandas Index\n        New index.\n\n    \"\"\"\n\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    if isinstance(index, pd.Index):\n\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                            start   = index[-1] + index.freq,\n                            periods = steps,\n                            freq    = index.freq\n                        )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(\n                            start = index[-1] + 1,\n                            stop  = index[-1] + 1 + steps\n                        )\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(\n                        start = 0,\n                        stop  = steps\n                    )\n\n    return new_index\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_numpy","title":"skforecast.utils.utils.transform_numpy","text":"<pre><code>transform_numpy(\n    array, transformer, fit=False, inverse_transform=False\n)\n</code></pre> <p>Transform raw values of a numpy ndarray with a scikit-learn alike  transformer, preprocessor or ColumnTransformer. The transformer used must  have the following methods: fit, transform, fit_transform and  inverse_transform. ColumnTransformers are not allowed since they do not  have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>numpy ndarray</code> <p>Array to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer, preprocessor, or ColumnTransformer.</code> <p>Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>array_transformed</code> <code>numpy ndarray</code> <p>Transformed array.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def transform_numpy(\n    array: np.ndarray,\n    transformer: object | None,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    array : numpy ndarray\n        Array to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default False\n        Train the transformer before applying it.\n    inverse_transform : bool, default False\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    array_transformed : numpy ndarray\n        Transformed array.\n\n    \"\"\"\n\n    if not isinstance(array, np.ndarray):\n        raise TypeError(\n            f\"`array` argument must be a numpy ndarray. Got {type(array)}\"\n        )\n\n    if transformer is None:\n        return array\n\n    array_ndim = array.ndim\n    if array_ndim == 1:\n        array = array.reshape(-1, 1)\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \n            message=\"X does not have valid feature names\", \n            category=UserWarning\n        )\n        if not inverse_transform:\n            if fit:\n                array_transformed = transformer.fit_transform(array)\n            else:\n                array_transformed = transformer.transform(array)\n        else:\n            array_transformed = transformer.inverse_transform(array)\n\n    if hasattr(array_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        array_transformed = array_transformed.toarray()\n\n    if isinstance(array_transformed, (pd.Series, pd.DataFrame)):\n        array_transformed = array_transformed.to_numpy()\n\n    if array_ndim == 1:\n        array_transformed = array_transformed.ravel()\n\n    return array_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_series","title":"skforecast.utils.utils.transform_series","text":"<pre><code>transform_series(\n    series, transformer, fit=False, inverse_transform=False\n)\n</code></pre> <p>Transform raw values of pandas Series with a scikit-learn alike  transformer, preprocessor or ColumnTransformer. The transformer used must  have the following methods: fit, transform, fit_transform and  inverse_transform. ColumnTransformers are not allowed since they do not  have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas Series</code> <p>Series to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer, preprocessor, or ColumnTransformer.</code> <p>Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>series_transformed</code> <code>pandas Series, pandas DataFrame</code> <p>Transformed Series. Depending on the transformer used, the output may  be a Series or a DataFrame.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def transform_series(\n    series: pd.Series,\n    transformer: object | None,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -&gt; pd.Series | pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas Series with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    series : pandas Series\n        Series to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default False\n        Train the transformer before applying it.\n    inverse_transform : bool, default False\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    series_transformed : pandas Series, pandas DataFrame\n        Transformed Series. Depending on the transformer used, the output may \n        be a Series or a DataFrame.\n\n    \"\"\"\n\n    if not isinstance(series, pd.Series):\n        raise TypeError(\n            f\"`series` argument must be a pandas Series. Got {type(series)}.\"\n        )\n\n    if transformer is None:\n        return series\n\n    if series.name is None:\n        series.name = 'no_name'\n\n    data = series.to_frame()\n\n    if fit and hasattr(transformer, 'fit'):\n        transformer.fit(data)\n\n    # If argument feature_names_in_ exits, is overwritten to allow using the \n    # transformer on other series than those that were passed during fit.\n    if hasattr(transformer, 'feature_names_in_') and transformer.feature_names_in_[0] != data.columns[0]:\n        transformer = deepcopy(transformer)\n        transformer.feature_names_in_ = np.array([data.columns[0]], dtype=object)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        if inverse_transform:\n            values_transformed = transformer.inverse_transform(data)\n        else:\n            values_transformed = transformer.transform(data)   \n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense array.\n        values_transformed = values_transformed.toarray()\n\n    if isinstance(values_transformed, np.ndarray) and values_transformed.shape[1] == 1:\n        series_transformed = pd.Series(\n                                 data  = values_transformed.ravel(),\n                                 index = data.index,\n                                 name  = data.columns[0]\n                             )\n    elif isinstance(values_transformed, pd.DataFrame) and values_transformed.shape[1] == 1:\n        series_transformed = values_transformed.squeeze()\n    else:\n        series_transformed = pd.DataFrame(\n                                 data    = values_transformed,\n                                 index   = data.index,\n                                 columns = transformer.get_feature_names_out()\n                             )\n\n    return series_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.transform_dataframe","title":"skforecast.utils.utils.transform_dataframe","text":"<pre><code>transform_dataframe(\n    df, transformer, fit=False, inverse_transform=False\n)\n</code></pre> <p>Transform raw values of pandas DataFrame with a scikit-learn alike  transformer, preprocessor or ColumnTransformer. The transformer used must  have the following methods: fit, transform, fit_transform and  inverse_transform. ColumnTransformers are not allowed since they do not  have inverse_transform method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame to be transformed.</p> required <code>transformer</code> <code>scikit-learn alike transformer, preprocessor, or ColumnTransformer.</code> <p>Scikit-learn alike transformer (preprocessor) with methods: fit, transform, fit_transform and inverse_transform.</p> required <code>fit</code> <code>bool</code> <p>Train the transformer before applying it.</p> <code>False</code> <code>inverse_transform</code> <code>bool</code> <p>Transform back the data to the original representation. This is not available when using transformers of class scikit-learn ColumnTransformers.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>df_transformed</code> <code>pandas DataFrame</code> <p>Transformed DataFrame.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def transform_dataframe(\n    df: pd.DataFrame,\n    transformer: object | None,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default False\n        Train the transformer before applying it.\n    inverse_transform : bool, default False\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    df_transformed : pandas DataFrame\n        Transformed DataFrame.\n\n    \"\"\"\n\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\n            f\"`df` argument must be a pandas DataFrame. Got {type(df)}\"\n        )\n\n    if transformer is None:\n        return df\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if hasattr(transformer, 'get_feature_names_out'):\n        feature_names_out = transformer.get_feature_names_out()\n    elif hasattr(transformer, 'categories_'):   \n        feature_names_out = transformer.categories_\n    else:\n        feature_names_out = df.columns\n\n    df_transformed = pd.DataFrame(\n                         data    = values_transformed,\n                         index   = df.index,\n                         columns = feature_names_out\n                     )\n\n    return df_transformed\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_optional_dependency","title":"skforecast.utils.utils.check_optional_dependency","text":"<pre><code>check_optional_dependency(package_name)\n</code></pre> <p>Check if an optional dependency is installed, if not raise an ImportError with installation instructions.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Name of the package to check.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_optional_dependency(\n    package_name: str\n) -&gt; None:\n    \"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError  \n    with installation instructions.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if importlib.util.find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(package_name=package_name)\n            msg = (\n                f\"\\n'{package_name}' is an optional dependency not included in the default \"\n                f\"skforecast installation. Please run: `pip install \\\"{package_version}\\\"` to install it.\"\n                f\"\\n\\nAlternately, you can install it by running `pip install skforecast[{extra}]`\"\n            )\n        except:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n\n        raise ImportError(msg)\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.multivariate_time_series_corr","title":"skforecast.utils.utils.multivariate_time_series_corr","text":"<pre><code>multivariate_time_series_corr(\n    time_series, other, lags, method=\"pearson\"\n)\n</code></pre> <p>Compute correlation between a time_series and the lagged values of other  time series. </p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>pandas Series</code> <p>Target time series.</p> required <code>other</code> <code>pandas DataFrame</code> <p>Time series whose lagged values are correlated to <code>time_series</code>.</p> required <code>lags</code> <code>int, list, numpy ndarray</code> <p>Lags to be included in the correlation analysis.</p> required <code>method</code> <code>str</code> <ul> <li>'pearson': standard correlation coefficient.</li> <li>'kendall': Kendall Tau correlation coefficient.</li> <li>'spearman': Spearman rank correlation.</li> </ul> <code>'pearson'</code> <p>Returns:</p> Name Type Description <code>corr</code> <code>pandas DataFrame</code> <p>Correlation values.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def multivariate_time_series_corr(\n    time_series: pd.Series,\n    other: pd.DataFrame,\n    lags: int | list[int] | np.ndarray[int],\n    method: str = 'pearson'\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute correlation between a time_series and the lagged values of other \n    time series. \n\n    Parameters\n    ----------\n    time_series : pandas Series\n        Target time series.\n    other : pandas DataFrame\n        Time series whose lagged values are correlated to `time_series`.\n    lags : int, list, numpy ndarray\n        Lags to be included in the correlation analysis.\n    method : str, default 'pearson'\n        - 'pearson': standard correlation coefficient.\n        - 'kendall': Kendall Tau correlation coefficient.\n        - 'spearman': Spearman rank correlation.\n\n    Returns\n    -------\n    corr : pandas DataFrame\n        Correlation values.\n\n    \"\"\"\n\n    if not len(time_series) == len(other):\n        raise ValueError(\"`time_series` and `other` must have the same length.\")\n\n    if not (time_series.index == other.index).all():\n        raise ValueError(\"`time_series` and `other` must have the same index.\")\n\n    if isinstance(lags, int):\n        lags = range(lags)\n\n    corr = {}\n    for col in other.columns:\n        lag_values = {}\n        for lag in lags:\n            lag_values[lag] = other[col].shift(lag)\n\n        lag_values = pd.DataFrame(lag_values)\n        lag_values.insert(0, None, time_series)\n        corr[col] = lag_values.corr(method=method).iloc[1:, 0]\n\n    corr = pd.DataFrame(corr)\n    corr.index = corr.index.astype('int64')\n    corr.index.name = \"lag\"\n\n    return corr\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.select_n_jobs_fit_forecaster","title":"skforecast.utils.utils.select_n_jobs_fit_forecaster","text":"<pre><code>select_n_jobs_fit_forecaster(forecaster_name, regressor)\n</code></pre> <p>Select the optimal number of jobs to use in the fitting process. This selection is based on heuristics and is not guaranteed to be optimal. </p> <p>The number of jobs is chosen as follows:</p> <ul> <li>If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate' and regressor_name is a linear regressor then <code>n_jobs = 1</code>,  otherwise <code>n_jobs = cpu_count() - 1</code>.</li> <li>If regressor is a <code>LGBMRegressor(n_jobs=1)</code>, then <code>n_jobs = cpu_count() - 1</code>.</li> <li>If regressor is a <code>LGBMRegressor</code> with internal n_jobs != 1, then <code>n_jobs = 1</code>. This is because <code>lightgbm</code> is highly optimized for gradient boosting and parallelizes operations at a very fine-grained level, making additional parallelization unnecessary and potentially harmful due to resource contention.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>forecaster_name</code> <code>str</code> <p>Forecaster name.</p> required <code>regressor</code> <code>regressor or pipeline compatible with the scikit-learn API</code> <p>An instance of a regressor or pipeline compatible with the scikit-learn API.</p> required <p>Returns:</p> Name Type Description <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def select_n_jobs_fit_forecaster(\n    forecaster_name: str,\n    regressor: object\n) -&gt; int:\n    \"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n\n    The number of jobs is chosen as follows:\n\n    - If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and regressor_name is a linear regressor then `n_jobs = 1`, \n    otherwise `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n\n    \"\"\"\n\n    if isinstance(regressor, Pipeline):\n        regressor = regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    if forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = joblib.cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = joblib.cpu_count() - 1\n    else:\n        n_jobs = 1\n\n    return n_jobs\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_preprocess_series","title":"skforecast.utils.utils.check_preprocess_series","text":"<pre><code>check_preprocess_series(series)\n</code></pre> <p>Check and preprocess <code>series</code> argument in <code>ForecasterRecursiveMultiSeries</code> class.</p> <ul> <li>If <code>series</code> is a wide-format pandas DataFrame, each column represents a different time series, and the index must be either a <code>DatetimeIndex</code> or  a <code>RangeIndex</code> with frequency or step size, as appropriate</li> <li>If <code>series</code> is a long-format pandas DataFrame with a MultiIndex, the  first level of the index must contain the series IDs, and the second  level must be a <code>DatetimeIndex</code> with the same frequency across all series.</li> <li>If series is a dictionary, each key must be a series ID, and each value  must be a named pandas Series. All series must have the same index, which  must be either a <code>DatetimeIndex</code> or a <code>RangeIndex</code>, and they must share the  same frequency or step size, as appropriate.</li> </ul> <p>When <code>series</code> is a pandas DataFrame, it is converted to a dictionary of pandas  Series, where the keys are the series IDs and the values are the Series with  the same index as the original DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>pandas DataFrame, dict</code> <p>Training time series.</p> required <p>Returns:</p> Name Type Description <code>series_dict</code> <code>dict</code> <p>Dictionary with the series used during training.</p> <code>series_indexes</code> <code>dict</code> <p>Dictionary with the index of each series.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_preprocess_series(\n    series: pd.DataFrame | dict[str, pd.Series | pd.DataFrame],\n) -&gt; tuple[dict[str, pd.Series], dict[str, pd.Index]]:\n    \"\"\"\n    Check and preprocess `series` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If `series` is a wide-format pandas DataFrame, each column represents a\n    different time series, and the index must be either a `DatetimeIndex` or \n    a `RangeIndex` with frequency or step size, as appropriate\n    - If `series` is a long-format pandas DataFrame with a MultiIndex, the \n    first level of the index must contain the series IDs, and the second \n    level must be a `DatetimeIndex` with the same frequency across all series.\n    - If series is a dictionary, each key must be a series ID, and each value \n    must be a named pandas Series. All series must have the same index, which \n    must be either a `DatetimeIndex` or a `RangeIndex`, and they must share the \n    same frequency or step size, as appropriate.\n\n    When `series` is a pandas DataFrame, it is converted to a dictionary of pandas \n    Series, where the keys are the series IDs and the values are the Series with \n    the same index as the original DataFrame.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    series_indexes : dict\n        Dictionary with the index of each series.\n\n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n            f\"Got {type(series)}.\"\n        )\n\n    if isinstance(series, pd.DataFrame):\n\n        if not isinstance(series.index, pd.MultiIndex):\n            _, _ = check_extract_values_and_index(\n                data=series, data_label='`series`', return_values=False\n            )\n            series = series.copy()\n            series.index.name = None\n            series_dict = series.to_dict(orient='series')\n        else:\n            if not isinstance(series.index.levels[1], pd.DatetimeIndex):\n                raise TypeError(\n                    f\"The second level of the MultiIndex in `series` must be a \"\n                    f\"pandas DatetimeIndex with the same frequency for each series. \"\n                    f\"Found {type(series.index.levels[1])}.\"\n                )\n\n            first_col = series.columns[0]\n            if len(series.columns) != 1:\n                warnings.warn(\n                    f\"`series` DataFrame has multiple columns. Only the values of \"\n                    f\"first column, '{first_col}', will be used as series values. \"\n                    f\"All other columns will be ignored.\",\n                    IgnoredArgumentWarning\n                )\n\n            series = series.copy()\n            series.index = series.index.set_names([series.index.names[0], None])\n            series_dict = {\n                series_id: series.loc[series_id][first_col].rename(series_id)\n                for series_id in series.index.levels[0]\n            }\n\n        warnings.warn(\n            \"Passing a DataFrame (either wide or long format) as `series` requires \"\n            \"additional internal transformations, which can increase computational \"\n            \"time. It is recommended to use a dictionary of pandas Series instead. \"\n            \"For more details, see: \"\n            \"https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting.html#input-data\",\n            InputTypeWarning\n        )\n\n    else:\n\n        not_valid_series = [\n            k \n            for k, v in series.items()\n            if not isinstance(v, (pd.Series, pd.DataFrame))\n        ]\n        if not_valid_series:\n            raise TypeError(\n                f\"If `series` is a dictionary, all series must be a named \"\n                f\"pandas Series or a pandas DataFrame with a single column. \"\n                f\"Review series: {not_valid_series}\"\n            )\n\n        series_dict = {\n            k: v.copy()\n            for k, v in series.items()\n        }\n\n    not_valid_index = []\n    indexes_freq = set()\n    series_indexes = {}\n    for k, v in series_dict.items():\n        if isinstance(v, pd.DataFrame):\n            if v.shape[1] != 1:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must be a named \"\n                    f\"pandas Series or a pandas DataFrame with a single column. \"\n                    f\"Review series: '{k}'\"\n                )\n            series_dict[k] = v.iloc[:, 0]\n\n        series_dict[k].name = k\n        if isinstance(v.index, pd.DatetimeIndex):\n            indexes_freq.add(v.index.freqstr)\n        elif isinstance(v.index, pd.RangeIndex):\n            indexes_freq.add(v.index.step)\n        else:\n            not_valid_index.append(k)\n\n        if v.isna().to_numpy().all():\n            raise ValueError(f\"All values of series '{k}' are NaN.\")\n\n        series_indexes[k] = v.index\n\n    if not_valid_index:\n        raise TypeError(\n            f\"If `series` is a dictionary, all series must have a Pandas \"\n            f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n            f\"Review series: {not_valid_index}\"\n        )\n    if None in indexes_freq:\n        raise ValueError(\n            \"If `series` is a dictionary, all series must have a Pandas \"\n            \"RangeIndex or DatetimeIndex with the same step/frequency. \"\n            \"If it a MultiIndex DataFrame, the second level must be a DatetimeIndex \"\n            \"with the same frequency for each series. Found series with no \"\n            \"frequency or step.\"\n        )\n    if not len(indexes_freq) == 1:\n        raise ValueError(\n            f\"If `series` is a dictionary, all series must have a Pandas \"\n            f\"RangeIndex or DatetimeIndex with the same step/frequency. \"\n            f\"If it a MultiIndex DataFrame, the second level must be a DatetimeIndex \"\n            f\"with the same frequency for each series. \"\n            f\"Found frequencies: {sorted(indexes_freq)}\"\n        )\n\n    return series_dict, series_indexes\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.check_preprocess_exog_multiseries","title":"skforecast.utils.utils.check_preprocess_exog_multiseries","text":"<pre><code>check_preprocess_exog_multiseries(\n    series_names_in_, series_index_type, exog, exog_dict\n)\n</code></pre> <p>Check and preprocess <code>exog</code> argument in <code>ForecasterRecursiveMultiSeries</code> class.</p> <ul> <li>If <code>exog</code> is a wide-format pandas DataFrame, it must share the same  index type as series. Each column represents a different exogenous variable,  and the same values are applied to all time series.</li> <li>If <code>exog</code> is a long-format pandas Series or DataFrame with a MultiIndex,  the first level contains the series IDs to which it belongs, and the  second level contains a pandas DatetimeIndex. One column must be created for each exogenous variable.</li> <li>If <code>exog</code> is a dictionary, each key must be the series ID to which it  belongs, and each value must be a named pandas Series/DataFrame with the same index type as <code>series</code> or None. While it is not necessary for  all values to include all the exogenous variables, the dtypes must be  consistent for the same exogenous variable across all series.</li> </ul> <p>When <code>exog</code> is a pandas DataFrame, it is converted to a dictionary of pandas  DataFrames, where the keys are the series IDs and the values are the Series  with the same index as the original DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>series_names_in_</code> <code>list</code> <p>Names of the series (levels) used during training.</p> required <code>series_index_type</code> <code>type</code> <p>Index type of the series used during training.</p> required <code>exog</code> <code>pandas Series, pandas DataFrame, dict</code> <p>Exogenous variable/s used during training.</p> required <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variable/s used during training.</p> required <p>Returns:</p> Name Type Description <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variable/s used during training.</p> <code>exog_names_in_</code> <code>list</code> <p>Names of the exogenous variables used during training.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def check_preprocess_exog_multiseries(\n    series_names_in_: list[str],\n    series_index_type: type,\n    exog: pd.Series | pd.DataFrame | dict[str, pd.Series | pd.DataFrame | None],\n    exog_dict: dict[str, pd.Series | pd.DataFrame | None],\n) -&gt; tuple[dict[str, pd.DataFrame | None], list[str]]:\n    \"\"\"\n    Check and preprocess `exog` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If `exog` is a wide-format pandas DataFrame, it must share the same \n    index type as series. Each column represents a different exogenous variable, \n    and the same values are applied to all time series.\n    - If `exog` is a long-format pandas Series or DataFrame with a MultiIndex, \n    the first level contains the series IDs to which it belongs, and the \n    second level contains a pandas DatetimeIndex. One column must be created\n    for each exogenous variable.\n    - If `exog` is a dictionary, each key must be the series ID to which it \n    belongs, and each value must be a named pandas Series/DataFrame with\n    the same index type as `series` or None. While it is not necessary for \n    all values to include all the exogenous variables, the dtypes must be \n    consistent for the same exogenous variable across all series.\n\n    When `exog` is a pandas DataFrame, it is converted to a dictionary of pandas \n    DataFrames, where the keys are the series IDs and the values are the Series \n    with the same index as the original DataFrame.\n\n    Parameters\n    ----------\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    series_index_type : type\n        Index type of the series used during training.\n    exog : pandas Series, pandas DataFrame, dict\n        Exogenous variable/s used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n\n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n        raise TypeError(\n            f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n            f\"Series/DataFrames or None. Got {type(exog)}.\"\n        )\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)): \n\n        exog = exog.copy().to_frame() if isinstance(exog, pd.Series) else exog.copy()\n        if isinstance(exog.index, pd.MultiIndex):\n            if not isinstance(exog.index.levels[1], pd.DatetimeIndex):\n                raise TypeError(\n                    f\"When input data are pandas MultiIndex DataFrame, \"\n                    f\"`series` and `exog` second level index must be a \"\n                    f\"pandas DatetimeIndex. Found `exog` index type: \"\n                    f\"{type(exog.index.levels[1])}.\"\n                )\n            exog.index = exog.index.set_names([exog.index.names[0], None])\n            exog_dict.update(\n                {\n                    series_id: exog.loc[series_id] \n                    for series_id in exog.index.levels[0]\n                    if series_id in series_names_in_\n                }\n            )\n            series_ids_in_exog = exog.index.levels[0]\n            warnings.warn(\n                \"Using a long-format DataFrame as `exog` requires additional transformations, \"\n                \"which can increase computational time. It is recommended to use a dictionary of \"\n                \"Series or DataFrames instead. For more information, see: \"\n                \"https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting#input-data\",\n                InputTypeWarning\n            )\n        else:\n            if not isinstance(exog.index, series_index_type):\n                raise TypeError(\n                    f\"`exog` must have the same index type as `series`, pandas \"\n                    f\"RangeIndex or pandas DatetimeIndex.\\n\"\n                    f\"    `series` index type : {series_index_type}.\\n\"\n                    f\"    `exog`   index type : {type(exog.index)}.\"\n                )\n            exog_dict = {series_id: exog for series_id in series_names_in_}\n            series_ids_in_exog = series_names_in_\n\n    else:\n\n        not_valid_exog = [\n            k \n            for k, v in exog.items()\n            if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n        ]\n        if not_valid_exog:\n            raise TypeError(\n                f\"If `exog` is a dictionary, all exog must be a named pandas \"\n                f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\"\n            )\n\n        # NOTE: Only elements already present in exog_dict are updated. Copy is\n        # needed to avoid modifying the original exog.\n        exog_dict.update(\n            {\n                k: v.copy()\n                for k, v in exog.items()\n                if k in series_names_in_ and v is not None\n            }\n        )\n        series_ids_in_exog = exog.keys()\n\n    series_not_in_exog = set(series_names_in_) - set(series_ids_in_exog)\n    if series_not_in_exog:\n        warnings.warn(\n            f\"No `exog` for series {series_not_in_exog}. All values \"\n            f\"of the exogenous variables for these series will be NaN.\",\n            MissingExogWarning\n        )\n\n    for k, v in exog_dict.items():\n        if v is not None:\n            check_exog(exog=v, allow_nan=True)\n            if isinstance(v, pd.Series):\n                v = v.to_frame()\n            exog_dict[k] = v\n\n    not_valid_index = [\n        k\n        for k, v in exog_dict.items()\n        if v is not None and not isinstance(v.index, series_index_type)\n    ]\n    if not_valid_index:\n        raise TypeError(\n            f\"All exog must have the same index type as `series`, which can be \"\n            f\"either a pandas RangeIndex or a pandas DatetimeIndex. If either \"\n            f\"`series` or `exog` is a pandas DataFrame with a MultiIndex, then \"\n            f\"both must be pandas DatetimeIndex. Review exog for series: {not_valid_index}.\"\n        )\n\n    if isinstance(exog, dict):\n        # NOTE: Check that all exog have the same dtypes for common columns\n        exog_dtypes_buffer = pd.DataFrame(\n            {k: df.dtypes for k, df in exog_dict.items() if df is not None}\n        )\n        exog_dtypes_nunique = exog_dtypes_buffer.nunique(axis=1)\n        if not (exog_dtypes_nunique == 1).all():\n            non_unique_dtypes_exogs = exog_dtypes_nunique[exog_dtypes_nunique != 1].index.to_list()\n            raise TypeError(\n                f\"Exog/s: {non_unique_dtypes_exogs} have different dtypes in different \"\n                f\"series. If any of these variables are categorical, note that this \"\n                f\"error can also occur when their internal categories \"\n                f\"(`series.cat.categories`) differ between series. Please ensure \"\n                f\"that all series have the same categories (and category order) \"\n                f\"for each categorical variable.\"\n            )\n\n        exog_names_in_ = list(\n            set(\n                column\n                for df in exog_dict.values()\n                if df is not None\n                for column in df.columns.to_list()\n            )\n        )\n    else:\n        exog_names_in_ = list(exog.columns) if isinstance(exog, pd.DataFrame) else [exog.name]\n\n    if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n        raise ValueError(\n            f\"`exog` cannot contain a column named the same as one of the series.\\n\"\n            f\"    `series` columns : {series_names_in_}.\\n\"\n            f\"    `exog`   columns : {exog_names_in_}.\"\n        )\n\n    return exog_dict, exog_names_in_\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.align_series_and_exog_multiseries","title":"skforecast.utils.utils.align_series_and_exog_multiseries","text":"<pre><code>align_series_and_exog_multiseries(\n    series_dict, exog_dict=None\n)\n</code></pre> <p>Align series and exog according to their index. If needed, reindexing is applied. Heading and trailing NaNs are removed from all series in  <code>series_dict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>series_dict</code> <code>dict</code> <p>Dictionary with the series used during training.</p> required <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variable/s used during training.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>series_dict</code> <code>dict</code> <p>Dictionary with the series used during training.</p> <code>exog_dict</code> <code>dict</code> <p>Dictionary with the exogenous variable/s used during training.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def align_series_and_exog_multiseries(\n    series_dict: dict[str, pd.Series],\n    exog_dict: dict[str, pd.DataFrame] | None = None\n) -&gt; tuple[dict[str, pd.Series], dict[str, pd.DataFrame | None]]:\n    \"\"\"\n    Align series and exog according to their index. If needed, reindexing is\n    applied. Heading and trailing NaNs are removed from all series in \n    `series_dict`.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Dictionary with the series used during training.\n    exog_dict : dict, default None\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n\n    \"\"\"\n\n    for k in series_dict.keys():\n        if np.isnan(series_dict[k].iat[0]) or np.isnan(series_dict[k].iat[-1]):\n            first_valid_index = series_dict[k].first_valid_index()\n            last_valid_index = series_dict[k].last_valid_index()\n            series_dict[k] = series_dict[k].loc[first_valid_index : last_valid_index]\n        else:\n            first_valid_index = series_dict[k].index[0]\n            last_valid_index = series_dict[k].index[-1]\n\n        if exog_dict[k] is not None:\n            if not series_dict[k].index.equals(exog_dict[k].index):\n                exog_dict[k] = exog_dict[k].loc[first_valid_index:last_valid_index]\n                if exog_dict[k].empty:\n                    warnings.warn(\n                        f\"`exog` for series '{k}' is empty after aligning \"\n                        f\"with the series index. Exog values will be NaN.\",\n                        MissingValuesWarning\n                    )\n                    exog_dict[k] = None\n                elif len(exog_dict[k]) != len(series_dict[k]):\n                    warnings.warn(\n                        f\"`exog` for series '{k}' doesn't have values for \"\n                        f\"all the dates in the series. Missing values will be \"\n                        f\"filled with NaN.\",\n                        MissingValuesWarning\n                    )\n                    exog_dict[k] = exog_dict[k].reindex(\n                        series_dict[k].index, fill_value = np.nan\n                    )\n\n    return series_dict, exog_dict\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.prepare_levels_multiseries","title":"skforecast.utils.utils.prepare_levels_multiseries","text":"<pre><code>prepare_levels_multiseries(\n    X_train_series_names_in_, levels=None\n)\n</code></pre> <p>Prepare list of levels to be predicted in multiseries Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>X_train_series_names_in_</code> <code>list</code> <p>Names of the series (levels) included in the matrix <code>X_train</code>.</p> required <code>levels</code> <code>(str, list)</code> <p>Names of the series (levels) to be predicted.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> <code>input_levels_is_list</code> <code>bool</code> <p>Indicates if input levels argument is a list.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def prepare_levels_multiseries(\n    X_train_series_names_in_: list[str],\n    levels: str | list[str] | None = None\n) -&gt; tuple[list[str], bool]:\n    \"\"\"\n    Prepare list of levels to be predicted in multiseries Forecasters.\n\n    Parameters\n    ----------\n    X_train_series_names_in_ : list\n        Names of the series (levels) included in the matrix `X_train`.\n    levels : str, list, default None\n        Names of the series (levels) to be predicted.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    input_levels_is_list : bool\n        Indicates if input levels argument is a list.\n\n    \"\"\"\n\n    input_levels_is_list = False\n    if levels is None:\n        levels = X_train_series_names_in_\n    elif isinstance(levels, str):\n        levels = [levels]\n    else:\n        input_levels_is_list = True\n\n    return levels, input_levels_is_list\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.preprocess_levels_self_last_window_multiseries","title":"skforecast.utils.utils.preprocess_levels_self_last_window_multiseries","text":"<pre><code>preprocess_levels_self_last_window_multiseries(\n    levels, input_levels_is_list, last_window_\n)\n</code></pre> <p>Preprocess <code>levels</code> and <code>last_window</code> (when using self.last_window_) arguments  in multiseries Forecasters when predicting. Only levels whose last window  ends at the same datetime index will be predicted together.</p> <p>Parameters:</p> Name Type Description Default <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> required <code>input_levels_is_list</code> <code>bool</code> <p>Indicates if input levels argument is a list.</p> required <code>last_window_</code> <code>dict</code> <p>Dictionary with the last window of each series (self.last_window_).</p> required <p>Returns:</p> Name Type Description <code>levels</code> <code>list</code> <p>Names of the series (levels) to be predicted.</p> <code>last_window</code> <code>pandas DataFrame</code> <p>Series values used to create the predictors (lags) needed in the  first iteration of the prediction (t + 1).</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def preprocess_levels_self_last_window_multiseries(\n    levels: list[str],\n    input_levels_is_list: bool,\n    last_window_: dict[str, pd.Series],\n) -&gt; tuple[list[str], pd.DataFrame]:\n    \"\"\"\n    Preprocess `levels` and `last_window` (when using self.last_window_) arguments \n    in multiseries Forecasters when predicting. Only levels whose last window \n    ends at the same datetime index will be predicted together.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    input_levels_is_list : bool\n        Indicates if input levels argument is a list.\n    last_window_ : dict\n        Dictionary with the last window of each series (self.last_window_).\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    last_window : pandas DataFrame\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n\n    \"\"\"\n\n    available_last_windows = set() if last_window_ is None else set(last_window_.keys())\n    not_available_last_window = set(levels) - available_last_windows\n    if not_available_last_window:\n        levels = [\n            level for level in levels \n            if level not in not_available_last_window\n        ]\n        if not levels:\n            raise ValueError(\n                f\"No series to predict. None of the series {not_available_last_window} \"\n                f\"are present in `last_window_` attribute. Provide `last_window` \"\n                f\"as argument in predict method.\"\n            )\n        else:\n            warnings.warn(\n                f\"Levels {not_available_last_window} are excluded from \"\n                f\"prediction since they were not stored in `last_window_` \"\n                f\"attribute during training. If you don't want to retrain \"\n                f\"the Forecaster, provide `last_window` as argument.\",\n                IgnoredArgumentWarning\n            )\n\n    last_index_levels = [\n        v.index[-1] \n        for k, v in last_window_.items()\n        if k in levels\n    ]\n    if len(set(last_index_levels)) &gt; 1:\n        max_index_levels = max(last_index_levels)\n        selected_levels = [\n            k\n            for k, v in last_window_.items()\n            if k in levels and v.index[-1] == max_index_levels\n        ]\n\n        series_excluded_from_last_window = set(levels) - set(selected_levels)\n        levels = selected_levels\n\n        if input_levels_is_list and series_excluded_from_last_window:\n            warnings.warn(\n                f\"Only series whose last window ends at the same index \"\n                f\"can be predicted together. Series that do not reach \"\n                f\"the maximum index, '{max_index_levels}', are excluded \"\n                f\"from prediction: {series_excluded_from_last_window}.\",\n                IgnoredArgumentWarning\n            )\n\n    last_window = pd.DataFrame(\n        {k: v \n         for k, v in last_window_.items() \n         if k in levels}\n    )\n\n    return levels, last_window\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.prepare_steps_direct","title":"skforecast.utils.utils.prepare_steps_direct","text":"<pre><code>prepare_steps_direct(max_step, steps=None)\n</code></pre> <p>Prepare list of steps to be predicted in Direct Forecasters.</p> <p>Parameters:</p> Name Type Description Default <code>max_step</code> <code>int, list, numpy ndarray</code> <p>Maximum number of future steps the forecaster will predict  when using predict methods.</p> required <code>steps</code> <code>(int, list, None)</code> <p>Predict n steps. The value of <code>steps</code> must be less than or equal to the  value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li>If <code>int</code>: Only steps within the range of 1 to int are predicted.</li> <li>If <code>list</code>: List of ints. Only the steps contained in the list  are predicted.</li> <li>If <code>None</code>: As many steps are predicted as were defined at  initialization.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>steps</code> <code>list</code> <p>Steps to be predicted.</p> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def prepare_steps_direct(\n    max_step: int | list[int] | np.ndarray[int],\n    steps: int | list[int] | None = None\n) -&gt; list[int]:\n    \"\"\"\n    Prepare list of steps to be predicted in Direct Forecasters.\n\n    Parameters\n    ----------\n    max_step : int, list, numpy ndarray\n        Maximum number of future steps the forecaster will predict \n        when using predict methods.\n    steps : int, list, None, default None\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n\n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n\n    Returns\n    -------\n    steps : list\n        Steps to be predicted.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        if isinstance(max_step, int):\n            steps = list(np.arange(max_step) + 1)\n        else:\n            steps = list(np.array(max_step))\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n\n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(\n                f\"`steps` argument must be an int, a list of ints or `None`. \"\n                f\"Got {type(steps)}.\"\n            )\n\n    # Required since numpy 2.0\n    steps = [int(step) for step in steps if step is not None]\n\n    return steps\n</code></pre>"},{"location":"api/utils.html#skforecast.utils.utils.set_skforecast_warnings","title":"skforecast.utils.utils.set_skforecast_warnings","text":"<pre><code>set_skforecast_warnings(\n    suppress_warnings, action=\"default\"\n)\n</code></pre> <p>Set skforecast warnings action.</p> <p>Parameters:</p> Name Type Description Default <code>suppress_warnings</code> <code>bool</code> <p>If <code>True</code>, skforecast warnings will be suppressed. If <code>False</code>, skforecast warnings will be shown as default. See  skforecast.exceptions.warn_skforecast_categories for more information.</p> required <code>action</code> <code>str</code> <p>Action to be taken when a warning is raised. See the warnings module for more information.</p> <code>`'default'`</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>skforecast\\utils\\utils.py</code> <pre><code>def set_skforecast_warnings(\n    suppress_warnings: bool,\n    action: str = 'default'\n) -&gt; None:\n    \"\"\"\n    Set skforecast warnings action.\n\n    Parameters\n    ----------\n    suppress_warnings : bool\n        If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n        warnings will be shown as default. See \n        skforecast.exceptions.warn_skforecast_categories for more information.\n    action : str, default `'default'`\n        Action to be taken when a warning is raised. See the warnings module\n        for more information.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.filterwarnings(action, category=category)\n</code></pre>"},{"location":"examples/examples_chinese.html","title":"Chinese (\u4e2d\u6587)","text":""},{"location":"examples/examples_chinese.html#_1","title":"\u793a\u4f8b\u4e0e\u6559\u7a0b","text":"<p>\u8fd9\u91cc\u63d0\u4f9b\u4e86\u5b9e\u7528\u6848\u4f8b\u4e0e\u8be6\u7ec6\u6559\u7a0b\uff0c\u5e2e\u52a9\u4f60\u5feb\u901f\u4e0a\u624b\u5e76\u638c\u63e1 skforecast \u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002\u4f60\u4e5f\u53ef\u4ee5\u5728 Python \u65f6\u95f4\u5e8f\u5217\u9884\u6d4b \u514d\u8d39\u83b7\u53d6\u5168\u90e8\u5b66\u4e60\u8d44\u6599\u3002</p>"},{"location":"examples/examples_chinese.html#_2","title":"\u5165\u95e8\u6307\u5357\uff1a\u57fa\u7840\u9884\u6d4b\u65b9\u6cd5","text":"<p>\u672c\u8282\u9762\u5411\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u521d\u5b66\u8005\uff0c\u6db5\u76d6\u6700\u5e38\u7528\u7684\u6a21\u578b\u4e0e\u6280\u672f\uff0c\u5e2e\u52a9\u4f60\u6253\u4e0b\u624e\u5b9e\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u57fa\u7840\u3002  </p> <p> Skforecast\uff1a\u7528\u673a\u5668\u5b66\u4e60\u505a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b </p> <p> ARIMA\u548cSARIMAX\u6a21\u578b</p> <p> \u57fa\u4e8e\u68af\u5ea6\u63d0\u5347\u7684\u9884\u6d4b\uff1aXGBoost\u3001LightGBM \u4e0e CatBoost</p>"},{"location":"examples/examples_english.html","title":"English","text":""},{"location":"examples/examples_english.html#examples-and-tutorials","title":"Examples and Tutorials","text":"<p>Practical examples and tutorials to help you understand and apply skforecast. You can also find them in the free track Forecasting with Python.</p>"},{"location":"examples/examples_english.html#getting-started-fundamental-forecasting","title":"Getting Started: Fundamental Forecasting","text":"<p>This section provides essential tutorials for users who are just getting started with time series forecasting. These examples cover the most fundamental models and techniques to help you build a strong foundation in forecasting.</p> <p> Skforecast: time series forecasting with machine learning</p> <p> ARIMA and SARIMAX models</p> <p> Forecasting with gradient boosting: XGBoost, LightGBM and CatBoost</p> <p> Forecasting with XGBoost</p> <p> Forecasting with LightGBM</p>"},{"location":"examples/examples_english.html#global-models-multi-series-forecasting","title":"Global Models: Multi-Series Forecasting","text":"<p>These tutorials focus on global models and multi-series forecasting, where you can explore the use of techniques that handle multiple time series simultaneously and compare performance across different forecasting approaches.</p> <p> Global Forecasting Models I: Multi-series forecasting</p> <p> Global Forecasting Models II: Comparative Analysis of Single and Multi-Series Forecasting Modeling</p> <p> Global Forecasting Models III: Modeling thousand time series with a single global model</p> <p> Global Forecasting Models IV: A step by step guide to global time series forecasting using Kaggle sticker sales data</p> <p> Global Forecasting Models V: The M5 Accuracy competition: the success of global forecasting models</p> <p> Forecasting with Deep Learning</p> <p> Clustering time series to improve forecasting</p>"},{"location":"examples/examples_english.html#advanced-techniques-beyond-basic-models","title":"Advanced Techniques: Beyond Basic Models","text":"<p>For experienced users looking to deepen their forecasting skills, this section provides advanced techniques, including probabilistic forecasting, handling missing values, and more sophisticated ensemble methods.</p> <p> Probabilistic forecasting</p> <p> Probabilistic forecasting: prediction intervals for multi-step time series forecasting</p> <p> Modelling time series trend with tree based models</p> <p> Forecasting time series with missing values</p> <p> Interpretable forecasting models</p> <p> Stacking ensemble of machine learning models to improve forecasting</p> <p> Time series anomaly detection</p> <p> Data leakage in pre-trained forecasting models</p>"},{"location":"examples/examples_english.html#real-world-challenges-and-case-studies","title":"Real-World Challenges and Case Studies","text":"<p>This section includes real-world applications of time series forecasting to tackle specific challenges, such as forecasting energy demand, web traffic, and even cryptocurrency prices. Learn how to apply forecasting techniques to practical use cases.</p> <p> Forecasting energy demand with machine learning</p> <p> Forecasting web traffic with machine learning and Python</p> <p> Intermittent demand forecasting</p> <p> Mitigating the impact of covid on forecasting models</p> <p> Bitcoin price prediction with Python</p>"},{"location":"examples/examples_spanish.html","title":"Spanish","text":""},{"location":"examples/examples_spanish.html#ejemplos-y-tutoriales","title":"Ejemplos y Tutoriales","text":"<p>Ejemplos pr\u00e1cticos y tutoriales para ayudarte a entender y aplicar skforecast. Tambi\u00e9n puedes encontrarlos todo este material gratuito en Forecasting con Python.</p>"},{"location":"examples/examples_spanish.html#primeros-pasos-forecasting-fundamental","title":"Primeros Pasos: Forecasting Fundamental","text":"<p>Esta secci\u00f3n proporciona tutoriales esenciales para los usuarios que est\u00e1n comenzando con el pron\u00f3stico de series temporales. Estos ejemplos cubren los modelos y t\u00e9cnicas m\u00e1s fundamentales para ayudarte a construir una base s\u00f3lida en forecasting.</p> <p> Skforecast: forecasting series temporales con machine learning</p> <p> Modelos ARIMA y SARIMAX</p> <p> Forecasting con gradient boosting: XGBoost, LightGBM y CatBoost</p> <p> Forecasting con XGBoost</p> <p> Forecasting con LightGBM</p> <p> Workshop predicci\u00f3n de series temporales con machine learning  Universidad de Deusto / Deustuko Unibertsitatea</p>"},{"location":"examples/examples_spanish.html#modelos-globales-forecasting-multi-serie","title":"Modelos Globales: Forecasting Multi-Serie","text":"<p>Estos tutoriales se centran en modelos globales y en el pron\u00f3stico de series m\u00faltiples, donde puedes explorar el uso de t\u00e9cnicas que manejan varias series temporales simult\u00e1neamente y comparar el rendimiento entre diferentes enfoques de forecasting.</p> <p> Modelos de Forecasting Globales I: Multi-series forecasting</p> <p> Modelos de Forecasting Globales II: An\u00e1lisis comparativo de modelos de una y m\u00faltiples series</p> <p> Modelos de Forecasting Globales III: Predecir mil series temporales con un \u00fanico modelo</p> <p> Modelos de Forecasting Globales IV: Guia paso a paso con Kaggle sticker sales</p> <p> Forecasting con Deep Learning</p> <p> Clustering de series temporales para mejorar los modelos de forecasting</p>"},{"location":"examples/examples_spanish.html#tecnicas-avanzadas-mas-alla-de-los-modelos-basicos","title":"T\u00e9cnicas Avanzadas: M\u00e1s All\u00e1 de los Modelos B\u00e1sicos","text":"<p>Para usuarios experimentados que buscan profundizar en sus habilidades de forecasting, esta secci\u00f3n ofrece t\u00e9cnicas avanzadas, como forecasting probabil\u00edstico y manejo de valores ausentes.</p> <p> Forecasting probabil\u00edstico</p> <p> Forecasting probabil\u00edstico: intervalos de predicci\u00f3n para forecasting multi-step</p> <p> Modelar series temporales con tendencia utilizando modelos de \u00e1rboles</p> <p> Forecasting de series incompletas con valores faltantes</p> <p> Interpretabilidad en modelos de forecasting</p> <p> Stacking ensemble de modelos para mejorar el forecasting</p> <p> Detecci\u00f3n de anomal\u00edas en series temporales</p> <p> Data leakage en modelos de forecasting preentrenados</p>"},{"location":"examples/examples_spanish.html#desafios-del-mundo-real-y-estudios-de-caso","title":"Desaf\u00edos del Mundo Real y Estudios de Caso","text":"<p>Esta secci\u00f3n incluye aplicaciones del mundo real del pron\u00f3stico de series temporales para abordar desaf\u00edos espec\u00edficos, como el pron\u00f3stico de la demanda de energ\u00eda, el tr\u00e1fico web e incluso los precios de criptomonedas. Aprende c\u00f3mo aplicar t\u00e9cnicas de forecasting a casos pr\u00e1cticos.</p> <p> Forecasting de la demanda el\u00e9ctrica</p> <p> Forecasting de las visitas a una p\u00e1gina web</p> <p> Predicci\u00f3n demanda intermitente</p> <p> Reducir el impacto del Covid en modelos de forecasting</p> <p> Predicci\u00f3n del precio de Bitcoin con Python</p>"},{"location":"faq/cyclical-features-time-series.html","title":"Cyclical features in time series","text":"<p>Cyclical features play an important role in time series prediction because they capture recurring patterns or oscillations within a data set. These patterns repeat at fixed intervals, and the effective incorporation of cyclical features into a machine learning model requires careful preprocessing and feature engineering.</p> <p>Due to the circular nature of cyclical features, it is not recommended to use them directly as numerical inputs in a machine learning model. Instead, they should be encoded in a format that captures their cyclical behavior. There are several common encoding techniques:</p> <ul> <li><p>One-hot encoding: If the cyclical feature consists of distinct categories, such as seasons or months, one-hot encoding can be used. This approach creates binary variables for each category, allowing the model to understand the presence or absence of specific categories.</p> </li> <li><p>Trigonometric coding: For periodic features such as time of day or day of the week, trigonometric functions such as sine and cosine can be used for coding. By mapping the cyclic feature onto a unit circle, these functions preserve the cyclic relationships. In addition, this method introduces only two additional features, making it an efficient coding technique.</p> </li> <li><p>Basis functions: Basis functions are mathematical functions that span a vector space and can be used to represent other functions within that space. When using basis functions, the cyclic feature is transformed into a new set of features based on the selected basis functions. Some commonly used basis functions for encoding cyclic features include Fourier basis functions, B-spline basis functions, and Gaussian basis functions. B-splines are a way to approximate nonlinear functions using a piecewise combination of polynomials.</p> </li> </ul> <p>By applying these encoding techniques, cyclic features can be effectively incorporated into a machine learning model, allowing it to capture and exploit the valuable recurring patterns present in time series data.</p> <p> \u270e Note </p> <p>The following examples are inspired by Time-related feature engineering, scikit-lego\u2019s documentation and Three Approaches to Encoding Time Information as Features for ML Models By Eryk Lewinson.</p> In\u00a0[1]: Copied! <pre># Data manipulation\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder, SplineTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklego.preprocessing import RepeatingBasisFunction\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\n</pre> # Data manipulation # ============================================================================== import numpy as np import pandas as pd  # Plots # ============================================================================== import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme  # Modelling and Forecasting # ============================================================================== from sklearn.ensemble import HistGradientBoostingRegressor from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, SplineTransformer from sklearn.compose import make_column_transformer from sklego.preprocessing import RepeatingBasisFunction from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster In\u00a0[2]: Copied! <pre># Data simulation\n# ==============================================================================\nnp.random.seed(123)\ndates = pd.date_range(start=\"2020-01-01\", end=\"2023-12-31\")\ndata = pd.DataFrame(index=dates)\ndata.index.name = \"date\"\ndata[\"day_idx\"] = range(len(data))\ndata['month'] = data.index.month\n\n# Create the components that will be combined to get the target series\nsignal_1 = 3 + 4 * np.sin(data[\"day_idx\"] / 365 * 2 * np.pi)\nsignal_2 = 3 * np.sin(data[\"day_idx\"] / 365 * 4 * np.pi + 365 / 2)\nnoise = np.random.normal(0, 0.85, len(data))\ny = signal_1 + signal_2 + noise\n\ndata[\"y\"] = y\ndata = data[[\"y\", \"month\"]]\ndata.head(3)\n</pre> # Data simulation # ============================================================================== np.random.seed(123) dates = pd.date_range(start=\"2020-01-01\", end=\"2023-12-31\") data = pd.DataFrame(index=dates) data.index.name = \"date\" data[\"day_idx\"] = range(len(data)) data['month'] = data.index.month  # Create the components that will be combined to get the target series signal_1 = 3 + 4 * np.sin(data[\"day_idx\"] / 365 * 2 * np.pi) signal_2 = 3 * np.sin(data[\"day_idx\"] / 365 * 4 * np.pi + 365 / 2) noise = np.random.normal(0, 0.85, len(data)) y = signal_1 + signal_2 + noise  data[\"y\"] = y data = data[[\"y\", \"month\"]] data.head(3) Out[2]: y month date 2020-01-01 2.928244 1 2020-01-02 4.866145 1 2020-01-03 4.425159 1 In\u00a0[3]: Copied! <pre># Split train-test\n# ==============================================================================\nend_train = '2022-06-30 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-test # ============================================================================== end_train = '2022-06-30 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train : 2020-01-01 00:00:00 --- 2022-06-30 00:00:00  (n=912)\nDates test  : 2022-07-01 00:00:00 --- 2023-12-31 00:00:00  (n=549)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(title=\"Time series\", label=\"train\", ax=ax)\ndata_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)\nax.legend();\n</pre> # Plot time series # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) data_train['y'].plot(title=\"Time series\", label=\"train\", ax=ax) data_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax) ax.legend(); In\u00a0[5]: Copied! <pre># One hot encoding of week_day and hour_day\n# ==============================================================================\none_hot_encoder = make_column_transformer(\n                      (\n                          OneHotEncoder(sparse_output=False, drop='if_binary'),\n                          ['month'],\n                      ),\n                      remainder=\"passthrough\",\n                      verbose_feature_names_out=False,\n                  ).set_output(transform=\"pandas\")\n\ndata_encoded_oh = one_hot_encoder.fit_transform(data)\ndata_encoded_oh.head(3)\n</pre> # One hot encoding of week_day and hour_day # ============================================================================== one_hot_encoder = make_column_transformer(                       (                           OneHotEncoder(sparse_output=False, drop='if_binary'),                           ['month'],                       ),                       remainder=\"passthrough\",                       verbose_feature_names_out=False,                   ).set_output(transform=\"pandas\")  data_encoded_oh = one_hot_encoder.fit_transform(data) data_encoded_oh.head(3) Out[5]: month_1 month_2 month_3 month_4 month_5 month_6 month_7 month_8 month_9 month_10 month_11 month_12 y date 2020-01-01 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.928244 2020-01-02 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.866145 2020-01-03 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.425159 In\u00a0[6]: Copied! <pre># Cyclical encoding with sine/cosine transformation\n# ==============================================================================\ndef sin_transformer(period):\n\t\"\"\"\n\tReturns a transformer that applies sine transformation to a variable using\n\tthe specified period.\n\t\"\"\"\n\treturn FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n\ndef cos_transformer(period):\n\t\"\"\"\n\tReturns a transformer that applies cosine transformation to a variable using\n\tthe specified period.\n\t\"\"\"\n\treturn FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n\ndata_encoded_sin_cos = data.copy()\ndata_encoded_sin_cos[\"month_sin\"] = sin_transformer(12).fit_transform(data_encoded_sin_cos['month'])\ndata_encoded_sin_cos[\"month_cos\"] = cos_transformer(12).fit_transform(data_encoded_sin_cos['month'])\ndata_encoded_sin_cos.head()\n</pre> # Cyclical encoding with sine/cosine transformation # ============================================================================== def sin_transformer(period): \t\"\"\" \tReturns a transformer that applies sine transformation to a variable using \tthe specified period. \t\"\"\" \treturn FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))  def cos_transformer(period): \t\"\"\" \tReturns a transformer that applies cosine transformation to a variable using \tthe specified period. \t\"\"\" \treturn FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))  data_encoded_sin_cos = data.copy() data_encoded_sin_cos[\"month_sin\"] = sin_transformer(12).fit_transform(data_encoded_sin_cos['month']) data_encoded_sin_cos[\"month_cos\"] = cos_transformer(12).fit_transform(data_encoded_sin_cos['month']) data_encoded_sin_cos.head() Out[6]: y month month_sin month_cos date 2020-01-01 2.928244 1 0.5 0.866025 2020-01-02 4.866145 1 0.5 0.866025 2020-01-03 4.425159 1 0.5 0.866025 2020-01-04 3.069222 1 0.5 0.866025 2020-01-05 4.021290 1 0.5 0.866025 In\u00a0[7]: Copied! <pre># Plot of the transformation\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(4., 3.5))\nsp = ax.scatter(\n        data_encoded_sin_cos[\"month_sin\"],\n        data_encoded_sin_cos[\"month_cos\"],\n        c=data_encoded_sin_cos[\"month\"],\n        cmap='viridis'\n     )\nax.set(\n    xlabel=\"sin(month)\",\n    ylabel=\"cos(month)\",\n)\n_ = fig.colorbar(sp)\ndata_encoded_sin_cos = data_encoded_sin_cos.drop(columns='month')\n</pre> # Plot of the transformation # ============================================================================== fig, ax = plt.subplots(figsize=(4., 3.5)) sp = ax.scatter(         data_encoded_sin_cos[\"month_sin\"],         data_encoded_sin_cos[\"month_cos\"],         c=data_encoded_sin_cos[\"month\"],         cmap='viridis'      ) ax.set(     xlabel=\"sin(month)\",     ylabel=\"cos(month)\", ) _ = fig.colorbar(sp) data_encoded_sin_cos = data_encoded_sin_cos.drop(columns='month') In\u00a0[8]: Copied! <pre># Create feature day of year\n# ==============================================================================\ndata['day_of_year'] = data.index.day_of_year\ndata.head(3)\n</pre> # Create feature day of year # ============================================================================== data['day_of_year'] = data.index.day_of_year data.head(3) Out[8]: y month day_of_year date 2020-01-01 2.928244 1 1 2020-01-02 4.866145 1 2 2020-01-03 4.425159 1 3 In\u00a0[9]: Copied! <pre># B-spline functions\n# ==============================================================================\ndef spline_transformer(period, degree=3, extrapolation=\"periodic\"):\n    \"\"\"\n    Returns a transformer that applies B-spline transformation.\n    \"\"\"\n    return SplineTransformer(\n               degree        = degree,\n               n_knots       = period + 1,\n               knots         = 'uniform',\n               extrapolation = extrapolation,\n               include_bias  = True\n           ).set_output(transform=\"pandas\")\n\nsplines_month = spline_transformer(period=12).fit_transform(data[['day_of_year']])\nsplines_month.columns = [f\"spline{i}\" for i in range(len(splines_month.columns))]\n</pre> # B-spline functions # ============================================================================== def spline_transformer(period, degree=3, extrapolation=\"periodic\"):     \"\"\"     Returns a transformer that applies B-spline transformation.     \"\"\"     return SplineTransformer(                degree        = degree,                n_knots       = period + 1,                knots         = 'uniform',                extrapolation = extrapolation,                include_bias  = True            ).set_output(transform=\"pandas\")  splines_month = spline_transformer(period=12).fit_transform(data[['day_of_year']]) splines_month.columns = [f\"spline{i}\" for i in range(len(splines_month.columns))] <p>The graph below shows the 12 spline functions generated using the day of the year as input. Since 12 splines are created with knots evenly distributed along the range 1 to 365 (day_of_year), each curve represents the proximity to the beginning of a particular month.</p> In\u00a0[10]: Copied! <pre># Location of the maximum value of each spline\n# ==============================================================================\nsplines_month.idxmax()\n</pre> # Location of the maximum value of each spline # ============================================================================== splines_month.idxmax() Out[10]: <pre>spline0    2020-12-01\nspline1    2020-01-01\nspline2    2020-01-31\nspline3    2020-03-02\nspline4    2020-04-01\nspline5    2020-05-02\nspline6    2020-06-01\nspline7    2020-07-02\nspline8    2020-08-01\nspline9    2020-08-31\nspline10   2020-10-01\nspline11   2020-10-31\ndtype: datetime64[ns]</pre> In\u00a0[11]: Copied! <pre># Plot of the B-splines functions for the first 365 days\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 4))\nsplines_month.head(365).plot(\n    ax       = ax,\n    subplots = True,\n    sharex   = True,\n    legend   = False,\n    yticks   = [],\n    title    = 'Splines functions for the first 365 days'\n);\n</pre> # Plot of the B-splines functions for the first 365 days # ============================================================================== fig, ax = plt.subplots(figsize=(7, 4)) splines_month.head(365).plot(     ax       = ax,     subplots = True,     sharex   = True,     legend   = False,     yticks   = [],     title    = 'Splines functions for the first 365 days' ); <pre>C:\\Users\\jaesc2\\AppData\\Local\\Temp\\ipykernel_106012\\1440543275.py:4: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n  splines_month.head(365).plot(\n</pre> In\u00a0[12]: Copied! <pre># Encoded data\n# ==============================================================================\ndata_encoded_splines = pd.concat([data, splines_month], axis=1)\ndata_encoded_splines = data_encoded_splines.drop(columns=['day_of_year', 'month'])\ndata_encoded_splines.head(3)\n</pre> # Encoded data # ============================================================================== data_encoded_splines = pd.concat([data, splines_month], axis=1) data_encoded_splines = data_encoded_splines.drop(columns=['day_of_year', 'month']) data_encoded_splines.head(3) Out[12]: y spline0 spline1 spline2 spline3 spline4 spline5 spline6 spline7 spline8 spline9 spline10 spline11 date 2020-01-01 2.928244 0.166667 0.666667 0.166667 0.000000 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-02 4.866145 0.150763 0.665604 0.183628 0.000006 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-03 4.425159 0.135904 0.662485 0.201563 0.000047 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 In\u00a0[13]: Copied! <pre># Radial basis functions\n# ==============================================================================\nrbf = RepeatingBasisFunction(\n          n_periods   = 12,\n          remainder   = 'drop',\n          column      = 'day_of_year',\n          input_range = (1, 366)\n      )\nrbf_month = rbf.fit_transform(data[['day_of_year']])\nrbf_month = pd.DataFrame(\n                data    = rbf_month,\n                index   = data.index,\n                columns = [f\"rbf_{i}\" for i in range(rbf_month.shape[1])]\n            )\nrbf_month.head(3)\n</pre> # Radial basis functions # ============================================================================== rbf = RepeatingBasisFunction(           n_periods   = 12,           remainder   = 'drop',           column      = 'day_of_year',           input_range = (1, 366)       ) rbf_month = rbf.fit_transform(data[['day_of_year']]) rbf_month = pd.DataFrame(                 data    = rbf_month,                 index   = data.index,                 columns = [f\"rbf_{i}\" for i in range(rbf_month.shape[1])]             ) rbf_month.head(3) Out[13]: rbf_0 rbf_1 rbf_2 rbf_3 rbf_4 rbf_5 rbf_6 rbf_7 rbf_8 rbf_9 rbf_10 rbf_11 date 2020-01-01 1.000000 0.367879 0.018316 0.000123 1.125352e-07 1.388794e-11 2.319523e-16 1.388794e-11 1.125352e-07 0.000123 0.018316 0.367879 2020-01-02 0.998920 0.392457 0.020867 0.000150 1.462326e-07 1.927304e-11 3.437695e-16 9.985890e-12 8.641586e-08 0.000101 0.016041 0.344096 2020-01-03 0.995686 0.417773 0.023723 0.000182 1.896101e-07 2.668848e-11 5.083902e-16 7.164680e-12 6.621552e-08 0.000083 0.014019 0.321155 In\u00a0[14]: Copied! <pre># Location of the maximum value of each rbf\n# ==============================================================================\nrbf_month.idxmax()\n</pre> # Location of the maximum value of each rbf # ============================================================================== rbf_month.idxmax() Out[14]: <pre>rbf_0    2020-01-01\nrbf_1    2020-01-31\nrbf_2    2020-03-02\nrbf_3    2020-04-01\nrbf_4    2020-05-02\nrbf_5    2020-06-01\nrbf_6    2020-07-01\nrbf_7    2020-08-01\nrbf_8    2020-08-31\nrbf_9    2020-10-01\nrbf_10   2020-10-31\nrbf_11   2020-12-01\ndtype: datetime64[ns]</pre> In\u00a0[15]: Copied! <pre># Encoded data\n# ==============================================================================\ndata_encoded_rbf = pd.concat([data, rbf_month], axis=1)\ndata_encoded_rbf = data_encoded_rbf.drop(columns=['day_of_year', 'month'])\ndata_encoded_rbf.head(3)\n</pre> # Encoded data # ============================================================================== data_encoded_rbf = pd.concat([data, rbf_month], axis=1) data_encoded_rbf = data_encoded_rbf.drop(columns=['day_of_year', 'month']) data_encoded_rbf.head(3) Out[15]: y rbf_0 rbf_1 rbf_2 rbf_3 rbf_4 rbf_5 rbf_6 rbf_7 rbf_8 rbf_9 rbf_10 rbf_11 date 2020-01-01 2.928244 1.000000 0.367879 0.018316 0.000123 1.125352e-07 1.388794e-11 2.319523e-16 1.388794e-11 1.125352e-07 0.000123 0.018316 0.367879 2020-01-02 4.866145 0.998920 0.392457 0.020867 0.000150 1.462326e-07 1.927304e-11 3.437695e-16 9.985890e-12 8.641586e-08 0.000101 0.016041 0.344096 2020-01-03 4.425159 0.995686 0.417773 0.023723 0.000182 1.896101e-07 2.668848e-11 5.083902e-16 7.164680e-12 6.621552e-08 0.000083 0.014019 0.321155 In\u00a0[16]: Copied! <pre># Create forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags      = [70]\n             )\n</pre> # Create forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags      = [70]              ) In\u00a0[17]: Copied! <pre># Train and validate a forecaster using each encoding method\n# ==============================================================================\ndatasets = [\n    data_encoded_oh, data_encoded_sin_cos, data_encoded_splines, data_encoded_rbf\n]\nencoding_methods = [\n    'one hot encoding', 'sine/cosine encoding', 'spline encoding', 'rbf encoding'\n]\n\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)\n\nfor i, data_encoded in enumerate(datasets):\n\n    cv = TimeSeriesFold(\n            steps              = 365,\n            initial_train_size = len(data_encoded.loc[:end_train]),\n            refit              = False,\n         )\n    metric, predictions = backtesting_forecaster(\n                              forecaster    = forecaster,\n                              y             = data_encoded['y'],\n                              exog          = data_encoded.drop(columns='y'),\n                              cv            = cv,\n                              metric        = 'mean_squared_error'\n                          )\n\n    print(f\"Backtest error using {encoding_methods[i]}: {metric.at[0, 'mean_squared_error']:.2f}\")\n    predictions.plot(label=encoding_methods[i], ax=ax)\n    ax.legend(labels=['test'] + encoding_methods)\n    \nplt.show();\n</pre> # Train and validate a forecaster using each encoding method # ============================================================================== datasets = [     data_encoded_oh, data_encoded_sin_cos, data_encoded_splines, data_encoded_rbf ] encoding_methods = [     'one hot encoding', 'sine/cosine encoding', 'spline encoding', 'rbf encoding' ]  fig, ax = plt.subplots(figsize=(7, 3)) data_test['y'].plot(title=\"Time series\", label=\"test\", ax=ax)  for i, data_encoded in enumerate(datasets):      cv = TimeSeriesFold(             steps              = 365,             initial_train_size = len(data_encoded.loc[:end_train]),             refit              = False,          )     metric, predictions = backtesting_forecaster(                               forecaster    = forecaster,                               y             = data_encoded['y'],                               exog          = data_encoded.drop(columns='y'),                               cv            = cv,                               metric        = 'mean_squared_error'                           )      print(f\"Backtest error using {encoding_methods[i]}: {metric.at[0, 'mean_squared_error']:.2f}\")     predictions.plot(label=encoding_methods[i], ax=ax)     ax.legend(labels=['test'] + encoding_methods)      plt.show(); <pre>  0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error using one hot encoding: 1.10\n</pre> <pre>  0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error using sine/cosine encoding: 1.12\n</pre> <pre>  0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error using spline encoding: 0.75\n</pre> <pre>  0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error using rbf encoding: 0.74\n</pre>"},{"location":"faq/cyclical-features-time-series.html#cyclical-features-in-time-series-forecasting","title":"Cyclical features in time series forecasting\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#one-hot-encoding","title":"One hot encoding\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#cyclical-encoding-with-sinecosine-transformation","title":"Cyclical encoding with sine/cosine transformation\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#b-splines-functions","title":"B-splines functions\u00b6","text":""},{"location":"faq/cyclical-features-time-series.html#radial-basis-functions-rbf","title":"Radial basis functions (RBF)\u00b6","text":"<p>The same encoding can be done using the <code>RepeatingBasisFunction</code> transformer from scikit-lego. The transformer is used to encode the day of the year into 12 radial basis functions.</p>"},{"location":"faq/cyclical-features-time-series.html#compare-forecasting-results","title":"Compare forecasting results\u00b6","text":"<p>A non-informative lag is included so that the impact of cyclical features can be assessed without being obscured by the autoregressive component.</p>"},{"location":"faq/forecasting-time-series-with-missing-values.html","title":"Forecasting time series with missing values","text":"<p>In many real use cases of forecasting, although historical data are available, it is common for the time series to be incomplete. The presence of missing values in the data is a major problem since most forecasting algorithms require the time series to be complete in order to train a model.</p> <p>A commonly employed strategy to overcome this problem is to impute missing values before training the model, for example, using a moving average. However, the quality of the imputations may not be good, impairing the training of the model. One way to improve the imputation strategy is to combine it with weighted time series forecasting. The latter consists of reducing the weight of the imputed observations and thus their influence during model training.</p> <p>This document shows two examples of how skforecast makes it easy to apply this strategy.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.datasets import fetch_dataset\nfrom lightgbm import LGBMRegressor\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme from skforecast.datasets import fetch_dataset from lightgbm import LGBMRegressor from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster <p> \u270e Note </p> <p>In this document, a forecaster of type <code>ForecasterRecursive</code> is used. The same strategy can be applied with any forecaster from skforecast.</p> In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(\"bicimad\")\ndata.head(3)\n</pre> # Data download # ============================================================================== data = fetch_dataset(\"bicimad\") data.head(3) <pre>bicimad\n-------\nThis dataset contains the daily users of the bicycle rental service (BiciMad) in\nthe city of Madrid (Spain) from 2014-06-23 to 2022-09-30.\nThe original data was obtained from: Portal de datos abiertos del Ayuntamiento\nde Madrid https://datos.madrid.es/portal/site/egob\nShape of the dataset: (3022, 1)\n</pre> Out[2]: users date 2014-06-23 99 2014-06-24 72 2014-06-25 119 In\u00a0[3]: Copied! <pre># Generating gaps with missing values\n# ==============================================================================\ngaps = [\n    ['2020-09-01', '2020-10-10'],\n    ['2020-11-08', '2020-12-15'],\n]\n\nfor gap in gaps:\n    data.loc[gap[0]:gap[1]] = np.nan\n</pre> # Generating gaps with missing values # ============================================================================== gaps = [     ['2020-09-01', '2020-10-10'],     ['2020-11-08', '2020-12-15'], ]  for gap in gaps:     data.loc[gap[0]:gap[1]] = np.nan In\u00a0[4]: Copied! <pre># Split data into train-test\n# ==============================================================================\ndata = data.loc['2020-06-01': '2021-06-01']\nend_train = '2021-03-01'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split data into train-test # ============================================================================== data = data.loc['2020-06-01': '2021-06-01'] end_train = '2021-03-01' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train : 2020-06-01 00:00:00 --- 2021-03-01 00:00:00  (n=274)\nDates test  : 2021-03-01 00:00:00 --- 2021-06-01 00:00:00  (n=93)\n</pre> In\u00a0[5]: Copied! <pre># Time series plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(8, 3.5))\ndata_train.users.plot(ax=ax, label='train', linewidth=1)\ndata_test.users.plot(ax=ax, label='test', linewidth=1)\n\nfor gap in gaps:\n    ax.plot(\n        [pd.to_datetime(gap[0]), pd.to_datetime(gap[1])],\n        [data.users[pd.to_datetime(gap[0]) - pd.Timedelta(days=1)],\n         data.users[pd.to_datetime(gap[1]) + pd.Timedelta(days=1)]],\n        color = 'red',\n        linestyle = '--',\n        label = 'gap'\n    )\n\nax.set_title('Number of users BiciMAD')\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nax.legend(by_label.values(), by_label.keys(), loc='lower right');\n</pre> # Time series plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(8, 3.5)) data_train.users.plot(ax=ax, label='train', linewidth=1) data_test.users.plot(ax=ax, label='test', linewidth=1)  for gap in gaps:     ax.plot(         [pd.to_datetime(gap[0]), pd.to_datetime(gap[1])],         [data.users[pd.to_datetime(gap[0]) - pd.Timedelta(days=1)],          data.users[pd.to_datetime(gap[1]) + pd.Timedelta(days=1)]],         color = 'red',         linestyle = '--',         label = 'gap'     )  ax.set_title('Number of users BiciMAD') handles, labels = plt.gca().get_legend_handles_labels() by_label = dict(zip(labels, handles)) ax.legend(by_label.values(), by_label.keys(), loc='lower right'); <pre>C:\\Users\\jaesc2\\AppData\\Local\\Temp\\ipykernel_68220\\3506748202.py:9: UserWarning: This axis already has a converter set and is updating to a potentially incompatible converter\n  ax.plot(\n</pre> In\u00a0[6]: Copied! <pre># Value imputation using linear interpolation\n# ======================================================================================\ndata['users_imputed'] = data['users'].interpolate(method='linear')\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n</pre> # Value imputation using linear interpolation # ====================================================================================== data['users_imputed'] = data['users'].interpolate(method='linear') data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :] <p>To minimize the influence on the model of imputed values, a custom function is defined to create weights following the rules:</p> <ul> <li><p>Weight of 0 if the index date has been imputed or is within 14 days ahead of an imputed day.</p> </li> <li><p>Weight of 1 otherwise.</p> </li> </ul> <p>If an observation has a weight of 0, it has no influence at all during model training.</p> <p> \u270e Note </p> <p>Imputed values should neither participate in the training process as a target nor as a predictor (lag). Therefore, values within a window size as large as the lags used should also be excluded.</p> In\u00a0[\u00a0]: Copied! <pre># Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is in any gap.\n    \"\"\"\n    gaps = [\n        ['2020-09-01', '2020-10-10'],\n        ['2020-11-08', '2020-12-15'],\n    ]\n    \n    missing_dates = [\n        pd.date_range(\n            start = pd.to_datetime(gap[0]) + pd.Timedelta('14d'),\n            end   = pd.to_datetime(gap[1]) + pd.Timedelta('14d'),\n            freq  = 'D'\n        ) \n        for gap in gaps\n    ]\n    missing_dates = pd.DatetimeIndex(np.concatenate(missing_dates))   \n    weights = np.where(index.isin(missing_dates), 0, 1)\n\n    return weights\n</pre> # Custom function to create weights # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is in any gap.     \"\"\"     gaps = [         ['2020-09-01', '2020-10-10'],         ['2020-11-08', '2020-12-15'],     ]          missing_dates = [         pd.date_range(             start = pd.to_datetime(gap[0]) + pd.Timedelta('14d'),             end   = pd.to_datetime(gap[1]) + pd.Timedelta('14d'),             freq  = 'D'         )          for gap in gaps     ]     missing_dates = pd.DatetimeIndex(np.concatenate(missing_dates))        weights = np.where(index.isin(missing_dates), 0, 1)      return weights <p><code>ForecasterRecursive</code> is trained including the <code>custom_weights</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterRecursive)\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor   = LGBMRegressor(random_state=123, verbose=-1),\n                 lags        = 14,\n                 weight_func = custom_weights\n             )\n\n# Backtesting: predict the next 7 days at a time.\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 7,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = True,\n         fixed_train_size   = False,\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster    = forecaster,\n                          y             = data.users_imputed,\n                          cv            = cv,\n                          metric        = 'mean_absolute_error',\n                          verbose       = False,\n                          show_progress = True\n                      )\n\nmetric\n</pre> # Create a recursive multi-step forecaster (ForecasterRecursive) # ============================================================================== forecaster = ForecasterRecursive(                  regressor   = LGBMRegressor(random_state=123, verbose=-1),                  lags        = 14,                  weight_func = custom_weights              )  # Backtesting: predict the next 7 days at a time. # ============================================================================== cv = TimeSeriesFold(          steps              = 7,          initial_train_size = len(data.loc[:end_train]),          refit              = True,          fixed_train_size   = False,      )  metric, predictions = backtesting_forecaster(                           forecaster    = forecaster,                           y             = data.users_imputed,                           cv            = cv,                           metric        = 'mean_absolute_error',                           verbose       = False,                           show_progress = True                       )  metric <pre>  0%|          | 0/14 [00:00&lt;?, ?it/s]</pre> Out[\u00a0]: mean_absolute_error 0 1904.830714 In\u00a0[9]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions.head(4)\n</pre> # Backtest predictions # ============================================================================== predictions.head(4) Out[9]: pred 2021-03-02 10524.159747 2021-03-03 10087.283682 2021-03-04 8882.926166 2021-03-05 9474.810215"},{"location":"faq/forecasting-time-series-with-missing-values.html#forecasting-time-series-with-missing-values","title":"Forecasting time series with missing values\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#impute-missing-values","title":"Impute missing values\u00b6","text":""},{"location":"faq/forecasting-time-series-with-missing-values.html#give-weight-of-zero-to-imputed-values","title":"Give weight of zero to imputed values\u00b6","text":""},{"location":"faq/forecasting-with-delayed-historical-data.html","title":"Forecasting with delayed historical data","text":"<p>In the world of forecasting, accurate predictions depend on historical data. In many real-world scenarios, however, the available data is often subject to delays.  Consider the retail industry, where sales data often arrive with delays ranging from a few days to several weeks. Such delays pose significant challenges for autoregressive models, which use past values of the target variable as predictors.</p> <p>One of the primary obstacles when working with delayed data is accurately evaluating model performance. Incorporating the delay into the evaluation becomes critical, as models must be evaluated based on the data available at the time of prediction. Failure to do so can lead to overly optimistic results, as the model may be accessing data that wasn't available during the prediction period.</p> <p>One way to mitigate this challenge is to include lags that are greater than the maximum delay that the historical data can have. For example, if the data is delayed by 7 days, the minimum lag should be 7 days. This ensures that the model always has access to the data it needs to make predictions. However, this approach will not always achieve great results because the model may be using data that is too far in the past to be useful for prediction.</p> <p> Predictions with lags (last window available) greater than the maximum delay. </p> In\u00a0[\u00a0]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\nfrom skforecast.plot import set_dark_theme\nimport warnings\nwarnings.filterwarnings(\"once\")\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import HistGradientBoostingRegressor from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster from skforecast.plot import set_dark_theme import warnings warnings.filterwarnings(\"once\") In\u00a0[2]: Copied! <pre># Download data and preprocessing\n# ==============================================================================\ndata = fetch_dataset(name='h2o', raw=True, verbose=False,\n                     kwargs_read_csv={'header': 0, 'names': ['y', 'datetime']})\n\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\nprint(f\"Length of time series: {len(data)}\")\nprint(f\"Frequency: {data.index.freqstr}\")\ndata.head(3)\n</pre> # Download data and preprocessing # ============================================================================== data = fetch_dataset(name='h2o', raw=True, verbose=False,                      kwargs_read_csv={'header': 0, 'names': ['y', 'datetime']})  data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  print(f\"Length of time series: {len(data)}\") print(f\"Frequency: {data.index.freqstr}\") data.head(3) <pre>Length of time series: 204\nFrequency: MS\n</pre> Out[2]: y datetime 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 In\u00a0[3]: Copied! <pre># Train-validation dates\n# ==============================================================================\nend_train = '2005-06-01 23:59:59'\ndata_train = data[:end_train].copy()\ndata_test  = data[end_train:].copy()\n\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[:end_train, 'y'].plot(ax=ax, label='train')\ndata.loc[end_train:, 'y'].plot(ax=ax, label='test')\nax.legend()\nplt.show()\n</pre> # Train-validation dates # ============================================================================== end_train = '2005-06-01 23:59:59' data_train = data[:end_train].copy() data_test  = data[end_train:].copy()  print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(6, 3)) data.loc[:end_train, 'y'].plot(ax=ax, label='train') data.loc[end_train:, 'y'].plot(ax=ax, label='test') ax.legend() plt.show() <pre>Train dates : 1991-07-01 00:00:00 --- 2005-06-01 00:00:00  (n=168)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> <p>The data used in this example is a time series of monthly values. Let's assume that the data is delayed by 3 months. This means that the data for January will not be available until April, the data for February will not be available until May, and so on.</p> <p>Ideally, we would like to forecast the entire next year using the last 12 months of data, starting with the month immediately preceding the forecast (lags 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, and 12). However, because the data are delayed by 3 months, it is not possible to use lags 1, 2, or 3 to predict the target variable because these data are not available at the time of the forecast. Therefore, the minimum lag must be 4.</p> In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags      = [4, 5, 6, 7, 8, 9, 10, 11, 12] \n             )\n\nforecaster.fit(y=data_train['y'])\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags      = [4, 5, 6, 7, 8, 9, 10, 11, 12]               )  forecaster.fit(y=data_train['y']) <p>Using a backtesting process, three years are forecast in batches of 12 months.</p> <p> \ud83d\udca1 Tip </p> <p>To a better understanding of backtesting process visit the Backtesting user guide.</p> In\u00a0[5]: Copied! <pre># Backtesting forecaster on test data\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data_train),\n         refit              = False,\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          cv                    = cv,\n                          metric                = 'mean_absolute_error',\n                          n_jobs                = 'auto',\n                          verbose               = True,\n                          show_progress         = True\n                      )\n\nmetric\n</pre> # Backtesting forecaster on test data # ============================================================================== cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data_train),          refit              = False,      )  metric, predictions = backtesting_forecaster(                           forecaster            = forecaster,                           y                     = data['y'],                           cv                    = cv,                           metric                = 'mean_absolute_error',                           n_jobs                = 'auto',                           verbose               = True,                           show_progress         = True                       )  metric <pre>Information of folds\n--------------------\nNumber of observations used for initial training: 168\nNumber of observations used for backtesting: 36\n    Number of folds: 3\n    Number skipped folds: 0 \n    Number of steps per fold: 12\n    Number of steps to exclude between last observed data (last window) and predictions (gap): 0\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=168)\n    Validation: 2005-07-01 00:00:00 -- 2006-06-01 00:00:00  (n=12)\nFold: 1\n    Training:   No training in this fold\n    Validation: 2006-07-01 00:00:00 -- 2007-06-01 00:00:00  (n=12)\nFold: 2\n    Training:   No training in this fold\n    Validation: 2007-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=12)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[5]: mean_absolute_error 0 0.065997 In\u00a0[6]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions.head(5)\n</pre> # Backtest predictions # ============================================================================== predictions.head(5) Out[6]: pred 2005-07-01 1.077316 2005-08-01 1.070779 2005-09-01 1.088995 2005-10-01 1.101434 2005-11-01 1.122724 In\u00a0[7]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[:end_train, 'y'].plot(ax=ax, label='train')\ndata.loc[end_train:, 'y'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[:end_train, 'y'].plot(ax=ax, label='train') data.loc[end_train:, 'y'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>Once the model has been validated, taking into account the delay, it can be used in production. In this case, the model will use the data available at the time of the forecast, which will be data starting 3 months ago.</p> <p>The way the model identifies the data to use is by position index. For example, lag 4 is the value at position 4 from the end of the last available window. The forecaster assumes that the last window provided ends just before the first step to be predicted, but because of the delay, the most recent data available will not be the most recent data in the time series. To ensure that the lags are taken from the correct position, the last window must be extended with dummy values. The number of dummy values must be equal to the number of steps between the last available data and the date just before the first forecast step. In this case, the lag is 3 months, so the number of dummy values must be 3.</p> <p>Let's take a real example, first we train the model with all available data.</p> In\u00a0[8]: Copied! <pre># Create and fit forecaster (whole data)\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = HistGradientBoostingRegressor(random_state=123),\n                 lags      = [4, 5, 6, 7, 8, 9, 10, 11] \n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and fit forecaster (whole data) # ============================================================================== forecaster = ForecasterRecursive(                  regressor = HistGradientBoostingRegressor(random_state=123),                  lags      = [4, 5, 6, 7, 8, 9, 10, 11]               )  forecaster.fit(y=data['y']) In\u00a0[9]: Copied! <pre># Last window \n# ==============================================================================\nlast_window = forecaster.last_window_\nlast_window\n</pre> # Last window  # ============================================================================== last_window = forecaster.last_window_ last_window Out[9]: y datetime 2007-08-01 1.078219 2007-09-01 1.110982 2007-10-01 1.109979 2007-11-01 1.163534 2007-12-01 1.176589 2008-01-01 1.219941 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 <p>Our latest available data date is <code>2008-06-01</code> and, as we know we have a 3 months delay, this means that we are actually sometime in September (the time at which we want to make predictions) and our first predicted point will be <code>2008-10-01</code>.</p> <p>Since the forecaster expects the last window to end in <code>2008-09-01</code> and the last available data is the <code>2008-06-01</code> value, the last window must be extended by 3 dummy values.</p> In\u00a0[10]: Copied! <pre># Dummy values to complete the last_window until the moment of prediction\n# ==============================================================================\n# These dummy values are never used by the model because they are always posterior to the\n# smallest lag.\ndate_start_prediction = pd.to_datetime(\"2008-09-30\")\ndummy_value = np.inf\n\nlast_window_extended = last_window.reindex(\n    pd.date_range(start=last_window.index[0], end=date_start_prediction, freq='MS'),\n    fill_value = dummy_value\n)\n\nlast_window_extended\n</pre> # Dummy values to complete the last_window until the moment of prediction # ============================================================================== # These dummy values are never used by the model because they are always posterior to the # smallest lag. date_start_prediction = pd.to_datetime(\"2008-09-30\") dummy_value = np.inf  last_window_extended = last_window.reindex(     pd.date_range(start=last_window.index[0], end=date_start_prediction, freq='MS'),     fill_value = dummy_value )  last_window_extended Out[10]: y 2007-08-01 1.078219 2007-09-01 1.110982 2007-10-01 1.109979 2007-11-01 1.163534 2007-12-01 1.176589 2008-01-01 1.219941 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 2008-07-01 inf 2008-08-01 inf 2008-09-01 inf <p> \u26a0 Warning </p> <p>Dummy values are never used by the model because they are always posterior to the smallest lag.</p> In\u00a0[11]: Copied! <pre># Predictions\n# ==============================================================================\npredictions = forecaster.predict(steps=12, last_window=last_window_extended)\npredictions.head(3)\n</pre> # Predictions # ============================================================================== predictions = forecaster.predict(steps=12, last_window=last_window_extended) predictions.head(3) Out[11]: <pre>2008-10-01    1.107654\n2008-11-01    1.182545\n2008-12-01    1.153173\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Plot predictions\n# ==============================================================================\nlast_window_used = (last_window_extended.index[0], last_window_extended.index[-4]) \ndummy_values = (last_window_extended.index[-3], last_window_extended.index[-1]) \n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(\n    [dummy_values[0], dummy_values[1]],\n    [last_window.iloc[-1, 0], predictions.iloc[0]],\n    color = 'red',\n    linestyle = '--',\n    label = 'Gap (Dummy values)'\n)\nax.fill_between(last_window_used, data['y'].min(), data['y'].max(), \n                facecolor='#f7931a', alpha=0.4, zorder=0, label='Last window used')\ndata['y'].plot(ax=ax, label='train')\npredictions.plot(ax=ax, label='predictions')\nax.legend()\nplt.show();\n</pre> # Plot predictions # ============================================================================== last_window_used = (last_window_extended.index[0], last_window_extended.index[-4])  dummy_values = (last_window_extended.index[-3], last_window_extended.index[-1])   fig, ax = plt.subplots(figsize=(8, 4)) ax.plot(     [dummy_values[0], dummy_values[1]],     [last_window.iloc[-1, 0], predictions.iloc[0]],     color = 'red',     linestyle = '--',     label = 'Gap (Dummy values)' ) ax.fill_between(last_window_used, data['y'].min(), data['y'].max(),                  facecolor='#f7931a', alpha=0.4, zorder=0, label='Last window used') data['y'].plot(ax=ax, label='train') predictions.plot(ax=ax, label='predictions') ax.legend() plt.show(); <p> \u270e Note </p> <p>Some forecasting models, such as ARIMA and SARIMAX, do not have as much flexibility in terms of changing the last window values. In these cases, forecasts must be made from the last available data to the desired forecast horizon. The forecast values for the delayed data may be discarded as they are already past values.</p> <p> \ud83d\udca1 Tip </p> <p>For a better understanding of how to deploy Forecaster models,  visit forecaster models in production.</p>"},{"location":"faq/forecasting-with-delayed-historical-data.html#forecasting-with-delayed-historical-data","title":"Forecasting with delayed historical data\u00b6","text":""},{"location":"faq/forecasting-with-delayed-historical-data.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"faq/forecasting-with-delayed-historical-data.html#forecasting-with-delayed-data","title":"Forecasting with delayed data\u00b6","text":""},{"location":"faq/forecasting-with-delayed-historical-data.html#forecasting-in-production","title":"Forecasting in production\u00b6","text":""},{"location":"faq/non-negative-predictions.html","title":"Avoid negative predictions when forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge, GammaRegressor\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Ridge, GammaRegressor from sklearn.preprocessing import FunctionTransformer from sklearn.metrics import mean_squared_error from xgboost import XGBRegressor from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Downloading data\n# ==============================================================================\ndata = fetch_dataset(\"bike_sharing\")\ndata = data[['users']].iloc[:1000].copy()\ndata\n</pre> # Downloading data # ============================================================================== data = fetch_dataset(\"bike_sharing\") data = data[['users']].iloc[:1000].copy() data <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 11)\n</pre> Out[2]: users date_time 2011-01-01 00:00:00 16.0 2011-01-01 01:00:00 40.0 2011-01-01 02:00:00 32.0 2011-01-01 03:00:00 13.0 2011-01-01 04:00:00 1.0 ... ... 2011-02-11 11:00:00 64.0 2011-02-11 12:00:00 71.0 2011-02-11 13:00:00 110.0 2011-02-11 14:00:00 84.0 2011-02-11 15:00:00 74.0 <p>1000 rows \u00d7 1 columns</p> In\u00a0[3]: Copied! <pre># Split train-test\n# ==============================================================================\nend_train = '2011-01-31 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(f\"Dates train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates test : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-test # ============================================================================== end_train = '2011-01-31 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(f\"Dates train: {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates test : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train: 2011-01-01 00:00:00 --- 2011-01-31 23:00:00  (n=744)\nDates test : 2011-02-01 00:00:00 --- 2011-02-11 15:00:00  (n=256)\n</pre> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['users'].plot(ax=ax, label='train')\ndata_test['users'].plot(ax=ax, label='test')\nax.set_title('Number of users')\nax.set_xlabel('')\nax.legend();\n</pre> # Plot time series # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) data_train['users'].plot(ax=ax, label='train') data_test['users'].plot(ax=ax, label='test') ax.set_title('Number of users') ax.set_xlabel('') ax.legend(); In\u00a0[5]: Copied! <pre># Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = Ridge(random_state=123),\n                 lags      = 24\n             )\n</pre> # Create a forecaster and train it # ============================================================================== forecaster = ForecasterRecursive(                  regressor = Ridge(random_state=123),                  lags      = 24              ) In\u00a0[\u00a0]: Copied! <pre># Backtesting predictions on test data\n# ==============================================================================\ncv = TimeSeriesFold(\n        steps              = 24,\n        initial_train_size = len(data_train),\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data['users'],\n                          cv         = cv,\n                          metric     = 'mean_squared_error'\n                      )\n</pre> # Backtesting predictions on test data # ============================================================================== cv = TimeSeriesFold(         steps              = 24,         initial_train_size = len(data_train),      )  metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data['users'],                           cv         = cv,                           metric     = 'mean_squared_error'                       ) <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.set_xlabel('')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.set_xlabel('') ax.legend(); <p>The graph above shows that some predictions are negative.</p> In\u00a0[8]: Copied! <pre># Negative predictions\n# ==============================================================================\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions # ============================================================================== predictions[predictions.pred &lt; 0] Out[8]: pred 2011-02-01 00:00:00 -23.337236 2011-02-01 01:00:00 -17.691405 2011-02-02 00:00:00 -5.243456 2011-02-02 01:00:00 -19.363139 2011-02-03 01:00:00 -6.441943 2011-02-04 01:00:00 -10.579940 2011-02-05 00:00:00 -6.026119 2011-02-05 01:00:00 -21.396841 2011-02-07 04:00:00 -3.412043 2011-02-07 05:00:00 -3.701964 2011-02-08 00:00:00 -17.045913 2011-02-08 01:00:00 -13.233004 2011-02-09 00:00:00 -25.315228 2011-02-09 01:00:00 -24.743686 2011-02-10 01:00:00 -5.704407 2011-02-11 01:00:00 -11.758940 In\u00a0[9]: Copied! <pre># Transform data into a logarithmic scale\n# =============================================================================\ndata_log = np.log1p(data)\ndata_train_log = np.log1p(data_train)\ndata_test_log  = np.log1p(data_test)\n</pre> # Transform data into a logarithmic scale # ============================================================================= data_log = np.log1p(data) data_train_log = np.log1p(data_train) data_test_log  = np.log1p(data_test) In\u00a0[10]: Copied! <pre># Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = Ridge(random_state=123),\n                 lags      = 24,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\n# Since the data has been transformed outside the forecaster, the predictions\n# and metric are not in the original scale. They need to be transformed back.\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data_log['users'],\n                          cv         = cv,\n                          metric     = 'mean_squared_error'\n                      )\n</pre> # Create a forecaster and train it # ============================================================================== forecaster = ForecasterRecursive(                  regressor = Ridge(random_state=123),                  lags      = 24,              )  # Backtesting predictions on test data # ============================================================================== # Since the data has been transformed outside the forecaster, the predictions # and metric are not in the original scale. They need to be transformed back. metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data_log['users'],                           cv         = cv,                           metric     = 'mean_squared_error'                       ) <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> In\u00a0[11]: Copied! <pre># Revert the transformation\n# ==============================================================================\npredictions = np.expm1(predictions)\npredictions.head(4)\n</pre> # Revert the transformation # ============================================================================== predictions = np.expm1(predictions) predictions.head(4) Out[11]: pred 2011-02-01 00:00:00 7.936823 2011-02-01 01:00:00 4.210682 2011-02-01 02:00:00 3.009993 2011-02-01 03:00:00 3.083922 In\u00a0[12]: Copied! <pre># Backtesting metric (test data)\n# ==============================================================================\n# The error metric is calculated once the transformation is reversed.\nmetric = mean_squared_error(y_true=data_test['users'], y_pred=predictions)\nprint(f\"Backtesting metric (test data): {metric}\")\n</pre> # Backtesting metric (test data) # ============================================================================== # The error metric is calculated once the transformation is reversed. metric = mean_squared_error(y_true=data_test['users'], y_pred=predictions) print(f\"Backtesting metric (test data): {metric}\") <pre>Backtesting metric (test data): 1991.9332571760067\n</pre> In\u00a0[13]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>Using scikit-learn FunctionTransformer it is possible to include custom transformers in the forecaster object, for example, a logarithmic transformation. If the <code>FunctionTransformer</code> has an inverse function, the output of the predict method is automatically transformed back to the original scale.</p> In\u00a0[14]: Copied! <pre># Create a custom transformer\n# =============================================================================\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"X does not have valid feature names, but FunctionTransformer was fitted with feature names\"\n)\ntransformer_y = FunctionTransformer(func=np.log1p, inverse_func=np.expm1, validate=True)\n\n# Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 24,\n                 transformer_y    = transformer_y,\n                 transformer_exog = None\n             )\nforecaster.fit(data['users'])\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data['users'],\n                          cv         = cv,\n                          metric     = 'mean_squared_error'\n                      )\n\n# Since the transformation is included in the forecaster, predictions are\n# automatically transformed back into the original scale. And the metric is\n# calculated in the original scale.\nmetric\n</pre> # Create a custom transformer # ============================================================================= import warnings warnings.filterwarnings(     \"ignore\",     message=\"X does not have valid feature names, but FunctionTransformer was fitted with feature names\" ) transformer_y = FunctionTransformer(func=np.log1p, inverse_func=np.expm1, validate=True)  # Create a forecaster and train it # ============================================================================== forecaster = ForecasterRecursive(                  regressor        = Ridge(random_state=123),                  lags             = 24,                  transformer_y    = transformer_y,                  transformer_exog = None              ) forecaster.fit(data['users'])  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data['users'],                           cv         = cv,                           metric     = 'mean_squared_error'                       )  # Since the transformation is included in the forecaster, predictions are # automatically transformed back into the original scale. And the metric is # calculated in the original scale. metric <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> Out[14]: mean_squared_error 0 1991.933257 <p> \u26a0 Warning </p> <p><code>FunctionTransformer</code> returns a userwarning un setting <code>validate=True</code>. To hide this warning use <code>warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")</code> before fitting or backtesting the forecaster.</p> In\u00a0[15]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions.head()\n</pre> # Backtest predictions # ============================================================================== predictions.head() Out[15]: pred 2011-02-01 00:00:00 7.936823 2011-02-01 01:00:00 4.210682 2011-02-01 02:00:00 3.009993 2011-02-01 03:00:00 3.083922 2011-02-01 04:00:00 3.865169 <p>The same results are obtained as if the data were log-transformed outside the model. However, by including the log transformation in the model, all inverse transformations are handled automatically.</p> <p>A forecaster with a linear model (scikit learn GammaRegressor) that uses a different link function is evaluated.</p> In\u00a0[16]: Copied! <pre># Create a forecaster and train it\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = GammaRegressor(alpha=1, max_iter=100000),\n                 lags      = 20,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data['users'],\n                          cv         = cv,\n                          metric     = 'mean_squared_error'\n                      )\n\nmetric\n</pre> # Create a forecaster and train it # ============================================================================== forecaster = ForecasterRecursive(                  regressor = GammaRegressor(alpha=1, max_iter=100000),                  lags      = 20,              )  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data['users'],                           cv         = cv,                           metric     = 'mean_squared_error'                       )  metric <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> Out[16]: mean_squared_error 0 3509.698436 In\u00a0[17]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions.head()\n</pre> # Backtest predictions # ============================================================================== predictions.head() Out[17]: pred 2011-02-01 00:00:00 6.685711 2011-02-01 01:00:00 10.238550 2011-02-01 02:00:00 11.950465 2011-02-01 03:00:00 9.194277 2011-02-01 04:00:00 6.372488 <p>In this case the gamma regressor model performed poorly, but if we look to the negative results:</p> In\u00a0[18]: Copied! <pre># Negative predictions\n# ======================================================================================\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions # ====================================================================================== predictions[predictions.pred &lt; 0] Out[18]: pred In\u00a0[19]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>Now a XGBoost model is trained, but with the objective function set to <code>reg:gamma</code>, so that the output is a mean of the gamma distribution, which is defined for positive values only.</p> In\u00a0[20]: Copied! <pre># Create forecaster and train\n# ==============================================================================\nparams = {\n    'tree_method': 'hist',\n    'objective': 'reg:gamma'\n}\n\nforecaster = ForecasterRecursive(\n                 regressor = XGBRegressor(**params),\n                 lags      = 24,\n             )\n\n# Backtesting predictions on test data\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data['users'],\n                          cv         = cv,\n                          metric     = 'mean_squared_error'\n                      )\n\nmetric\n</pre> # Create forecaster and train # ============================================================================== params = {     'tree_method': 'hist',     'objective': 'reg:gamma' }  forecaster = ForecasterRecursive(                  regressor = XGBRegressor(**params),                  lags      = 24,              )  # Backtesting predictions on test data # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data['users'],                           cv         = cv,                           metric     = 'mean_squared_error'                       )  metric <pre>  0%|          | 0/11 [00:00&lt;?, ?it/s]</pre> Out[20]: mean_squared_error 0 1256.619357 In\u00a0[21]: Copied! <pre># Negative predictions\npredictions[predictions.pred &lt; 0]\n</pre> # Negative predictions predictions[predictions.pred &lt; 0] Out[21]: pred In\u00a0[22]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['users'].plot(ax=ax, label='test')\npredictions['pred'].plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['users'].plot(ax=ax, label='test') predictions['pred'].plot(ax=ax, label='predictions') ax.legend(); <p>The forecaster's performance is better than using a Ridge regressor with a log-transformer, and no negative predictions are made.</p>"},{"location":"faq/non-negative-predictions.html#avoid-negative-predictions-in-forecasting","title":"Avoid Negative Predictions in Forecasting\u00b6","text":"<p>When training a forecasting model, even if none of the training observations are negative, some predictions may turn out to be negative. This is quite common when trying to predict attendance, sales, or rainfall, among other things. To avoid this, it is possible to use the following approaches:</p> <ul> <li><p>Use a log+K transformation to always have positive values in the transformed time series. K is a positive integer to avoid the undefinability of the $log(x)$ function at 0. Despite the simplicity of the approach, one must be aware that some caveats may arise depending on the metric we use to optimize our models.</p> </li> <li><p>Use a different link function in the models. The link function provides the relationship between the linear predictor and the expected value of the response variable. When using machine learning models, several algorithms support different objective functions to account for this situation. For example, when predicting the number of visitors, one can use <code>count:poisson</code> in XGBoost or LightGBM as well as gamma regression if the target is always strictly positive.</p> </li> </ul> <p>The following tutorial will explore both possibilities.</p>"},{"location":"faq/non-negative-predictions.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"faq/non-negative-predictions.html#forecaster","title":"Forecaster\u00b6","text":""},{"location":"faq/non-negative-predictions.html#modeling-time-series-in-logarithmic-scale","title":"Modeling time series in logarithmic scale\u00b6","text":""},{"location":"faq/non-negative-predictions.html#include-a-logarithmic-transformer-as-part-of-the-forecaster","title":"Include a logarithmic transformer as part of the forecaster\u00b6","text":""},{"location":"faq/non-negative-predictions.html#usage-of-link-functions","title":"Usage of link functions\u00b6","text":""},{"location":"faq/non-negative-predictions.html#usage-of-xgboost-objective-functions","title":"Usage of XGBoost objective functions\u00b6","text":""},{"location":"faq/parallelization-skforecast.html","title":"Parallelization in skforecast","text":"<p> \u26a0 Warning </p> <p>The automatic selection of the parallelization level relies on heuristics and is therefore not guaranteed to be optimal. In addition, it is important to keep in mind that many regressors already parallelize their fitting procedures inherently. As a result, introducing additional parallelization may not necessarily improve overall performance. For a more detailed look at parallelization, visit <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code>.</p> In\u00a0[2]: Copied! <pre># Libraries\n# ==============================================================================\nimport platform\nimport psutil\nimport skforecast\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport sklearn\nimport time\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\n\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.direct import ForecasterDirect\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\nfrom skforecast.direct import ForecasterDirectMultiVariate\n\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import grid_search_forecaster_multiseries\nfrom skforecast.model_selection import backtesting_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import platform import psutil import skforecast import pandas as pd import numpy as np import scipy import sklearn import time import warnings import seaborn as sns import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.ensemble import HistGradientBoostingRegressor from sklearn.preprocessing import StandardScaler from lightgbm import LGBMRegressor  from skforecast.recursive import ForecasterRecursive from skforecast.direct import ForecasterDirect from skforecast.recursive import ForecasterRecursiveMultiSeries from skforecast.direct import ForecasterDirectMultiVariate  from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import grid_search_forecaster_multiseries from skforecast.model_selection import backtesting_forecaster_multiseries In\u00a0[3]: Copied! <pre># Versions\n# ==============================================================================\nprint(f\"Python version      : {platform.python_version()}\")\nprint(f\"scikit-learn version: {sklearn.__version__}\")\nprint(f\"skforecast version  : {skforecast.__version__}\")\nprint(f\"pandas version      : {pd.__version__}\")\nprint(f\"numpy version       : {np.__version__}\")\nprint(f\"scipy version       : {scipy.__version__}\")\nprint(\"\")\n\n# System information\n# ==============================================================================\nprint(f\"Processor type: {platform.processor()}\")\nprint(f\"Platform type: {platform.platform()}\")\nprint(f\"Operating system: {platform.system()}\")\nprint(f\"Operating system release: {platform.release()}\")\nprint(f\"Operating system version: {platform.version()}\")\nprint(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\")\nprint(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\")\n</pre> # Versions # ============================================================================== print(f\"Python version      : {platform.python_version()}\") print(f\"scikit-learn version: {sklearn.__version__}\") print(f\"skforecast version  : {skforecast.__version__}\") print(f\"pandas version      : {pd.__version__}\") print(f\"numpy version       : {np.__version__}\") print(f\"scipy version       : {scipy.__version__}\") print(\"\")  # System information # ============================================================================== print(f\"Processor type: {platform.processor()}\") print(f\"Platform type: {platform.platform()}\") print(f\"Operating system: {platform.system()}\") print(f\"Operating system release: {platform.release()}\") print(f\"Operating system version: {platform.version()}\") print(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\") print(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\") <pre>Python version      : 3.12.4\nscikit-learn version: 1.5.2\nskforecast version  : 0.14.0\npandas version      : 2.2.3\nnumpy version       : 2.0.2\nscipy version       : 1.14.1\n\nProcessor type: Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\nPlatform type: Windows-11-10.0.26100-SP0\nOperating system: Windows\nOperating system release: 11\nOperating system version: 10.0.26100\nNumber of physical cores: 4\nNumber of logical cores: 8\n</pre> In\u00a0[4]: Copied! <pre># Data\n# ==============================================================================\nn = 5_000\nrgn = np.random.default_rng(seed=123)\ny = pd.Series(rgn.random(size=(n)), name=\"y\")\nexog = pd.DataFrame(rgn.random(size=(n, 10)))\nexog.columns = [f\"exog_{i}\" for i in range(exog.shape[1])]\nmulti_series = pd.DataFrame(rgn.random(size=(n, 10)))\nmulti_series.columns = [f\"series_{i + 1}\" for i in range(multi_series.shape[1])]\ny_train = y[:-int(n / 2)]\ndisplay(y.head())\ndisplay(exog.head())   \ndisplay(multi_series.head())\n</pre> # Data # ============================================================================== n = 5_000 rgn = np.random.default_rng(seed=123) y = pd.Series(rgn.random(size=(n)), name=\"y\") exog = pd.DataFrame(rgn.random(size=(n, 10))) exog.columns = [f\"exog_{i}\" for i in range(exog.shape[1])] multi_series = pd.DataFrame(rgn.random(size=(n, 10))) multi_series.columns = [f\"series_{i + 1}\" for i in range(multi_series.shape[1])] y_train = y[:-int(n / 2)] display(y.head()) display(exog.head())    display(multi_series.head()) <pre>0    0.682352\n1    0.053821\n2    0.220360\n3    0.184372\n4    0.175906\nName: y, dtype: float64</pre> exog_0 exog_1 exog_2 exog_3 exog_4 exog_5 exog_6 exog_7 exog_8 exog_9 0 0.593121 0.353471 0.336277 0.399734 0.915459 0.822278 0.480418 0.929802 0.950948 0.863556 1 0.764104 0.638191 0.956624 0.178105 0.434077 0.137480 0.837667 0.768947 0.244235 0.815336 2 0.475312 0.312415 0.353596 0.272162 0.772064 0.110216 0.596551 0.688549 0.651380 0.191837 3 0.039253 0.962713 0.189194 0.910629 0.169796 0.697751 0.830913 0.484824 0.634634 0.862865 4 0.872447 0.861421 0.394829 0.877763 0.286779 0.131008 0.450185 0.898167 0.590147 0.045838 series_1 series_2 series_3 series_4 series_5 series_6 series_7 series_8 series_9 series_10 0 0.967448 0.580646 0.643348 0.461737 0.450859 0.894496 0.037967 0.097698 0.094356 0.893528 1 0.207450 0.194904 0.377063 0.975065 0.351034 0.812253 0.265956 0.262733 0.784995 0.674256 2 0.520431 0.985069 0.039559 0.541797 0.612761 0.640336 0.823467 0.768387 0.561777 0.600835 3 0.866694 0.165510 0.819767 0.691179 0.717778 0.392694 0.094067 0.271990 0.467866 0.041054 4 0.406310 0.657688 0.630730 0.694424 0.943934 0.888538 0.470363 0.518283 0.719674 0.010789 In\u00a0[5]: Copied! <pre>warnings.filterwarnings(\"ignore\")\n\nprint(\"-------------------\")\nprint(\"ForecasterRecursive\")\nprint(\"-------------------\")\nsteps = 100\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5, verbose=-1),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5, verbose=-1),\n    HistGradientBoostingRegressor(random_state=77, max_iter=50, max_depth=5,),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'max_iter': [50, 50], 'max_depth': [5, 5]}\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterRecursive(\n                     regressor        = regressor,\n                     lags             = lags,\n                     transformer_exog = StandardScaler()\n                 )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n             steps              = steps,\n             initial_train_size = len(y_train),\n             refit              = True,\n             fixed_train_size   = False,\n         )\n    metric, backtest_predictions = backtesting_forecaster(\n                                       forecaster    = forecaster,\n                                       y             = y,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = -1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                       forecaster    = forecaster,\n                                       y             = y,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = 1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n             steps              = steps,\n             initial_train_size = len(y_train),\n             refit              = False,\n         )\n    metric, backtest_predictions = backtesting_forecaster(\n                                       forecaster    = forecaster,\n                                       y             = y,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = -1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                       forecaster    = forecaster,\n                                       y             = y,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = 1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n             steps              = steps,\n             initial_train_size = len(y_train),\n             refit              = False,\n         )\n    results_grid = grid_search_forecaster(\n                       forecaster    = forecaster,\n                       y             = y,\n                       exog          = exog,\n                       cv            = cv,\n                       param_grid    = param_grid,\n                       lags_grid     = lags_grid,\n                       metric        = 'mean_squared_error',\n                       return_best   = False,\n                       verbose       = False,\n                       show_progress = False,\n                       n_jobs        = -1\n                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                       forecaster    = forecaster,\n                       y             = y,\n                       exog          = exog,\n                       cv            = cv,\n                       param_grid    = param_grid,\n                       lags_grid     = lags_grid,\n                       metric        = 'mean_squared_error',\n                       return_best   = False,\n                       verbose       = False,\n                       show_progress = False,\n                       n_jobs        = 1\n                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n    \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n    \"method\": np.tile(methods, len(regressors)),\n    \"elapsed_time\": elapsed_times\n})\nresults[\"regressor\"] = results[\"regressor\"].str.replace(\"\\n              \", \" \")\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(\n    index=[\"regressor\", \"method\"],\n    columns=\"parallel\",\n    values=\"elapsed_time\"\n).reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nbars = sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nfor container in bars.containers:\n    ax.bar_label(container, fmt='%.1f', padding=3, fontsize=8)\nax.set_title(\"Parallel vs Sequential (ForecasterRecursive)\")\nax.set_ylabel(\"Percent improvement\")\nax.set_xlabel(\"Method\")\nax.legend(fontsize=8, loc='lower left', bbox_to_anchor=(0, -0.31), ncols=1);\n</pre> warnings.filterwarnings(\"ignore\")  print(\"-------------------\") print(\"ForecasterRecursive\") print(\"-------------------\") steps = 100 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5, verbose=-1),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5, verbose=-1),     HistGradientBoostingRegressor(random_state=77, max_iter=50, max_depth=5,), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'max_iter': [50, 50], 'max_depth': [5, 5]} ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterRecursive(                      regressor        = regressor,                      lags             = lags,                      transformer_exog = StandardScaler()                  )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     cv = TimeSeriesFold(              steps              = steps,              initial_train_size = len(y_train),              refit              = True,              fixed_train_size   = False,          )     metric, backtest_predictions = backtesting_forecaster(                                        forecaster    = forecaster,                                        y             = y,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = -1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                        forecaster    = forecaster,                                        y             = y,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = 1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     cv = TimeSeriesFold(              steps              = steps,              initial_train_size = len(y_train),              refit              = False,          )     metric, backtest_predictions = backtesting_forecaster(                                        forecaster    = forecaster,                                        y             = y,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = -1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                        forecaster    = forecaster,                                        y             = y,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = 1                                    )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     cv = TimeSeriesFold(              steps              = steps,              initial_train_size = len(y_train),              refit              = False,          )     results_grid = grid_search_forecaster(                        forecaster    = forecaster,                        y             = y,                        exog          = exog,                        cv            = cv,                        param_grid    = param_grid,                        lags_grid     = lags_grid,                        metric        = 'mean_squared_error',                        return_best   = False,                        verbose       = False,                        show_progress = False,                        n_jobs        = -1                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster(                        forecaster    = forecaster,                        y             = y,                        exog          = exog,                        cv            = cv,                        param_grid    = param_grid,                        lags_grid     = lags_grid,                        metric        = 'mean_squared_error',                        return_best   = False,                        verbose       = False,                        show_progress = False,                        n_jobs        = 1                    )     end = time.time()     elapsed_times.append(end - start)   methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),     \"method\": np.tile(methods, len(regressors)),     \"elapsed_time\": elapsed_times }) results[\"regressor\"] = results[\"regressor\"].str.replace(\"\\n              \", \" \") results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(     index=[\"regressor\", \"method\"],     columns=\"parallel\",     values=\"elapsed_time\" ).reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot)  fig, ax = plt.subplots(figsize=(10, 5)) bars = sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) for container in bars.containers:     ax.bar_label(container, fmt='%.1f', padding=3, fontsize=8) ax.set_title(\"Parallel vs Sequential (ForecasterRecursive)\") ax.set_ylabel(\"Percent improvement\") ax.set_xlabel(\"Method\") ax.legend(fontsize=8, loc='lower left', bbox_to_anchor=(0, -0.31), ncols=1); <pre>-------------------\nForecasterRecursive\n-------------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77,\n              verbose=-1) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=-1, random_state=77,\n              verbose=-1) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nHistGradientBoostingRegressor(max_depth=5, max_iter=50, random_state=77) {'max_iter': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n</pre> regressor method False True pct_improvement 0 HistGradientBoostingRegressor(max_depth=5, max... backtest_no_refit 2.097107 0.848587 59.535365 1 HistGradientBoostingRegressor(max_depth=5, max... backtest_refit 6.490748 2.794112 56.952390 2 HistGradientBoostingRegressor(max_depth=5, max... create_train_X_y 0.005000 NaN NaN 3 HistGradientBoostingRegressor(max_depth=5, max... fit 0.527734 NaN NaN 4 HistGradientBoostingRegressor(max_depth=5, max... gridSearch_no_refit 8.432383 3.252936 61.423293 5 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 2.136809 0.972203 54.502080 6 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 4.557765 3.956567 13.190632 7 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.005999 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 0.114982 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 8.766446 4.257316 51.436232 10 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 1.095819 0.515112 52.992953 11 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 4.761011 2.660067 44.128113 12 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.005002 NaN NaN 13 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 0.239274 NaN NaN 14 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 4.372416 2.121355 51.483235 15 Ridge(alpha=0.1, random_state=77) backtest_no_refit 0.242297 0.119852 50.535045 16 Ridge(alpha=0.1, random_state=77) backtest_refit 0.596156 8.014275 -1244.324804 17 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.007996 NaN NaN 18 Ridge(alpha=0.1, random_state=77) fit 0.050004 NaN NaN 19 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 0.681082 0.359283 47.248207 In\u00a0[6]: Copied! <pre>print(\"----------------\")\nprint(\"ForecasterDirect\")\nprint(\"----------------\")\nsteps = 10\nlags = 10\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5, verbose=-1),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5, verbose=-1),\n    HistGradientBoostingRegressor(random_state=77, max_iter=50, max_depth=5,),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'max_iter': [50, 50], 'max_depth': [5, 5]}\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterDirect(\n                     regressor        = regressor,\n                     steps            = steps,\n                     lags             = lags,\n                     transformer_exog = StandardScaler()\n                 )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(y=y, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n             steps              = steps,\n             initial_train_size = int(len(y) * 0.9),\n             refit              = True,\n             fixed_train_size   = False,\n         )\n    metric, backtest_predictions = backtesting_forecaster(\n                                       forecaster    = forecaster,\n                                       y             = y,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = -1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                       forecaster    = forecaster,\n                                       y             = y,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = 1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n             steps              = steps,\n             initial_train_size = int(len(y) * 0.9),\n             refit              = False,\n         )\n    metric, backtest_predictions = backtesting_forecaster(\n                                       forecaster    = forecaster,\n                                       y             = y,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = -1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster(\n                                       forecaster    = forecaster,\n                                       y             = y,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = 1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                       forecaster    = forecaster,\n                       y             = y,\n                       exog          = exog,\n                       cv            = cv,\n                       param_grid    = param_grid,\n                       lags_grid     = lags_grid,\n                       metric        = 'mean_squared_error',\n                       return_best   = False,\n                       verbose       = False,\n                       show_progress = False,\n                       n_jobs        = -1\n                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster(\n                       forecaster    = forecaster,\n                       y             = y,\n                       exog          = exog,\n                       cv            = cv,\n                       param_grid    = param_grid,\n                       lags_grid     = lags_grid,\n                       metric        = 'mean_squared_error',\n                       return_best   = False,\n                       verbose       = False,\n                       show_progress = False,\n                       n_jobs        = 1\n                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n    \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n    \"method\": np.tile(methods, len(regressors)),\n    \"elapsed_time\": elapsed_times\n})\nresults[\"regressor\"] = results[\"regressor\"].str.replace(\"\\n              \", \" \")\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(\n    index=[\"regressor\", \"method\"],\n    columns=\"parallel\",\n    values=\"elapsed_time\"\n).reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nbars = sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nfor container in bars.containers:\n    ax.bar_label(container, fmt='%.1f', padding=3, fontsize=8)\nax.set_title(\"Parallel vs Sequential (ForecasterDirect)\")\nax.set_ylabel(\"Percent improvement\")\nax.set_xlabel(\"Method\")\nax.legend(fontsize=8, loc='lower left', bbox_to_anchor=(0, -0.31), ncols=1);\n</pre> print(\"----------------\") print(\"ForecasterDirect\") print(\"----------------\") steps = 10 lags = 10 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5, verbose=-1),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5, verbose=-1),     HistGradientBoostingRegressor(random_state=77, max_iter=50, max_depth=5,), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'max_iter': [50, 50], 'max_depth': [5, 5]} ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterDirect(                      regressor        = regressor,                      steps            = steps,                      lags             = lags,                      transformer_exog = StandardScaler()                  )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(y=y, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     cv = TimeSeriesFold(              steps              = steps,              initial_train_size = int(len(y) * 0.9),              refit              = True,              fixed_train_size   = False,          )     metric, backtest_predictions = backtesting_forecaster(                                        forecaster    = forecaster,                                        y             = y,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = -1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                        forecaster    = forecaster,                                        y             = y,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = 1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     cv = TimeSeriesFold(              steps              = steps,              initial_train_size = int(len(y) * 0.9),              refit              = False,          )     metric, backtest_predictions = backtesting_forecaster(                                        forecaster    = forecaster,                                        y             = y,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = -1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster(                                        forecaster    = forecaster,                                        y             = y,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = 1                                    )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster(                        forecaster    = forecaster,                        y             = y,                        exog          = exog,                        cv            = cv,                        param_grid    = param_grid,                        lags_grid     = lags_grid,                        metric        = 'mean_squared_error',                        return_best   = False,                        verbose       = False,                        show_progress = False,                        n_jobs        = -1                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster(                        forecaster    = forecaster,                        y             = y,                        exog          = exog,                        cv            = cv,                        param_grid    = param_grid,                        lags_grid     = lags_grid,                        metric        = 'mean_squared_error',                        return_best   = False,                        verbose       = False,                        show_progress = False,                        n_jobs        = 1                    )     end = time.time()     elapsed_times.append(end - start)   methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),     \"method\": np.tile(methods, len(regressors)),     \"elapsed_time\": elapsed_times }) results[\"regressor\"] = results[\"regressor\"].str.replace(\"\\n              \", \" \") results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(     index=[\"regressor\", \"method\"],     columns=\"parallel\",     values=\"elapsed_time\" ).reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot)  fig, ax = plt.subplots(figsize=(10, 5)) bars = sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) for container in bars.containers:     ax.bar_label(container, fmt='%.1f', padding=3, fontsize=8) ax.set_title(\"Parallel vs Sequential (ForecasterDirect)\") ax.set_ylabel(\"Percent improvement\") ax.set_xlabel(\"Method\") ax.legend(fontsize=8, loc='lower left', bbox_to_anchor=(0, -0.31), ncols=1); <pre>----------------\nForecasterDirect\n----------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77,\n              verbose=-1) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=-1, random_state=77,\n              verbose=-1) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nHistGradientBoostingRegressor(max_depth=5, max_iter=50, random_state=77) {'max_iter': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n</pre> regressor method False True pct_improvement 0 HistGradientBoostingRegressor(max_depth=5, max... backtest_no_refit 2.728197 5.023978 -84.150110 1 HistGradientBoostingRegressor(max_depth=5, max... backtest_refit 71.989915 28.746552 60.068641 2 HistGradientBoostingRegressor(max_depth=5, max... create_train_X_y 0.009108 NaN NaN 3 HistGradientBoostingRegressor(max_depth=5, max... fit 1.012895 NaN NaN 4 HistGradientBoostingRegressor(max_depth=5, max... gridSearch_no_refit 8.722155 27.459687 -214.826857 5 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 1.175862 1.919050 -63.203676 6 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 25.116236 28.374661 -12.973379 7 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.010000 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 0.386074 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 6.524236 11.774790 -80.477689 10 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 1.028363 1.998308 -94.319227 11 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 31.491574 12.525722 60.225165 12 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.006005 NaN NaN 13 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 0.723918 NaN NaN 14 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 6.923338 11.685491 -68.784073 15 Ridge(alpha=0.1, random_state=77) backtest_no_refit 0.158813 0.132391 16.637442 16 Ridge(alpha=0.1, random_state=77) backtest_refit 1.816731 0.620145 65.864817 17 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.006006 NaN NaN 18 Ridge(alpha=0.1, random_state=77) fit 0.062544 NaN NaN 19 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 0.548561 0.618236 -12.701411 In\u00a0[7]: Copied! <pre>print(\"------------------------------\")\nprint(\"ForecasterRecursiveMultiSeries\")\nprint(\"------------------------------\")\nsteps = 100\nlags = 50\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5, verbose=-1),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5, verbose=-1),\n    HistGradientBoostingRegressor(random_state=77, max_iter=50, max_depth=5,),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'max_iter': [50, 50], 'max_depth': [5, 5]}\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterRecursiveMultiSeries(\n                     regressor        = regressor,\n                     lags             = lags,\n                     transformer_exog = StandardScaler()\n                 )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n             initial_train_size = len(y_train),\n             refit              = True,\n             fixed_train_size   = False,\n             steps              = steps,\n         )\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                       forecaster    = forecaster,\n                                       series        = multi_series,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = -1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting refit and no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                       forecaster    = forecaster,\n                                       series        = multi_series,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = 1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n            initial_train_size = len(y_train),\n            refit              = False,\n            steps              = steps,\n         )\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                       forecaster    = forecaster,\n                                       series        = multi_series,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = -1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                       forecaster    = forecaster,\n                                       series        = multi_series,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = 1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                       forecaster    = forecaster,\n                       series        = multi_series,\n                       exog          = exog,\n                       cv            = cv,\n                       param_grid    = param_grid,\n                       lags_grid     = lags_grid,\n                       metric        = 'mean_squared_error',\n                       return_best   = False,\n                       verbose       = False,\n                       show_progress = False,\n                       n_jobs        = -1\n                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n             initial_train_size = len(y_train),\n             refit              = False,\n             steps              = steps,\n         )\n    results_grid = grid_search_forecaster_multiseries(\n                       forecaster    = forecaster,\n                       series        = multi_series,\n                       exog          = exog,\n                       cv            = cv,\n                       param_grid    = param_grid,\n                       lags_grid     = lags_grid,\n                       metric        = 'mean_squared_error',\n                       return_best   = False,\n                       verbose       = False,\n                       show_progress = False,\n                       n_jobs        = 1\n                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n    \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n    \"method\": np.tile(methods, len(regressors)),\n    \"elapsed_time\": elapsed_times\n})\nresults[\"regressor\"] = results[\"regressor\"].str.replace(\"\\n              \", \" \")\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(\n    index=[\"regressor\", \"method\"],\n    columns=\"parallel\",\n    values=\"elapsed_time\"\n).reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nbars = sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nfor container in bars.containers:\n    ax.bar_label(container, fmt='%.1f', padding=3, fontsize=8)\nax.set_title(\"Parallel vs Sequential (ForecasterRecursiveMultiSeries)\")\nax.set_ylabel(\"Percent improvement\")\nax.set_xlabel(\"Method\")\nax.legend(fontsize=8, loc='lower left', bbox_to_anchor=(0, -0.31), ncols=1);\n</pre> print(\"------------------------------\") print(\"ForecasterRecursiveMultiSeries\") print(\"------------------------------\") steps = 100 lags = 50 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5, verbose=-1),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5, verbose=-1),     HistGradientBoostingRegressor(random_state=77, max_iter=50, max_depth=5,), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'max_iter': [50, 50], 'max_depth': [5, 5]} ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterRecursiveMultiSeries(                      regressor        = regressor,                      lags             = lags,                      transformer_exog = StandardScaler()                  )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     cv = TimeSeriesFold(              initial_train_size = len(y_train),              refit              = True,              fixed_train_size   = False,              steps              = steps,          )     metric, backtest_predictions = backtesting_forecaster_multiseries(                                        forecaster    = forecaster,                                        series        = multi_series,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = -1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting refit and no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                        forecaster    = forecaster,                                        series        = multi_series,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = 1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     cv = TimeSeriesFold(             initial_train_size = len(y_train),             refit              = False,             steps              = steps,          )     metric, backtest_predictions = backtesting_forecaster_multiseries(                                        forecaster    = forecaster,                                        series        = multi_series,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = -1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                        forecaster    = forecaster,                                        series        = multi_series,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = 1                                    )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                        forecaster    = forecaster,                        series        = multi_series,                        exog          = exog,                        cv            = cv,                        param_grid    = param_grid,                        lags_grid     = lags_grid,                        metric        = 'mean_squared_error',                        return_best   = False,                        verbose       = False,                        show_progress = False,                        n_jobs        = -1                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     cv = TimeSeriesFold(              initial_train_size = len(y_train),              refit              = False,              steps              = steps,          )     results_grid = grid_search_forecaster_multiseries(                        forecaster    = forecaster,                        series        = multi_series,                        exog          = exog,                        cv            = cv,                        param_grid    = param_grid,                        lags_grid     = lags_grid,                        metric        = 'mean_squared_error',                        return_best   = False,                        verbose       = False,                        show_progress = False,                        n_jobs        = 1                    )     end = time.time()     elapsed_times.append(end - start)   methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),     \"method\": np.tile(methods, len(regressors)),     \"elapsed_time\": elapsed_times }) results[\"regressor\"] = results[\"regressor\"].str.replace(\"\\n              \", \" \") results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(     index=[\"regressor\", \"method\"],     columns=\"parallel\",     values=\"elapsed_time\" ).reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot)  fig, ax = plt.subplots(figsize=(10, 5)) bars = sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) for container in bars.containers:     ax.bar_label(container, fmt='%.1f', padding=3, fontsize=8) ax.set_title(\"Parallel vs Sequential (ForecasterRecursiveMultiSeries)\") ax.set_ylabel(\"Percent improvement\") ax.set_xlabel(\"Method\") ax.legend(fontsize=8, loc='lower left', bbox_to_anchor=(0, -0.31), ncols=1); <pre>------------------------------\nForecasterRecursiveMultiSeries\n------------------------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77,\n              verbose=-1) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=-1, random_state=77,\n              verbose=-1) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nHistGradientBoostingRegressor(max_depth=5, max_iter=50, random_state=77) {'max_iter': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit and no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n</pre> regressor method False True pct_improvement 0 HistGradientBoostingRegressor(max_depth=5, max... backtest_no_refit 1.106152 0.664156 39.957987 1 HistGradientBoostingRegressor(max_depth=5, max... backtest_refit 8.177982 4.231594 48.256264 2 HistGradientBoostingRegressor(max_depth=5, max... create_train_X_y 0.073507 NaN NaN 3 HistGradientBoostingRegressor(max_depth=5, max... fit 0.396801 NaN NaN 4 HistGradientBoostingRegressor(max_depth=5, max... gridSearch_no_refit 4.788807 2.866729 40.136880 5 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 2.918234 1.537057 47.329215 6 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 15.102185 11.231904 25.627290 7 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.093560 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 0.421425 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 12.388050 6.388853 48.427290 10 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 1.801216 1.084726 39.778153 11 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 19.794949 9.823516 50.373624 12 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.053997 NaN NaN 13 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 1.159806 NaN NaN 14 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 5.983032 4.391494 26.600865 15 Ridge(alpha=0.1, random_state=77) backtest_no_refit 0.476766 3.180120 -567.018602 16 Ridge(alpha=0.1, random_state=77) backtest_refit 2.896543 1.427100 50.730913 17 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.085904 NaN NaN 18 Ridge(alpha=0.1, random_state=77) fit 0.187410 NaN NaN 19 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 1.479561 1.118693 24.390221 In\u00a0[8]: Copied! <pre>print(\"----------------------------\")\nprint(\"ForecasterDirectMultiVariate\")\nprint(\"----------------------------\")\nsteps = 5\nlags = 10\nregressors = [\n    Ridge(random_state=77, alpha=0.1),\n    LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5, verbose=-1),\n    LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5, verbose=-1),\n    HistGradientBoostingRegressor(random_state=77, max_iter=50, max_depth=5,),\n]\nparam_grids = [\n    {'alpha': [0.1, 0.1, 0.1]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'n_estimators': [50, 50], 'max_depth': [5, 5]},\n    {'max_iter': [50, 50], 'max_depth': [5, 5]}\n]\nlags_grid = [50, 50, 50]\nelapsed_times = []\n\nfor regressor, param_grid in zip(regressors, param_grids):\n    print(\"\")\n    print(regressor, param_grid)\n    print(\"\")\n    forecaster = ForecasterDirectMultiVariate(\n                     regressor        = regressor,\n                     lags             = lags,\n                     steps            = steps,\n                     level            = \"series_1\",\n                     transformer_exog = StandardScaler()\n                 )\n    \n    print(\"Profiling fit\")\n    start = time.time()\n    forecaster.fit(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling create_train_X_y\")\n    start = time.time()\n    _ = forecaster.create_train_X_y(series=multi_series, exog=exog)\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n             steps              = steps,\n             initial_train_size = int(len(y) * 0.9),\n             refit              = True,\n             fixed_train_size   = False,\n         )\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                       forecaster    = forecaster,\n                                       series        = multi_series,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = -1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n\n    print(\"Profiling backtesting refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                       forecaster    = forecaster,\n                                       series        = multi_series,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = 1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit parallel\")\n    start = time.time()\n    cv = TimeSeriesFold(\n             steps              = steps,\n             initial_train_size = int(len(y) * 0.9),\n             refit              = False,\n         )\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                       forecaster    = forecaster,\n                                       series        = multi_series,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = -1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling backtesting no refit no parallel\")\n    start = time.time()\n    metric, backtest_predictions = backtesting_forecaster_multiseries(\n                                       forecaster    = forecaster,\n                                       series        = multi_series,\n                                       exog          = exog,\n                                       cv            = cv,\n                                       metric        = 'mean_squared_error',\n                                       interval      = None,\n                                       n_boot        = 500,\n                                       random_state  = 123,\n                                       verbose       = False,\n                                       show_progress = False,\n                                       n_jobs        = 1\n                                   )\n    end = time.time()\n    elapsed_times.append(end - start)    \n    \n    print(\"Profiling GridSearch no refit parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                       forecaster    = forecaster,\n                       series        = multi_series,\n                       exog          = exog,\n                       cv            = cv,\n                       param_grid    = param_grid,\n                       lags_grid     = lags_grid,\n                       metric        = 'mean_squared_error',\n                       return_best   = False,\n                       verbose       = False,\n                       show_progress = False,\n                       n_jobs        = -1\n                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n    \n    print(\"Profiling GridSearch no refit no parallel\")\n    start = time.time()\n    results_grid = grid_search_forecaster_multiseries(\n                       forecaster    = forecaster,\n                       series        = multi_series,\n                       exog          = exog,\n                       cv            = cv,\n                       param_grid    = param_grid,\n                       lags_grid     = lags_grid,\n                       metric        = 'mean_squared_error',\n                       return_best   = False,\n                       verbose       = False,\n                       show_progress = False,\n                       n_jobs        = 1\n                   )\n    end = time.time()\n    elapsed_times.append(end - start)\n\nmethods = [\n    \"fit\",\n    \"create_train_X_y\",\n    \"backtest_refit_parallel\",\n    \"backtest_refit_noparallel\",\n    \"backtest_no_refit_parallel\",\n    \"backtest_no_refit_noparallel\",\n    \"gridSearch_no_refit_parallel\",\n    \"gridSearch_no_refit_noparallel\"\n]\n\nresults = pd.DataFrame({\n    \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),\n    \"method\": np.tile(methods, len(regressors)),\n    \"elapsed_time\": elapsed_times\n})\nresults[\"regressor\"] = results[\"regressor\"].str.replace(\"\\n              \", \" \")\nresults['parallel'] = results.method.str.contains(\"_parallel\")\nresults['method'] = results.method.str.replace(\"_parallel\", \"\")\nresults['method'] = results.method.str.replace(\"_noparallel\", \"\")\nresults = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])\n\nresults_pivot = results.pivot_table(index=[\"regressor\", \"method\"], columns=\"parallel\", values=\"elapsed_time\").reset_index()\nresults_pivot.columns.name = None\nresults_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100\ndisplay(results_pivot)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nbars = sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax)\nfor container in bars.containers:\n    ax.bar_label(container, fmt='%.1f', padding=3, fontsize=8)\nax.set_title(\"Parallel vs Sequential (ForecasterDirectMultiVariate)\")\nax.set_ylabel(\"Percent improvement\")\nax.set_xlabel(\"Method\")\nax.legend(fontsize=8, loc='lower left', bbox_to_anchor=(0, -0.31), ncols=1);\n</pre> print(\"----------------------------\") print(\"ForecasterDirectMultiVariate\") print(\"----------------------------\") steps = 5 lags = 10 regressors = [     Ridge(random_state=77, alpha=0.1),     LGBMRegressor(random_state=77, n_jobs=1, n_estimators=50, max_depth=5, verbose=-1),     LGBMRegressor(random_state=77, n_jobs=-1, n_estimators=50, max_depth=5, verbose=-1),     HistGradientBoostingRegressor(random_state=77, max_iter=50, max_depth=5,), ] param_grids = [     {'alpha': [0.1, 0.1, 0.1]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'n_estimators': [50, 50], 'max_depth': [5, 5]},     {'max_iter': [50, 50], 'max_depth': [5, 5]} ] lags_grid = [50, 50, 50] elapsed_times = []  for regressor, param_grid in zip(regressors, param_grids):     print(\"\")     print(regressor, param_grid)     print(\"\")     forecaster = ForecasterDirectMultiVariate(                      regressor        = regressor,                      lags             = lags,                      steps            = steps,                      level            = \"series_1\",                      transformer_exog = StandardScaler()                  )          print(\"Profiling fit\")     start = time.time()     forecaster.fit(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling create_train_X_y\")     start = time.time()     _ = forecaster.create_train_X_y(series=multi_series, exog=exog)     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit parallel\")     start = time.time()     cv = TimeSeriesFold(              steps              = steps,              initial_train_size = int(len(y) * 0.9),              refit              = True,              fixed_train_size   = False,          )     metric, backtest_predictions = backtesting_forecaster_multiseries(                                        forecaster    = forecaster,                                        series        = multi_series,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = -1                                    )     end = time.time()     elapsed_times.append(end - start)      print(\"Profiling backtesting refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                        forecaster    = forecaster,                                        series        = multi_series,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = 1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit parallel\")     start = time.time()     cv = TimeSeriesFold(              steps              = steps,              initial_train_size = int(len(y) * 0.9),              refit              = False,          )     metric, backtest_predictions = backtesting_forecaster_multiseries(                                        forecaster    = forecaster,                                        series        = multi_series,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = -1                                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling backtesting no refit no parallel\")     start = time.time()     metric, backtest_predictions = backtesting_forecaster_multiseries(                                        forecaster    = forecaster,                                        series        = multi_series,                                        exog          = exog,                                        cv            = cv,                                        metric        = 'mean_squared_error',                                        interval      = None,                                        n_boot        = 500,                                        random_state  = 123,                                        verbose       = False,                                        show_progress = False,                                        n_jobs        = 1                                    )     end = time.time()     elapsed_times.append(end - start)              print(\"Profiling GridSearch no refit parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                        forecaster    = forecaster,                        series        = multi_series,                        exog          = exog,                        cv            = cv,                        param_grid    = param_grid,                        lags_grid     = lags_grid,                        metric        = 'mean_squared_error',                        return_best   = False,                        verbose       = False,                        show_progress = False,                        n_jobs        = -1                    )     end = time.time()     elapsed_times.append(end - start)          print(\"Profiling GridSearch no refit no parallel\")     start = time.time()     results_grid = grid_search_forecaster_multiseries(                        forecaster    = forecaster,                        series        = multi_series,                        exog          = exog,                        cv            = cv,                        param_grid    = param_grid,                        lags_grid     = lags_grid,                        metric        = 'mean_squared_error',                        return_best   = False,                        verbose       = False,                        show_progress = False,                        n_jobs        = 1                    )     end = time.time()     elapsed_times.append(end - start)  methods = [     \"fit\",     \"create_train_X_y\",     \"backtest_refit_parallel\",     \"backtest_refit_noparallel\",     \"backtest_no_refit_parallel\",     \"backtest_no_refit_noparallel\",     \"gridSearch_no_refit_parallel\",     \"gridSearch_no_refit_noparallel\" ]  results = pd.DataFrame({     \"regressor\": np.repeat(np.array([str(regressor) for regressor in regressors]), len(methods)),     \"method\": np.tile(methods, len(regressors)),     \"elapsed_time\": elapsed_times }) results[\"regressor\"] = results[\"regressor\"].str.replace(\"\\n              \", \" \") results['parallel'] = results.method.str.contains(\"_parallel\") results['method'] = results.method.str.replace(\"_parallel\", \"\") results['method'] = results.method.str.replace(\"_noparallel\", \"\") results = results.sort_values(by=[\"regressor\", \"method\", \"parallel\"])  results_pivot = results.pivot_table(index=[\"regressor\", \"method\"], columns=\"parallel\", values=\"elapsed_time\").reset_index() results_pivot.columns.name = None results_pivot[\"pct_improvement\"] = (results_pivot[False] - results_pivot[True]) / results_pivot[False] * 100 display(results_pivot)  fig, ax = plt.subplots(figsize=(10, 5)) bars = sns.barplot(data=results_pivot.dropna(), x=\"method\", y=\"pct_improvement\", hue=\"regressor\", ax=ax) for container in bars.containers:     ax.bar_label(container, fmt='%.1f', padding=3, fontsize=8) ax.set_title(\"Parallel vs Sequential (ForecasterDirectMultiVariate)\") ax.set_ylabel(\"Percent improvement\") ax.set_xlabel(\"Method\") ax.legend(fontsize=8, loc='lower left', bbox_to_anchor=(0, -0.31), ncols=1); <pre>----------------------------\nForecasterDirectMultiVariate\n----------------------------\n\nRidge(alpha=0.1, random_state=77) {'alpha': [0.1, 0.1, 0.1]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=1, random_state=77,\n              verbose=-1) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nLGBMRegressor(max_depth=5, n_estimators=50, n_jobs=-1, random_state=77,\n              verbose=-1) {'n_estimators': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n\nHistGradientBoostingRegressor(max_depth=5, max_iter=50, random_state=77) {'max_iter': [50, 50], 'max_depth': [5, 5]}\n\nProfiling fit\nProfiling create_train_X_y\nProfiling backtesting refit parallel\nProfiling backtesting refit no parallel\nProfiling backtesting no refit parallel\nProfiling backtesting no refit no parallel\nProfiling GridSearch no refit parallel\nProfiling GridSearch no refit no parallel\n</pre> regressor method False True pct_improvement 0 HistGradientBoostingRegressor(max_depth=5, max... backtest_no_refit 2.853674 9.233681 -223.571717 1 HistGradientBoostingRegressor(max_depth=5, max... backtest_refit 129.822874 92.496529 28.751748 2 HistGradientBoostingRegressor(max_depth=5, max... create_train_X_y 0.018000 NaN NaN 3 HistGradientBoostingRegressor(max_depth=5, max... fit 1.330104 NaN NaN 4 HistGradientBoostingRegressor(max_depth=5, max... gridSearch_no_refit 24.548890 77.863022 -217.175326 5 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 2.891752 3.798469 -31.355314 6 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 111.030921 86.056100 22.493573 7 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.029996 NaN NaN 8 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 0.717866 NaN NaN 9 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 23.100491 25.343611 -9.710274 10 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_no_refit 2.516266 3.437793 -36.622786 11 LGBMRegressor(max_depth=5, n_estimators=50, n_... backtest_refit 186.235264 65.961237 64.581768 12 LGBMRegressor(max_depth=5, n_estimators=50, n_... create_train_X_y 0.013000 NaN NaN 13 LGBMRegressor(max_depth=5, n_estimators=50, n_... fit 1.724441 NaN NaN 14 LGBMRegressor(max_depth=5, n_estimators=50, n_... gridSearch_no_refit 34.885274 39.199495 -12.366885 15 Ridge(alpha=0.1, random_state=77) backtest_no_refit 0.715881 0.862378 -20.463955 16 Ridge(alpha=0.1, random_state=77) backtest_refit 7.449928 3.115467 58.181243 17 Ridge(alpha=0.1, random_state=77) create_train_X_y 0.015999 NaN NaN 18 Ridge(alpha=0.1, random_state=77) fit 0.114108 NaN NaN 19 Ridge(alpha=0.1, random_state=77) gridSearch_no_refit 2.783162 3.158578 -13.488834"},{"location":"faq/parallelization-skforecast.html#parallelization-in-skforecast","title":"Parallelization in skforecast\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#parallelization-rules","title":"Parallelization rules\u00b6","text":"<p>The <code>n_jobs</code> argument facilitates the parallelization of specific functionalities to enhance speed within the skforecast library. Parallelization has been strategically integrated at two key levels: during the process of forecaster fitting and during the backtesting phase, which also encompasses hyperparameter search. When the <code>n_jobs</code> argument is set to its default value of <code>'auto'</code>, the library dynamically determines the number of jobs to employ, guided by the ensuing guidelines:</p> <p>Regressor</p> <p>If regressor is a <code>LGBMRegressor</code> with internal<code> n_jobs != 1</code>, then <code>n_jobs = 1</code> in forecasting fitting and backtesting. This is because <code>lightgbm</code> is highly optimized for gradient boosting and parallelizes operations at a very fine-grained level, making additional parallelization unnecessary and potentially harmful due to resource contention.</p> <p>Forecaster Fitting</p> <ul> <li><p>If the forecaster is either <code>ForecasterDirect</code> or <code>ForecasterDirectMultiVariate</code>, and the underlying regressor happens to be a linear regressor, then <code>n_jobs</code> is set to 1.</p> </li> <li><p>Otherwise, if none of the above conditions hold, the <code>n_jobs</code> value is determined as <code>cpu_count() - 1</code>, aligning with the number of available CPU cores.</p> </li> </ul> <p>Backtesting</p> <ul> <li><p>If <code>refit</code> is an integer, then <code>n_jobs = 1</code>. This is because parallelization doesn`t work with intermittent refit.</p> </li> <li><p>If forecaster is <code>ForecasterRecursive</code> and the underlying regressor is linear, <code>n_jobs</code> is set to 1.</p> </li> <li><p>If forecaster is <code>ForecasterRecursive</code>, the underlying regressor is not a linear regressor, then <code>n_jobs</code> is set to <code>cpu_count() - 1</code>.</p> </li> <li><p>If forecaster is <code>ForecasterDirect</code> or <code>ForecasterDirectMultiVariate</code> and <code>refit = True</code>, then <code>n_jobs</code> is set to <code>cpu_count() - 1</code>.</p> </li> <li><p>If forecaster is <code>ForecasterDirect</code> or <code>ForecasterDirectMultiVariate</code> and <code>refit = False</code>, then <code>n_jobs</code> is set to 1.</p> </li> <li><p>If forecaster is <code>ForecasterRecursiveMultiSeries</code>, then <code>n_jobs</code> is set to <code>cpu_count() - 1</code>.</p> </li> <li><p>If forecaster is <code>ForecasterSarimax</code> or <code>ForecasterEquivalentDate</code>, then <code>n_jobs = 1</code>.</p> </li> </ul>"},{"location":"faq/parallelization-skforecast.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterrecursive","title":"Benchmark ForecasterRecursive\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterdirect","title":"Benchmark ForecasterDirect\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterrecursivemultiseries","title":"Benchmark ForecasterRecursiveMultiSeries\u00b6","text":""},{"location":"faq/parallelization-skforecast.html#benchmark-forecasterdirectmultivariate","title":"Benchmark ForecasterDirectMultiVariate\u00b6","text":""},{"location":"faq/parameters-search-backtesting-vs-one-step-ahead.html","title":"Backtesting vs One-step-ahead","text":"<p>Hyperparameter and lag tuning involves systematically testing different values or combinations of hyperparameters (and/or lags) to find the optimal configuration that gives the best performance. The skforecast library provides two different methods to evaluate each candidate configuration:</p> <ul> <li><p>Backtesting: In this method, the model predicts several steps ahead in each iteration, using the same forecast horizon and retraining frequency strategy that would be used if the model were deployed. This simulates a real forecasting scenario where the model is retrained and updated over time. More information here.</p> </li> <li><p>One-Step Ahead: Evaluates the model using only one-step-ahead predictions. This method is faster because it requires fewer iterations, but it only tests the model's performance in the immediate next time step ($t+1$).</p> </li> </ul> <p>Each method uses a different evaluation strategy, so they may produce different results. However, in the long run, both methods are expected to converge to similar selections of optimal hyperparameters. The one-step-ahead method is much faster than backtesting because it requires fewer iterations, but it only tests the model's performance in the immediate next time step. It is recommended to backtest the final model for a more accurate multi-step performance estimate.</p> <p>The document compares the performance of these two methods when applied to various datasets and forecaster types. The process is outlined as follows:</p> <ul> <li><p>Optimal hyperparameters and lags are identified through a search using both backtesting and one-step-ahead evaluation methods. This search is performed on the validation partition, and the best configuration is stored along with the time taken to complete the search.</p> </li> <li><p>Finally, the selected best configuration is evaluated on the test partition using a backtesting procedure.</p> </li> </ul> <p>It is important to note that the final evaluation is consistently performed using backtesting to simulate a real-world multi-step forecasting scenario.</p> <p>The results show a significant reduction in the time required to find the optimal configuration using the one-step-ahead method (top panel). However, the performance of the selected configuration on the test partition is similar for both methods (lower panel), with no clear winner. These results are consistent for both grid search and Bayesian search approaches.</p> <p> \u270e Note </p> <p>The purpose of this analysis is to compare the time and forecasting performance of the two available evaluation methods, not to compare different forecasters.</p> In\u00a0[\u00a0]: Copied! <pre># Libraries\n# ==============================================================================\nimport platform\nimport psutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom copy import copy\nimport sklearn\nimport skforecast\nimport lightgbm\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.direct import ForecasterDirect\nfrom skforecast.model_selection import TimeSeriesFold, OneStepAheadFold\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.model_selection import bayesian_search_forecaster\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\nfrom skforecast.direct import ForecasterDirectMultiVariate\nfrom skforecast.model_selection import backtesting_forecaster_multiseries\nfrom skforecast.model_selection import grid_search_forecaster_multiseries\nfrom skforecast.model_selection import bayesian_search_forecaster_multiseries\nfrom skforecast.preprocessing import reshape_series_long_to_dict\nfrom skforecast.preprocessing import reshape_exog_long_to_dict\n\n# Warnings\n# ==============================================================================\nimport warnings\nfrom skforecast.exceptions import IgnoredArgumentWarning\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore', category=IgnoredArgumentWarning)\n</pre> # Libraries # ============================================================================== import platform import psutil import numpy as np import pandas as pd import matplotlib.pyplot as plt from time import time from copy import copy import sklearn import skforecast import lightgbm from lightgbm import LGBMRegressor from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from skforecast.datasets import fetch_dataset from skforecast.plot import set_dark_theme from skforecast.recursive import ForecasterRecursive from skforecast.direct import ForecasterDirect from skforecast.model_selection import TimeSeriesFold, OneStepAheadFold from skforecast.model_selection import grid_search_forecaster from skforecast.model_selection import bayesian_search_forecaster from skforecast.model_selection import backtesting_forecaster from skforecast.recursive import ForecasterRecursiveMultiSeries from skforecast.direct import ForecasterDirectMultiVariate from skforecast.model_selection import backtesting_forecaster_multiseries from skforecast.model_selection import grid_search_forecaster_multiseries from skforecast.model_selection import bayesian_search_forecaster_multiseries from skforecast.preprocessing import reshape_series_long_to_dict from skforecast.preprocessing import reshape_exog_long_to_dict  # Warnings # ============================================================================== import warnings from skforecast.exceptions import IgnoredArgumentWarning warnings.filterwarnings('ignore') warnings.simplefilter('ignore', category=IgnoredArgumentWarning) In\u00a0[2]: Copied! <pre># Versions\n# ==============================================================================\nprint(f\"Python version      : {platform.python_version()}\")\nprint(f\"scikit-learn version: {sklearn.__version__}\")\nprint(f\"skforecast version  : {skforecast.__version__}\")\nprint(f\"lightgbm version    : {lightgbm.__version__}\")\nprint(f\"pandas version      : {pd.__version__}\")\nprint(f\"numpy version       : {np.__version__}\")\nprint(\"\")\n\n# System information\n# ==============================================================================\nprint(f\"Machine type: {platform.machine()}\")\nprint(f\"Processor type: {platform.processor()}\")\nprint(f\"Platform type: {platform.platform()}\")\nprint(f\"Operating system: {platform.system()}\")\nprint(f\"Operating system release: {platform.release()}\")\nprint(f\"Operating system version: {platform.version()}\")\nprint(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\")\nprint(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\")\n</pre> # Versions # ============================================================================== print(f\"Python version      : {platform.python_version()}\") print(f\"scikit-learn version: {sklearn.__version__}\") print(f\"skforecast version  : {skforecast.__version__}\") print(f\"lightgbm version    : {lightgbm.__version__}\") print(f\"pandas version      : {pd.__version__}\") print(f\"numpy version       : {np.__version__}\") print(\"\")  # System information # ============================================================================== print(f\"Machine type: {platform.machine()}\") print(f\"Processor type: {platform.processor()}\") print(f\"Platform type: {platform.platform()}\") print(f\"Operating system: {platform.system()}\") print(f\"Operating system release: {platform.release()}\") print(f\"Operating system version: {platform.version()}\") print(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\") print(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\") <pre>Python version      : 3.12.9\nscikit-learn version: 1.5.2\nskforecast version  : 0.15.0\nlightgbm version    : 4.6.0\npandas version      : 2.2.3\nnumpy version       : 1.26.4\n\nMachine type: x86_64\nProcessor type: x86_64\nPlatform type: Linux-5.15.0-1077-aws-x86_64-with-glibc2.31\nOperating system: Linux\nOperating system release: 5.15.0-1077-aws\nOperating system version: #84~20.04.1-Ubuntu SMP Mon Jan 20 22:14:54 UTC 2025\nNumber of physical cores: 4\nNumber of logical cores: 8\n</pre> In\u00a0[\u00a0]: Copied! <pre># Import data\n# ==============================================================================\ndata_bike = fetch_dataset('bike_sharing_extended_features', verbose=False)\n\ndata_sales = fetch_dataset(name=\"items_sales\", verbose=False)\ndata_sales = data_sales * 100\ndata_sales['day_of_week'] = data_sales.index.dayofweek\n\ndata_website = fetch_dataset(name=\"website_visits\", raw=True, verbose=False)\ndata_website['date'] = pd.to_datetime(data_website['date'], format='%d/%m/%y')\ndata_website = data_website.set_index('date')\ndata_website = data_website.asfreq('1D')\ndata_website = data_website.sort_index()\ndata_website['month'] = data_website.index.month\ndata_website['month_day'] = data_website.index.day\ndata_website['week_day'] = data_website.index.day_of_week\ndata_website = pd.get_dummies(data_website, columns=['month', 'week_day', 'month_day'], dtype='int64')\n\ndata_electricity = fetch_dataset(name='vic_electricity', raw=False, verbose=False)\ndata_electricity = data_electricity.drop(columns=\"Date\")\ndata_electricity = (\n    data_electricity\n    .resample(rule=\"h\", closed=\"left\", label=\"right\")\n    .agg({\n        \"Demand\": \"mean\",\n        \"Temperature\": \"mean\",\n        \"Holiday\": \"mean\",\n    })\n)\ndata_electricity = data_electricity.loc['2012-01-01 00:00:00': '2013-12-30 23:00:00'].copy()\n\nseries_dict = pd.read_csv(\n    'https://raw.githubusercontent.com/skforecast/skforecast-datasets/main/data/demo_multi_series.csv'\n)\nexog_dict = pd.read_csv(\n    'https://raw.githubusercontent.com/skforecast/skforecast-datasets/main/data/demo_multi_series_exog.csv'\n)\nseries_dict['timestamp'] = pd.to_datetime(series_dict['timestamp'])\nexog_dict['timestamp'] = pd.to_datetime(exog_dict['timestamp'])\n\nseries_dict = reshape_series_long_to_dict(\n    data      = series_dict,\n    series_id = 'series_id',\n    index     = 'timestamp',\n    values    = 'value',\n    freq      = 'D'\n)\nexog_dict = reshape_exog_long_to_dict(\n    data      = exog_dict,\n    series_id = 'series_id',\n    index     = 'timestamp',\n    freq      = 'D'\n)\n</pre> # Import data # ============================================================================== data_bike = fetch_dataset('bike_sharing_extended_features', verbose=False)  data_sales = fetch_dataset(name=\"items_sales\", verbose=False) data_sales = data_sales * 100 data_sales['day_of_week'] = data_sales.index.dayofweek  data_website = fetch_dataset(name=\"website_visits\", raw=True, verbose=False) data_website['date'] = pd.to_datetime(data_website['date'], format='%d/%m/%y') data_website = data_website.set_index('date') data_website = data_website.asfreq('1D') data_website = data_website.sort_index() data_website['month'] = data_website.index.month data_website['month_day'] = data_website.index.day data_website['week_day'] = data_website.index.day_of_week data_website = pd.get_dummies(data_website, columns=['month', 'week_day', 'month_day'], dtype='int64')  data_electricity = fetch_dataset(name='vic_electricity', raw=False, verbose=False) data_electricity = data_electricity.drop(columns=\"Date\") data_electricity = (     data_electricity     .resample(rule=\"h\", closed=\"left\", label=\"right\")     .agg({         \"Demand\": \"mean\",         \"Temperature\": \"mean\",         \"Holiday\": \"mean\",     }) ) data_electricity = data_electricity.loc['2012-01-01 00:00:00': '2013-12-30 23:00:00'].copy()  series_dict = pd.read_csv(     'https://raw.githubusercontent.com/skforecast/skforecast-datasets/main/data/demo_multi_series.csv' ) exog_dict = pd.read_csv(     'https://raw.githubusercontent.com/skforecast/skforecast-datasets/main/data/demo_multi_series_exog.csv' ) series_dict['timestamp'] = pd.to_datetime(series_dict['timestamp']) exog_dict['timestamp'] = pd.to_datetime(exog_dict['timestamp'])  series_dict = reshape_series_long_to_dict(     data      = series_dict,     series_id = 'series_id',     index     = 'timestamp',     values    = 'value',     freq      = 'D' ) exog_dict = reshape_exog_long_to_dict(     data      = exog_dict,     series_id = 'series_id',     index     = 'timestamp',     freq      = 'D' ) In\u00a0[4]: Copied! <pre># Functions to compare results using backtesting and one step ahead\n# ==============================================================================\ndef run_benchmark(\n    data,\n    forecaster_to_benchmark,\n    search_method = None,\n    lags_grid = None,\n    param_grid = None,\n    search_space = None,\n    end_train = None,\n    end_validation = None,\n    target = None,\n    exog_features = None,\n    steps = None,\n    metric = None\n):\n    \"\"\"\n    Compare results of grid search and bayesian search using backtesting and one-step-ahead.\n    \"\"\"\n    \n    # backtesting\n    forecaster = copy(forecaster_to_benchmark)\n    start  = time()\n    cv = TimeSeriesFold(\n            initial_train_size = len(data.loc[:end_train]),\n            steps              = steps,\n            refit              = False,\n         )\n    if search_method == 'grid_search':\n        results_1 = grid_search_forecaster(\n                        forecaster    = forecaster,\n                        y             = data.loc[:end_validation, target],\n                        exog          = data.loc[:end_validation, exog_features] if exog_features else None,\n                        cv            = cv,\n                        param_grid    = param_grid,\n                        lags_grid     = lags_grid,\n                        metric        = metric,\n                        return_best   = False,\n                        n_jobs        = 'auto',\n                        verbose       = False,\n                        show_progress = False\n                    )\n    else:\n        results_1, _ = bayesian_search_forecaster(\n                           forecaster    = forecaster,\n                           y             = data.loc[:end_validation, target],\n                           exog          = data.loc[:end_validation, exog_features] if exog_features else None,\n                           cv            = cv,\n                           search_space  = search_space,\n                           metric        = metric,\n                           n_trials      = 15,\n                           random_state  = 123,\n                           return_best   = False,\n                           n_jobs        = 'auto',\n                           verbose       = False,\n                           show_progress = False\n                       )\n\n    end = time()\n    time_1 = end - start\n    best_params = results_1.loc[0, 'params']\n    best_lags = results_1.loc[0, 'lags']\n    forecaster.set_params(best_params)\n    forecaster.set_lags(lags=best_lags)\n    cv = TimeSeriesFold(\n            initial_train_size = len(data.loc[:end_validation]),\n            steps              = steps,\n            refit              = False,\n         )\n    metric_1, _ = backtesting_forecaster(\n                      forecaster    = forecaster,\n                      y             = data.loc[:, target],\n                      exog          = data.loc[:, exog_features] if exog_features else None,\n                      cv            = cv,\n                      metric        = metric,\n                      verbose       = False,\n                      show_progress = False\n                  )\n\n    # One step ahead\n    forecaster = copy(forecaster_to_benchmark)\n    start  = time()\n    cv = OneStepAheadFold(initial_train_size = len(data.loc[:end_train]))\n    if search_method == 'grid_search':\n        results_2 = grid_search_forecaster(\n                        forecaster    = forecaster,\n                        y             = data.loc[:end_validation, target],\n                        exog          = data.loc[:end_validation, exog_features] if exog_features else None,\n                        cv            = cv,\n                        param_grid    = param_grid,\n                        lags_grid     = lags_grid,\n                        metric        = metric,\n                        return_best   = False,\n                        verbose       = False,\n                        show_progress = False\n                    )\n    else:\n        results_2, _ = bayesian_search_forecaster(\n                           forecaster    = forecaster,\n                           y             = data.loc[:end_validation, target],\n                           exog          = data.loc[:end_validation, exog_features] if exog_features else None,\n                           cv            = cv,\n                           search_space  = search_space,\n                           metric        = metric,\n                           n_trials      = 15,\n                           random_state  = 123,\n                           return_best   = False,\n                           verbose       = False,\n                           show_progress = False\n                       )\n\n    end = time()\n    time_2 = end - start\n    best_params = results_2.loc[0, 'params']\n    best_lags = results_2.loc[0, 'lags']\n    forecaster.set_params(best_params)\n    forecaster.set_lags(lags=best_lags)\n    cv = TimeSeriesFold(\n            initial_train_size = len(data.loc[:end_validation]),\n            steps              = steps,\n            refit              = False,\n         )\n    metric_2, _ = backtesting_forecaster(\n                      forecaster    = forecaster,\n                      y             = data.loc[:, target],\n                      exog          = data.loc[:, exog_features] if exog_features else None,\n                      cv            = cv,\n                      metric        = metric,\n                      verbose       = False,\n                      show_progress = False\n                  )\n\n    print(\"-----------------\")\n    print(\"Benchmark results\")\n    print(\"-----------------\")\n    print('Execution time backtesting   :', time_1)\n    print('Execution time one step ahead:', time_2)\n    print(f\"Same lags   : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")\n    print(f\"Same params : {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")\n    print(\"\")\n    print(\"Method: backtesting\")\n    print(f\"    lags   : {results_1.loc[0, 'lags']}\")\n    print(f\"    params : {results_1.loc[0, 'params']}\")\n    print(f\"    {metric}: {metric_1.loc[0, metric]}\")\n    print(\"\")\n    print(\"Method: one step ahead\")\n    print(f\"    lags   : {results_2.loc[0, 'lags']}\")\n    print(f\"    params : {results_2.loc[0, 'params']}\")\n    print(f\"    {metric}: {metric_2.loc[0, metric]}\")\n    \n    return time_1, time_2, metric_1.loc[0, metric], metric_2.loc[0, metric]\n\n\n# Functions to compare results using backtesting and one step ahead\n# ==============================================================================\ndef run_benchmark_multiseries(\n    data = None,\n    forecaster_to_benchmark = None,\n    search_method = None,\n    lags_grid = None,\n    param_grid = None,\n    search_space = None,\n    end_train = None,\n    end_validation = None,\n    levels = None,\n    exog_features = None,\n    steps = None,\n    metric = None\n):\n    \"\"\"\n    Compare results of grid search using backtesting and one-step-ahead.\n    \"\"\"\n    \n    # Backtesting\n    forecaster = copy(forecaster_to_benchmark)\n    start  = time()\n    cv = TimeSeriesFold(\n                initial_train_size = len(data.loc[:end_train]),\n                steps              = steps,\n                refit              = False,\n             )\n    if search_method == 'grid_search':\n        results_1 = grid_search_forecaster_multiseries(\n                        forecaster    = forecaster,\n                        series        = data.loc[:end_validation, levels],\n                        levels        = levels,\n                        exog          = data.loc[:end_validation, exog_features] if exog_features else None,\n                        cv            = cv,\n                        param_grid    = param_grid,\n                        lags_grid     = lags_grid,\n                        metric        = metric,\n                        return_best   = False,\n                        n_jobs        = 'auto',\n                        verbose       = False,\n                        show_progress = False\n                    )\n    else:\n        results_1, _ = bayesian_search_forecaster_multiseries(\n                           forecaster    = forecaster,\n                           series        = data.loc[:end_validation, levels],\n                           exog          = data.loc[:end_validation, exog_features] if exog_features else None,\n                           levels        = levels,\n                           search_space  = search_space,\n                           cv            = cv,\n                           metric        = metric,\n                           n_trials      = 15,\n                           random_state  = 123,\n                           return_best   = False,\n                           n_jobs        = 'auto',\n                           verbose       = False,\n                           show_progress = False\n                       )\n    end = time()\n    time_1 = end - start\n    best_params = results_1.loc[0, 'params']\n    best_lags = results_1.loc[0, 'lags']\n    forecaster.set_params(best_params)\n    forecaster.set_lags(lags=best_lags)\n    cv = TimeSeriesFold(\n            initial_train_size = len(data.loc[:end_validation]),\n            steps              = steps,\n            refit              = False,\n         )\n    metric_1, _ = backtesting_forecaster_multiseries(\n                      forecaster    = forecaster,\n                      series        = data.loc[:, levels],\n                      exog          = data.loc[:, exog_features] if exog_features else None,\n                      cv            = cv,\n                      levels        = levels,\n                      metric        = metric,\n                      verbose       = False,\n                      show_progress = False\n                  )\n\n    # One step ahead\n    forecaster = copy(forecaster_to_benchmark)\n    start  = time()\n    cv = OneStepAheadFold(initial_train_size = len(data.loc[:end_train]))\n    if search_method == 'grid_search':\n        results_2 = grid_search_forecaster_multiseries(\n                        forecaster    = forecaster,\n                        series        = data.loc[:end_validation, levels],\n                        exog          = data.loc[:end_validation, exog_features] if exog_features else None,\n                        cv            = cv,\n                        levels        = levels,\n                        param_grid    = param_grid,\n                        lags_grid     = lags_grid,\n                        metric        = metric,\n                        return_best   = False,\n                        verbose       = False,\n                        show_progress = False\n                    )\n    else:\n        results_2, _ = bayesian_search_forecaster_multiseries(\n                           forecaster    = forecaster,\n                           series        = data.loc[:end_validation, levels],\n                           exog          = data.loc[:end_validation, exog_features] if exog_features else None,\n                           cv            = cv,\n                           levels        = levels,\n                           search_space  = search_space,\n                           metric        = metric,\n                           n_trials      = 15,\n                           random_state  = 123,\n                           return_best   = False,\n                           verbose       = False,\n                           show_progress = False\n                       )\n\n    end = time()\n    time_2 = end - start\n    best_params = results_2.loc[0, 'params']\n    best_lags = results_2.loc[0, 'lags']\n    forecaster.set_params(best_params)\n    forecaster.set_lags(lags=best_lags)\n    cv = TimeSeriesFold(\n            initial_train_size = len(data.loc[:end_validation]),\n            steps              = steps,\n            refit              = False,\n         )\n    metric_2, _ = backtesting_forecaster_multiseries(\n                      forecaster    = forecaster,\n                      series        = data.loc[:, levels],\n                      exog          = data.loc[:, exog_features] if exog_features else None,\n                      cv            = cv,\n                      levels        = levels,\n                      metric        = metric,\n                      verbose       = False,\n                      show_progress = False\n                  )\n\n    print(\"Benchmark results\")\n    print(\"-----------------\")\n    print('Execution time backtesting   :', time_1)\n    print('Execution time one step ahead:', time_2)\n    print(f\"Same lags   : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")\n    print(f\"Same params : {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")\n    print(\"\")\n    print(\"Method: backtesting\")\n    print(f\"    lags   : {results_1.loc[0, 'lags']}\")\n    print(f\"    params : {results_1.loc[0, 'params']}\")\n    print(f\"    {metric_1.loc[0, metric]}\")\n    print(\"\")\n    print(\"Method: one step ahead\")\n    print(f\"    lags   : {results_2.loc[0, 'lags']}\")\n    print(f\"    params : {results_2.loc[0, 'params']}\")\n    print(f\"    {metric_2.loc[0, metric]}\")\n    \n    return time_1, time_2, metric_1.loc[0, metric], metric_2.loc[0, metric]\n\n\ndef summarize_results(results, metric, title, plot=True, save_plot=None, fig_size=(8, 4)):\n    \"\"\"\n    Summarize results of benchmark.\n    \"\"\"\n\n    results = pd.DataFrame(\n        results,\n        columns=[\n            \"dataset\",\n            \"forecaster\",\n            \"time_search_backtesting\",\n            \"time_search_one_step\",\n            \"metric_backtesting\",\n            \"metric_one_step\",\n        ]\n    )\n    results['ratio_speed'] = (\n        results['time_search_backtesting'] / results['time_search_one_step']\n    ).round(2)\n    results['ratio_metric'] = (\n        results['metric_backtesting'] / results['metric_one_step']\n    ).round(2)\n    results[\"dataset_forecaster\"] = (\n        results[\"dataset\"]\n        + \" \\n \"\n        + results[\"forecaster\"].str.replace(\"Forecaster\", \"\")\n    )\n    display(results)\n\n    if plot:\n        set_dark_theme()\n        fig, axs = plt.subplots(2, 1, figsize=fig_size, sharex=True)\n        results.plot.bar(\n            x='dataset_forecaster',\n            y=['time_search_backtesting', 'time_search_one_step'],\n            ax=axs[0],\n        )\n        axs[0].set_ylabel('time (s)')\n        axs[0].legend([\"backtesting\", \"one-step-ahead\"])\n        results.plot.bar(\n            x='dataset_forecaster',\n            y=['metric_backtesting', 'metric_one_step'],\n            ax=axs[1],\n            legend=False\n        )\n        axs[1].set_ylabel(f'{metric}')\n        axs[1].set_xlabel('')\n        plt.xticks(rotation=90)\n        plt.suptitle(title)\n        plt.tight_layout()\n\n        if save_plot:\n            plt.savefig(save_plot, dpi=300, bbox_inches='tight')\n</pre> # Functions to compare results using backtesting and one step ahead # ============================================================================== def run_benchmark(     data,     forecaster_to_benchmark,     search_method = None,     lags_grid = None,     param_grid = None,     search_space = None,     end_train = None,     end_validation = None,     target = None,     exog_features = None,     steps = None,     metric = None ):     \"\"\"     Compare results of grid search and bayesian search using backtesting and one-step-ahead.     \"\"\"          # backtesting     forecaster = copy(forecaster_to_benchmark)     start  = time()     cv = TimeSeriesFold(             initial_train_size = len(data.loc[:end_train]),             steps              = steps,             refit              = False,          )     if search_method == 'grid_search':         results_1 = grid_search_forecaster(                         forecaster    = forecaster,                         y             = data.loc[:end_validation, target],                         exog          = data.loc[:end_validation, exog_features] if exog_features else None,                         cv            = cv,                         param_grid    = param_grid,                         lags_grid     = lags_grid,                         metric        = metric,                         return_best   = False,                         n_jobs        = 'auto',                         verbose       = False,                         show_progress = False                     )     else:         results_1, _ = bayesian_search_forecaster(                            forecaster    = forecaster,                            y             = data.loc[:end_validation, target],                            exog          = data.loc[:end_validation, exog_features] if exog_features else None,                            cv            = cv,                            search_space  = search_space,                            metric        = metric,                            n_trials      = 15,                            random_state  = 123,                            return_best   = False,                            n_jobs        = 'auto',                            verbose       = False,                            show_progress = False                        )      end = time()     time_1 = end - start     best_params = results_1.loc[0, 'params']     best_lags = results_1.loc[0, 'lags']     forecaster.set_params(best_params)     forecaster.set_lags(lags=best_lags)     cv = TimeSeriesFold(             initial_train_size = len(data.loc[:end_validation]),             steps              = steps,             refit              = False,          )     metric_1, _ = backtesting_forecaster(                       forecaster    = forecaster,                       y             = data.loc[:, target],                       exog          = data.loc[:, exog_features] if exog_features else None,                       cv            = cv,                       metric        = metric,                       verbose       = False,                       show_progress = False                   )      # One step ahead     forecaster = copy(forecaster_to_benchmark)     start  = time()     cv = OneStepAheadFold(initial_train_size = len(data.loc[:end_train]))     if search_method == 'grid_search':         results_2 = grid_search_forecaster(                         forecaster    = forecaster,                         y             = data.loc[:end_validation, target],                         exog          = data.loc[:end_validation, exog_features] if exog_features else None,                         cv            = cv,                         param_grid    = param_grid,                         lags_grid     = lags_grid,                         metric        = metric,                         return_best   = False,                         verbose       = False,                         show_progress = False                     )     else:         results_2, _ = bayesian_search_forecaster(                            forecaster    = forecaster,                            y             = data.loc[:end_validation, target],                            exog          = data.loc[:end_validation, exog_features] if exog_features else None,                            cv            = cv,                            search_space  = search_space,                            metric        = metric,                            n_trials      = 15,                            random_state  = 123,                            return_best   = False,                            verbose       = False,                            show_progress = False                        )      end = time()     time_2 = end - start     best_params = results_2.loc[0, 'params']     best_lags = results_2.loc[0, 'lags']     forecaster.set_params(best_params)     forecaster.set_lags(lags=best_lags)     cv = TimeSeriesFold(             initial_train_size = len(data.loc[:end_validation]),             steps              = steps,             refit              = False,          )     metric_2, _ = backtesting_forecaster(                       forecaster    = forecaster,                       y             = data.loc[:, target],                       exog          = data.loc[:, exog_features] if exog_features else None,                       cv            = cv,                       metric        = metric,                       verbose       = False,                       show_progress = False                   )      print(\"-----------------\")     print(\"Benchmark results\")     print(\"-----------------\")     print('Execution time backtesting   :', time_1)     print('Execution time one step ahead:', time_2)     print(f\"Same lags   : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")     print(f\"Same params : {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")     print(\"\")     print(\"Method: backtesting\")     print(f\"    lags   : {results_1.loc[0, 'lags']}\")     print(f\"    params : {results_1.loc[0, 'params']}\")     print(f\"    {metric}: {metric_1.loc[0, metric]}\")     print(\"\")     print(\"Method: one step ahead\")     print(f\"    lags   : {results_2.loc[0, 'lags']}\")     print(f\"    params : {results_2.loc[0, 'params']}\")     print(f\"    {metric}: {metric_2.loc[0, metric]}\")          return time_1, time_2, metric_1.loc[0, metric], metric_2.loc[0, metric]   # Functions to compare results using backtesting and one step ahead # ============================================================================== def run_benchmark_multiseries(     data = None,     forecaster_to_benchmark = None,     search_method = None,     lags_grid = None,     param_grid = None,     search_space = None,     end_train = None,     end_validation = None,     levels = None,     exog_features = None,     steps = None,     metric = None ):     \"\"\"     Compare results of grid search using backtesting and one-step-ahead.     \"\"\"          # Backtesting     forecaster = copy(forecaster_to_benchmark)     start  = time()     cv = TimeSeriesFold(                 initial_train_size = len(data.loc[:end_train]),                 steps              = steps,                 refit              = False,              )     if search_method == 'grid_search':         results_1 = grid_search_forecaster_multiseries(                         forecaster    = forecaster,                         series        = data.loc[:end_validation, levels],                         levels        = levels,                         exog          = data.loc[:end_validation, exog_features] if exog_features else None,                         cv            = cv,                         param_grid    = param_grid,                         lags_grid     = lags_grid,                         metric        = metric,                         return_best   = False,                         n_jobs        = 'auto',                         verbose       = False,                         show_progress = False                     )     else:         results_1, _ = bayesian_search_forecaster_multiseries(                            forecaster    = forecaster,                            series        = data.loc[:end_validation, levels],                            exog          = data.loc[:end_validation, exog_features] if exog_features else None,                            levels        = levels,                            search_space  = search_space,                            cv            = cv,                            metric        = metric,                            n_trials      = 15,                            random_state  = 123,                            return_best   = False,                            n_jobs        = 'auto',                            verbose       = False,                            show_progress = False                        )     end = time()     time_1 = end - start     best_params = results_1.loc[0, 'params']     best_lags = results_1.loc[0, 'lags']     forecaster.set_params(best_params)     forecaster.set_lags(lags=best_lags)     cv = TimeSeriesFold(             initial_train_size = len(data.loc[:end_validation]),             steps              = steps,             refit              = False,          )     metric_1, _ = backtesting_forecaster_multiseries(                       forecaster    = forecaster,                       series        = data.loc[:, levels],                       exog          = data.loc[:, exog_features] if exog_features else None,                       cv            = cv,                       levels        = levels,                       metric        = metric,                       verbose       = False,                       show_progress = False                   )      # One step ahead     forecaster = copy(forecaster_to_benchmark)     start  = time()     cv = OneStepAheadFold(initial_train_size = len(data.loc[:end_train]))     if search_method == 'grid_search':         results_2 = grid_search_forecaster_multiseries(                         forecaster    = forecaster,                         series        = data.loc[:end_validation, levels],                         exog          = data.loc[:end_validation, exog_features] if exog_features else None,                         cv            = cv,                         levels        = levels,                         param_grid    = param_grid,                         lags_grid     = lags_grid,                         metric        = metric,                         return_best   = False,                         verbose       = False,                         show_progress = False                     )     else:         results_2, _ = bayesian_search_forecaster_multiseries(                            forecaster    = forecaster,                            series        = data.loc[:end_validation, levels],                            exog          = data.loc[:end_validation, exog_features] if exog_features else None,                            cv            = cv,                            levels        = levels,                            search_space  = search_space,                            metric        = metric,                            n_trials      = 15,                            random_state  = 123,                            return_best   = False,                            verbose       = False,                            show_progress = False                        )      end = time()     time_2 = end - start     best_params = results_2.loc[0, 'params']     best_lags = results_2.loc[0, 'lags']     forecaster.set_params(best_params)     forecaster.set_lags(lags=best_lags)     cv = TimeSeriesFold(             initial_train_size = len(data.loc[:end_validation]),             steps              = steps,             refit              = False,          )     metric_2, _ = backtesting_forecaster_multiseries(                       forecaster    = forecaster,                       series        = data.loc[:, levels],                       exog          = data.loc[:, exog_features] if exog_features else None,                       cv            = cv,                       levels        = levels,                       metric        = metric,                       verbose       = False,                       show_progress = False                   )      print(\"Benchmark results\")     print(\"-----------------\")     print('Execution time backtesting   :', time_1)     print('Execution time one step ahead:', time_2)     print(f\"Same lags   : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")     print(f\"Same params : {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")     print(\"\")     print(\"Method: backtesting\")     print(f\"    lags   : {results_1.loc[0, 'lags']}\")     print(f\"    params : {results_1.loc[0, 'params']}\")     print(f\"    {metric_1.loc[0, metric]}\")     print(\"\")     print(\"Method: one step ahead\")     print(f\"    lags   : {results_2.loc[0, 'lags']}\")     print(f\"    params : {results_2.loc[0, 'params']}\")     print(f\"    {metric_2.loc[0, metric]}\")          return time_1, time_2, metric_1.loc[0, metric], metric_2.loc[0, metric]   def summarize_results(results, metric, title, plot=True, save_plot=None, fig_size=(8, 4)):     \"\"\"     Summarize results of benchmark.     \"\"\"      results = pd.DataFrame(         results,         columns=[             \"dataset\",             \"forecaster\",             \"time_search_backtesting\",             \"time_search_one_step\",             \"metric_backtesting\",             \"metric_one_step\",         ]     )     results['ratio_speed'] = (         results['time_search_backtesting'] / results['time_search_one_step']     ).round(2)     results['ratio_metric'] = (         results['metric_backtesting'] / results['metric_one_step']     ).round(2)     results[\"dataset_forecaster\"] = (         results[\"dataset\"]         + \" \\n \"         + results[\"forecaster\"].str.replace(\"Forecaster\", \"\")     )     display(results)      if plot:         set_dark_theme()         fig, axs = plt.subplots(2, 1, figsize=fig_size, sharex=True)         results.plot.bar(             x='dataset_forecaster',             y=['time_search_backtesting', 'time_search_one_step'],             ax=axs[0],         )         axs[0].set_ylabel('time (s)')         axs[0].legend([\"backtesting\", \"one-step-ahead\"])         results.plot.bar(             x='dataset_forecaster',             y=['metric_backtesting', 'metric_one_step'],             ax=axs[1],             legend=False         )         axs[1].set_ylabel(f'{metric}')         axs[1].set_xlabel('')         plt.xticks(rotation=90)         plt.suptitle(title)         plt.tight_layout()          if save_plot:             plt.savefig(save_plot, dpi=300, bbox_inches='tight') In\u00a0[5]: Copied! <pre># Results\n# ==============================================================================\nresults_grid_search = []\nmetric = 'mean_absolute_error'\n</pre> # Results # ============================================================================== results_grid_search = [] metric = 'mean_absolute_error' In\u00a0[6]: Copied! <pre># Dataset bike_sharing_extended_features - ForecasterRecursive\n# ==============================================================================\nend_train = '2012-03-31 23:59:00'\nend_validation = '2012-08-31 23:59:00'\nexog_features = [\n    'month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin',\n    'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',\n    'sunset_hour_sin', 'sunset_hour_cos', 'holiday_previous_day', 'holiday_next_day',\n    'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day',\n    'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day',\n    'temp', 'holiday'\n]\n\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10\n             )\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5],\n    'learning_rate': [0.01, 0.1]\n}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_bike,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\n\nresults_grid_search.append([\n    'bike',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset bike_sharing_extended_features - ForecasterRecursive # ============================================================================== end_train = '2012-03-31 23:59:00' end_validation = '2012-08-31 23:59:00' exog_features = [     'month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin',     'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',     'sunset_hour_sin', 'sunset_hour_cos', 'holiday_previous_day', 'holiday_next_day',     'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day',     'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day',     'temp', 'holiday' ]  forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10              ) lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  param_grid = {     'n_estimators': [100, 200],     'max_depth': [3, 5],     'learning_rate': [0.01, 0.1] }  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_bike,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric )  results_grid_search.append([     'bike',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 87.1414406299591\nExecution time one step ahead: 7.115885972976685\nSame lags   : False\nSame params : True\n\nMethod: backtesting\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n    mean_absolute_error: 58.276762590192014\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n    mean_absolute_error: 64.04254202108999\n</pre> In\u00a0[7]: Copied! <pre># Dataset bike_sharing_extended_features - ForecasterDirect\n# ==============================================================================\nforecaster = ForecasterDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 steps         = 24,\n                 lags          = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\nparam_grid = {'alpha': np.logspace(-3, 3, 20)}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_bike,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'bike',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset bike_sharing_extended_features - ForecasterDirect # ============================================================================== forecaster = ForecasterDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  steps         = 24,                  lags          = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  param_grid = {'alpha': np.logspace(-3, 3, 20)}  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_bike,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_grid_search.append([     'bike',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 67.9793062210083\nExecution time one step ahead: 1.2969775199890137\nSame lags   : False\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 112.88378916846884}\n    mean_absolute_error: 79.14111581771634\n\nMethod: one step ahead\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'alpha': 12.742749857031322}\n    mean_absolute_error: 111.95615163625295\n</pre> In\u00a0[8]: Copied! <pre># Dataset website_visits - ForecasterRecursive\n# ==============================================================================\nend_train = '2021-03-30 23:59:00'\nend_validation = '2021-06-30 23:59:00'\nexog_features = [col for col in data_website.columns if col.startswith(('month_', 'week_day_', 'month_day_'))]\n\nforecaster = ForecasterRecursive(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 lags          = 10\n             )\n\nlags_grid = [7, 14, 21, [7, 14, 21]]\n\nparam_grid = {'alpha': np.logspace(-3, 3, 20)}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_website,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 7,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'website',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset website_visits - ForecasterRecursive # ============================================================================== end_train = '2021-03-30 23:59:00' end_validation = '2021-06-30 23:59:00' exog_features = [col for col in data_website.columns if col.startswith(('month_', 'week_day_', 'month_day_'))]  forecaster = ForecasterRecursive(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  lags          = 10              )  lags_grid = [7, 14, 21, [7, 14, 21]]  param_grid = {'alpha': np.logspace(-3, 3, 20)}  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_website,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 7,     metric                  = metric ) results_grid_search.append([     'website',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 5.345601797103882\nExecution time one step ahead: 0.4459536075592041\nSame lags   : True\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 6.158482110660261}\n    mean_absolute_error: 162.11396980738706\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 2.976351441631316}\n    mean_absolute_error: 162.3516346601722\n</pre> In\u00a0[9]: Copied! <pre># Dataset website_visits - ForecasterDirect\n# ==============================================================================\nforecaster = ForecasterDirect(\n                 regressor = Ridge(random_state=123),\n                 steps     = 24,\n                 lags      = 10\n             )\n\nlags_grid = [7, 14, 21, [7, 14, 21]]\n\nparam_grid = {'alpha': np.logspace(-3, 3, 20)}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_website,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'website',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset website_visits - ForecasterDirect # ============================================================================== forecaster = ForecasterDirect(                  regressor = Ridge(random_state=123),                  steps     = 24,                  lags      = 10              )  lags_grid = [7, 14, 21, [7, 14, 21]]  param_grid = {'alpha': np.logspace(-3, 3, 20)}  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_website,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_grid_search.append([     'website',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 8.905436277389526\nExecution time one step ahead: 0.7390425205230713\nSame lags   : True\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 6.158482110660261}\n    mean_absolute_error: 277.8362513175169\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 1.438449888287663}\n    mean_absolute_error: 236.28560218972729\n</pre> In\u00a0[10]: Copied! <pre># Dataset vic_electricity - ForecasterRecursive\n# ==============================================================================\nend_train = '2013-06-30 23:59:00'\nend_validation = '2013-11-30 23:59:00'\nexog_features = ['Temperature', 'Holiday']\n\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5],\n    'learning_rate': [0.01, 0.1]\n}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_electricity,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'Demand',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'electricity',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset vic_electricity - ForecasterRecursive # ============================================================================== end_train = '2013-06-30 23:59:00' end_validation = '2013-11-30 23:59:00' exog_features = ['Temperature', 'Holiday']  forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  param_grid = {     'n_estimators': [100, 200],     'max_depth': [3, 5],     'learning_rate': [0.01, 0.1] }  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_electricity,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'Demand',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_grid_search.append([     'electricity',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 83.48313307762146\nExecution time one step ahead: 6.249357223510742\nSame lags   : False\nSame params : True\n\nMethod: backtesting\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n    mean_absolute_error: 194.83553235066182\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n    mean_absolute_error: 188.8782299908785\n</pre> In\u00a0[11]: Copied! <pre># Dataset vic_electricity - ForecasterDirect\n# ==============================================================================\nforecaster = ForecasterDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 steps         = 24,\n                 lags          = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\nparam_grid = {'alpha': np.logspace(-3, 3, 20)}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_electricity,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'Demand',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'electricity',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset vic_electricity - ForecasterDirect # ============================================================================== forecaster = ForecasterDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  steps         = 24,                  lags          = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]  param_grid = {'alpha': np.logspace(-3, 3, 20)}  time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_electricity,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'Demand',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric ) results_grid_search.append([     'electricity',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 62.373592138290405\nExecution time one step ahead: 1.0504238605499268\nSame lags   : True\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 6.158482110660261}\n    mean_absolute_error: 304.22332781257694\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 1.438449888287663}\n    mean_absolute_error: 301.7070971763055\n</pre> In\u00a0[12]: Copied! <pre># Dataset sales - ForecasterRecursiveMultiSeries\n# ==============================================================================\nend_train = '2014-05-15 23:59:00'\nend_validation = '2014-07-15 23:59:00'\nlevels = ['item_1', 'item_2', 'item_3']\nexog_features = ['day_of_week']\n\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = \"ordinal\",\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 differentiation    = None,\n                 dropna_from_series = False,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nlags_grid = {\n    '24 lags': 24,\n    '48 lags': 48\n}\n\nparam_grid = {\n    'n_estimators': [50, 200],\n    'max_depth': [3, 7]\n}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(\n    data                    = data_sales,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    levels                  = levels,\n    exog_features           = exog_features,\n    steps                   = 36,\n    metric                  = metric\n)\nresults_grid_search.append([\n    'sales',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset sales - ForecasterRecursiveMultiSeries # ============================================================================== end_train = '2014-05-15 23:59:00' end_validation = '2014-07-15 23:59:00' levels = ['item_1', 'item_2', 'item_3'] exog_features = ['day_of_week']  forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = \"ordinal\",                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  differentiation    = None,                  dropna_from_series = False,                  fit_kwargs         = None,                  forecaster_id      = None              )  lags_grid = {     '24 lags': 24,     '48 lags': 48 }  param_grid = {     'n_estimators': [50, 200],     'max_depth': [3, 7] }  time_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(     data                    = data_sales,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     levels                  = levels,     exog_features           = exog_features,     steps                   = 36,     metric                  = metric ) results_grid_search.append([     'sales',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>Benchmark results\n-----------------\nExecution time backtesting   : 1.6175434589385986\nExecution time one step ahead: 0.9103469848632812\nSame lags   : False\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n    params : {'max_depth': 7, 'n_estimators': 200}\n    137.16940500432474\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'max_depth': 3, 'n_estimators': 50}\n    134.76669158338447\n</pre> In\u00a0[13]: Copied! <pre># Dataset sales - ForecasterDirectMultiVariate\n# ==============================================================================\nforecaster = ForecasterDirectMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 steps              = 5,\n                 level              = 'item_1',\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nlags_grid = {\n    '24 lags': 24,\n    '48 lags': 48\n}\n\nparam_grid = {\n    'n_estimators': [50, 200],\n    'max_depth': [3, 7]\n}\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(\n    data                    = data_sales,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'grid_search',\n    lags_grid               = lags_grid,\n    param_grid              = param_grid,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    levels                  = levels,\n    exog_features           = exog_features,\n    steps                   = 5,\n    metric                  = metric\n)\n\nresults_grid_search.append([\n    'sales',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset sales - ForecasterDirectMultiVariate # ============================================================================== forecaster = ForecasterDirectMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  steps              = 5,                  level              = 'item_1',                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  fit_kwargs         = None,                  forecaster_id      = None              )  lags_grid = {     '24 lags': 24,     '48 lags': 48 }  param_grid = {     'n_estimators': [50, 200],     'max_depth': [3, 7] }  time_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(     data                    = data_sales,     forecaster_to_benchmark = forecaster,     search_method           = 'grid_search',     lags_grid               = lags_grid,     param_grid              = param_grid,     end_train               = end_train,     end_validation          = end_validation,     levels                  = levels,     exog_features           = exog_features,     steps                   = 5,     metric                  = metric )  results_grid_search.append([     'sales',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>Benchmark results\n-----------------\nExecution time backtesting   : 6.571660757064819\nExecution time one step ahead: 1.1719706058502197\nSame lags   : False\nSame params : True\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n    params : {'max_depth': 7, 'n_estimators': 50}\n    100.16441146410313\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'max_depth': 7, 'n_estimators': 50}\n    95.20010578089475\n</pre> In\u00a0[14]: Copied! <pre># Dataset series_dict - ForecasterRecursiveMultiSeries\n# ==============================================================================\nend_train = '2016-05-31 23:59:00'\nend_validation = '2016-07-31 23:59:00'\nlevels = ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004']\nseries_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()}\nexog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()}\nseries_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()}\nexog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}\n\nforecaster_to_benchmark = ForecasterRecursiveMultiSeries(\n                              regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                              lags               = 24,\n                              encoding           = \"ordinal\",\n                              transformer_series = None,\n                              transformer_exog   = None,\n                              weight_func        = None,\n                              series_weights     = None,\n                              differentiation    = None,\n                              dropna_from_series = False,\n                              fit_kwargs         = None,\n                              forecaster_id      = None\n                          )\nlags_grid = [7, 14]\nparam_grid = {\n    'n_estimators': [50, 200],\n    'max_depth': [3, 7]\n}\n\n# Backtesting\nforecaster = copy(forecaster_to_benchmark)\nstart  = time()\ncv = TimeSeriesFold(\n        initial_train_size = 100,\n        steps              = 24,\n        refit              = False\n     )\nresults_1 = grid_search_forecaster_multiseries(\n                forecaster        = forecaster,\n                series            = {k: v.loc[: end_validation,] for k, v in series_dict.items()},\n                exog              = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},\n                cv                = cv,\n                param_grid        = param_grid,\n                lags_grid         = lags_grid,\n                metric            = metric,\n                return_best       = False,\n                n_jobs            = 'auto',\n                verbose           = False,\n                show_progress     = False,\n                suppress_warnings = True\n            )\nend = time()\ntime_1 = end - start\nbest_params = results_1.loc[0, 'params']\nbest_lags = results_1.loc[0, 'lags']\nforecaster.set_params(best_params)\nforecaster.set_lags(lags=best_lags)\ncv = TimeSeriesFold(\n        initial_train_size = 213,\n        steps              = 24,\n        refit              = False\n     )\nmetric_1, pred_1 = backtesting_forecaster_multiseries(\n                       forecaster        = forecaster,\n                       series            = series_dict,\n                       exog              = exog_dict,\n                       cv                = cv,\n                       levels            = levels,\n                       metric            = metric,\n                       verbose           = False,\n                       show_progress     = False,\n                       suppress_warnings = True\n                   )\n\n# One step ahead\nforecaster = copy(forecaster_to_benchmark)\nstart  = time()\ncv = OneStepAheadFold(initial_train_size = 100)\nresults_2 = grid_search_forecaster_multiseries(\n                forecaster        = forecaster,\n                series            = {k: v.loc[: end_validation,] for k, v in series_dict.items()},\n                exog              = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},\n                cv                = cv,\n                levels            = levels,\n                param_grid        = param_grid,\n                lags_grid         = lags_grid,\n                metric            = metric,\n                return_best       = False,\n                verbose           = False,\n                show_progress     = False,\n                suppress_warnings = True\n            )\nend = time()\ntime_2 = end - start\nbest_params = results_2.loc[0, 'params']\nbest_lags = results_2.loc[0, 'lags']\nforecaster.set_params(best_params)\nforecaster.set_lags(lags=best_lags)\ncv = TimeSeriesFold(\n        initial_train_size = 213,\n        steps              = 24,\n        refit              = False\n     )\nmetric_2, pred_2 = backtesting_forecaster_multiseries(\n                       forecaster        = forecaster,\n                       series            = series_dict,\n                       exog              = exog_dict,\n                       cv                = cv,\n                       levels            = levels,\n                       metric            = metric,\n                       verbose           = False,\n                       show_progress     = False,\n                       suppress_warnings = True\n                   )\n\nprint(\"Benchmark results\")\nprint(\"-----------------\")\nprint('Execution time backtesting   :', time_1)\nprint('Execution time one step ahead:', time_2)\nprint(f\"Same lags   : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")\nprint(f\"Same params : {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")\nprint(\"\")\nprint(\"Method: backtesting\")\nprint(f\"    lags   : {results_1.loc[0, 'lags']}\")\nprint(f\"    params : {results_1.loc[0, 'params']}\")\nprint(f\"    {metric_1.loc[0, metric]}\")\nprint(\"\")\nprint(\"Method: one step ahead\")\nprint(f\"    lags   : {results_2.loc[0, 'lags']}\")\nprint(f\"    params : {results_2.loc[0, 'params']}\")\nprint(f\"    {metric_2.loc[0, metric]}\")\n\nresults_grid_search.append([\n    'series_dict',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1.loc[0, metric],\n    metric_2.loc[0, metric],\n])\n</pre> # Dataset series_dict - ForecasterRecursiveMultiSeries # ============================================================================== end_train = '2016-05-31 23:59:00' end_validation = '2016-07-31 23:59:00' levels = ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004'] series_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()} exog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()} series_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()} exog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}  forecaster_to_benchmark = ForecasterRecursiveMultiSeries(                               regressor          = LGBMRegressor(random_state=123, verbose=-1),                               lags               = 24,                               encoding           = \"ordinal\",                               transformer_series = None,                               transformer_exog   = None,                               weight_func        = None,                               series_weights     = None,                               differentiation    = None,                               dropna_from_series = False,                               fit_kwargs         = None,                               forecaster_id      = None                           ) lags_grid = [7, 14] param_grid = {     'n_estimators': [50, 200],     'max_depth': [3, 7] }  # Backtesting forecaster = copy(forecaster_to_benchmark) start  = time() cv = TimeSeriesFold(         initial_train_size = 100,         steps              = 24,         refit              = False      ) results_1 = grid_search_forecaster_multiseries(                 forecaster        = forecaster,                 series            = {k: v.loc[: end_validation,] for k, v in series_dict.items()},                 exog              = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},                 cv                = cv,                 param_grid        = param_grid,                 lags_grid         = lags_grid,                 metric            = metric,                 return_best       = False,                 n_jobs            = 'auto',                 verbose           = False,                 show_progress     = False,                 suppress_warnings = True             ) end = time() time_1 = end - start best_params = results_1.loc[0, 'params'] best_lags = results_1.loc[0, 'lags'] forecaster.set_params(best_params) forecaster.set_lags(lags=best_lags) cv = TimeSeriesFold(         initial_train_size = 213,         steps              = 24,         refit              = False      ) metric_1, pred_1 = backtesting_forecaster_multiseries(                        forecaster        = forecaster,                        series            = series_dict,                        exog              = exog_dict,                        cv                = cv,                        levels            = levels,                        metric            = metric,                        verbose           = False,                        show_progress     = False,                        suppress_warnings = True                    )  # One step ahead forecaster = copy(forecaster_to_benchmark) start  = time() cv = OneStepAheadFold(initial_train_size = 100) results_2 = grid_search_forecaster_multiseries(                 forecaster        = forecaster,                 series            = {k: v.loc[: end_validation,] for k, v in series_dict.items()},                 exog              = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},                 cv                = cv,                 levels            = levels,                 param_grid        = param_grid,                 lags_grid         = lags_grid,                 metric            = metric,                 return_best       = False,                 verbose           = False,                 show_progress     = False,                 suppress_warnings = True             ) end = time() time_2 = end - start best_params = results_2.loc[0, 'params'] best_lags = results_2.loc[0, 'lags'] forecaster.set_params(best_params) forecaster.set_lags(lags=best_lags) cv = TimeSeriesFold(         initial_train_size = 213,         steps              = 24,         refit              = False      ) metric_2, pred_2 = backtesting_forecaster_multiseries(                        forecaster        = forecaster,                        series            = series_dict,                        exog              = exog_dict,                        cv                = cv,                        levels            = levels,                        metric            = metric,                        verbose           = False,                        show_progress     = False,                        suppress_warnings = True                    )  print(\"Benchmark results\") print(\"-----------------\") print('Execution time backtesting   :', time_1) print('Execution time one step ahead:', time_2) print(f\"Same lags   : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\") print(f\"Same params : {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\") print(\"\") print(\"Method: backtesting\") print(f\"    lags   : {results_1.loc[0, 'lags']}\") print(f\"    params : {results_1.loc[0, 'params']}\") print(f\"    {metric_1.loc[0, metric]}\") print(\"\") print(\"Method: one step ahead\") print(f\"    lags   : {results_2.loc[0, 'lags']}\") print(f\"    params : {results_2.loc[0, 'params']}\") print(f\"    {metric_2.loc[0, metric]}\")  results_grid_search.append([     'series_dict',     type(forecaster).__name__,     time_1,     time_2,     metric_1.loc[0, metric],     metric_2.loc[0, metric], ]) <pre>Benchmark results\n-----------------\nExecution time backtesting   : 1.8693697452545166\nExecution time one step ahead: 0.3610079288482666\nSame lags   : False\nSame params : False\n\nMethod: backtesting\n    lags   : [1 2 3 4 5 6 7]\n    params : {'max_depth': 3, 'n_estimators': 50}\n    180.46141171905165\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'max_depth': 7, 'n_estimators': 50}\n    164.23659500870002\n</pre> In\u00a0[15]: Copied! <pre># Results\n# ==============================================================================\nsummarize_results(\n    results   = results_grid_search,\n    metric    = metric,\n    plot      = True,\n    fig_size  = (8, 6),\n    title     = 'Grid search using backtesting vs one-step-ahead',\n    save_plot = \"../img/grid_search_benchmarck.png\"\n)\n</pre> # Results # ============================================================================== summarize_results(     results   = results_grid_search,     metric    = metric,     plot      = True,     fig_size  = (8, 6),     title     = 'Grid search using backtesting vs one-step-ahead',     save_plot = \"../img/grid_search_benchmarck.png\" ) dataset forecaster time_search_backtesting time_search_one_step metric_backtesting metric_one_step ratio_speed ratio_metric dataset_forecaster 0 bike ForecasterRecursive 87.141441 7.115886 58.276763 64.042542 12.25 0.91 bike \\n Recursive 1 bike ForecasterDirect 67.979306 1.296978 79.141116 111.956152 52.41 0.71 bike \\n Direct 2 website ForecasterRecursive 5.345602 0.445954 162.113970 162.351635 11.99 1.00 website \\n Recursive 3 website ForecasterDirect 8.905436 0.739043 277.836251 236.285602 12.05 1.18 website \\n Direct 4 electricity ForecasterRecursive 83.483133 6.249357 194.835532 188.878230 13.36 1.03 electricity \\n Recursive 5 electricity ForecasterDirect 62.373592 1.050424 304.223328 301.707097 59.38 1.01 electricity \\n Direct 6 sales ForecasterRecursiveMultiSeries 1.617543 0.910347 137.169405 134.766692 1.78 1.02 sales \\n RecursiveMultiSeries 7 sales ForecasterDirectMultiVariate 6.571661 1.171971 100.164411 95.200106 5.61 1.05 sales \\n DirectMultiVariate 8 series_dict ForecasterRecursiveMultiSeries 1.869370 0.361008 180.461412 164.236595 5.18 1.10 series_dict \\n RecursiveMultiSeries In\u00a0[16]: Copied! <pre># Table to store results\n# ==============================================================================\nresults_bayesian_search = []\nmetric = 'mean_absolute_error'\n</pre> # Table to store results # ============================================================================== results_bayesian_search = [] metric = 'mean_absolute_error' In\u00a0[17]: Copied! <pre># Dataset bike_sharing_extended_features - ForecasterRecursive\n# ==============================================================================\nend_train = '2012-03-31 23:59:00'\nend_validation = '2012-08-31 23:59:00'\nexog_features = [\n    'month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin',\n    'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',\n    'sunset_hour_sin', 'sunset_hour_cos', 'holiday_previous_day', 'holiday_next_day',\n    'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day',\n    'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day',\n    'temp', 'holiday'\n]\n\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators' : trial.suggest_int('n_estimators', 400, 1200, step=100),\n        'max_depth'    : trial.suggest_int('max_depth', 3, 10, step=1),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n        'gamma'        : trial.suggest_float('gamma', 0, 1),\n        'reg_alpha'    : trial.suggest_float('reg_alpha', 0, 1),\n        'reg_lambda'   : trial.suggest_float('reg_lambda', 0, 1),\n        'lags'         : trial.suggest_categorical('lags', lags_grid)\n    }\n\n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_bike,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'bike_sharing',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset bike_sharing_extended_features - ForecasterRecursive # ============================================================================== end_train = '2012-03-31 23:59:00' end_validation = '2012-08-31 23:59:00' exog_features = [     'month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin',     'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos',     'sunset_hour_sin', 'sunset_hour_cos', 'holiday_previous_day', 'holiday_next_day',     'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day',     'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day',     'temp', 'holiday' ]  forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]   def search_space(trial):     search_space  = {         'n_estimators' : trial.suggest_int('n_estimators', 400, 1200, step=100),         'max_depth'    : trial.suggest_int('max_depth', 3, 10, step=1),         'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),         'gamma'        : trial.suggest_float('gamma', 0, 1),         'reg_alpha'    : trial.suggest_float('reg_alpha', 0, 1),         'reg_lambda'   : trial.suggest_float('reg_lambda', 0, 1),         'lags'         : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_bike,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric )  results_bayesian_search.append([     'bike_sharing',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OneStepAheadValidationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 One-step-ahead predictions are used for faster model comparison, but they may not    \u2502\n\u2502 fully represent multi-step prediction performance. It is recommended to backtest the \u2502\n\u2502 final model for a more accurate multi-step performance estimate.                     \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : OneStepAheadValidationWarning                                             \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:675                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 87.15898752212524\nExecution time one step ahead: 27.95840358734131\nSame lags   : True\nSame params : True\n\nMethod: backtesting\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'n_estimators': 1200, 'max_depth': 10, 'learning_rate': 0.017833474222028703, 'gamma': 0.2285821738161964, 'reg_alpha': 0.2379772800670556, 'reg_lambda': 0.9887301767538853}\n    mean_absolute_error: 55.80577702511616\n\nMethod: one step ahead\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'n_estimators': 1200, 'max_depth': 10, 'learning_rate': 0.017833474222028703, 'gamma': 0.2285821738161964, 'reg_alpha': 0.2379772800670556, 'reg_lambda': 0.9887301767538853}\n    mean_absolute_error: 55.80577702511616\n</pre> In\u00a0[18]: Copied! <pre># Dataset bike_sharing_extended_features - ForecasterDirect\n# ==============================================================================\nforecaster = ForecasterDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 steps         = 24,\n                 lags          = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\n\ndef search_space(trial):\n    search_space  = {\n        'alpha': trial.suggest_float('alpha', 0.001, 1000, log=True),\n        'lags' : trial.suggest_categorical('lags', lags_grid)\n    }\n\n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_bike,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'bike_sharing',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset bike_sharing_extended_features - ForecasterDirect # ============================================================================== forecaster = ForecasterDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  steps         = 24,                  lags          = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]   def search_space(trial):     search_space  = {         'alpha': trial.suggest_float('alpha', 0.001, 1000, log=True),         'lags' : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_bike,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric )  results_bayesian_search.append([     'bike_sharing',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OneStepAheadValidationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 One-step-ahead predictions are used for faster model comparison, but they may not    \u2502\n\u2502 fully represent multi-step prediction performance. It is recommended to backtest the \u2502\n\u2502 final model for a more accurate multi-step performance estimate.                     \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : OneStepAheadValidationWarning                                             \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:675                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 17.606075286865234\nExecution time one step ahead: 0.9842092990875244\nSame lags   : False\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 121.0898788312409}\n    mean_absolute_error: 79.1498337214025\n\nMethod: one step ahead\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'alpha': 15.094374246471325}\n    mean_absolute_error: 111.96208734026862\n</pre> In\u00a0[19]: Copied! <pre># Dataset website_visits - ForecasterRecursive\n# ==============================================================================\nend_train = '2021-03-30 23:59:00'\nend_validation = '2021-06-30 23:59:00'\nexog_features = [col for col in data_website.columns if col.startswith(('month_', 'week_day_', 'month_day_'))]\n\nforecaster = ForecasterRecursive(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 lags          = 10\n             )\n\nlags_grid = [7, 14, 21, [7, 14, 21]]\n\n\ndef search_space(trial):\n    search_space  = {\n        'alpha': trial.suggest_float('alpha', 0.001, 1000, log=True),\n        'lags' : trial.suggest_categorical('lags', lags_grid)\n    } \n    \n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_website,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 7,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'website',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset website_visits - ForecasterRecursive # ============================================================================== end_train = '2021-03-30 23:59:00' end_validation = '2021-06-30 23:59:00' exog_features = [col for col in data_website.columns if col.startswith(('month_', 'week_day_', 'month_day_'))]  forecaster = ForecasterRecursive(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  lags          = 10              )  lags_grid = [7, 14, 21, [7, 14, 21]]   def search_space(trial):     search_space  = {         'alpha': trial.suggest_float('alpha', 0.001, 1000, log=True),         'lags' : trial.suggest_categorical('lags', lags_grid)     }           return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_website,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 7,     metric                  = metric )  results_bayesian_search.append([     'website',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OneStepAheadValidationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 One-step-ahead predictions are used for faster model comparison, but they may not    \u2502\n\u2502 fully represent multi-step prediction performance. It is recommended to backtest the \u2502\n\u2502 final model for a more accurate multi-step performance estimate.                     \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : OneStepAheadValidationWarning                                             \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:675                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 1.0920460224151611\nExecution time one step ahead: 0.26500821113586426\nSame lags   : False\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n    params : {'alpha': 0.07474245141964296}\n    mean_absolute_error: 136.76802274106467\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 0.03182234592129467}\n    mean_absolute_error: 173.5282998809151\n</pre> In\u00a0[20]: Copied! <pre># Dataset website_visits - ForecasterDirect\n# ==============================================================================\nforecaster = ForecasterDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 lags          = 10,\n                 steps         = 7\n             )\n\nlags_grid = [7, 14, 21, [7, 14, 21]]\n\n\ndef search_space(trial):\n    search_space  = {\n        'alpha': trial.suggest_float('alpha', 0.001, 1000, log=True),\n        'lags' : trial.suggest_categorical('lags', lags_grid)\n    } \n    \n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_website,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'users',\n    exog_features           = exog_features,\n    steps                   = 7,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'website',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset website_visits - ForecasterDirect # ============================================================================== forecaster = ForecasterDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  lags          = 10,                  steps         = 7              )  lags_grid = [7, 14, 21, [7, 14, 21]]   def search_space(trial):     search_space  = {         'alpha': trial.suggest_float('alpha', 0.001, 1000, log=True),         'lags' : trial.suggest_categorical('lags', lags_grid)     }           return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_website,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'users',     exog_features           = exog_features,     steps                   = 7,     metric                  = metric )  results_bayesian_search.append([     'website',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OneStepAheadValidationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 One-step-ahead predictions are used for faster model comparison, but they may not    \u2502\n\u2502 fully represent multi-step prediction performance. It is recommended to backtest the \u2502\n\u2502 final model for a more accurate multi-step performance estimate.                     \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : OneStepAheadValidationWarning                                             \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:675                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 1.5587255954742432\nExecution time one step ahead: 0.3095226287841797\nSame lags   : False\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n    params : {'alpha': 0.07474245141964296}\n    mean_absolute_error: 139.40123604696382\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'alpha': 0.03182234592129467}\n    mean_absolute_error: 153.6723680506666\n</pre> In\u00a0[21]: Copied! <pre># Dataset vic_electricity - ForecasterRecursive\n# ==============================================================================\nend_train = '2013-06-30 23:59:00'\nend_validation = '2013-11-30 23:59:00'\nexog_features = ['Temperature', 'Holiday']\n\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10\n             )\n\nlags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]\n\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators' : trial.suggest_int('n_estimators', 400, 1200, step=100),\n        'max_depth'    : trial.suggest_int('max_depth', 3, 10, step=1),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n        'gamma'        : trial.suggest_float('gamma', 0, 1),\n        'reg_alpha'    : trial.suggest_float('reg_alpha', 0, 1),\n        'reg_lambda'   : trial.suggest_float('reg_lambda', 0, 1),\n        'lags'         : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_electricity,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'Demand',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'electricity',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset vic_electricity - ForecasterRecursive # ============================================================================== end_train = '2013-06-30 23:59:00' end_validation = '2013-11-30 23:59:00' exog_features = ['Temperature', 'Holiday']  forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10              )  lags_grid = [48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169)]   def search_space(trial):     search_space  = {         'n_estimators' : trial.suggest_int('n_estimators', 400, 1200, step=100),         'max_depth'    : trial.suggest_int('max_depth', 3, 10, step=1),         'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),         'gamma'        : trial.suggest_float('gamma', 0, 1),         'reg_alpha'    : trial.suggest_float('reg_alpha', 0, 1),         'reg_lambda'   : trial.suggest_float('reg_lambda', 0, 1),         'lags'         : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_electricity,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'Demand',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric )  results_bayesian_search.append([     'electricity',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OneStepAheadValidationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 One-step-ahead predictions are used for faster model comparison, but they may not    \u2502\n\u2502 fully represent multi-step prediction performance. It is recommended to backtest the \u2502\n\u2502 final model for a more accurate multi-step performance estimate.                     \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : OneStepAheadValidationWarning                                             \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:675                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 90.56433486938477\nExecution time one step ahead: 24.597376823425293\nSame lags   : False\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'n_estimators': 1200, 'max_depth': 8, 'learning_rate': 0.020288327487155415, 'gamma': 0.9893221948178936, 'reg_alpha': 0.0026751307734329544, 'reg_lambda': 0.0033431281459104997}\n    mean_absolute_error: 196.74829952595292\n\nMethod: one step ahead\n    lags   : [  1   2   3  23  24  25 167 168 169]\n    params : {'n_estimators': 1200, 'max_depth': 10, 'learning_rate': 0.056896300053531614, 'gamma': 0.2725691628660212, 'reg_alpha': 0.24605588251006993, 'reg_lambda': 0.9687485406819449}\n    mean_absolute_error: 191.37491441780287\n</pre> In\u00a0[22]: Copied! <pre># Dataset vic_electricity - ForecasterDirect\n# ==============================================================================\nforecaster = ForecasterDirect(\n                 regressor     = Ridge(random_state=123),\n                 transformer_y = StandardScaler(),\n                 lags          = 10,\n                 steps         = 24\n             )\n\nlags_grid = (48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169))\n\n\ndef search_space(trial):\n    search_space  = {\n        'alpha': trial.suggest_float('alpha', 0.001, 1000, log=True),\n        'lags' : trial.suggest_categorical('lags', lags_grid)\n    }\n\n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark(\n    data                    = data_electricity,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    target                  = 'Demand',\n    exog_features           = exog_features,\n    steps                   = 24,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'electricity',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset vic_electricity - ForecasterDirect # ============================================================================== forecaster = ForecasterDirect(                  regressor     = Ridge(random_state=123),                  transformer_y = StandardScaler(),                  lags          = 10,                  steps         = 24              )  lags_grid = (48, 72, (1, 2, 3, 23, 24, 25, 167, 168, 169))   def search_space(trial):     search_space  = {         'alpha': trial.suggest_float('alpha', 0.001, 1000, log=True),         'lags' : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark(     data                    = data_electricity,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     target                  = 'Demand',     exog_features           = exog_features,     steps                   = 24,     metric                  = metric )  results_bayesian_search.append([     'electricity',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OneStepAheadValidationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 One-step-ahead predictions are used for faster model comparison, but they may not    \u2502\n\u2502 fully represent multi-step prediction performance. It is recommended to backtest the \u2502\n\u2502 final model for a more accurate multi-step performance estimate.                     \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : OneStepAheadValidationWarning                                             \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:675                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>-----------------\nBenchmark results\n-----------------\nExecution time backtesting   : 16.454455375671387\nExecution time one step ahead: 0.6429948806762695\nSame lags   : True\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 16.432489069228232}\n    mean_absolute_error: 307.13365278620506\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'alpha': 0.36149961722510493}\n    mean_absolute_error: 300.87028133154746\n</pre> In\u00a0[23]: Copied! <pre># Dataset sales - ForecasterRecursiveMultiSeries\n# ==============================================================================\nend_train = '2014-05-15 23:59:00'\nend_validation = '2014-07-15 23:59:00'\nlevels = ['item_1', 'item_2', 'item_3']\nexog_features = ['day_of_week']\n\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = \"ordinal\",\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 differentiation    = None,\n                 dropna_from_series = False,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nlags_grid = [48, 72]\n\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators' : trial.suggest_int('n_estimators', 50, 200),\n        'max_depth'    : trial.suggest_int('max_depth', 3, 10, step=1),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n        'lags'         : trial.suggest_categorical('lags', lags_grid)\n    } \n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(\n    data                    = data_sales,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    levels                  = levels,\n    exog_features           = exog_features,\n    steps                   = 36,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'sales',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset sales - ForecasterRecursiveMultiSeries # ============================================================================== end_train = '2014-05-15 23:59:00' end_validation = '2014-07-15 23:59:00' levels = ['item_1', 'item_2', 'item_3'] exog_features = ['day_of_week']  forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = \"ordinal\",                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  differentiation    = None,                  dropna_from_series = False,                  fit_kwargs         = None,                  forecaster_id      = None              )  lags_grid = [48, 72]   def search_space(trial):     search_space  = {         'n_estimators' : trial.suggest_int('n_estimators', 50, 200),         'max_depth'    : trial.suggest_int('max_depth', 3, 10, step=1),         'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),         'lags'         : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(     data                    = data_sales,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     levels                  = levels,     exog_features           = exog_features,     steps                   = 36,     metric                  = metric )  results_bayesian_search.append([     'sales',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OneStepAheadValidationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 One-step-ahead predictions are used for faster model comparison, but they may not    \u2502\n\u2502 fully represent multi-step prediction performance. It is recommended to backtest the \u2502\n\u2502 final model for a more accurate multi-step performance estimate.                     \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : OneStepAheadValidationWarning                                             \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:675                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>Benchmark results\n-----------------\nExecution time backtesting   : 3.746009349822998\nExecution time one step ahead: 2.683635950088501\nSame lags   : True\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'n_estimators': 199, 'max_depth': 3, 'learning_rate': 0.01901626315047264}\n    135.45451272241843\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'n_estimators': 198, 'max_depth': 3, 'learning_rate': 0.06045266837878549}\n    123.59899056676193\n</pre> In\u00a0[24]: Copied! <pre># Dataset sales - ForecasterDirectMultiVariate\n# ==============================================================================\nforecaster = ForecasterDirectMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 steps              = 5,\n                 level              = 'item_1',\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nlags_grid = [48, 72]\n\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators' : trial.suggest_int('n_estimators', 50, 200),\n        'max_depth'    : trial.suggest_int('max_depth', 3, 10, step=1),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),\n        'lags'         : trial.suggest_categorical('lags', lags_grid)\n    }\n\n    return search_space\n\n\ntime_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(\n    data                    = data_sales,\n    forecaster_to_benchmark = forecaster,\n    search_method           = 'bayesian_search',\n    search_space            = search_space,\n    end_train               = end_train,\n    end_validation          = end_validation,\n    levels                  = levels,\n    exog_features           = exog_features,\n    steps                   = 5,\n    metric                  = metric\n)\n\nresults_bayesian_search.append([\n    'sales',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1,\n    metric_2,\n])\n</pre> # Dataset sales - ForecasterDirectMultiVariate # ============================================================================== forecaster = ForecasterDirectMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  steps              = 5,                  level              = 'item_1',                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  fit_kwargs         = None,                  forecaster_id      = None              )  lags_grid = [48, 72]   def search_space(trial):     search_space  = {         'n_estimators' : trial.suggest_int('n_estimators', 50, 200),         'max_depth'    : trial.suggest_int('max_depth', 3, 10, step=1),         'learning_rate': trial.suggest_float('learning_rate', 0.01, 1),         'lags'         : trial.suggest_categorical('lags', lags_grid)     }      return search_space   time_1, time_2, metric_1, metric_2 = run_benchmark_multiseries(     data                    = data_sales,     forecaster_to_benchmark = forecaster,     search_method           = 'bayesian_search',     search_space            = search_space,     end_train               = end_train,     end_validation          = end_validation,     levels                  = levels,     exog_features           = exog_features,     steps                   = 5,     metric                  = metric )  results_bayesian_search.append([     'sales',     type(forecaster).__name__,     time_1,     time_2,     metric_1,     metric_2, ]) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 IgnoredArgumentWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `levels` argument have no use when the forecaster is of type                         \u2502\n\u2502 `ForecasterDirectMultiVariate`. The level of this forecaster is 'item_1', to predict \u2502\n\u2502 another level, change the `level` argument when initializing the forecaster.         \u2502\n\u2502                                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : IgnoredArgumentWarning                                                    \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:891                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=IgnoredArgumentWarning)          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 IgnoredArgumentWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `levels` argument have no use when the forecaster is of type                         \u2502\n\u2502 `ForecasterDirectMultiVariate`. The level of this forecaster is 'item_1', to predict \u2502\n\u2502 another level, change the `level` argument when initializing the forecaster.         \u2502\n\u2502                                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : IgnoredArgumentWarning                                                    \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:891                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=IgnoredArgumentWarning)          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OneStepAheadValidationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 One-step-ahead predictions are used for faster model comparison, but they may not    \u2502\n\u2502 fully represent multi-step prediction performance. It is recommended to backtest the \u2502\n\u2502 final model for a more accurate multi-step performance estimate.                     \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : OneStepAheadValidationWarning                                             \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:675                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 IgnoredArgumentWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `levels` argument have no use when the forecaster is of type                         \u2502\n\u2502 `ForecasterDirectMultiVariate`. The level of this forecaster is 'item_1', to predict \u2502\n\u2502 another level, change the `level` argument when initializing the forecaster.         \u2502\n\u2502                                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : IgnoredArgumentWarning                                                    \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:891                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=IgnoredArgumentWarning)          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 IgnoredArgumentWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `levels` argument have no use when the forecaster is of type                         \u2502\n\u2502 `ForecasterDirectMultiVariate`. The level of this forecaster is 'item_1', to predict \u2502\n\u2502 another level, change the `level` argument when initializing the forecaster.         \u2502\n\u2502                                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : IgnoredArgumentWarning                                                    \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/ubuntu/anaconda3/envs/skforecast_15_py12/lib/python3.12/site-packages/skforeca \u2502\n\u2502 st/model_selection/_utils.py:891                                                     \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=IgnoredArgumentWarning)          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>Benchmark results\n-----------------\nExecution time backtesting   : 18.90007495880127\nExecution time one step ahead: 4.8505072593688965\nSame lags   : False\nSame params : False\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n    params : {'n_estimators': 178, 'max_depth': 4, 'learning_rate': 0.029392307095288957}\n    98.66981600939468\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n    params : {'n_estimators': 98, 'max_depth': 5, 'learning_rate': 0.23598059857016607}\n    101.07276932380157\n</pre> In\u00a0[25]: Copied! <pre># Dataset series_dict - ForecasterRecursiveMultiSeries\n# ==============================================================================\nend_train = '2016-05-31 23:59:00'\nend_validation = '2016-07-31 23:59:00'\nlevels = ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004']\nseries_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()}\nexog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()}\nseries_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()}\nexog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}\n\nforecaster_to_benchmark = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = \"ordinal\",\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 differentiation    = None,\n                 dropna_from_series = False,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\n\ndef search_space(trial):\n    search_space  = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n        'max_depth'   : trial.suggest_int('max_depth', 3, 7, step=1),\n        'lags'        : trial.suggest_categorical('lags', [7, 14])\n    } \n    return search_space\n\n\n# Backtesting\nforecaster = copy(forecaster_to_benchmark)\ncv = TimeSeriesFold(\n        initial_train_size = 100,\n        steps              = 24,\n        refit              = False,\n     )\nstart  = time()\nresults_1, _ = bayesian_search_forecaster_multiseries(\n                   forecaster         = forecaster,\n                   series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},\n                   exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},\n                   cv                 = cv,\n                   search_space       = search_space,\n                   n_trials           = 10,\n                   metric             = metric,\n                   return_best        = False,\n                   n_jobs             = 'auto',\n                   verbose            = False,\n                   show_progress      = False,\n                   suppress_warnings  = True\n               )\nend = time()\ntime_1 = end - start\nbest_params = results_1.loc[0, 'params']\nbest_lags = results_1.loc[0, 'lags']\nforecaster.set_params(best_params)\nforecaster.set_lags(lags=best_lags)\n\ncv = TimeSeriesFold(\n        initial_train_size = 213,\n        steps              = 24,\n        refit              = False,\n     )\nmetric_1, pred_1 = backtesting_forecaster_multiseries(\n                       forecaster         = forecaster,\n                       series             = series_dict,\n                       exog               = exog_dict,\n                       cv                 = cv,\n                       levels             = levels,\n                       metric             = metric,\n                       verbose            = False,\n                       show_progress      = False,\n                       suppress_warnings  = True\n                   )\n\n# One step ahead\nforecaster = copy(forecaster_to_benchmark)\ncv = OneStepAheadFold(initial_train_size = 100)\nstart  = time()\nresults_2, _ = bayesian_search_forecaster_multiseries(\n                   forecaster         = forecaster,\n                   series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},\n                   exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},\n                   cv                 = cv,\n                   levels             = levels,\n                   search_space       = search_space,\n                   n_trials           = 10,\n                   metric             = metric,\n                   return_best        = False,\n                   verbose            = False,\n                   show_progress      = False,\n                   suppress_warnings  = True\n               )\nend = time()\ntime_2 = end - start\nbest_params = results_2.loc[0, 'params']\nbest_lags = results_2.loc[0, 'lags']\nforecaster.set_params(best_params)\nforecaster.set_lags(lags=best_lags)\n\ncv = TimeSeriesFold(\n        initial_train_size = 213,\n        steps              = 24,\n        refit              = False,\n     )\nmetric_2, pred_2 = backtesting_forecaster_multiseries(\n                       forecaster         = forecaster,\n                       series             = series_dict,\n                       exog               = exog_dict,\n                       cv                 = cv,\n                       levels             = levels,\n                       metric             = metric,\n                       verbose            = False,\n                       show_progress      = False,\n                       suppress_warnings  = True\n                   )\n\nprint(\"Benchmark results\")\nprint(\"-----------------\")\nprint('Execution time backtesting   :', time_1)\nprint('Execution time one step ahead:', time_2)\nprint(f\"Same lags   : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\")\nprint(f\"Same params : {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\")\nprint(\"\")\nprint(\"Method: backtesting\")\nprint(f\"    lags   : {results_1.loc[0, 'lags']}\")\nprint(f\"    params : {results_1.loc[0, 'params']}\")\nprint(f\"    {metric_1.loc[0, metric]}\")\nprint(\"\")\nprint(\"Method: one step ahead\")\nprint(f\"    lags   : {results_2.loc[0, 'lags']}\")\nprint(f\"    params : {results_2.loc[0, 'params']}\")\nprint(f\"    {metric_2.loc[0, metric]}\")\n\nresults_bayesian_search.append([\n    'series_dict',\n    type(forecaster).__name__,\n    time_1,\n    time_2,\n    metric_1.loc[0, metric],\n    metric_2.loc[0, metric],\n])\n</pre> # Dataset series_dict - ForecasterRecursiveMultiSeries # ============================================================================== end_train = '2016-05-31 23:59:00' end_validation = '2016-07-31 23:59:00' levels = ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004'] series_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()} exog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()} series_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()} exog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}  forecaster_to_benchmark = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = \"ordinal\",                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  differentiation    = None,                  dropna_from_series = False,                  fit_kwargs         = None,                  forecaster_id      = None              )   def search_space(trial):     search_space  = {         'n_estimators': trial.suggest_int('n_estimators', 50, 200),         'max_depth'   : trial.suggest_int('max_depth', 3, 7, step=1),         'lags'        : trial.suggest_categorical('lags', [7, 14])     }      return search_space   # Backtesting forecaster = copy(forecaster_to_benchmark) cv = TimeSeriesFold(         initial_train_size = 100,         steps              = 24,         refit              = False,      ) start  = time() results_1, _ = bayesian_search_forecaster_multiseries(                    forecaster         = forecaster,                    series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},                    exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},                    cv                 = cv,                    search_space       = search_space,                    n_trials           = 10,                    metric             = metric,                    return_best        = False,                    n_jobs             = 'auto',                    verbose            = False,                    show_progress      = False,                    suppress_warnings  = True                ) end = time() time_1 = end - start best_params = results_1.loc[0, 'params'] best_lags = results_1.loc[0, 'lags'] forecaster.set_params(best_params) forecaster.set_lags(lags=best_lags)  cv = TimeSeriesFold(         initial_train_size = 213,         steps              = 24,         refit              = False,      ) metric_1, pred_1 = backtesting_forecaster_multiseries(                        forecaster         = forecaster,                        series             = series_dict,                        exog               = exog_dict,                        cv                 = cv,                        levels             = levels,                        metric             = metric,                        verbose            = False,                        show_progress      = False,                        suppress_warnings  = True                    )  # One step ahead forecaster = copy(forecaster_to_benchmark) cv = OneStepAheadFold(initial_train_size = 100) start  = time() results_2, _ = bayesian_search_forecaster_multiseries(                    forecaster         = forecaster,                    series             = {k: v.loc[: end_validation,] for k, v in series_dict.items()},                    exog               = {k: v.loc[: end_validation,] for k, v in exog_dict.items()},                    cv                 = cv,                    levels             = levels,                    search_space       = search_space,                    n_trials           = 10,                    metric             = metric,                    return_best        = False,                    verbose            = False,                    show_progress      = False,                    suppress_warnings  = True                ) end = time() time_2 = end - start best_params = results_2.loc[0, 'params'] best_lags = results_2.loc[0, 'lags'] forecaster.set_params(best_params) forecaster.set_lags(lags=best_lags)  cv = TimeSeriesFold(         initial_train_size = 213,         steps              = 24,         refit              = False,      ) metric_2, pred_2 = backtesting_forecaster_multiseries(                        forecaster         = forecaster,                        series             = series_dict,                        exog               = exog_dict,                        cv                 = cv,                        levels             = levels,                        metric             = metric,                        verbose            = False,                        show_progress      = False,                        suppress_warnings  = True                    )  print(\"Benchmark results\") print(\"-----------------\") print('Execution time backtesting   :', time_1) print('Execution time one step ahead:', time_2) print(f\"Same lags   : {np.array_equal(results_1.loc[0, 'lags'], results_2.loc[0, 'lags'])}\") print(f\"Same params : {results_1.loc[0, 'params'] == results_2.loc[0, 'params']}\") print(\"\") print(\"Method: backtesting\") print(f\"    lags   : {results_1.loc[0, 'lags']}\") print(f\"    params : {results_1.loc[0, 'params']}\") print(f\"    {metric_1.loc[0, metric]}\") print(\"\") print(\"Method: one step ahead\") print(f\"    lags   : {results_2.loc[0, 'lags']}\") print(f\"    params : {results_2.loc[0, 'params']}\") print(f\"    {metric_2.loc[0, metric]}\")  results_bayesian_search.append([     'series_dict',     type(forecaster).__name__,     time_1,     time_2,     metric_1.loc[0, metric],     metric_2.loc[0, metric], ]) <pre>Benchmark results\n-----------------\nExecution time backtesting   : 2.2047343254089355\nExecution time one step ahead: 0.6358184814453125\nSame lags   : True\nSame params : True\n\nMethod: backtesting\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'n_estimators': 77, 'max_depth': 3}\n    208.60243551060555\n\nMethod: one step ahead\n    lags   : [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    params : {'n_estimators': 77, 'max_depth': 3}\n    208.60243551060555\n</pre> In\u00a0[26]: Copied! <pre># Results\n# ==============================================================================\nsummarize_results(\n    results   = results_bayesian_search,\n    metric    = metric,\n    plot      = True,\n    fig_size  = (8, 6),\n    title     = 'Bayesian search using backtesting vs one-step-ahead',\n    save_plot = \"../img/bayesian_search_benchmarck.png\"\n)\n</pre> # Results # ============================================================================== summarize_results(     results   = results_bayesian_search,     metric    = metric,     plot      = True,     fig_size  = (8, 6),     title     = 'Bayesian search using backtesting vs one-step-ahead',     save_plot = \"../img/bayesian_search_benchmarck.png\" ) dataset forecaster time_search_backtesting time_search_one_step metric_backtesting metric_one_step ratio_speed ratio_metric dataset_forecaster 0 bike_sharing ForecasterRecursive 87.158988 27.958404 55.805777 55.805777 3.12 1.00 bike_sharing \\n Recursive 1 bike_sharing ForecasterDirect 17.606075 0.984209 79.149834 111.962087 17.89 0.71 bike_sharing \\n Direct 2 website ForecasterRecursive 1.092046 0.265008 136.768023 173.528300 4.12 0.79 website \\n Recursive 3 website ForecasterDirect 1.558726 0.309523 139.401236 153.672368 5.04 0.91 website \\n Direct 4 electricity ForecasterRecursive 90.564335 24.597377 196.748300 191.374914 3.68 1.03 electricity \\n Recursive 5 electricity ForecasterDirect 16.454455 0.642995 307.133653 300.870281 25.59 1.02 electricity \\n Direct 6 sales ForecasterRecursiveMultiSeries 3.746009 2.683636 135.454513 123.598991 1.40 1.10 sales \\n RecursiveMultiSeries 7 sales ForecasterDirectMultiVariate 18.900075 4.850507 98.669816 101.072769 3.90 0.98 sales \\n DirectMultiVariate 8 series_dict ForecasterRecursiveMultiSeries 2.204734 0.635818 208.602436 208.602436 3.47 1.00 series_dict \\n RecursiveMultiSeries"},{"location":"faq/parameters-search-backtesting-vs-one-step-ahead.html#hyperparameters-and-lags-search-backtesting-vs-one-step-ahead","title":"Hyperparameters and lags search: backtesting vs one-step-ahead\u00b6","text":""},{"location":"faq/parameters-search-backtesting-vs-one-step-ahead.html#results","title":"Results\u00b6","text":""},{"location":"faq/parameters-search-backtesting-vs-one-step-ahead.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"faq/parameters-search-backtesting-vs-one-step-ahead.html#benchmark","title":"Benchmark\u00b6","text":""},{"location":"faq/parameters-search-backtesting-vs-one-step-ahead.html#grid-search","title":"Grid search\u00b6","text":""},{"location":"faq/parameters-search-backtesting-vs-one-step-ahead.html#bayesian-search","title":"Bayesian search\u00b6","text":""},{"location":"faq/probabilistic-forecasting-calibrate-intervals.html","title":"Calibration of probabilistic forecasting intervals","text":"<p>When using probabilistic forecasting methods, it is often necessary to estimate prediction intervals for the forecasted values. Prediction intervals quantify the uncertainty in forecasts and allow one to assess the probability that the actual value will fall within the predicted range. One of the most commonly used methods for calibrating prediction intervals is the conformal prediction framework.</p> <p>Conformal prediction is a framework for constructing prediction intervals that are guaranteed to contain the true value with a specified probability (coverage probability). In addition to generating prediction intervals from point forecasts, conformal methods can also calibrate intervals produced by other techniques, such as quantile regression or bootstrapped residuals. In such cases, the conformal method adjusts the intervals\u2014either expanding or shrinking them\u2014to ensure they achieve the desired coverage.</p> <p>          Conformal calibration of prediction intervals. Source:                                Introduction To Conformal Prediction With Python: A Short Guide For Quantifying Uncertainty Of Machine Learning Models                        by Christoph Molnar          </p> <p>Skforecast provides this functionality through the <code>ConformalIntervalCalibrator</code> transformer that can be used for single series forecasting models as well as global forecasting models.</p> <p> \ud83d\udca1 Tip </p> <p> <p>This document demonstrates how <code>ConformalIntervalCalibrator</code> can improve various types of prediction intervals \u2014whether overly conservative or under-conservative\u2014 using both symmetric and asymmetric conformal calibration. In these examples, the transformer is fitted and applied to the same dataset. However, in forecasting problems, the transformer should be fitted using a calibration set and then applied to the prediction intervals of the forecast set.</p> <p>This document demonstrates how the <code>ConformalIntervalCalibrator</code> can improve various types of prediction intervals - whether overly conservative or underly conservative - using both symmetric and asymmetric conformal calibration. In these examples, the calibrator is fitted and applied to the same dataset. In real scenario problems, however, the transformer should be fitted using a calibration set and then applied to the prediction intervals of the test set.</p> </p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.metrics import calculate_coverage\nfrom skforecast.preprocessing import ConformalIntervalCalibrator\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme from skforecast.metrics import calculate_coverage from skforecast.preprocessing import ConformalIntervalCalibrator In\u00a0[2]: Copied! <pre># Simulation of interval\n# ==============================================================================\nrng = np.random.default_rng(42)\ninterval = pd.DataFrame({\n        'lower_bound': np.sin(np.linspace(0, 4 * np.pi, 100)),\n        'upper_bound': np.sin(np.linspace(0, 4 * np.pi, 100)) + 5\n    },\n    index=pd.date_range(start='2024-01-01', periods=100, freq='D')\n)\ny_true = (interval['lower_bound'] + interval['upper_bound']) / 2 + rng.normal(0, 0.5, 100)\ny_true.name = \"series_1\"\ny_true.iloc[1::5] = interval.iloc[1::5, 0] - rng.normal(1, 1, 20)\ny_true.iloc[3::5] = interval.iloc[1::5, 1] + rng.normal(1, 1, 20)\n\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ninterval.plot(ax=ax, linestyle=\"--\")\ny_true.plot(ax=ax, label='True values')\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.legend(loc=\"upper right\", fontsize=8, ncol=3)\nplt.show()\n\ncoverage = calculate_coverage(\n    y_true=y_true,\n    lower_bound=interval[\"lower_bound\"],\n    upper_bound=interval[\"upper_bound\"],\n)\nprint(f'Coverage: {coverage:.2f}')\n</pre> # Simulation of interval # ============================================================================== rng = np.random.default_rng(42) interval = pd.DataFrame({         'lower_bound': np.sin(np.linspace(0, 4 * np.pi, 100)),         'upper_bound': np.sin(np.linspace(0, 4 * np.pi, 100)) + 5     },     index=pd.date_range(start='2024-01-01', periods=100, freq='D') ) y_true = (interval['lower_bound'] + interval['upper_bound']) / 2 + rng.normal(0, 0.5, 100) y_true.name = \"series_1\" y_true.iloc[1::5] = interval.iloc[1::5, 0] - rng.normal(1, 1, 20) y_true.iloc[3::5] = interval.iloc[1::5, 1] + rng.normal(1, 1, 20)  set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) interval.plot(ax=ax, linestyle=\"--\") y_true.plot(ax=ax, label='True values') ax.set_yticklabels([]) ax.set_xticklabels([]) ax.legend(loc=\"upper right\", fontsize=8, ncol=3) plt.show()  coverage = calculate_coverage(     y_true=y_true,     lower_bound=interval[\"lower_bound\"],     upper_bound=interval[\"upper_bound\"], ) print(f'Coverage: {coverage:.2f}') <pre>Coverage: 0.67\n</pre> In\u00a0[3]: Copied! <pre># Calibrate interval\n# ==============================================================================\ncalibrator = ConformalIntervalCalibrator(nominal_coverage=0.8, symmetric_calibration=False)\ncalibrator.fit(y_true=y_true, y_pred_interval=interval)\nprint(calibrator)\n\ninterval_calibrated = calibrator.transform(interval)\nfig, ax = plt.subplots(figsize=(7, 3.5))\ninterval.plot(ax=ax, linestyle=\"--\")\ninterval_calibrated[\"lower_bound\"].plot(ax=ax, color=\"#30a2da\", label=\"Calibrated lower bound\")\ninterval_calibrated[\"upper_bound\"].plot(ax=ax, color=\"#fc4f30\", label=\"Calibrated upper bound\")\ny_true.plot(ax=ax, label=\"True values\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.legend(loc=\"upper right\", fontsize=8, ncol=4)\nplt.show()\n\ncoverage = calculate_coverage(\n               y_true      = y_true,\n               lower_bound = interval_calibrated[\"lower_bound\"],\n               upper_bound = interval_calibrated[\"upper_bound\"],\n           )\nprint(f\"Coverage after calibration: {coverage:.2f}\")\n</pre> # Calibrate interval # ============================================================================== calibrator = ConformalIntervalCalibrator(nominal_coverage=0.8, symmetric_calibration=False) calibrator.fit(y_true=y_true, y_pred_interval=interval) print(calibrator)  interval_calibrated = calibrator.transform(interval) fig, ax = plt.subplots(figsize=(7, 3.5)) interval.plot(ax=ax, linestyle=\"--\") interval_calibrated[\"lower_bound\"].plot(ax=ax, color=\"#30a2da\", label=\"Calibrated lower bound\") interval_calibrated[\"upper_bound\"].plot(ax=ax, color=\"#fc4f30\", label=\"Calibrated upper bound\") y_true.plot(ax=ax, label=\"True values\") ax.set_yticklabels([]) ax.set_xticklabels([]) ax.legend(loc=\"upper right\", fontsize=8, ncol=4) plt.show()  coverage = calculate_coverage(                y_true      = y_true,                lower_bound = interval_calibrated[\"lower_bound\"],                upper_bound = interval_calibrated[\"upper_bound\"],            ) print(f\"Coverage after calibration: {coverage:.2f}\") <pre>=========================== \nConformalIntervalCalibrator \n=========================== \nNominal coverage: 0.8 \nCoverage in fit data: {'series_1': 0.67} \nSymmetric interval: False \nSymmetric correction factor: {'series_1': 0.9488126715432241} \nAsymmetric correction factor lower: {'series_1': 0.7640361794875151} \nAsymmetric correction factor upper: {'series_1': 1.0384612609432264} \nFitted series: ['series_1'] \n\n</pre> <pre>Coverage after calibration: 0.80\n</pre> <p>The interval has been expanded by a factor of 0.948 in both directions, achieving a coverage probability of 0.80.</p> In\u00a0[4]: Copied! <pre># Simulation of interval\n# ==============================================================================\nrng = np.random.default_rng(42)\ninterval = pd.DataFrame({\n        'lower_bound': np.sin(np.linspace(0, 4 * np.pi, 100)),\n        'upper_bound': np.sin(np.linspace(0, 4 * np.pi, 100)) + 5\n    },\n    index=pd.date_range(start='2024-01-01', periods=100, freq='D')\n)\ny_true = (interval['lower_bound'] + interval['upper_bound']) / 2 + rng.normal(0, 0.5, 100)\ny_true.name = \"series_1\"\ny_true.iloc[1::9] = interval.iloc[1::9, 0] - rng.normal(1, 1, 11)\ny_true.iloc[3::9] = interval.iloc[1::9, 1] + rng.normal(1, 1, 11)\n\nfig, ax = plt.subplots(figsize=(7, 3))\ninterval.plot(ax=ax, linestyle=\"--\")\ny_true.plot(ax=ax, label='True values')\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.legend(loc=\"upper right\", fontsize=8, ncol=3)\nplt.show()\n\ncoverage = calculate_coverage(\n               y_true      = y_true,\n               lower_bound = interval[\"lower_bound\"],\n               upper_bound = interval[\"upper_bound\"],\n           )\nprint(f'Coverage: {coverage:.2f}')\n</pre> # Simulation of interval # ============================================================================== rng = np.random.default_rng(42) interval = pd.DataFrame({         'lower_bound': np.sin(np.linspace(0, 4 * np.pi, 100)),         'upper_bound': np.sin(np.linspace(0, 4 * np.pi, 100)) + 5     },     index=pd.date_range(start='2024-01-01', periods=100, freq='D') ) y_true = (interval['lower_bound'] + interval['upper_bound']) / 2 + rng.normal(0, 0.5, 100) y_true.name = \"series_1\" y_true.iloc[1::9] = interval.iloc[1::9, 0] - rng.normal(1, 1, 11) y_true.iloc[3::9] = interval.iloc[1::9, 1] + rng.normal(1, 1, 11)  fig, ax = plt.subplots(figsize=(7, 3)) interval.plot(ax=ax, linestyle=\"--\") y_true.plot(ax=ax, label='True values') ax.set_yticklabels([]) ax.set_xticklabels([]) ax.legend(loc=\"upper right\", fontsize=8, ncol=3) plt.show()  coverage = calculate_coverage(                y_true      = y_true,                lower_bound = interval[\"lower_bound\"],                upper_bound = interval[\"upper_bound\"],            ) print(f'Coverage: {coverage:.2f}') <pre>Coverage: 0.80\n</pre> In\u00a0[5]: Copied! <pre># Calibrate interval\n# ==============================================================================\ncalibrator = ConformalIntervalCalibrator(nominal_coverage=0.7, symmetric_calibration=False)\ncalibrator.fit(y_true=y_true, y_pred_interval=interval)\nprint(calibrator)\ninterval_calibrated = calibrator.transform(interval)\nfig, ax = plt.subplots(figsize=(7, 3.5))\ninterval.plot(ax=ax, linestyle=\"--\")\ninterval_calibrated[\"lower_bound\"].plot(ax=ax, color=\"#30a2da\", label=\"Calibrated lower bound\")\ninterval_calibrated[\"upper_bound\"].plot(ax=ax, color=\"#fc4f30\", label=\"Calibrated upper bound\")\ny_true.plot(ax=ax, label=\"True values\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.legend(loc=\"upper right\", fontsize=8, ncol=4)\nplt.show()\n\ncoverage = calculate_coverage(\n               y_true      = y_true,\n               lower_bound = interval_calibrated[\"lower_bound\"],\n               upper_bound = interval_calibrated[\"upper_bound\"],\n           )\nprint(f\"Coverage after calibration: {coverage:.2f}\")\n</pre> # Calibrate interval # ============================================================================== calibrator = ConformalIntervalCalibrator(nominal_coverage=0.7, symmetric_calibration=False) calibrator.fit(y_true=y_true, y_pred_interval=interval) print(calibrator) interval_calibrated = calibrator.transform(interval) fig, ax = plt.subplots(figsize=(7, 3.5)) interval.plot(ax=ax, linestyle=\"--\") interval_calibrated[\"lower_bound\"].plot(ax=ax, color=\"#30a2da\", label=\"Calibrated lower bound\") interval_calibrated[\"upper_bound\"].plot(ax=ax, color=\"#fc4f30\", label=\"Calibrated upper bound\") y_true.plot(ax=ax, label=\"True values\") ax.set_yticklabels([]) ax.set_xticklabels([]) ax.legend(loc=\"upper right\", fontsize=8, ncol=4) plt.show()  coverage = calculate_coverage(                y_true      = y_true,                lower_bound = interval_calibrated[\"lower_bound\"],                upper_bound = interval_calibrated[\"upper_bound\"],            ) print(f\"Coverage after calibration: {coverage:.2f}\") <pre>=========================== \nConformalIntervalCalibrator \n=========================== \nNominal coverage: 0.7 \nCoverage in fit data: {'series_1': 0.8} \nSymmetric interval: False \nSymmetric correction factor: {'series_1': -1.8807575829634011} \nAsymmetric correction factor lower: {'series_1': -1.775690512731013} \nAsymmetric correction factor upper: {'series_1': -2.0540377453500485} \nFitted series: ['series_1'] \n\n</pre> <pre>Coverage after calibration: 0.70\n</pre> <p>The resulting interval has been shrunk by a factor of 1.88 in both directions, achieving a coverage probability of 0.70.</p> In\u00a0[6]: Copied! <pre># Simulation of interval\n# ==============================================================================\nrng = np.random.default_rng(42)\ninterval = pd.DataFrame({\n        'lower_bound': np.sin(np.linspace(0, 4 * np.pi, 100)),\n        'upper_bound': np.sin(np.linspace(0, 4 * np.pi, 100)) + 5\n    },\n    index=pd.date_range(start='2024-01-01', periods=100, freq='D')\n)\ny_true = (interval['lower_bound'] + interval['upper_bound']) / 2 + rng.normal(0, 0.5, 100)\ny_true.name = \"series_1\"\ny_true.iloc[1::6] = interval.iloc[1::6, 0] - rng.normal(1, 1, 17)\ny_true.iloc[3::4] = interval.iloc[1::4, 1] + rng.normal(3, 2, 25)\n\nfig, ax = plt.subplots(figsize=(7, 3))\ninterval.plot(ax=ax, linestyle=\"--\")\ny_true.plot(ax=ax, label='True values')\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.legend(loc=\"upper right\", fontsize=8, ncol=3)\nplt.show()\n\ncoverage = calculate_coverage(\n               y_true      = y_true,\n               lower_bound = interval[\"lower_bound\"],\n               upper_bound = interval[\"upper_bound\"],\n           )\nprint(f'Coverage: {coverage:.2f}')\n</pre> # Simulation of interval # ============================================================================== rng = np.random.default_rng(42) interval = pd.DataFrame({         'lower_bound': np.sin(np.linspace(0, 4 * np.pi, 100)),         'upper_bound': np.sin(np.linspace(0, 4 * np.pi, 100)) + 5     },     index=pd.date_range(start='2024-01-01', periods=100, freq='D') ) y_true = (interval['lower_bound'] + interval['upper_bound']) / 2 + rng.normal(0, 0.5, 100) y_true.name = \"series_1\" y_true.iloc[1::6] = interval.iloc[1::6, 0] - rng.normal(1, 1, 17) y_true.iloc[3::4] = interval.iloc[1::4, 1] + rng.normal(3, 2, 25)  fig, ax = plt.subplots(figsize=(7, 3)) interval.plot(ax=ax, linestyle=\"--\") y_true.plot(ax=ax, label='True values') ax.set_yticklabels([]) ax.set_xticklabels([]) ax.legend(loc=\"upper right\", fontsize=8, ncol=3) plt.show()  coverage = calculate_coverage(                y_true      = y_true,                lower_bound = interval[\"lower_bound\"],                upper_bound = interval[\"upper_bound\"],            ) print(f'Coverage: {coverage:.2f}') <pre>Coverage: 0.66\n</pre> <p>The plot shows that there are more points above the prediction interval than below it. Also, the points above the interval are farther away than the points below the lower interval. This suggests that the upper interval should be expanded more than the lower interval.</p> In\u00a0[7]: Copied! <pre># Calibrate interval\n# ==============================================================================\ncalibrator = ConformalIntervalCalibrator(nominal_coverage=0.8, symmetric_calibration=False)\ncalibrator.fit(y_true=y_true, y_pred_interval=interval)\nprint(calibrator)\ninterval_calibrated = calibrator.transform(interval)\nfig, ax = plt.subplots(figsize=(7, 3.5))\ninterval.plot(ax=ax, linestyle=\"--\")\ninterval_calibrated[\"lower_bound\"].plot(ax=ax, color=\"#30a2da\", label=\"Calibrated lower bound\")\ninterval_calibrated[\"upper_bound\"].plot(ax=ax, color=\"#fc4f30\", label=\"Calibrated upper bound\")\ny_true.plot(ax=ax, label=\"True values\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.legend(loc=\"upper right\", fontsize=8, ncol=4)\nplt.show()\n\ncoverage = calculate_coverage(\n               y_true      = y_true,\n               lower_bound = interval_calibrated[\"lower_bound\"],\n               upper_bound = interval_calibrated[\"upper_bound\"],\n           )\nprint(f\"Coverage after calibration: {coverage:.2f}\")\n</pre> # Calibrate interval # ============================================================================== calibrator = ConformalIntervalCalibrator(nominal_coverage=0.8, symmetric_calibration=False) calibrator.fit(y_true=y_true, y_pred_interval=interval) print(calibrator) interval_calibrated = calibrator.transform(interval) fig, ax = plt.subplots(figsize=(7, 3.5)) interval.plot(ax=ax, linestyle=\"--\") interval_calibrated[\"lower_bound\"].plot(ax=ax, color=\"#30a2da\", label=\"Calibrated lower bound\") interval_calibrated[\"upper_bound\"].plot(ax=ax, color=\"#fc4f30\", label=\"Calibrated upper bound\") y_true.plot(ax=ax, label=\"True values\") ax.set_yticklabels([]) ax.set_xticklabels([]) ax.legend(loc=\"upper right\", fontsize=8, ncol=4) plt.show()  coverage = calculate_coverage(                y_true      = y_true,                lower_bound = interval_calibrated[\"lower_bound\"],                upper_bound = interval_calibrated[\"upper_bound\"],            ) print(f\"Coverage after calibration: {coverage:.2f}\") <pre>=========================== \nConformalIntervalCalibrator \n=========================== \nNominal coverage: 0.8 \nCoverage in fit data: {'series_1': 0.66} \nSymmetric interval: False \nSymmetric correction factor: {'series_1': 1.2730256977929353} \nAsymmetric correction factor lower: {'series_1': -1.6431477452861944} \nAsymmetric correction factor upper: {'series_1': 3.691274308503413} \nFitted series: ['series_1'] \n\n</pre> <pre>Coverage after calibration: 0.80\n</pre> <p>The asymmetric calibration has expanded the upper interval by a factor of 3.69 while shrinking the lower interval by a factor of 1.64.</p> <p>The positive correction factor for the upper interval indicates that more points fall above the interval than expected for an upper limit representing the 0.90 quantile. To correct this, the interval is expanded, reducing the number of points that exceed the upper bound.</p> <p>Conversely, the negative correction factor for the lower interval suggests that fewer points fall below the interval than expected for a lower limit representing the 0.10 quantile. As a result, the interval is shrunk to ensure that more points fall below the lower bound.</p> <p>The reason the upper and lower bounds correspond to the 0.90 and 0.10 quantiles, respectively, is that the desired coverage probability is 80%. Internally, the transformer assumes that the upper bound should represent the quantile $(1 - \\frac{(1 - 0.8)}{2})$ and the lower bound should represent the quantile $(\\frac{(1 - 0.8)}{2})$.</p>"},{"location":"faq/probabilistic-forecasting-calibrate-intervals.html#calibration-of-probabilistic-forecasting-intervals","title":"Calibration of probabilistic forecasting intervals\u00b6","text":""},{"location":"faq/probabilistic-forecasting-calibrate-intervals.html#symmetric-calibration","title":"Symmetric calibration\u00b6","text":"<p>In symmetric calibration, prediction intervals are expanded or contracted by a constant factor to ensure that the coverage probability is achieved. This means that both the upper and lower prediction intervals are adjusted by the same factor.</p>"},{"location":"faq/probabilistic-forecasting-calibrate-intervals.html#under-conservative-intervals","title":"Under-conservative intervals\u00b6","text":"<p>Underconservative intervals (optimistic intervals) have a lower than desired coverage probability, meaning they are too narrow and exclude the true value too often. In this case, conformal calibration expands the intervals to increase the coverage.</p>"},{"location":"faq/probabilistic-forecasting-calibrate-intervals.html#over-conservative-intervals","title":"Over-conservative intervals\u00b6","text":"<p>Overly conservative intervals (pessimistic intervals) have a higher than desired coverage probability, meaning they are too wide and include the true value too often. In this case, the conformal calibration shrinks the intervals to reduce the coverage.</p> <p>To illustrate, the initial interval has a coverage probability of 80% and the desired coverage probability is 70%.</p>"},{"location":"faq/probabilistic-forecasting-calibrate-intervals.html#asymetric-calibration","title":"Asymetric calibration\u00b6","text":"<p>Asymmetric calibration allows the upper and lower prediction intervals to be adjusted by different factors. This is useful when the prediction intervals are not symmetrically distributed around the point forecast.</p>"},{"location":"faq/probabilistic-forecasting-crps-score.html","title":"Continuous Ranked Probability Score (CRPS)","text":"<p>In point estimate forecasting, the model outputs a single value that ideally represents the most likely value of the time series at future steps. In this scenario, the quality of the predictions can be assessed by comparing the predicted value with the true value of the series. Examples of metrics used for this purpose are the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE).</p> <p>In probabilistic forecasting, however, the model does not produce a single value, but rather a representation of the entire distribution of possible predicted values. In practice, this is often represented by a sample of the underlying distribution (e.g. 50 possible predicted values) or by specific quantiles that capture most of the information in the distribution.</p> <p>One of the main applications of probabilistic forecasting is the estimation of prediction intervals - ranges within which the actual value is expected to fall with a certain probability. In this case, the model should aim to achieve the desired coverage (e.g. 80%) while minimising the width of the prediction interval.</p> <p>The Continuous Ranked Probability Score (CRPS) is a generalisation of the Mean Absolute Error (MAE) tailored to probabilistic forecasting. Unlike the MAE, which compares point predictions to observations, the CRPS evaluates the accuracy of an entire predicted probability distribution against the observed value. It does this by comparing the empirical cumulative distribution function (CDF) of the predicted values with the step-function CDF of the true value.</p> <p>Two key components of the CRPS are the empirical CDF of the predicted values, \ud835\udc39(\ud835\udc66), and the CDF of the observed value, \ud835\udc3b(\ud835\udc66). The CRPS is then calculated as the integral of the squared difference between these two functions over the entire real line:</p> <ul> <li><p>Empirical CDF of the forecast, $F(y)$: This is constructed from the ensemble of predicted values. Each predicted value contributes a \"step\" in the cumulative distribution. The predicted values are therefore treated  as a sample of the underlying probability distribution.</p> </li> <li><p>CDF of the observed Value, $H(y)$: This is a step function that transitions from 0 to 1 at the true observed value. It represents the probability that the observed value falls below a given threshold.</p> </li> </ul> <p>The CRPS measures the area between the two CDFs, $F(y)$ and $H(y)$, across all possible values of $y$. Mathematically, it is expressed as:</p> <p>$$\\text{CRPS}(F, H) = \\int_{-\\infty}^{\\infty} \\big(F(y) - H(y)\\big)^2 \\, dy$$</p> <p>This integral quantifies the squared difference between the forecasted and observed distributions.</p> <p>The CRPS can be computed for a single observation or for a set of observations. In the latter case, the CRPS is averaged over all observations to provide a summary measure of the model's performance.</p> <p>CRPS is widely used in probabilistic forecasting because it provides a unified framework for evaluating both the sharpness (narrowness) and calibration (accuracy) of predictive distributions. By doing so, it ensures that models are not only accurate in their point predictions but also appropriately represent uncertainty. Smaller values of CRPS indicate a better match between the forecast and the observed outcome.</p> <p> Example of CRPS calculation </p> <p>Skforecast provides different output options for probabilistic forecasting, two of which are:</p> <ul> <li><p><code>predict_bootstrapping</code>: Returns multiple predicted values for each forecasted step. Each value is a variation of the forecast generated through bootstrapping. For a given step (i), (n) predictions are estimated.</p> </li> <li><p><code>predict_quantile</code>: Returns the estimated values for multiple quantiles. Internally, the forecaster uses <code>predict_bootstrapping</code> and then calculates the desired quantiles.</p> </li> </ul> <p>For both outputs, the CRPS (Continuous Ranked Probability Score) can be calculated to evaluate the forecasting performance of the model.</p> <p> \ud83d\udca1 Tip </p> <p>For more examples on how to use probabilistic forecasting, check out the following articles:</p> <ul> <li>              Probabilistic forecasting with machine learning          </li> <li>              Probabilistic forecasting: prediction intervals for multi-step time series forecasting          </li> <li>              Continuous Ranked Probability Score (CRPS) in probabilistic forecasting          </li> </ul> <p>The Continuous Ranked Probability Score (CRPS) is calculated by comparing the empirical cumulative distribution function (ECDF) of the forecasted values to the step function CDF of the true value. When the available information consists of the true value (<code>y_true</code>) and a sample of predictions (<code>y_pred</code>), the CRPS can be calculated by following these steps:</p> <ol> <li><p>Generate the Empirical Cumulative Distribution Function (ECDF) of the predictions:</p> <ul> <li>Sort the predictions.</li> <li>Use each sorted prediction as a step in the ECDF.</li> </ul> </li> <li><p>Generate the Cumulative Distribution Function (CDF) of the true value:</p> <ul> <li>Since there is only a single true value, this is represented as a step function that jumps from 0 to 1 at the observed value (<code>y_true</code>).</li> </ul> </li> <li><p>Calculate the CRPS by integrating the area between both curves:</p> <ul> <li>Create a grid of values to evaluate the ECDF. This grid is the combination of the predictions and the true value.</li> <li>Compute the squared differences between the forecasted ECDF and the true CDF, and then summing the areas between the two curves.</li> </ul> </li> </ol> In\u00a0[1]: Copied! <pre># Libraries\n# ======================================================================================\nimport pandas as pd\nimport numpy as np\nfrom skforecast.metrics import crps_from_predictions\nfrom skforecast.metrics import crps_from_quantiles\nfrom scipy.interpolate import interp1d\nimport properscoring as ps\nfrom CRPS import CRPS\nfrom pymc_marketing.metrics import crps\n</pre> # Libraries # ====================================================================================== import pandas as pd import numpy as np from skforecast.metrics import crps_from_predictions from skforecast.metrics import crps_from_quantiles from scipy.interpolate import interp1d import properscoring as ps from CRPS import CRPS from pymc_marketing.metrics import crps <pre>WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n</pre> In\u00a0[2]: Copied! <pre># Simulate data: true value and and array of 100 predicted values for the same true value\n# ======================================================================================\ny_true = 500\ny_pred = np.random.normal(500, 10, 100)\ncrps_from_predictions(y_true, y_pred)\n</pre> # Simulate data: true value and and array of 100 predicted values for the same true value # ====================================================================================== y_true = 500 y_pred = np.random.normal(500, 10, 100) crps_from_predictions(y_true, y_pred) Out[2]: <pre>2.1767579851785803</pre> <p>The result of <code>skforecast.metrics.crps_from_predictions</code> is compared with other implemented functions in the <code>properscoring</code>, <code>CRPS</code> and <code>pymc_marketing</code> libraries.</p> In\u00a0[3]: Copied! <pre># properscoring, CRPS, pymc-mar\n# ==============================================================================\nprint(f\"properscoring : {ps.crps_ensemble(y_true, y_pred)}\")\nprint(f\"CRPS          : {CRPS(y_pred, y_true).compute()[0]}\")\nprint(f\"pymc-marketing: {crps(y_true, y_pred.reshape(-1, 1))}\")\n</pre> # properscoring, CRPS, pymc-mar # ============================================================================== print(f\"properscoring : {ps.crps_ensemble(y_true, y_pred)}\") print(f\"CRPS          : {CRPS(y_pred, y_true).compute()[0]}\") print(f\"pymc-marketing: {crps(y_true, y_pred.reshape(-1, 1))}\") <pre>properscoring : 2.1767579851785785\nCRPS          : 2.1767579851785803\npymc-marketing: 2.1767579851785808\n</pre> <p>When forecasting multiple steps, the CRPS can be calculated for each step and then averaged to provide a summary measure of the model's performance.</p> In\u00a0[4]: Copied! <pre># CRPS for multiple steps\n# ==============================================================================\nrng = np.random.default_rng(123)\nn_steps = 40\nn_bootstraps = 100\npredictions = pd.DataFrame({\n    'y_true': rng.normal(100, 10, n_steps),\n    'y_pred': [rng.normal(5, 5, n_bootstraps) for _ in range(n_steps)]\n})\n\npredictions['crps_from_predictions'] = predictions.apply(lambda x: crps_from_predictions(x['y_true'], x['y_pred']), axis=1)\npredictions['properscoring'] = predictions.apply(lambda x: ps.crps_ensemble(x['y_true'], x['y_pred']), axis=1)\npredictions['CRPS'] = predictions.apply(lambda x: CRPS(x['y_pred'], x['y_true']).compute()[0], axis=1)\npredictions['pymc_marqueting'] = predictions.apply(lambda x: crps(x['y_true'], x['y_pred'].reshape(-1, 1)), axis=1)\ndisplay(predictions.head())\n\nassert np.allclose(predictions['properscoring'], predictions['CRPS'])\nassert np.allclose(predictions['properscoring'], predictions['pymc_marqueting'])\nassert np.allclose(predictions['crps_from_predictions'], predictions['properscoring'])\n</pre> # CRPS for multiple steps # ============================================================================== rng = np.random.default_rng(123) n_steps = 40 n_bootstraps = 100 predictions = pd.DataFrame({     'y_true': rng.normal(100, 10, n_steps),     'y_pred': [rng.normal(5, 5, n_bootstraps) for _ in range(n_steps)] })  predictions['crps_from_predictions'] = predictions.apply(lambda x: crps_from_predictions(x['y_true'], x['y_pred']), axis=1) predictions['properscoring'] = predictions.apply(lambda x: ps.crps_ensemble(x['y_true'], x['y_pred']), axis=1) predictions['CRPS'] = predictions.apply(lambda x: CRPS(x['y_pred'], x['y_true']).compute()[0], axis=1) predictions['pymc_marqueting'] = predictions.apply(lambda x: crps(x['y_true'], x['y_pred'].reshape(-1, 1)), axis=1) display(predictions.head())  assert np.allclose(predictions['properscoring'], predictions['CRPS']) assert np.allclose(predictions['properscoring'], predictions['pymc_marqueting']) assert np.allclose(predictions['crps_from_predictions'], predictions['properscoring']) y_true y_pred crps_from_predictions properscoring CRPS pymc_marqueting 0 90.108786 [8.640637789445714, -1.3080015845984816, 12.14... 82.100538 82.100538 82.100538 82.100538 1 96.322133 [3.1678558225107745, 6.737363230274925, 5.6735... 88.410864 88.410864 88.410864 88.410864 2 112.879253 [6.709160916434245, 10.896201858093296, 0.9120... 105.460630 105.460630 105.460630 105.460630 3 101.939744 [14.521434285699028, 1.1295876122380442, 15.13... 94.259885 94.259885 94.259885 94.259885 4 109.202309 [13.80532539228533, 5.482203757147254, 6.46324... 101.908526 101.908526 101.908526 101.908526 In\u00a0[5]: Copied! <pre># Average CRPS\n# ==============================================================================\nmean_crps = predictions[['crps_from_predictions', 'properscoring', 'CRPS', 'pymc_marqueting']].mean()\nmean_crps\n</pre> # Average CRPS # ============================================================================== mean_crps = predictions[['crps_from_predictions', 'properscoring', 'CRPS', 'pymc_marqueting']].mean() mean_crps Out[5]: <pre>crps_from_predictions    93.974753\nproperscoring            93.974753\nCRPS                     93.974753\npymc_marqueting          93.974753\ndtype: float64</pre> <p>A quantile is a value that divides a data set into intervals, with a specific percentage of the data lying below it. Essentially, it is a point on the cumulative distribution function (CDF) that represents a threshold at which a given proportion of the data is less than or equal to that value.</p> <p>For example, the 40th percentile (or 0.4 quantile) is the value below which 40% of the data points fall. To find it, you would examine the CDF, which shows the cumulative proportion of the data as you move along the values of the data set. The 0.4 quantile corresponds to the point where the CDF reaches 0.4 on the vertical axis, indicating that 40% of the data lies at or below this value.</p> <p>This relationship between quantiles and the CDF means that, given several quantile values, it is possible to reconstruct the CDF. This is essential for calculating the Continuous Ranked Probability Score (CRPS), which measures the accuracy of probabilistic forecasts by comparing how well the predicted distribution matches the true value.</p> <p>Given a set of quantiles, their associated probabilities, and the true value, the CRPS can be calculated as follows:</p> <ol> <li><p>Construct the Empirical Cumulative Distribution Function (ECDF) using the quantiles and their corresponding probabilities.</p> </li> <li><p>Generate the CDF for the true value: Since the true value is a single point, its CDF is represented as a step function that jumps from 0 to 1 at the observed value.</p> </li> <li><p>Calculate the CRPS as the squared diference between the two curves.</p> </li> </ol> In\u00a0[6]: Copied! <pre># CRPS score from quantiles\n# ==============================================================================\ny_true = 3.0\n\nquantile_levels = np.array([\n    0.00, 0.025, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55,\n    0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.975, 1.00\n])\npred_quantiles = np.array([\n    0.1, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5,\n    8.0, 8.5, 9.0, 9.5, 10.0, 10.5, 11.0, 11.5\n])\n\ncrps_from_quantiles(y_true, pred_quantiles, quantile_levels)\n</pre> # CRPS score from quantiles # ============================================================================== y_true = 3.0  quantile_levels = np.array([     0.00, 0.025, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55,     0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.975, 1.00 ]) pred_quantiles = np.array([     0.1, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5,     8.0, 8.5, 9.0, 9.5, 10.0, 10.5, 11.0, 11.5 ])  crps_from_quantiles(y_true, pred_quantiles, quantile_levels) Out[6]: <pre>1.7339183102042313</pre> <p>Again, results are compared versus the <code>properscoring</code> package. In this case, a warapper function is used to calculate the CRPS score from the predicted quantiles using <code>crps_quadrature</code>.</p> In\u00a0[7]: Copied! <pre>def crps_from_quantiles_properscoring(y_true, predicted_quantiles, quantile_levels):\n    \"\"\"\n    Calculate the Continuous Ranked Probability Score (CRPS) for a given true value\n    and predicted quantiles using the function crps_quadrature from the properscoring\n    library.\n\n    Parameters\n    ----------\n    y_true : float\n        The true value of the random variable.\n    predicted_quantiles : np.array\n        The predicted quantile values.\n    quantile_levels : np.array\n        The quantile levels corresponding to the predicted quantiles.\n\n    Returns\n    -------\n    float\n        The CRPS score.\n    \"\"\"\n    if len(predicted_quantiles) != len(quantile_levels):\n        raise ValueError(\n            \"The number of predicted quantiles and quantile levels must be equal.\"\n        )\n    \n    # Ensure predicted_quantiles are sorted\n    sort_idx = np.argsort(predicted_quantiles)\n    predicted_quantiles = predicted_quantiles[sort_idx]\n    quantile_levels = quantile_levels[sort_idx]\n\n    def empirical_cdf(x):\n        # Interpolate between quantile levels and quantile values\n        cdf_func = interp1d(\n            predicted_quantiles,\n            quantile_levels,\n            bounds_error=False,\n            fill_value=(0.0, 1.0),\n        )\n        return cdf_func(x)\n\n    # Integration bounds\n    xmin = np.min(predicted_quantiles) * 0.9\n    xmax = np.max(predicted_quantiles) * 1.1\n\n    # Compute CRPS\n    crps = ps.crps_quadrature(np.array([y_true]), empirical_cdf, xmin, xmax)\n\n    return crps[0]\n\n\ncrps_from_quantiles_properscoring(y_true, pred_quantiles, quantile_levels)\n</pre> def crps_from_quantiles_properscoring(y_true, predicted_quantiles, quantile_levels):     \"\"\"     Calculate the Continuous Ranked Probability Score (CRPS) for a given true value     and predicted quantiles using the function crps_quadrature from the properscoring     library.      Parameters     ----------     y_true : float         The true value of the random variable.     predicted_quantiles : np.array         The predicted quantile values.     quantile_levels : np.array         The quantile levels corresponding to the predicted quantiles.      Returns     -------     float         The CRPS score.     \"\"\"     if len(predicted_quantiles) != len(quantile_levels):         raise ValueError(             \"The number of predicted quantiles and quantile levels must be equal.\"         )          # Ensure predicted_quantiles are sorted     sort_idx = np.argsort(predicted_quantiles)     predicted_quantiles = predicted_quantiles[sort_idx]     quantile_levels = quantile_levels[sort_idx]      def empirical_cdf(x):         # Interpolate between quantile levels and quantile values         cdf_func = interp1d(             predicted_quantiles,             quantile_levels,             bounds_error=False,             fill_value=(0.0, 1.0),         )         return cdf_func(x)      # Integration bounds     xmin = np.min(predicted_quantiles) * 0.9     xmax = np.max(predicted_quantiles) * 1.1      # Compute CRPS     crps = ps.crps_quadrature(np.array([y_true]), empirical_cdf, xmin, xmax)      return crps[0]   crps_from_quantiles_properscoring(y_true, pred_quantiles, quantile_levels) Out[7]: <pre>1.7342500001706027</pre> <p>Results are similar but not identical. This may be due to differences in the implementation of the CRPS calculation or the numerical methods used to approximate the integral. The skforecast team is working on validating the implementation of the CRPS function in the library.</p>"},{"location":"faq/probabilistic-forecasting-crps-score.html#continuous-ranked-probability-score-crps-in-probabilistic-forecasting","title":"Continuous Ranked Probability Score (CRPS) in probabilistic forecasting\u00b6","text":""},{"location":"faq/probabilistic-forecasting-crps-score.html#crps-and-skforecast","title":"CRPS and Skforecast\u00b6","text":""},{"location":"faq/probabilistic-forecasting-crps-score.html#crps-from-a-sample-of-predictions","title":"CRPS from a sample of predictions\u00b6","text":""},{"location":"faq/probabilistic-forecasting-crps-score.html#crps-from-quantiles","title":"CRPS from quantiles\u00b6","text":""},{"location":"faq/profiling-skforecast.html","title":"Profiling skforecast","text":"<p>This document shows the profiling of the main classes, methods and functions available in skforecast. Understanding the bottlenecks will help to:</p> <ul> <li>Use it more efficiently</li> <li>Improve the code for future releases</li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport platform\nimport psutil\n\nimport sklearn\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\n\nimport skforecast\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.direct import ForecasterDirect\nfrom skforecast.model_selection import grid_search_forecaster, backtesting_forecaster\n\n%load_ext pyinstrument\n</pre> # Libraries # ============================================================================== import time import numpy as np import pandas as pd import matplotlib.pyplot as plt import platform import psutil  import sklearn from sklearn.linear_model import LinearRegression, Ridge from sklearn.ensemble import HistGradientBoostingRegressor from lightgbm import LGBMRegressor  import skforecast from skforecast.recursive import ForecasterRecursive from skforecast.direct import ForecasterDirect from skforecast.model_selection import grid_search_forecaster, backtesting_forecaster  %load_ext pyinstrument In\u00a0[2]: Copied! <pre># Versions\n# ==============================================================================\nprint(f\"Python version      : {platform.python_version()}\")\nprint(f\"scikit-learn version: {sklearn.__version__}\")\nprint(f\"skforecast version  : {skforecast.__version__}\")\nprint(f\"pandas version      : {pd.__version__}\")\nprint(f\"numpy version       : {np.__version__}\")\nprint(f\"psutil version      : {psutil.__version__}\")\nprint(\"\")\n\n# System information\n# ==============================================================================\nprint(f\"Machine type: {platform.machine()}\")\nprint(f\"Processor type: {platform.processor()}\")\nprint(f\"Platform type: {platform.platform()}\")\nprint(f\"Operating system: {platform.system()}\")\nprint(f\"Operating system release: {platform.release()}\")\nprint(f\"Operating system version: {platform.version()}\")\nprint(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\")\nprint(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\")\n</pre> # Versions # ============================================================================== print(f\"Python version      : {platform.python_version()}\") print(f\"scikit-learn version: {sklearn.__version__}\") print(f\"skforecast version  : {skforecast.__version__}\") print(f\"pandas version      : {pd.__version__}\") print(f\"numpy version       : {np.__version__}\") print(f\"psutil version      : {psutil.__version__}\") print(\"\")  # System information # ============================================================================== print(f\"Machine type: {platform.machine()}\") print(f\"Processor type: {platform.processor()}\") print(f\"Platform type: {platform.platform()}\") print(f\"Operating system: {platform.system()}\") print(f\"Operating system release: {platform.release()}\") print(f\"Operating system version: {platform.version()}\") print(f\"Number of physical cores: {psutil.cpu_count(logical=False)}\") print(f\"Number of logical cores: {psutil.cpu_count(logical=True)}\") <pre>Python version      : 3.12.11\nscikit-learn version: 1.6.1\nskforecast version  : 0.17.0\npandas version      : 2.3.1\nnumpy version       : 2.1.3\npsutil version      : 7.0.0\n\nMachine type: AMD64\nProcessor type: Intel64 Family 6 Model 141 Stepping 1, GenuineIntel\nPlatform type: Windows-11-10.0.26100-SP0\nOperating system: Windows\nOperating system release: 11\nOperating system version: 10.0.26100\nNumber of physical cores: 8\nNumber of logical cores: 16\n</pre> <p>A time series of length 1000 with random values is created.</p> In\u00a0[3]: Copied! <pre># Data\n# ==============================================================================\nnp.random.seed(123)\nn = 1_000\ndata = pd.Series(data = np.random.normal(size=n))\n</pre> # Data # ============================================================================== np.random.seed(123) n = 1_000 data = pd.Series(data = np.random.normal(size=n)) <p>To isolate the training process of the regressor from the other parts of the code, a dummy regressor class is created. This dummy regressor has a fit method that does nothing, and a predict method that returns a constant value.</p> In\u00a0[4]: Copied! <pre>class DummyRegressor(LinearRegression):\n    \"\"\"\n    Dummy regressor with dummy fit and predict methods.\n    \"\"\"\n    \n    def fit(self, X, y):\n        pass\n\n    def predict(self, y):\n        predictions = np.ones(shape = len(y))\n        return predictions\n</pre> class DummyRegressor(LinearRegression):     \"\"\"     Dummy regressor with dummy fit and predict methods.     \"\"\"          def fit(self, X, y):         pass      def predict(self, y):         predictions = np.ones(shape = len(y))         return predictions In\u00a0[5]: Copied! <pre>%%pyinstrument\n\nforecaster = ForecasterRecursive(\n                 regressor = DummyRegressor(),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> %%pyinstrument  forecaster = ForecasterRecursive(                  regressor = DummyRegressor(),                  lags      = 24              )  forecaster.fit(y=data) <p>Almost all of the time spent by <code>fit</code> is required by the <code>create_train_X_y</code> method.</p> In\u00a0[6]: Copied! <pre>%%pyinstrument\n\nforecaster = ForecasterRecursive(\n                 regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> %%pyinstrument  forecaster = ForecasterRecursive(                  regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),                  lags      = 24              )  forecaster.fit(y=data) <p>When training a forecaster with a real machine learning regressor, the time spent by <code>create_train_X_y</code> is negligible compared to the time needed by the <code>fit</code> method of the regressor. Therefore, improving the speed of <code>create_train_X_y</code> will not have much impact.</p> <p>Understand how the <code>create_train_X_y</code> method is influenced by the length of the series and the number of lags.</p> In\u00a0[7]: Copied! <pre># Profiling `create_train_X_y` for different length of series and number of lags\n# ======================================================================================\nseries_length = np.linspace(1000, 1000000, num=5, dtype=int)\nn_lags = [5, 10, 50, 100, 200]\nresults = {}\n\nfor lags in n_lags:\n    execution_time = []\n    forecaster = ForecasterRecursive(\n                     regressor = DummyRegressor(),\n                     lags      = lags\n                 )\n\n    for n in series_length:\n        y = pd.Series(data = np.random.normal(size=n))\n        tic = time.perf_counter()\n        _ = forecaster.create_train_X_y(y=y)\n        toc = time.perf_counter()\n        execution_time.append(toc - tic)\n\n    results[lags] = execution_time\n\nresults = pd.DataFrame(\n              data = results,\n              index = series_length\n          )\n\nresults\n</pre> # Profiling `create_train_X_y` for different length of series and number of lags # ====================================================================================== series_length = np.linspace(1000, 1000000, num=5, dtype=int) n_lags = [5, 10, 50, 100, 200] results = {}  for lags in n_lags:     execution_time = []     forecaster = ForecasterRecursive(                      regressor = DummyRegressor(),                      lags      = lags                  )      for n in series_length:         y = pd.Series(data = np.random.normal(size=n))         tic = time.perf_counter()         _ = forecaster.create_train_X_y(y=y)         toc = time.perf_counter()         execution_time.append(toc - tic)      results[lags] = execution_time  results = pd.DataFrame(               data = results,               index = series_length           )  results Out[7]: 5 10 50 100 200 1000 0.000817 0.003778 0.006570 0.027220 0.083118 250750 0.005524 0.008127 0.030577 0.073946 0.120689 500500 0.009436 0.015381 0.070102 0.146196 0.289218 750250 0.013998 0.024984 0.104574 0.335631 0.616191 1000000 0.019030 0.036879 0.181628 0.612827 0.798728 In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots(figsize=(7, 4))\nresults.plot(ax=ax, marker='.')\nax.set_xlabel('length of series')\nax.set_ylabel('time (seconds)')\nax.set_title('Profiling create_train_X_y()')\nax.legend(title='number of lags');\n</pre> fig, ax = plt.subplots(figsize=(7, 4)) results.plot(ax=ax, marker='.') ax.set_xlabel('length of series') ax.set_ylabel('time (seconds)') ax.set_title('Profiling create_train_X_y()') ax.legend(title='number of lags'); In\u00a0[9]: Copied! <pre>forecaster = ForecasterRecursive(\n                 regressor = DummyRegressor(),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> forecaster = ForecasterRecursive(                  regressor = DummyRegressor(),                  lags      = 24              )  forecaster.fit(y=data) In\u00a0[10]: Copied! <pre>%%pyinstrument\n\n_ = forecaster.predict(steps=1000)\n</pre> %%pyinstrument  _ = forecaster.predict(steps=1000) In\u00a0[11]: Copied! <pre>forecaster = ForecasterRecursive(\n                 regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),\n                 lags      = 24\n             )\n\nforecaster.fit(y=data)\n</pre> forecaster = ForecasterRecursive(                  regressor = HistGradientBoostingRegressor(max_iter=10, random_state=123),                  lags      = 24              )  forecaster.fit(y=data) In\u00a0[12]: Copied! <pre>%%pyinstrument\n\n_ = forecaster.predict(steps=1000)\n</pre> %%pyinstrument  _ = forecaster.predict(steps=1000) <p>Inside the <code>predict</code> method, the <code>append</code> action is the most expensive but, similar to what happen with <code>fit</code>, it is negligible compared to the time need by the <code>predict</code> method of the regressor.</p>"},{"location":"faq/profiling-skforecast.html#profiling-skforecast","title":"Profiling skforecast\u00b6","text":""},{"location":"faq/profiling-skforecast.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"faq/profiling-skforecast.html#dummy-regressor","title":"Dummy regressor\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-fit","title":"Profiling fit\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-create_train_x_y","title":"Profiling create_train_X_y\u00b6","text":""},{"location":"faq/profiling-skforecast.html#profiling-predict","title":"Profiling predict\u00b6","text":""},{"location":"faq/table-of-contents.html","title":"Frequently Asked Questions and forecasting tips","text":"<p>Thank you for choosing skforecast and visiting our Frequently Asked Questions (FAQ) page. Here, we aim to provide solutions to commonly encountered issues on our Github repository. If your question is not answered on this page, we encourage you to create a new issue on our Github so that it can be addressed, and other users can also benefit from it. Additionally, we have included some forecasting tips to help you get the most out of skforecast.</p> <p>General Forecasting Tips</p> <ul> <li>Avoid negative predictions when forecasting</li> <li>Forecasting time series with missing values</li> <li>Forecasting time series with trend: differentiation</li> <li>Forecasting time series with trend: tree based models</li> <li>Forecasting with delayed historical data</li> <li>Backtesting vs One-step-ahead</li> <li>Continuous Ranked Probability Score (CRPS)</li> <li>Calibration of probabilistic forecasting intervals</li> <li>Avoid data leakage in pre-trained forecasting models</li> </ul> <p>Feature Engineering</p> <ul> <li>Cyclical features in time series</li> <li>Time series aggregation</li> </ul> <p>Performance Optimization</p> <ul> <li>Parallelization in skforecast</li> <li>Profiling skforecast</li> </ul>"},{"location":"faq/time-series-aggregation.html","title":"Time series aggregation","text":"<p>Time series aggregation involves summarizing or transforming data over specific time intervals. Two of the most common use cases are</p> <ul> <li><p>Aggregating data from one frequency to another. For example, converting hourly data to daily data.</p> </li> <li><p>Aggregating data across a sliding window. For example, calculating a rolling average over the last 7 days.</p> </li> </ul> <p>These aggregations not only greatly reduce the total volume of data, but also help you find interesting features for your model faster.</p> <p>Pandas provides an easy and efficient way to aggregate data from time series. This document shows how to use the <code>resampling</code> and <code>rolling</code> methods to aggregate data, with special emphasis on how to avoid data leakage, which is a common mistake when aggregating time series.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== import pandas as pd from skforecast.datasets import fetch_dataset In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name='vic_electricity')\ndata = data[['Demand', 'Temperature', 'Holiday']]\ndata.head(5)\n</pre> # Download data # ============================================================================== data = fetch_dataset(name='vic_electricity') data = data[['Demand', 'Temperature', 'Holiday']] data.head(5) <pre>vic_electricity\n---------------\nHalf-hourly electricity demand for Victoria, Australia\nO'Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022).tsibbledata: Diverse\nDatasets for 'tsibble'. https://tsibbledata.tidyverts.org/,\nhttps://github.com/tidyverts/tsibbledata/.\nhttps://tsibbledata.tidyverts.org/reference/vic_elec.html\nShape of the dataset: (52608, 4)\n</pre> Out[2]: Demand Temperature Holiday Time 2011-12-31 13:00:00 4382.825174 21.40 True 2011-12-31 13:30:00 4263.365526 21.05 True 2011-12-31 14:00:00 4048.966046 20.70 True 2011-12-31 14:30:00 3877.563330 20.55 True 2011-12-31 15:00:00 4036.229746 20.40 True <p>This dataset contains the electricity demand in Victoria (Australia) at half-hourly frequency.</p> In\u00a0[3]: Copied! <pre># Index Frequency\n# ==============================================================================\nprint(f\"Frequency: {data.index.freq}\")\n</pre> # Index Frequency # ============================================================================== print(f\"Frequency: {data.index.freq}\") <pre>Frequency: &lt;30 * Minutes&gt;\n</pre> <p>To change the frequency of a time series, use the <code>resample</code> method. This method allows you to specify a frequency and an aggregation function. It works similarly to the <code>groupby</code> method, but it works with time series indices.</p> <p>When aggregating data, it is very important to use the <code>closed</code> and <code>label</code> arguments correctly. This avoids introducing future information into the training (data leakage).</p> <ul> <li><p>The <code>closed</code> argument specifies whether the interval is closed on the left-side, right-side, both or neither.</p> </li> <li><p>The <code>label</code> argument specifies whether the result is labeled with the beginning or the end of the interval.</p> </li> </ul> <p>Suppose that values are available for 10:10, 10:30, 10:45, 11:00, 11:12, and 11:30. To obtain the hourly average, the value assigned to 11:00 must be calculated using the values for 10:10, 10:30, and 10:45; and the value assigned to 12:00 must be calculated using the value for 11:00, 11:12 and 11:30. The 11:00 average does not include the 11:00 point value because in reality the value is not available at that exact time.</p> <p>In this case, the correct arguments are <code>closed='left'</code> and <code>label='right'</code>.</p> <p> Diagram of data aggregation using the resample method without including future information. </p> <p>For example, the code in the next cell converts the data from half-hourly to hourly frequency. Since there are multiple columns, an aggregation function must be specified for each column. In this case, the <code>sum</code> is calculated for the <code>Demand</code> column and the <code>average</code> is calculated for the rest.</p> In\u00a0[4]: Copied! <pre># Aggregate data from 30 minutes to 1 hour\n# ==============================================================================\ndata = data.resample(rule='1h', closed='left', label ='right').agg({\n           'Demand': 'sum',\n           'Temperature': 'mean',\n           'Holiday': 'mean'\n       })\n\ndata\n</pre> # Aggregate data from 30 minutes to 1 hour # ============================================================================== data = data.resample(rule='1h', closed='left', label ='right').agg({            'Demand': 'sum',            'Temperature': 'mean',            'Holiday': 'mean'        })  data Out[4]: Demand Temperature Holiday Time 2011-12-31 14:00:00 8646.190700 21.225 1.0 2011-12-31 15:00:00 7926.529376 20.625 1.0 2011-12-31 16:00:00 7901.826990 20.325 1.0 2011-12-31 17:00:00 7255.721350 19.850 1.0 2011-12-31 18:00:00 6792.503352 19.025 1.0 ... ... ... ... 2014-12-31 09:00:00 8139.251100 21.600 0.0 2014-12-31 10:00:00 7818.461408 20.300 0.0 2014-12-31 11:00:00 7801.201802 19.650 0.0 2014-12-31 12:00:00 7516.472988 18.100 0.0 2014-12-31 13:00:00 7571.301440 17.200 0.0 <p>26304 rows \u00d7 3 columns</p> <p>Rolling window aggregation is used to calculate statistics over a sliding window of time. For example, the 24h rolling average is the average of the last 24 hours of data. As with the <code>resample</code> method, it is very important to use the <code>closed='left'</code> and <code>center=False</code> arguments correctly to avoid introducing future information into the training (data leakage).</p> In\u00a0[5]: Copied! <pre># Rolling mean for 4 hours\n# ==============================================================================\ndata.rolling(window=4, min_periods=4, closed='left', center=False).mean()\n</pre> # Rolling mean for 4 hours # ============================================================================== data.rolling(window=4, min_periods=4, closed='left', center=False).mean() Out[5]: Demand Temperature Holiday Time 2011-12-31 14:00:00 NaN NaN NaN 2011-12-31 15:00:00 NaN NaN NaN 2011-12-31 16:00:00 NaN NaN NaN 2011-12-31 17:00:00 NaN NaN NaN 2011-12-31 18:00:00 7932.567104 20.50625 1.0 ... ... ... ... 2014-12-31 09:00:00 8490.517461 23.71250 0.0 2014-12-31 10:00:00 8482.825404 23.41250 0.0 2014-12-31 11:00:00 8314.896216 22.67500 0.0 2014-12-31 12:00:00 8076.417548 21.30000 0.0 2014-12-31 13:00:00 7818.846825 19.91250 0.0 <p>26304 rows \u00d7 3 columns</p> <p>The average values for <code>2011-12-31 18:00:00</code> are calculated from the values of the previous 4 hours (from 14:00:00 to 17:00:00).</p> In\u00a0[6]: Copied! <pre># '2011-12-31 18:00:00' mean\n# ==============================================================================\ndata.iloc[0:4, :].mean()\n</pre> # '2011-12-31 18:00:00' mean # ============================================================================== data.iloc[0:4, :].mean() Out[6]: <pre>Demand         7932.567104\nTemperature      20.506250\nHoliday           1.000000\ndtype: float64</pre> <p> \u26a0 Warning </p> <p>When transforming time series data, such as aggregating, it is very important to avoid data leakage, which means using information from the future to calculate the current value.</p>"},{"location":"faq/time-series-aggregation.html#time-series-aggregation","title":"Time series aggregation\u00b6","text":""},{"location":"faq/time-series-aggregation.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"faq/time-series-aggregation.html#change-frequency-resample","title":"Change frequency (resample)\u00b6","text":""},{"location":"faq/time-series-aggregation.html#rolling-window-aggregation","title":"Rolling window aggregation\u00b6","text":""},{"location":"introduction-forecasting/introduction-forecasting.html","title":"Introduction to forecasting","text":""},{"location":"introduction-forecasting/introduction-forecasting.html#time-series-and-forecasting","title":"Time series and forecasting","text":"<p>A time series is a sequence of data arranged chronologically and spaced at equal or irregular intervals. The forecasting process consists of predicting the future value of a time series, either by modeling the series solely based on its past behavior (autoregressive) or by incorporating other external variables.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#machine-learning-for-forecasting","title":"Machine learning for forecasting","text":"<p>To apply machine learning models to forecasting problems, the time series needs to be transformed into a matrix where each value is associated with a specific time window (known as lags) that precedes it. In the context of time series, a lag with respect to a time step t is defined as the value of the series at previous time steps. For instance, lag 1 represents the value at time step t-1, while lag m represents the value at time step t-m.</p> <p>This transformation is essential for machine learning models to capture the dependencies and patterns that exist between past and future values in a time series. By using lags as input features, machine learning models can learn from the past and make predictions about future values. The number of lags used as input features in the matrix is an important hyperparameter that needs to be carefully tuned to obtain the best performance of the model.</p> <p> Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix </p> <p>This type of transformation also allows to include additional variables.</p> <p> Time series transformation including an exogenous variable </p> <p>Once data have been rearranged into the new shape, any regression model can be trained to predict the next value (step) of the series. During model training, every row is considered a separate data instance, where values at lags 1, 2, ... p are considered predictors for the target quantity of the time series at time step p+1. </p> <p> Diagram of training a machine learning model with time series data </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#single-step-forecasting","title":"Single-step forecasting","text":"<p>Single-step prediction is used when the goal is to predict only the next value of the series.</p> <p> Diagram of single-step forecasting </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multi-step-forecasting","title":"Multi-step forecasting","text":"<p>When working with time series, it is seldom needed to predict only the next element in the series (t+1). Instead, the most common goal is to predict a whole future interval (t+1, ..., t+n)  or a far point in time (t+n). Several strategies allow generating this type of prediction.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#recursive-multi-step-forecasting","title":"Recursive multi-step forecasting","text":"<p>Since the value t(n-1) is required to predict t(n), and t(n-1) is unknown, a recursive process is applied in which, each new prediction, is based on the previous one. This process is known as recursive forecasting or recursive multi-step forecasting and can be easily generated with the <code>ForecasterRecursive</code> class.</p> <p> Diagram of recursive multi-step forecasting </p> <p> Recursive forecasting </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#direct-multi-step-forecasting","title":"Direct multi-step forecasting","text":"<p>Direct multi-step forecasting consists of training a different model for each step of the forecast horizon. For example, to predict the next 5 values of a time series, 5 different models are trained, one for each step. As a result, the predictions are independent of each other. This entire process is automated in the <code>ForecasterDirect</code> class. </p> <p> Diagram of direct multi-step forecasting </p> <p> Direct forecasting </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#multiple-output-forecasting","title":"Multiple output forecasting","text":"<p>Some machine learning models, such as long short-term memory (LSTM) neural networks, can predict multiple values of a sequence simultaneously (one-shot). This strategy implemented in the <code>ForecasterRnn</code> class.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#global-forecasting-models","title":"Global forecasting models","text":"<p>Univariate time series forecasting focuses on modeling a single time series as a linear or nonlinear function of its own past values (lags), using historical observations to predict future ones. </p> <p>Global forecasting builds a single predictive model that considers all time series simultaneously. This approach seeks to learn the shared patterns that underlie the different series, helping to reduce the influence of noise present in individual time series. It is computationally efficient, easier to maintain, and often yields more robust generalization across series. Two main strategies are used to implement global forecasting models.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#independent-multi-series-forecasting","title":"Independent Multi-Series Forecasting","text":"<p>In independent multi-series forecasting, a single model is trained using all time series, but each series is treated independently\u2014past values of one series are not used to predict another. Modeling them together is still beneficial when the series share similar temporal dynamics. For example, sales of products A and B in the same store may not be directly related, but both are influenced by the same underlying store-level patterns.</p> <p> Transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-series context </p> <p>To predict the next n steps, the strategy of recursive multi-step forecasting is applied</p> <p> Diagram of recursive forecasting with multiple independent time series </p> <p>The <code>ForecasterRecursiveMultiSeries</code> class cover this process. </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#dependent-multi-series-forecasting-multivariate-time-series","title":"Dependent Multi-Series Forecasting (multivariate time series)","text":"<p>In dependent multi-series forecasting (also known as multivariate time series forecasting), all series are modeled jointly under the assumption that each series depends not only on its own past values, but also on the past values of the other series. The forecaster is expected to learn both the individual dynamics of each series and the relationships between them.</p> <p>A typical example is the set of sensor readings (such as flow, temperature, and pressure) collected from an industrial machine like a compressor, where the variables influence each other over time.</p> <p> Transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-variate-series context </p> <p>Using the <code>ForecasterDirectMultiVariate</code> class, it is possible to easily build machine learning models for multivariate forecasting.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#forecasters","title":"Forecasters","text":"<p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>The skforecast library offers a variety of forecaster types, each tailored to specific requirements such as single or multiple time series, direct or recursive strategies, or custom predictors. Regardless of the specific forecaster type, all instances share the same API.</p> Forecaster Single series Multiple series Recursive strategy Direct strategy Probabilistic prediction Time series differentiation Exogenous features Window features ForecasterRecursive \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterDirect \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterRecursiveMultiSeries \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterDirectMultiVariate \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterRNN \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterSarimax \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f <p>To understand what can be done when initializing a forecaster with skforecast visit Forecaster parameters and Forecaster attributes.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#exogenous-variables-features","title":"Exogenous variables (features)","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process. The inclusion of exogenous variables can enhance the accuracy of forecasts.</p> <p>In skforecast, exogenous variables can be easily included as predictors in all forecasting models. To ensure that their effects are accurately accounted for, it is crucial to include these variables during both the training and prediction phases. This will help to optimize the accuracy of forecasts and provide more reliable predictions.</p> <p> Time series transformation including an exogenous variable </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-forecasting-models","title":"Backtesting forecasting models","text":"<p>In time series forecasting, the process of backtesting consists of evaluating the performance of a predictive model by applying it retrospectively to historical data. Therefore, it is a special type of cross-validation applied to the previous period(s).</p> <p>The purpose of backtesting is to evaluate the accuracy and effectiveness of a model and identify any potential issues or areas of improvement. By testing the model on historical data, one can assess how well it performs on data that it has not seen before. This is an important step in the modeling process, as it helps to ensure that the model is robust and reliable.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data.</p> <p>Overall, backtesting is an essential step in the development of a time series forecasting model. By rigorously testing the model on historical data, one can improve its accuracy and ensure that it is effective at predicting future values of the time series.</p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-without-refit","title":"Backtesting without refit","text":"<p>Backtesting without refit is a strategy where the model is trained only once and used sequentially without updating it, following the temporal order of the data. This approach is advantageous as it is much faster than other methods that require retraining the model each time. However, the model may lose its predictive power over time as it does not incorporate the latest information available.</p> <p> </p> <p> Backtesting without refit </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-refit-and-increasing-training-size-fixed-origin","title":"Backtesting with refit and increasing training size (fixed origin)","text":"<p>In this approach, the model is trained before making predictions each time, and all available data up to that point is used in the training process. This differs from standard cross-validation, where the data is randomly distributed between training and validation sets.</p> <p>Instead of randomizing the data, this backtesting sequentially increases the size of the training set while maintaining the temporal order of the data. By doing this, the model can be tested on progressively larger amounts of historical data, providing a more accurate assessment of its predictive capabilities.</p> <p> </p> <p> Backtesting with refit and increasing training size (fixed origin) </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-refit-and-fixed-training-size-rolling-origin","title":"Backtesting with refit and fixed training size (rolling origin)","text":"<p>In this approach, the model is trained using a fixed window of past observations, and the testing is performed on a rolling basis, where the training window is moved forward in time. The size of the training window is kept constant, allowing for the model to be tested on different sections of the data. This technique is particularly useful when there is a limited amount of data available, or when the data is non-stationary, and the model's performance may vary over time. Is also known as time series cross-validation or walk-forward validation.</p> <p> </p> <p> Backtesting with refit and fixed training size (rolling origin) </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-with-intermittent-refit","title":"Backtesting with intermittent refit","text":"<p>The model is retrained every n iterations, a method often used when the model retraining is limited to certain time intervals, such as weekly, but a different time window, such as a day, needs to be used for prediction.</p> <p>This refit strategy can be implemented using either a fixed or rolling origin, providing flexibility in adapting the model to new data.</p> <p> Backtesting with intermittent refit </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#backtesting-including-gap","title":"Backtesting including gap","text":"<p>This approach introduces a time gap between the training and test sets, replicating a scenario where predictions cannot be made immediately after the end of the training data.</p> <p>For example, consider the goal of predicting the 24 hours of day D+1, but the predictions need to be made at 11:00 to allow sufficient flexibility. At 11:00 on day D, the task is to forecast hours [12 - 23] of the same day and hours [0 - 23] of day D+1. Thus, a total of 36 hours into the future must be predicted, with only the last 24 hours to be stored.</p> <p> Backtesting with refit and gap </p>"},{"location":"introduction-forecasting/introduction-forecasting.html#which-strategy-should-i-use","title":"Which strategy should I use?","text":"<p>To ensure an accurate evaluation of your model and gain confidence in its predictive performance on new data, it is critical to employ an appropriate backtesting strategy.</p> <p>To determine an appropriate strategy, factors such as use case characteristics, available computing resources, and time intervals between predictions must be considered. These factors determine when the model should be refitted and the prediction horizon that should be used.</p> <ul> <li> <p>Prediction horizon: Suppose you need to predict the users of an App every Monday for the entire week. In this case, each iteration would be a seven-step prediction representing the seven days of the week.</p> </li> <li> <p>Refit strategy: Continuing with the example above, at the end of the week you need to decide whether or not to update the model. Training the model with additional data can improve its predictive ability, but it requires more time and computational resources, which may not always be readily available. A reasonable approach is to retrain the model when the error metric shows a consistent upward trend. This behavior can be effectively simulated using the backtesting framework.</p> </li> </ul> <p>As an example, backtesting is performed using the data from this skforecast example. The same backtest is run with different <code>refit</code> strategies: <code>False</code> (no refit between predictions), refit every <code>30</code> days, every <code>14</code> days, every <code>7</code> days and <code>True</code> (refit after every predictions). Notice that the significant increase in time does not correspond to a decrease in error.</p> refit value execution time (s) metric False 1.4 262.5 30 4.0 263.4 14 6.3 262.5 7 11.1 261.4 True 69.1 258.3 <p>For a code example illustrating the backtesting process, refer to the Backtesting user guide.</p>"},{"location":"more/about-skforecast.html","title":"About skforecast","text":""},{"location":"more/about-skforecast.html#history","title":"History","text":"<p>skforecast was born from the need to make forecasting easy, accessible, and effective for everyone. Since its inception in 2021, our aim has been to bridge the gap between machine learning and practical time series forecasting. Thanks to our amazing community, the library continues to grow, evolve, and help people worldwide forecast better.</p>"},{"location":"more/about-skforecast.html#governance","title":"Governance","text":"<p>skforecast is an open-source project maintained by its core development team, with guidance and valuable contributions from the community. We strongly believe in openness, transparency, and collaboration. Everyone is encouraged to participate through issues, discussions, and pull requests on GitHub.</p>"},{"location":"more/about-skforecast.html#core-development-team","title":"Core Development Team","text":"<p>Meet the core developers behind skforecast.</p> Joaqu\u00edn Amat Rodrigo @JoaquinAmatRodrigo LinkedIn Javier Escobar Ortiz @JavierEscobarOrtiz LinkedIn"},{"location":"more/about-skforecast.html#main-contributors","title":"Main Contributors","text":"<p>Special mention to the main contributors who have significantly impacted the library development:</p> Name GitHub Fernando Carazo Melo @FernandoCarazoMelo"},{"location":"more/about-skforecast.html#contributors","title":"Contributors","text":"Get Involved <p>We value your input! Here are a few ways you can participate:</p> <ul> <li>Report bugs and suggest new features on our GitHub Issues page.</li> <li>Contribute to the project by submitting code, adding new features, or improving the documentation.</li> <li>Share your feedback on LinkedIn to help spread the word about skforecast!</li> </ul> <p>Together, we can make time series forecasting accessible to everyone.</p> Name GitHub Josh Wong @josh-wong Edgar Bahilo Rodr\u00edguez @edgBR Mwainwright @syndct Kishan Manani @KishManani Sergio Quijano @Sergio-Quijano-Stratesys Fernando da Silva @schoulten Lillian Jensen, MPH @tyg3rr Ivan Liu @IvanLiuTW Ignacio Moya @imMoya Pablo Rodr\u00edguez P\u00e9rez @pablorodriper g-rubio @g-rubio Gin\u00e9s Meca @GinesMeca <p> Thank you for helping us make skforecast better! \ud83c\udf89</p>"},{"location":"more/about-skforecast.html#citing-skforecast","title":"Citing skforecast","text":"<p>If you use skforecast for a scientific publication, we would appreciate citations to the published software.</p> <p>Zenodo</p> <pre><code>Amat Rodrigo, Joaquin, &amp; Escobar Ortiz, Javier. (2025). skforecast (v0.17.0). Zenodo. https://doi.org/10.5281/zenodo.8382788\n</code></pre> <p>APA: <pre><code>Amat Rodrigo, J., &amp; Escobar Ortiz, J. (2025). skforecast (Version 0.17.0) [Computer software]. https://doi.org/10.5281/zenodo.8382788\n</code></pre></p> <p>BibTeX: <pre><code>@software{skforecast,\n  author  = {Amat Rodrigo, Joaquin and Escobar Ortiz, Javier},\n  title   = {skforecast},\n  version = {0.17.0},\n  month   = {8},\n  year    = {2025},\n  license = {BSD-3-Clause},\n  url     = {https://skforecast.org/},\n  doi     = {10.5281/zenodo.8382788}\n}\n</code></pre></p> <p>View the citation file.</p>"},{"location":"more/about-skforecast.html#license","title":"License","text":"<p>Skforecast software: BSD-3-Clause License</p> <p>Skforecast documentation: CC BY-NC-SA 4.0</p> <p>Trademark: The trademark skforecast is registered with the European Union Intellectual Property Office (EUIPO) under the application number 019109684. Unauthorized use of this trademark, its logo, or any associated visual identity elements is strictly prohibited without the express consent of the owner.</p>"},{"location":"more/about-skforecast.html#artwork","title":"Artwork","text":"<p>Official skforecast logo for presentations, blog posts, or any material mentioning or promoting skforecast. Created by Thelma Alfonso Arias.</p> <p> </p> <p>The trademark skforecast is registered with the European Union Intellectual Property Office (EUIPO) under the application number 019109684. Unauthorized use of this trademark, its logo, or any associated visual identity elements is strictly prohibited without the express consent of the owner.</p>"},{"location":"more/consulting.html","title":"Consulting &amp; Professional services","text":"<p>If you need professional help with Machine Learning, our core development team offer expert consulting services in areas such as:</p> <ul> <li>Forecasting: Gain insights into future trends with accurate time-series modeling and demand prediction tailored to your business needs.</li> <li>General Machine Learning: From classification and regression to anomaly detection, we design and implement models that drive data-driven decision-making.</li> <li>Advisory &amp; Strategy: Not sure where to start? We help define the right AI approach, ensuring your data strategy aligns with your business goals.</li> <li>Workshops &amp; Training: Empower your team with hands-on training and customized workshops designed to enhance their ML skills and knowledge.</li> </ul> <p>Interested in working with us? We'd love to hear from you! Let\u2019s build something great together! \ud83d\ude80</p> Joaqu\u00edn Amat Rodrigo j.amatrodrigo@gmail.com LinkedIn Javier Escobar Ortiz javier.escobar.ortiz@gmail.com LinkedIn"},{"location":"more/funding.html","title":"Funding","text":"<p>If you found skforecast useful, you can support us with a donation. Your contribution will help us continue developing, maintaining, and improving this project. Every contribution, no matter the size, makes a difference. Thank you for your support!</p> <p>\u2615 Buy us a coffee</p> <p> </p> <p>\u2764\ufe0f Become a GitHub Sponsor</p> <p> </p> <p>\ud83d\udcb3 Donate via PayPal </p> <p></p>"},{"location":"more/funding.html#sponsor-for-companies","title":"\ud83e\udd1d Sponsor for Companies","text":"<p>If you\u2019d like to sponsor skforecast in a unique way or explore potential partnerships, let\u2019s talk!</p> Joaqu\u00edn Amat Rodrigo j.amatrodrigo@gmail.com LinkedIn Javier Escobar Ortiz javier.escobar.ortiz@gmail.com LinkedIn"},{"location":"quick-start/forecaster-attributes.html","title":"Forecaster Attributes","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.datasets import load_demo_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.recursive import ForecasterRecursive\n</pre> # Libraries # ============================================================================== from sklearn.ensemble import RandomForestRegressor from skforecast.datasets import load_demo_dataset from skforecast.preprocessing import RollingFeatures from skforecast.recursive import ForecasterRecursive In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ny = load_demo_dataset()\n\n# Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = RandomForestRegressor(random_state=123),\n                 lags            = 5,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[7])\n             )\n\nforecaster.fit(y=y)\nforecaster\n</pre> # Download data # ============================================================================== y = load_demo_dataset()  # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = RandomForestRegressor(random_state=123),                  lags            = 5,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[7])              )  forecaster.fit(y=y) forecaster Out[2]: ForecasterRecursive General Information <ul> <li>Regressor: RandomForestRegressor</li> <li>Lags: [1 2 3 4 5]</li> <li>Window features: ['roll_mean_7']</li> <li>Window size: 7</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-11 16:07:40</li> <li>Last fit date: 2025-08-11 16:07:40</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[3]: Copied! <pre># List of attributes\n# ==============================================================================\nfor attribute, value in forecaster.__dict__.items():\n    print(attribute)\n</pre> # List of attributes # ============================================================================== for attribute, value in forecaster.__dict__.items():     print(attribute) <pre>regressor\ntransformer_y\ntransformer_exog\nweight_func\nsource_code_weight_func\ndifferentiation\ndifferentiation_max\ndifferentiator\nlast_window_\nindex_type_\nindex_freq_\ntraining_range_\nseries_name_in_\nexog_in_\nexog_names_in_\nexog_type_in_\nexog_dtypes_in_\nexog_dtypes_out_\nX_train_window_features_names_out_\nX_train_exog_names_out_\nX_train_features_names_out_\nin_sample_residuals_\nout_sample_residuals_\nin_sample_residuals_by_bin_\nout_sample_residuals_by_bin_\ncreation_date\nis_fitted\nfit_date\nskforecast_version\npython_version\nforecaster_id\n_probabilistic_mode\nlags\nlags_names\nmax_lag\nwindow_features\nwindow_features_names\nmax_size_window_features\nwindow_size\nwindow_features_class_names\nfit_kwargs\nbinner_kwargs\nbinner\nbinner_intervals_\n</pre> In\u00a0[4]: Copied! <pre># Forecaster regressor\n# ==============================================================================\nforecaster.regressor\n</pre> # Forecaster regressor # ============================================================================== forecaster.regressor Out[4]: <pre>RandomForestRegressor(random_state=123)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted Parameters n_estimators\u00a0 100 criterion\u00a0 'squared_error' max_depth\u00a0 None min_samples_split\u00a0 2 min_samples_leaf\u00a0 1 min_weight_fraction_leaf\u00a0 0.0 max_features\u00a0 1.0 max_leaf_nodes\u00a0 None min_impurity_decrease\u00a0 0.0 bootstrap\u00a0 True oob_score\u00a0 False n_jobs\u00a0 None random_state\u00a0 123 verbose\u00a0 0 warm_start\u00a0 False ccp_alpha\u00a0 0.0 max_samples\u00a0 None monotonic_cst\u00a0 None In\u00a0[5]: Copied! <pre># Show regressor parameters\n# ==============================================================================\nforecaster.regressor.get_params(deep=True)\n</pre> # Show regressor parameters # ============================================================================== forecaster.regressor.get_params(deep=True) Out[5]: <pre>{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'criterion': 'squared_error',\n 'max_depth': None,\n 'max_features': 1.0,\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': 123,\n 'verbose': 0,\n 'warm_start': False}</pre> <p> \u270e Note </p> <p>In the forecasters that follows a Direct Strategy (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>), one instance of the regressor is trained for each step. All of them are stored in <code>self.regressors_</code></p> In\u00a0[6]: Copied! <pre># Forecaster lags\n# ==============================================================================\nforecaster.lags\n</pre> # Forecaster lags # ============================================================================== forecaster.lags Out[6]: <pre>array([1, 2, 3, 4, 5])</pre> In\u00a0[7]: Copied! <pre># Lags information\n# ==============================================================================\nprint(\"Lags names : \", forecaster.lags_names)\nprint(\"Max lag    : \", forecaster.max_lag)\n</pre> # Lags information # ============================================================================== print(\"Lags names : \", forecaster.lags_names) print(\"Max lag    : \", forecaster.max_lag) <pre>Lags names :  ['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5']\nMax lag    :  5\n</pre> In\u00a0[8]: Copied! <pre># Forecaster window features\n# ==============================================================================\nforecaster.window_features\n</pre> # Forecaster window features # ============================================================================== forecaster.window_features Out[8]: <pre>[RollingFeatures(\n     stats           = ['mean'],\n     window_sizes    = [7],\n     Max window size = 7,\n     min_periods     = [7],\n     features_names  = ['roll_mean_7'],\n     fillna          = None\n     kwargs_stats    = {'ewm': {'alpha': 0.3}},\n )]</pre> In\u00a0[9]: Copied! <pre># Window features information\n# ==============================================================================\nprint(\"Window features names   : \", forecaster.window_features_names)\nprint(\"Max window size wf      : \", forecaster.max_size_window_features)\nprint(\"Window features classes : \", forecaster.window_features_class_names)\n</pre> # Window features information # ============================================================================== print(\"Window features names   : \", forecaster.window_features_names) print(\"Max window size wf      : \", forecaster.max_size_window_features) print(\"Window features classes : \", forecaster.window_features_class_names) <pre>Window features names   :  ['roll_mean_7']\nMax window size wf      :  7\nWindow features classes :  ['RollingFeatures']\n</pre> In\u00a0[10]: Copied! <pre># Forecaster window size\n# ==============================================================================\nprint(\"Max lag            : \", forecaster.max_lag)\nprint(\"Max window size wf : \", forecaster.max_size_window_features)\nprint(\"Window size        : \", forecaster.window_size)\n</pre> # Forecaster window size # ============================================================================== print(\"Max lag            : \", forecaster.max_lag) print(\"Max window size wf : \", forecaster.max_size_window_features) print(\"Window size        : \", forecaster.window_size) <pre>Max lag            :  5\nMax window size wf :  7\nWindow size        :  7\n</pre> In\u00a0[11]: Copied! <pre># Forecaster last window\n# ==============================================================================\nforecaster.last_window_\n</pre> # Forecaster last window # ============================================================================== forecaster.last_window_ Out[11]: y datetime 2007-12-01 1.176589 2008-01-01 1.219941 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 <p> \ud83d\udca1 Tip </p> <p>Learn how to get your forecasters into production and get the most out of them with <code>last_window</code>. Using forecasting models in production.</p> <p> \u270e Note </p> <p>In the forecasters that follows a Direct Strategy (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>) and in the  Global Forecasting Models: Independent multi-series forecasting (<code>ForecasterRecursiveMultiSeries</code>) this parameter is a <code>dict</code> containing the residuals for each regressor/serie.</p> In\u00a0[12]: Copied! <pre># Forecaster in-sample residuals\n# ==============================================================================\nforecaster.fit(y=y, store_in_sample_residuals=True)\n\nprint(\"Length:\", len(forecaster.in_sample_residuals_))\nforecaster.in_sample_residuals_[:5]\n</pre> # Forecaster in-sample residuals # ============================================================================== forecaster.fit(y=y, store_in_sample_residuals=True)  print(\"Length:\", len(forecaster.in_sample_residuals_)) forecaster.in_sample_residuals_[:5] <pre>Length: 197\n</pre> Out[12]: <pre>array([-0.13137208, -0.03279934, -0.00417238, -0.02469213,  0.00563615])</pre> In\u00a0[13]: Copied! <pre># Forecaster out-of-sample residuals\n# ==============================================================================\nforecaster.out_sample_residuals_\n</pre> # Forecaster out-of-sample residuals # ============================================================================== forecaster.out_sample_residuals_"},{"location":"quick-start/forecaster-attributes.html#understanding-the-forecaster-attributes","title":"Understanding the forecaster attributes\u00b6","text":"<p>During the process of creating and training a forecaster, the object stores a lot of information in its attributes that can be useful to the user. We will explore the main attributes included in a <code>ForecasterRecursive</code>, but this can be extrapolated to any of the skforecast forecasters.</p>"},{"location":"quick-start/forecaster-attributes.html#create-and-train-a-forecaster","title":"Create and train a forecaster\u00b6","text":"<p>To be able to create and train a forecaster, at least <code>regressor</code> and <code>lags</code> and/or <code>window_features</code> must be specified.</p>"},{"location":"quick-start/forecaster-attributes.html#regressor","title":"Regressor\u00b6","text":"<p>Skforecast is a Python library that facilitates using <code>scikit-learn</code> regressors as multi-step forecasters and also works with any regressor compatible with the scikit-learn API.</p>"},{"location":"quick-start/forecaster-attributes.html#lags","title":"Lags\u00b6","text":"<p>Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.</p>"},{"location":"quick-start/forecaster-attributes.html#window-features","title":"Window features\u00b6","text":"<p>When forecasting time series data, it may be useful to consider additional characteristics beyond just the lagged values. For example, the moving average of the previous n values may help to capture the trend in the series. The <code>window_features</code> argument allows the inclusion of additional predictors created with the previous values of the series.</p>"},{"location":"quick-start/forecaster-attributes.html#window-size","title":"Window size\u00b6","text":"<p>The size of the data window needed to create the predictors. It is the maximum between the maximum lag and the maximum window required by the window features.</p>"},{"location":"quick-start/forecaster-attributes.html#last-window","title":"Last window\u00b6","text":"<p>Last window the forecaster has seen during training. It stores the values needed to predict the next <code>step</code> immediately after the training data.</p>"},{"location":"quick-start/forecaster-attributes.html#in-sample-residuals","title":"In-sample residuals\u00b6","text":"<p>The residuals of the in-sample predictions. This is the difference between the true values and the predictions made by the forecaster on the training data.</p> <p>In order to save conputational resources, the residuals are only stored if the user specifies <code>store_residuals=True</code> when training the forecaster. If the forecaster is already trained and the user wants to store the residuals, the method <code>set_in_sample_residuals()</code> can be used.</p> <p>If the forecaster includes a transformer, the residuals are stored in the transformed scale.</p>"},{"location":"quick-start/forecaster-attributes.html#out-of-sample-residuals","title":"Out-of-sample residuals\u00b6","text":"<p>Residuals from models predicting non training data. If <code>transformer_y</code> is not <code>None</code>, residuals are assumed to be in the transformed scale. Use <code>set_out_sample_residuals</code>  method to set values.</p> <p>As no values have been added, the parameter is <code>None</code>.</p>"},{"location":"quick-start/forecaster-parameters.html","title":"Understanding the forecaster parameters","text":"<p>Understanding what can be done when initializing a forecaster with skforecast can have a significant impact on the accuracy and effectiveness of the model. This guide highlights key considerations to keep in mind when initializing a forecaster and how these functionalities can be used to create more powerful and accurate forecasting models in Python.</p> <p>We will explore the arguments that can be included in a <code>ForecasterRecursive</code>, but this can be extrapolated to any of the skforecast forecasters.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.recursive import ForecasterRecursive\n\nforecaster = ForecasterRecursive(\n                 regressor        = None,\n                 lags             = None,\n                 window_features  = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = None,\n                 binner_kwargs    = None,\n                 forecaster_id    = None\n             )\n</code></pre> <p>Tip</p> <p>To be able to create and train a forecaster, at least <code>regressor</code> and <code>lags</code> and/or <code>window_features</code> must be specified.</p>"},{"location":"quick-start/forecaster-parameters.html#general-parameters","title":"General parameters","text":""},{"location":"quick-start/forecaster-parameters.html#regressor","title":"Regressor","text":"<p>Skforecast is a Python library that facilitates using scikit-learn regressors as multi-step forecasters and also works with any regressor compatible with the scikit-learn API. Therefore, any of these regressors can be used to create a forecaster:</p> <ul> <li> <p>HistGradientBoostingRegressor</p> </li> <li> <p>LGBMRegressor</p> </li> <li> <p>XGBoost</p> </li> <li> <p>CatBoost</p> </li> </ul> <pre><code># Create a forecaster\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom skforecast.recursive import ForecasterRecursive\n\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = None\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#lags","title":"Lags","text":"<p>To apply machine learning models to forecasting problems, the time series needs to be transformed into a matrix where each value is associated with a specific time window (known as lags) that precedes it. In the context of time series, a lag with respect to a time step t is defined as the value of the series at previous time steps. For instance, lag 1 represents the value at time step t-1, while lag m represents the value at time step t-m.</p> <p>Learn more about machine learning for forecasting. </p> <p> Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix. </p> <pre><code># Create a forecaster using 5 lags\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom skforecast.recursive import ForecasterRecursive\n\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 5\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#window-features","title":"Window Features","text":"<p>When forecasting time series data, it may be useful to consider additional characteristics beyond just the lagged values. For example, the moving average of the previous n values may help to capture the trend in the series. The <code>window_features</code> argument allows the inclusion of additional predictors created with the previous values of the series.</p> <p>More information: Window and custom features.</p> <pre><code># Create a forecaster with window features\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.recursive import ForecasterRecursive\n\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'mean', 'min', 'max'],\n                      window_sizes = [20, 10, 10, 10]\n                  )\n\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 5,\n                 window_features = window_features\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#transformers","title":"Transformers","text":"<p>Skforecast has two arguments in all the forecasters that allow more detailed control over input data transformations. This feature is particularly useful as many machine learning models require specific data pre-processing transformations. For example, linear models may benefit from features being scaled, or categorical features being transformed into numerical values.</p> <p>Both arguments expect an instance of a transformer (preprocessor) compatible with the <code>scikit-learn</code> preprocessing API with the methods: <code>fit</code>, <code>transform</code>, <code>fit_transform</code> and, <code>inverse_transform</code>.</p> <p>More information: Scikit-learn transformers and pipelines.</p> <p>Example</p> <p>In this example, a scikit-learn <code>StandardScaler</code> preprocessor is used for both the time series and the exogenous variables.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom skforecast.recursive import ForecasterRecursive\n\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(random_state=123, verbose=-1),\n                 lags             = 5,\n                 window_features  = None,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = StandardScaler()\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#custom-weights","title":"Custom weights","text":"<p>The <code>weight_func</code> parameter allows the user to define custom weights for each observation in the time series. These custom weights can be used to assign different levels of importance to different time periods. For example, assign higher weights to recent data points and lower weights to older data points to emphasize the importance of recent observations in the forecast model.</p> <p>More information: Weighted time series forecasting.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom skforecast.recursive import ForecasterRecursive\n\n# Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(random_state=123, verbose=-1),\n                 lags             = 5,\n                 window_features  = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = custom_weights\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#differentiation","title":"Differentiation","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Skforecast, version 0.10.0 or higher, introduces a novel differentiation parameter within its Forecasters. </p> <p>More information: Time series differentiation.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom skforecast.recursive import ForecasterRecursive\n\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(random_state=123, verbose=-1),\n                 lags             = 5,\n                 window_features  = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = 1\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#inclusion-of-kwargs-in-the-regressor-fit-method","title":"Inclusion of kwargs in the regressor fit method","text":"<p>Some regressors include the possibility to add some additional configuration during the fitting method. The predictor parameter <code>fit_kwargs</code> allows these arguments to be set when the forecaster is declared.</p> <p>Danger</p> <p>To add weights to the forecaster, it must be done through the <code>weight_func</code> argument and not through a <code>fit_kwargs</code>.</p> <p>Example</p> <p>The following example demonstrates the inclusion of categorical features in an LGBM regressor. This must be done during the <code>LGBMRegressor</code> fit method. Fit parameters lightgbm</p> <p>More information: [Categorical features]../user_guides/categorical-features.html#native-implementation-for-categorical-features).</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.recursive import ForecasterRecursive\nfrom lightgbm import LGBMRegressor\n\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(),\n                 lags             = 5,\n                 window_features  = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = {'categorical_feature': ['exog_1', 'exog_2']}\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#intervals-conditioned-on-predicted-values-binned-residuals","title":"Intervals conditioned on predicted values (binned residuals)","text":"<p>When creating prediction intervals, skforecast uses a <code>QuantileBinner</code> class to bin data into quantile-based bins using <code>numpy.percentile</code>. This class is similar to KBinsDiscretizer but faster for binning data into quantile-based bins. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i].</p> <p>More information: Intervals conditioned on predicted values (binned residuals).</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom skforecast.recursive import ForecasterRecursive\nfrom lightgbm import LGBMRegressor\n\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(),\n                 lags             = 5,\n                 window_features  = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = None,\n                 binner_kwargs    = {'n_bins': 10}\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#forecaster-id","title":"Forecaster ID","text":"<p>Name used as an identifier of the forecaster. It may be used, for example to identify the time series being modeled.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom skforecast.recursive import ForecasterRecursive\n\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(random_state=123, verbose=-1),\n                 lags             = 5,\n                 window_features  = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 differentiation  = None,\n                 fit_kwargs       = None,\n                 binner_kwargs    = None,\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#direct-multi-step-parameters","title":"Direct multi-step parameters","text":"<p>For the Forecasters that follow a direct multi-step strategy (<code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>), there are two additional parameters in addition to those mentioned above.</p>"},{"location":"quick-start/forecaster-parameters.html#steps","title":"Steps","text":"<p>Direct multi-step forecasting consists of training a different model for each step of the forecast horizon. For example, to predict the next 5 values of a time series, 5 different models are trained, one for each step. As a result, the predictions are independent of each other. </p> <p>The number of models to be trained is specified by the <code>steps</code> parameter.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom skforecast.direct import ForecasterDirect\n\nforecaster = ForecasterDirect(\n                 regressor        = LGBMRegressor(random_state=123, verbose=-1),\n                 steps            = 5,\n                 lags             = 5,\n                 window_features  = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 fit_kwargs       = None,\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"quick-start/forecaster-parameters.html#number-of-jobs","title":"Number of jobs","text":"<p>The <code>n_jobs</code> parameter allows multi-process parallelization to train regressors for all <code>steps</code> simultaneously. </p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> <pre><code># Create a forecaster\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom skforecast.direct import ForecasterDirect\n\nforecaster = ForecasterDirect(\n                 regressor        = LGBMRegressor(random_state=123, verbose=-1),\n                 steps            = 5,\n                 lags             = 5,\n                 window_features  = None,\n                 transformer_y    = None,\n                 transformer_exog = None,\n                 weight_func      = None,\n                 fit_kwargs       = None,\n                 n_jobs           = 'auto',\n                 forecaster_id    = 'my_forecaster'\n             )\n</code></pre>"},{"location":"quick-start/how-to-install.html","title":"Installation Guide","text":"<p>This guide will help you install <code>skforecast</code>, a powerful library for time series forecasting in Python. The default installation of <code>skforecast</code> includes only the essential dependencies required for basic functionality. Additional optional dependencies can be installed for extended features.</p> <p> </p>"},{"location":"quick-start/how-to-install.html#basic-installation","title":"Basic installation","text":"<p>Skforecast requires Python 3.9 or higher. It is available on PyPI and can be installed using <code>pip</code>. You can also install it via conda from the conda-forge channel.</p> <p>To install the basic version of <code>skforecast</code> with its core dependencies, run:</p> <pre><code>pip install skforecast\n</code></pre> <p>Specific version:</p> <pre><code>pip install skforecast==0.17.0\n</code></pre> <p>Latest (unstable):</p> <pre><code>pip install git+https://github.com/skforecast/skforecast@master\n</code></pre> <p>The following dependencies are installed with the default installation:</p> <ul> <li>numpy&gt;=1.24</li> <li>pandas&gt;=1.5</li> <li>tqdm&gt;=4.57</li> <li>scikit-learn&gt;=1.2</li> <li>optuna&gt;=2.10</li> <li>joblib&gt;=1.1</li> <li>numba&gt;=0.59</li> <li>rich&gt;=13.9</li> </ul>"},{"location":"quick-start/how-to-install.html#optional-dependencies","title":"Optional dependencies","text":"<p>To install the full version with all optional dependencies:</p> <pre><code>pip install skforecast[full]\n</code></pre> <p>For specific use cases, you can install these dependencies as needed:</p>"},{"location":"quick-start/how-to-install.html#sarimax","title":"Sarimax","text":"<pre><code>pip install skforecast[sarimax]\n</code></pre> <ul> <li>statsmodels&gt;=0.12, &lt;0.15</li> </ul>"},{"location":"quick-start/how-to-install.html#plotting","title":"Plotting","text":"<pre><code>pip install skforecast[plotting]\n</code></pre> <ul> <li>matplotlib&gt;=3.3, &lt;3.11</li> <li>seaborn&gt;=0.11, &lt;0.14</li> <li>statsmodels&gt;=0.12, &lt;0.15</li> </ul>"},{"location":"quick-start/how-to-install.html#deep-learning","title":"Deep Learning","text":"<pre><code>pip install skforecast[deeplearning]\n</code></pre> <ul> <li>keras&gt;=3.0, &lt;4.0</li> <li>matplotlib&gt;=3.3, &lt;3.11</li> </ul>"},{"location":"quick-start/quick-start-skforecast.html","title":"Quick start","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom skforecast.datasets import load_demo_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import matplotlib.pyplot as plt from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error from skforecast.datasets import load_demo_dataset from skforecast.preprocessing import RollingFeatures from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = load_demo_dataset()\ndata.head(5)\n</pre> # Download data # ============================================================================== data = load_demo_dataset() data.head(5) Out[2]: <pre>datetime\n1991-07-01    0.429795\n1991-08-01    0.400906\n1991-09-01    0.432159\n1991-10-01    0.492543\n1991-11-01    0.502369\nFreq: MS, Name: y, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre># Data partition train-test\n# ==============================================================================\nend_train = '2005-06-01 23:59:00'\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \" \n    f\"(n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\nax.legend()\nplt.show();\n</pre> # Data partition train-test # ============================================================================== end_train = '2005-06-01 23:59:00' print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"      f\"(n={len(data.loc[:end_train])})\" ) print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') ax.legend() plt.show(); <pre>Train dates : 1991-07-01 00:00:00 --- 2005-06-01 00:00:00  (n=168)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> In\u00a0[4]: Copied! <pre># Create and fit a recursive multi-step forecaster (ForecasterRecursive)\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=10)\n             )\n\nforecaster.fit(y=data.loc[:end_train])\nforecaster\n</pre> # Create and fit a recursive multi-step forecaster (ForecasterRecursive) # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=10)              )  forecaster.fit(y=data.loc[:end_train]) forecaster Out[4]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]</li> <li>Window features: ['roll_mean_10']</li> <li>Window size: 15</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-11 16:08:45</li> <li>Last fit date: 2025-08-11 16:08:46</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p> \ud83d\udca1 Tip </p> <p>To understand what can be done when initializing a forecaster with skforecast visit Forecaster parameters and Forecaster attributes.</p> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=len(data.loc[end_train:]))\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=len(data.loc[end_train:])) predictions.head(3) Out[5]: <pre>2005-07-01    1.026507\n2005-08-01    1.042429\n2005-09-01    1.116730\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error on test data\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data.loc[end_train:],\n                y_pred = predictions\n            )\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error on test data # ============================================================================== error_mse = mean_squared_error(                 y_true = data.loc[end_train:],                 y_pred = predictions             ) print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.006632513357651682\n</pre> In\u00a0[8]: Copied! <pre># Backtesting\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 10,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = True,\n         fixed_train_size   = False\n     )\n\nmetric, predictions_backtest = backtesting_forecaster(\n                                   forecaster    = forecaster,\n                                   y             = data,\n                                   cv            = cv,\n                                   metric        = 'mean_squared_error',\n                                   verbose       = True,\n                               )\n\nmetric\n</pre> # Backtesting # ============================================================================== cv = TimeSeriesFold(          steps              = 10,          initial_train_size = len(data.loc[:end_train]),          refit              = True,          fixed_train_size   = False      )  metric, predictions_backtest = backtesting_forecaster(                                    forecaster    = forecaster,                                    y             = data,                                    cv            = cv,                                    metric        = 'mean_squared_error',                                    verbose       = True,                                )  metric <pre>Information of folds\n--------------------\nNumber of observations used for initial training: 168\nNumber of observations used for backtesting: 36\n    Number of folds: 4\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude between last observed data (last window) and predictions (gap): 0\n    Last fold only includes 6 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=168)\n    Validation: 2005-07-01 00:00:00 -- 2006-04-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2006-04-01 00:00:00  (n=178)\n    Validation: 2006-05-01 00:00:00 -- 2007-02-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2007-02-01 00:00:00  (n=188)\n    Validation: 2007-03-01 00:00:00 -- 2007-12-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2007-12-01 00:00:00  (n=198)\n    Validation: 2008-01-01 00:00:00 -- 2008-06-01 00:00:00  (n=6)\n\n</pre> <pre>  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> Out[8]: mean_squared_error 0 0.006816 In\u00a0[9]: Copied! <pre># Grid search hyperparameter and lags\n# ==============================================================================\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Folds\ncv = TimeSeriesFold(\n         steps              = 10,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False,\n     )\n\nresults_grid = grid_search_forecaster(\n                   forecaster  = forecaster,\n                   y           = data,\n                   param_grid  = param_grid,\n                   lags_grid   = lags_grid,\n                   cv          = cv,\n                   metric      = 'mean_squared_error',\n                   return_best = True,\n               )\n</pre> # Grid search hyperparameter and lags # ============================================================================== # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Folds cv = TimeSeriesFold(          steps              = 10,          initial_train_size = len(data.loc[:end_train]),          refit              = False,      )  results_grid = grid_search_forecaster(                    forecaster  = forecaster,                    y           = data,                    param_grid  = param_grid,                    lags_grid   = lags_grid,                    cv          = cv,                    metric      = 'mean_squared_error',                    return_best = True,                ) <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10] \n  Parameters: {'max_depth': 15, 'n_estimators': 50}\n  Backtesting metric: 0.017861588026122758\n</pre> In\u00a0[10]: Copied! <pre># Grid results\n# ==============================================================================\nresults_grid\n</pre> # Grid results # ============================================================================== results_grid Out[10]: lags lags_label params mean_squared_error max_depth n_estimators 0 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 50} 0.017862 15 50 1 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 50} 0.017862 10 50 2 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 100} 0.018772 5 100 3 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 10, 'n_estimators': 100} 0.018898 10 100 4 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 15, 'n_estimators': 100} 0.018898 15 100 5 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] {'max_depth': 5, 'n_estimators': 50} 0.019198 5 50 6 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.035032 10 50 7 [1, 2, 3] [1, 2, 3] {'max_depth': 15, 'n_estimators': 50} 0.035032 15 50 8 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.035168 5 50 9 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.040312 10 100 10 [1, 2, 3] [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.040312 15 100 11 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.040562 5 100 12 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.042146 5 100 13 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.042147 10 100 14 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.042147 15 100 15 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 50} 0.043385 5 50 16 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 50} 0.043385 10 50 17 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 50} 0.043385 15 50 <p>Since <code>return_best = True</code>, the forecaster object is updated with the best configuration found and trained with the whole data set. This means that the final model obtained from grid search will have the best combination of lags and hyperparameters that resulted in the highest performance metric. This final model can then be used for future predictions on new data.</p> In\u00a0[11]: Copied! <pre># Print forecaster information\n# ==============================================================================\nforecaster\n</pre> # Print forecaster information # ============================================================================== forecaster Out[11]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10]</li> <li>Window features: ['roll_mean_10']</li> <li>Window size: 10</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-11 16:08:45</li> <li>Last fit date: 2025-08-11 16:08:48</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 15, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 50, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1, 'device': 'cpu'}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p>"},{"location":"quick-start/quick-start-skforecast.html#quick-start-skforecast","title":"Quick start skforecast\u00b6","text":"<p>Welcome to a quick start guide to using skforecast! In this guide, we will provide you with a code example that demonstrates how to create, validate, and optimize a recursive multi-step forecaster, <code>ForecasterRecursive</code>, using skforecast.</p> <p>A Forecaster object in the skforecast library is a comprehensive container that provides essential functionality and methods for training a forecasting model and generating predictions for future points in time.</p> <p>If you need more detailed documentation or guidance, you can visit the User Guides section.</p> <p>Without further ado, let's jump into the code example!</p>"},{"location":"quick-start/quick-start-skforecast.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"quick-start/quick-start-skforecast.html#train-a-forecaster","title":"Train a forecaster\u00b6","text":"<p>Let's start by training a forecaster! For a more in-depth guide to using <code>ForecasterRecursive</code>, visit the User guide.</p>"},{"location":"quick-start/quick-start-skforecast.html#prediction","title":"Prediction\u00b6","text":"<p>After training the forecaster, the <code>predict</code> method can be used to make predictions for the future $n$ steps.</p>"},{"location":"quick-start/quick-start-skforecast.html#backtesting-forecaster-validation","title":"Backtesting: forecaster validation\u00b6","text":"<p>In time series forecasting, backtesting refers to the process of validating a predictive model using historical data. The technique involves moving backwards in time, step-by-step, to assess how well a model would have performed if it had been used to make predictions during that time period. Backtesting is a form of cross-validation that is applied to previous periods in the time series.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data. For more detailed documentation on backtesting, visit: User guide Backtesting forecaster.</p>"},{"location":"quick-start/quick-start-skforecast.html#hyperparameter-tuning-and-lags-selection","title":"Hyperparameter tuning and lags selection\u00b6","text":"<p>Hyperparameter tuning is a crucial aspect of developing accurate and effective machine learning models. In machine learning, hyperparameters are values that cannot be learned from data and must be set by the user before the model is trained. These hyperparameters can significantly impact the performance of the model, and tuning them carefully can improve its accuracy and generalization to new data. In the case of forecasting models, the lags included in the model can be considered as an additional hyperparameter.</p> <p>Hyperparameter tuning involves systematically testing different values or combinations of hyperparameters (including lags) to find the optimal configuration that produces the best results. The Skforecast library offers various hyperparameter tuning strategies, including grid search, random search, and Bayesian search. For more detailed documentation on Hyperparameter tuning, visit: Hyperparameter tuning and lags selection.</p>"},{"location":"releases/releases.html","title":"Changelog","text":"<p>All significant changes to this project are documented in this release file.</p> Legend Feature New feature Enhancement Improvement in existing functionality API Change Changes in the API Fix Bug fix"},{"location":"releases/releases.html#0.17.0","title":"0.17.0 Aug 11, 2025","text":"<p>The main changes in this release are:</p> <ul> <li> <p>Feature <code>ForecasterEquivalentDate</code> can now predict intervals using the conformal prediction framework.</p> </li> <li> <p>Feature Created module <code>experimental</code>, this module contains experimental features that are not yet fully tested or may change in future releases.</p> </li> <li> <p>Enhancement The <code>ForecasterRNN</code> and the function <code>create_and_compile_model</code> have been refactored to allow for the inclusion of exogenous variables. The forecaster can also make interval predictions using the conformal prediction framework.</p> </li> <li> <p>API Change Input data passed to all functions/classes must have either a pandas <code>RangeIndex</code> or <code>DatetimeIndex</code>. Previously, if the input did not meet this condition, a <code>RangeIndex</code> starting at 0 was automatically generated. This behavior has been removed to ensure consistent and explicit handling of input data.</p> </li> <li> <p>API Change <code>ForecasterRecursiveMultiSeries</code> now accepts three input types for the <code>series</code> data: a wide-format DataFrame, where each column corresponds to a different time series; a long-format DataFrame with a MultiIndex, where the first level indicates the series name and the second level is the time index; or a dictionary with series names as keys and pandas <code>Series</code> as values.</p> </li> <li> <p>API Change <code>ForecasterRecursiveMultiSeries</code> now accepts <code>exog</code> input as a wide-format DataFrame, where each column corresponds to a different exogenous variable; a long-format DataFrame with a MultiIndex, where the first level indicates the series name to which it belongs and the second level is the time index; or a dictionary with series names as keys and pandas <code>Series</code> or <code>DataFrames</code> as values.</p> </li> <li> <p>Fix A bug that prevented the use of <code>initial_train_size</code> as a date with the <code>OneStepAheadFold</code> during the hyperparameter search has been fixed.</p> </li> <li> <p>Fix A bug that caused the data types to be set incorrectly when creating the predicting matrix with the <code>create_predict_X</code> method or when <code>return_predictors=True</code> in the <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code> functions has been fixed. The dtypes of the predictors are now set to match those of the training data.</p> </li> <li> <p>Fix A bug that prevented the use of a <code>pd.RangeIndex</code> with the <code>OneStepAheadFold</code> during the hyperparameter search has been fixed.</p> </li> </ul> <p>Added</p> <ul> <li> <p>Added attribute <code>exog_dtypes_out_</code> in all forecasters to store the data types of the exogenous variables used in training after the transformation applied by <code>transformer_exog</code>. If <code>transformer_exog</code> is not used, it is equal to <code>exog_dtypes_in_</code>.</p> </li> <li> <p>Added function <code>reshape_series_wide_to_long</code> in the <code>preprocessing</code> module. This function reshapes a wide-format DataFrame where each column corresponds to a series into a long-format DataFrame with with a MultiIndex. The first level of the index is the series name and the second level is the time index.</p> </li> <li> <p>Added metric <code>symmetric_mean_absolute_percentage_error</code> in the <code>metrics</code> module. This metric calculates the symmetric mean absolute percentage error (SMAPE) between the true values and the predicted values.</p> </li> <li> <p><code>ForecasterEquivalentDate</code> can now predict intervals using the conformal prediction framework.</p> </li> <li> <p>Created module <code>experimental</code>, this module contains experimental features that are not yet fully tested or may change in future releases.</p> </li> <li> <p>Include function <code>calculate_distance_from_holiday</code> in the <code>experimental</code> module. It calculates the number of days to the next holiday and the number of days since the last holiday in a DataFrame with a date column.</p> </li> <li> <p>The <code>ForecasterRNN</code> and the function <code>create_and_compile_model</code> now support the inclusion of exogenous variables.</p> </li> <li> <p>Added method <code>predict_interval</code> to the <code>ForecasterRNN</code> using the conformal prediction framework.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Input data passed to all functions/classes must have either a pandas <code>RangeIndex</code> or <code>DatetimeIndex</code>. Previously, if the input did not meet this condition, a <code>RangeIndex</code> starting at 0 was automatically generated. This behavior has been removed to ensure consistent and explicit handling of input data.</p> </li> <li> <p><code>ForecasterRecursiveMultiSeries</code> now accepts three input types for the series data: a wide-format DataFrame, where each column corresponds to a different time series; a long-format DataFrame with a MultiIndex, where the first level indicates the series name and the second level is the time index; or a dictionary with series names as keys and pandas Series as values.</p> </li> <li> <p><code>ForecasterRecursiveMultiSeries</code> now accepts <code>exog</code> input as a wide-format DataFrame, where each column corresponds to a different exogenous variable; a long-format DataFrame with a MultiIndex, where the first level indicates the series name to which it belongs and the second level is the time index; or a dictionary with series names as keys and pandas <code>Series</code> or <code>DataFrames</code> as values.</p> </li> <li> <p>When predicting, <code>ForecasterRecursiveMultiSeries</code> does not require the exog input to have the same type as the one used during training.</p> </li> <li> <p>Function <code>series_long_to_dict</code> renamed to <code>reshape_series_long_to_dict</code> in the <code>preprocessing</code> module. This function reshapes a long-format DataFrame with time series data into a dictionary format where each entry corresponds to a series.</p> </li> <li> <p>Function <code>exog_long_to_dict</code> renamed to <code>reshape_exog_long_to_dict</code> in the <code>preprocessing</code> module. This function reshapes a long-format DataFrame with exogenous variables into a dictionary format where each entry corresponds to the exogenous variables of a series.</p> </li> <li> <p>The <code>create_and_compile_model</code> function has been refactored. All arguments related with layers and compilation are now passed as a dictionary using the following arguments: <code>recurrent_layers_kwargs</code>, <code>dense_layers_kwargs</code>, <code>output_dense_layer_kwargs</code>, and <code>compile_kwargs</code>.</p> </li> <li> <p>The arguments <code>lags</code> and <code>steps</code> were removed from the <code>ForecasterRNN</code> initialization. These arguments are now inferred from the regressor architecture.</p> </li> <li> <p>Remove <code>preprocess_y</code>, <code>preprocess_last_window</code> and <code>preprocess_exog</code> in favor of <code>check_extract_values_and_index</code> in the <code>utils</code> module. This function checks if the index is a pandas <code>DatetimeIndex</code> or <code>RangeIndex</code> and extracts the values and index accordingly.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>A bug that prevented the use of <code>initial_train_size</code> as a date with the <code>OneStepAheadFold</code> during the hyperparameter search has been fixed.</p> </li> <li> <p>A bug that caused the data types to be set incorrectly when creating the predicting matrix with the <code>create_predict_X</code> method or when <code>return_predictors=True</code> in the <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code> functions has been fixed. The dtypes of the predictors are now set to match those of the training data.</p> </li> <li> <p>A bug that prevented the use of a <code>pd.RangeIndex</code> with the <code>OneStepAheadFold</code> during the hyperparameter search has been fixed.</p> </li> </ul>"},{"location":"releases/releases.html#0.16.0","title":"0.16.0 May 01, 2025","text":"<p>The main changes in this release are:</p> <ul> <li>Enhancement Refactored the internal codebase of all forecasters to enhance performance, primarily by replacing pandas DataFrames with more efficient NumPy arrays.</li> </ul> <p>Added</p> <ul> <li> <p>Function <code>set_cpu_gpu_device()</code> in the <code>utils</code> module to set the device of the regressor to 'cpu' or 'gpu'. It is used to ensure that the recursive prediction is done in cpu even if the regressor is set to 'gpu'. This allows to avoid the bottleneck of the recursive prediction when using a gpu. Only applied to recursive forecasters when the regressor is a <code>XGBoost</code>, <code>LightGBM</code> or <code>CatBoost</code> model.</p> </li> <li> <p>Added <code>series_name_in_</code> attribute in single series forecasters to store the name of the series used to fit the forecaster.</p> </li> <li> <p>Added argument <code>return_predictors</code> to <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code> to return the predictors generated during the backtesting process along with the predictions.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Refactored the internal codebase of all forecasters to enhance performance, primarily by replacing pandas DataFrames with more efficient NumPy arrays.</p> </li> <li> <p>In-sample residuals in direct forecasters has been simplified.</p> </li> <li> <p>The method <code>create_predict_X</code> in the <code>ForecasterRecursiveMultiSeries</code> now returns a long-format DataFrame with the predictors. The columns are <code>level</code> and one column for each predictor. The index is the same as the prediction index.</p> </li> <li> <p>The method <code>create_predict_X</code> in the <code>ForecasterDirectMultiVariate</code> now includes the <code>level</code> column in the returned DataFrame. The columns are <code>level</code> and one column for each predictor. The index is the same as the prediction index.</p> </li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.15.1","title":"0.15.1 Mar 18, 2025","text":"<ul> <li>Fix Minor release to fix a bug when importing module <code>skforecast.sarimax</code>.</li> </ul> <p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li>Fixed import error when importing the <code>skforecast.sarimax</code> module.</li> </ul>"},{"location":"releases/releases.html#0.15.0","title":"0.15.0 Mar 10, 2025","text":"<p>The main changes in this release are:</p> <ul> <li> <p>Feature Added conformal framework for probabilistic forecasting. Generate prediction intervals using the conformal prediction split method.</p> </li> <li> <p>Feature Binned residuals are now available in the <code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code> forecasters. </p> </li> <li> <p>Feature New class <code>ConformalIntervalCalibrator</code> to perform conformal calibration. This class is used to calibrate the prediction intervals using the conformal prediction framework.</p> </li> <li> <p>API Change Probabilistic predictions in <code>ForecasterRecursiveMultiSeries</code> and <code>ForecasterDirectMultiVariate</code> are now returned as a long format DataFrame.</p> </li> <li> <p>API Change Fit argument <code>store_in_sample_residuals</code> has changed default value to <code>False</code>. This means in-sample residuals are not stored by default. To store them, call new method <code>set_in_sample_residuals</code> after fitting the forecaster using the same training data. </p> </li> </ul> <p>Added</p> <ul> <li> <p>Support for <code>Python 3.13</code>.</p> </li> <li> <p>Added <code>rich&gt;=13.9.4</code> library as hard dependence.</p> </li> <li> <p>New argument <code>method  = 'conformal'</code> in <code>predict_interval</code> method or <code>interval_method = 'conformal'</code> in backtesting functions to use the conformal prediction framework.</p> </li> <li> <p>New class <code>ConformalIntervalCalibrator</code> to perform conformal calibration. This class is used to calibrate the prediction intervals using the conformal prediction framework.</p> </li> <li> <p>Binned residuals are now available in the <code>ForecasterRecursiveMultiSeries</code>, <code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code> forecasters. </p> </li> <li> <p>New method <code>set_in_sample_residuals</code> to store the in-sample residuals after fitting the forecaster using the same training data.</p> </li> <li> <p>Functions <code>crps_from_predictions</code> and <code>crps_from_quantiles</code> in module <code>metrics</code> to calculate the Continuous Ranked Probability Score (CRPS).</p> </li> <li> <p>Function <code>calculate_coverage</code> in module <code>metrics</code> to calculate the coverage of the predicted intervals.</p> </li> <li> <p>The <code>differentiation</code> argument in <code>ForecasterRecursiveMultiSeries</code> can now be a dict to differentiate each series independently. This is useful if the user wants to differentiate each series with a different order or not differentiate some of them.</p> </li> <li> <p>Added statistic <code>ewm</code> (exponential weighted mean) in <code>RollingFeatures</code>. Alpha can be specified using the new argument <code>kwargs_stats</code>, default <code>{'ewm': {'alpha': 0.3}}</code>.</p> </li> <li> <p>Added method <code>_repr_html_</code> to <code>ForecasterSarimax</code>, <code>TimeSeriesFold</code> and <code>OneStepAheadFold</code> to display the object in HTML format.</p> </li> <li> <p>Added argument <code>consolidate_dtypes</code> in <code>exog_long_to_dict</code> function to ensure that the data types of the exogenous variables are consistent across all series when <code>np.nan</code> values are added and integer columns are converted to float.</p> </li> <li> <p>Added <code>calculate_lag_autocorrelation</code> function to the <code>plot</code> module to calculate the autocorrelation and partial autocorrelation of a time series.</p> </li> <li> <p>Added datasets <code>m5</code>, <code>ett_m1</code>, <code>ett_m2</code>, <code>ett_m2_extended</code> and <code>expenditures_australia</code> and <code>public_transport_madrid</code> to the <code>datasets</code> module.</p> </li> <li> <p>Added function <code>create_mean_pinball_loss</code> in the <code>metrics</code> module to create a function to calculate the mean pinball loss for a given quantile.</p> </li> <li> <p>Added function <code>check_one_step_ahead_input</code> to check the input data when using a <code>OneStepAheadFold</code> in the <code>model_selection</code> functions.</p> </li> <li> <p>Function <code>set_warnings_style</code> in the <code>exceptions</code> module to set the style of the skforecast warnings issued by the library.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>ForecasterRecursiveMultiSeries</code> and <code>ForecasterDirectMultiVariate</code> forecasters use conformal prediction framework as default for probabilistic forecasting, <code>method = 'conformal'</code> in <code>predict_interval</code> method.</p> </li> <li> <p><code>backtesting_forecaster_multiseries</code> uses conformal prediction framework as default for probabilistic forecasting, <code>interval_method = 'conformal'</code>.</p> </li> <li> <p><code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code> use binned residuals as default for probabilistic forecasting, <code>use_binned_residuals = True</code>.</p> </li> <li> <p>Fit argument <code>store_in_sample_residuals</code> has changed default value to <code>False</code>. This means in-sample residuals are not stored by default. To store them, call new method <code>set_in_sample_residuals</code> after fitting the forecaster using the same training data. </p> </li> <li> <p>Predictions from <code>predict_bootstrapping</code> in <code>ForecasterRecursiveMultiSeries</code> and <code>ForecasterDirectMultiVariate</code> are now returned as a long format DataFrame with the bootstrapping predictions. The columns are <code>level</code>, <code>pred_boot_0</code>, <code>pred_boot_1</code>, ..., <code>pred_boot_n_boot</code>.</p> </li> <li> <p>Predictions from <code>predict_interval</code> in <code>ForecasterRecursiveMultiSeries</code> and <code>ForecasterDirectMultiVariate</code> are now returned as long format DataFrame with the predictions and the lower and upper bounds of the estimated interval. The columns are <code>level</code>, <code>pred</code>, <code>lower_bound</code>, <code>upper_bound</code>.</p> </li> <li> <p>Predictions from <code>predict_quantiles</code> in <code>ForecasterRecursiveMultiSeries</code> and <code>ForecasterDirectMultiVariate</code> are now returned as long format DataFrame with the quantiles predicted by the forecaster. For example, if <code>quantiles = [0.05, 0.5, 0.95]</code>, the columns are <code>level</code>, <code>q_0.05</code>, <code>q_0.5</code>, <code>q_0.95</code>.</p> </li> <li> <p>Predictions from <code>predict_dist</code> in <code>ForecasterRecursiveMultiSeries</code> and <code>ForecasterDirectMultiVariate</code> are now returned as long format DataFrame with the parameters of the fitted distribution for each step. The columns are <code>level</code>, <code>param_0</code>, <code>param_1</code>, ..., <code>param_n</code>, where <code>param_i</code> are the parameters of the distribution.</p> </li> <li> <p><code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiSeriesCustom</code> has been deleted (deprecated since skforecast 0.14.0). Window features can be added using the <code>window_features</code> argument in the <code>ForecasterRecursive</code>, <code>ForecasterDirect</code>, <code>ForecasterDirectMultiVariate</code> and <code>ForecasterRecursiveMultiSeries</code>.</p> </li> <li> <p>Argument <code>dropna</code> in <code>exog_long_to_dict</code> function has been renamed to <code>drop_all_nan_cols</code>.</p> </li> <li> <p>Argument <code>initial_train_size</code> can be a <code>str</code> or a <code>pandas datetime</code> in <code>TimeSeriesFold</code> and <code>OneStepAheadFold</code>. If so, the cv object will use the specified date to split the data. (contribution by @g-rubio #898).</p> </li> <li> <p><code>set_dark_theme</code> background color changed to <code>#001633</code> to improve readability.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Now <code>ForecasterRecursiveMultiSeries</code> can be saved correctly when <code>weight_func</code> is a <code>dict</code> with <code>None</code> for any series. It now use the method <code>_weight_func_all_1</code> to create the weight function for these series.</p> </li> <li> <p>Fix <code>transform_numpy</code> function in the <code>utils</code> module to work when transformers output in <code>scikit-learn</code> is <code>set_output(transform='pandas')</code>.</p> </li> </ul>"},{"location":"releases/releases.html#0.14.0","title":"0.14.0 Nov 11, 2024","text":"<p>The main changes in this release are:</p> <p>This release has undergone a major refactoring to improve the performance of the library. Visit the migration guide section for more information.</p> <ul> <li> <p>Feature Window features can be added to the training matrix using the <code>window_features</code> argument in all forecasters. You can use the <code>RollingFeatures</code> class to create these features or create your own object. Create window and custom features.</p> </li> <li> <p>Feature <code>model_selection</code> functions now have a new argument <code>cv</code>. This argument expect an object of type <code>TimeSeriesFold</code> (backtesting) or <code>OneStepAheadFold</code> which allows to define the validation strategy using the arguments <code>initial_train_size</code>, <code>steps</code>, <code>gap</code>, <code>refit</code>, <code>fixed_train_size</code>, <code>skip_folds</code> and <code>allow_incomplete_folds</code>.</p> </li> <li> <p>Feature Hyperparameter search now allows to follow a one-step-ahead validation strategy using a <code>OneStepAheadFold</code> as <code>cv</code> argument in the <code>model_selection</code> functions.</p> </li> <li> <p>Enhancement Refactor the prediction process in <code>ForecasterRecursiveMultiSeries</code> to improve performance when predicting multiple series.</p> </li> <li> <p>Enhancement The bootstrapping process in the <code>predict_bootstrapping</code> method of all forecasters has been optimized to improve performance. This may result in slightly different results when using the same seed as in previous versions.</p> </li> <li> <p>Enhancement Exogenous variables can be added to the training matrix if they do not contain the first window size observations. This is useful when exogenous variables are not available in early historical data. Visit the exogenous variables section for more information.</p> </li> <li> <p>API Change Package structure has been changed to improve code organization. The forecasters have been grouped into the <code>recursive</code>, <code>direct</code> amd <code>deep_learning</code> modules. Visit the migration guide section for more information.</p> </li> <li> <p>API Change <code>ForecasterAutoregCustom</code> has been deprecated. Window features can be added using the <code>window_features</code> argument in the <code>ForecasterRecursive</code>.</p> </li> <li> <p>API Change Refactor the <code>set_out_sample_residuals</code> method in all forecasters, it now expects <code>y_true</code> and <code>y_pred</code> as arguments instead of <code>residuals</code>. This method is used to store the residuals of the out-of-sample predictions.</p> </li> <li> <p>API Change The <code>pmdarima.ARIMA</code> regressor is no longer supported by the <code>ForecasterSarimax</code>. You can use the skforecast <code>Sarimax</code> model or, to continue using it, use skforecast 0.13.0 or lower.</p> </li> <li> <p>Fix Fixed a bug where the <code>create_predict_X</code> method in recursive Forecasters did not correctly generate the matrix correctly when using transformations and/or differentiations</p> </li> </ul> <p>Added</p> <ul> <li> <p>Added <code>numba&gt;=0.59</code> as hard dependency.</p> </li> <li> <p>Added <code>window_features</code> argument to all forecasters. This argument allows the user to add window features to the training matrix. See <code>RollingFeatures</code>.</p> </li> <li> <p>Hyperparameter search now allows to follow a one-step-ahead validation strategy using a <code>OneStepAheadFold</code> as <code>cv</code> argument in the <code>model_selection</code> functions.</p> </li> <li> <p>Differentiation has been extended to all forecasters. The <code>differentiation</code> argument has been added to all forecasters to model the n-order differentiated time series.</p> </li> <li> <p>Create <code>transform_numpy</code> function in the <code>utils</code> module to carry out the transformation of the modeled time series and exogenous variables as numpy arrays.</p> </li> <li> <p><code>random_state</code> argument in the <code>fit</code> method of <code>ForecasterRecursive</code> to set a seed for the random generator so that the stored sample residuals are always deterministic.</p> </li> <li> <p>New private method <code>_train_test_split_one_step_ahead</code> in all forecasters.</p> </li> <li> <p>New private function <code>_calculate_metrics_one_step_ahead</code> to <code>model_selection</code> module to calculate the metrics when predicting one step ahead.</p> </li> <li> <p>The <code>steps</code> argument in the predict method of the <code>ForecasterRecursive</code> can now be a str or a pandas datetime. If so, the method will predict up to the specified date. (contribution by @imMoya #811).</p> </li> <li> <p>Exogenous variables can be added to the training matrix if they do not contain the first window size observations. This is useful when exogenous variables are not available in early historical data.</p> </li> <li> <p>Added support for different activation functions in the <code>create_and_compile_model</code> function. (contribution by @pablorodriper #824).</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiSeriesCustom</code> has been deprecated. Window features can be added using the <code>window_features</code> argument in the <code>ForecasterRecursive</code> and <code>ForecasterRecursiveMultiSeries</code>.</p> </li> <li> <p>Refactor <code>recursive_predict</code> in <code>ForecasterRecursiveMultiSeries</code> to predict all series at once and include option of adding residuals. This improves performance when predicting multiple series.</p> </li> <li> <p>Refactor <code>predict_bootstrapping</code> in all Forecasters. The bootstrapping process has been optimized to improve performance. This may result in slightly different results when using the same seed as in previous versions.</p> </li> <li> <p>Change the default value of <code>encoding</code> to <code>ordinal</code> in <code>ForecasterRecursiveMultiSeries</code>. This will avoid conflicts if the regressor does not support categorical variables by default.</p> </li> <li> <p>Removed argument <code>engine</code> from <code>bayesian_search_forecaster</code> and <code>bayesian_search_forecaster_multiseries</code>.</p> </li> <li> <p>The <code>pmdarima.ARIMA</code> regressor is no longer supported by the <code>ForecasterSarimax</code>. You can use the skforecast <code>Sarimax</code> model or, to continue using it, use skforecast 0.13.0 or lower.</p> </li> <li> <p><code>initialize_lags</code> in <code>utils</code> now returns the maximum lag, <code>max_lag</code>.</p> </li> <li> <p>Removed attribute <code>window_size_diff</code> from all Forecasters. The window size extended by the order of differentiation is now calculated on <code>window_size</code>.</p> </li> <li> <p><code>lags</code> can be <code>None</code> when initializing any Forecaster that includes window features.</p> </li> <li> <p><code>model_selection</code> module has been divided internally into different modules to improve code organization (<code>_validation</code>, <code>_search</code>, <code>_split</code>).</p> </li> <li> <p>Functions from <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> modules have been moved to the <code>model_selection</code> module.</p> </li> <li> <p><code>model_selection</code> functions now have a new argument <code>cv</code>. This argument expect an object of type <code>TimeSeriesFold</code> or <code>OneStepAheadFold</code> which allows to define the validation strategy using the arguments <code>initial_train_size</code>, <code>steps</code>, <code>gap</code>, <code>refit</code>, <code>fixed_train_size</code>, <code>skip_folds</code> and <code>allow_incomplete_folds</code>.</p> </li> <li> <p>Added <code>feature_selection</code> module. The functions <code>select_features</code> and <code>select_features_multiseries</code> have been moved to this module.</p> </li> <li> <p>The functions <code>select_features</code> and <code>select_features_multiseries</code> now have 3 returns: <code>selected_lags</code>, <code>selected_window_features</code> and <code>selected_exog</code>.</p> </li> <li> <p>Refactor the <code>set_out_sample_residuals</code> method in all forecasters, it now expects <code>y_true</code> and <code>y_pred</code> as arguments instead of <code>residuals</code>.</p> </li> <li> <p><code>exog_to_direct</code> and <code>exog_to_direct_numpy</code> in <code>utils</code> now returns a the names of the columns of the transformed exogenous variables.</p> </li> <li> <p>Renamed attributes in all Forecasters:</p> <ul> <li> <p><code>encoding_mapping</code> has been renamed to <code>encoding_mapping_</code>.</p> </li> <li> <p><code>last_window</code> has been renamed to <code>last_window_</code>.</p> </li> <li> <p><code>index_type</code> has been renamed to <code>index_type_</code>.</p> </li> <li> <p><code>index_freq</code> has been renamed to <code>index_freq_</code>.</p> </li> <li> <p><code>training_range</code> has been renamed to <code>training_range_</code>.</p> </li> <li> <p><code>series_col_names</code> has been renamed to <code>series_names_in_</code>.</p> </li> <li> <p><code>included_exog</code> has been renamed to <code>exog_in_</code>.</p> </li> <li> <p><code>exog_type</code> has been renamed to <code>exog_type_in_</code>.</p> </li> <li> <p><code>exog_dtypes</code> has been renamed to <code>exog_dtypes_in_</code>.</p> </li> <li> <p><code>exog_col_names</code> has been renamed to <code>exog_names_in_</code>.</p> </li> <li> <p><code>series_X_train</code> has been renamed to <code>X_train_series_names_in_</code>.</p> </li> <li> <p><code>X_train_col_names</code> has been renamed to <code>X_train_features_names_out_</code>.</p> </li> <li> <p><code>binner_intervals</code> has been renamed to <code>binner_intervals_</code>.</p> </li> <li> <p><code>in_sample_residuals</code> has been renamed to <code>in_sample_residuals_</code>.</p> </li> <li> <p><code>out_sample_residuals</code> has been renamed to <code>out_sample_residuals_</code>.</p> </li> <li> <p><code>fitted</code> has been renamed to <code>is_fitted</code>.</p> </li> </ul> </li> <li> <p>Renamed arguments in different functions and methods:</p> <ul> <li> <p><code>in_sample_residuals</code> has been renamed to <code>use_in_sample_residuals</code>.</p> </li> <li> <p><code>binned_residuals</code> has been renamed to <code>use_binned_residuals</code>.</p> </li> <li> <p><code>series_col_names</code> has been renamed to <code>series_names_in_</code> in the <code>check_predict_input</code>, <code>check_preprocess_exog_multiseries</code> and <code>initialize_transformer_series</code> functions in the <code>utils</code> module.</p> </li> <li> <p><code>series_X_train</code> has been renamed to <code>X_train_series_names_in_</code> in the <code>prepare_levels_multiseries</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>exog_col_names</code> has been renamed to <code>exog_names_in_</code> in the <code>check_predict_input</code> and <code>check_preprocess_exog_multiseries</code> functions in the <code>utils</code> module.</p> </li> <li> <p><code>index_type</code> has been renamed to <code>index_type_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>index_freq</code> has been renamed to <code>index_freq_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>included_exog</code> has been renamed to <code>exog_in_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>exog_type</code> has been renamed to <code>exog_type_in_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>exog_dtypes</code> has been renamed to <code>exog_dtypes_in_</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>fitted</code> has been renamed to <code>is_fitted</code> in the <code>check_predict_input</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>use_in_sample</code> has been renamed to <code>use_in_sample_residuals</code> in the <code>prepare_residuals_multiseries</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>in_sample_residuals</code> has been renamed to <code>use_in_sample_residuals</code> in the <code>backtesting_forecaster</code>, <code>backtesting_forecaster_multiseries</code> and <code>check_backtesting_input</code> (<code>utils</code> module) functions.</p> </li> </ul> </li> <li> <p><code>binned_residuals</code> has been renamed to <code>use_binned_residuals</code> in the <code>backtesting_forecaster</code> function.</p> <ul> <li> <p><code>in_sample_residuals</code> has been renamed to <code>in_sample_residuals_</code> in the <code>prepare_residuals_multiseries</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>out_sample_residuals</code> has been renamed to <code>out_sample_residuals_</code> in the <code>prepare_residuals_multiseries</code> function in the <code>utils</code> module.</p> </li> <li> <p><code>last_window</code> has been renamed to <code>last_window_</code> in the <code>preprocess_levels_self_last_window_multiseries</code> function in the <code>utils</code> module.</p> </li> </ul> </li> </ul> <p>Fixed</p> <ul> <li>Fixed a bug where the <code>create_predict_X</code> method in recursive Forecasters did not correctly generate the matrix correctly when using transformations and/or differentiations.</li> </ul>"},{"location":"releases/releases.html#0.13.0","title":"0.13.0 Aug 01, 2024","text":"<p>The main changes in this release are:</p> <ul> <li> <p>Feature Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> are able to predict series not seen during training. This is useful when the user wants to predict a new series that was not included in the training data.</p> </li> <li> <p>Feature <code>encoding</code> can be set to <code>None</code> in Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code>. This option does not add the encoded series ids to the regressor training matrix.</p> </li> <li> <p>Feature New <code>create_predict_X</code> method in all recursive and direct Forecasters to allow the user to inspect the matrix passed to the predict method of the regressor.</p> </li> <li> <p>Feature New module <code>metrics</code> with functions to calculate metrics for time series forecasting such as <code>mean_absolute_scaled_error</code> and <code>root_mean_squared_scaled_error</code>. Visit Time Series Forecasting Metrics for more information.</p> </li> <li> <p>Feature New argument <code>add_aggregated_metric</code> in <code>backtesting_forecaster_multiseries</code> to include, in addition to the metrics for each level, the aggregated metric of all levels using the average (arithmetic mean), weighted average (weighted by the number of predicted values of each level) or pooling (the values of all levels are pooled and then the metric is calculated).</p> </li> <li> <p>Feature New argument <code>skip_folds</code> in <code>model_selection</code> and <code>model_selection_multiseries</code> functions. It allows the user to skip some folds during backtesting, which can be useful to speed up the backtesting process and thus the hyperparameter search.</p> </li> <li> <p>API Change backtesting procedures now pass the training series to the metric functions so it can be used to calculate metrics that depend on the training series.</p> </li> <li> <p>API Change Changed the default value of the <code>transformer_series</code> argument to <code>None</code> in the Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code>. In most cases, tree-based models are used as regressors in these forecasters, so no transformation is applied by default as it is not necessary.</p> </li> </ul> <p>Added</p> <ul> <li> <p>Support for <code>Python 3.12</code>.</p> </li> <li> <p><code>keras</code> has been added as an optional dependency, tag <code>deeplearning</code>, to use the <code>ForecasterRnn</code>.</p> </li> <li> <p><code>PyTorch</code> backend for the <code>ForecasterRnn</code>.</p> </li> <li> <p>New <code>create_predict_X</code> method in all recursive and direct Forecasters to allow the user to inspect the matrix passed to the predict method of the regressor.</p> </li> <li> <p>New <code>_create_predict_inputs</code> method in all Forecasters to unify the inputs of the predict methods.</p> </li> <li> <p>New plot function <code>plot_prediction_intervals</code> in the <code>plot</code> module to plot predicted intervals.</p> </li> <li> <p>New module <code>metrics</code> with functions to calculate metrics for time series forecasting such as <code>mean_absolute_scaled_error</code> and <code>root_mean_squared_scaled_error</code>.</p> </li> <li> <p>New argument <code>skip_folds</code> in <code>model_selection</code> and <code>model_selection_multiseries</code> functions. It allows the user to skip some folds during backtesting, which can be useful to speed up the backtesting process and thus the hyperparameter search.</p> </li> <li> <p>New function <code>plot_prediction_intervals</code> in module <code>plot</code>.</p> </li> <li> <p>Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> are able to predict series not seen during training. This is useful when the user wants to predict a new series that was not included in the training data.</p> </li> <li> <p><code>encoding</code> can be set to <code>None</code> in Global Forecasters <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code>. This option does not add the encoded series ids to the regressor training matrix.</p> </li> <li> <p>New argument <code>add_aggregated_metric</code> in <code>backtesting_forecaster_multiseries</code> to include, in addition to the metrics for each level, the aggregated metric of all levels using the average (arithmetic mean), weighted average (weighted by the number of predicted values of each level) or pooling (the values of all levels are pooled and then the metric is calculated).</p> </li> <li> <p>New argument <code>aggregate_metric</code> in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>bayesian_search_forecaster_multiseries</code> to select the aggregation method used to combine the metric(s) of all levels during the hyperparameter search. The available methods are: mean (arithmetic mean), weighted (weighted by the number of predicted values of each level) and pool (the values of all levels are pooled and then the metric is calculated). If more than one metric and/or aggregation method is used, all are reported in the results, but the first of each is used to select the best model.</p> </li> <li> <p>New class <code>DateTimeFeatureTransformer</code> and function <code>create_datetime_features</code> in the <code>preprocessing</code> module to create datetime and calendar features from a datetime index.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Deprecated <code>python 3.8</code> compatibility.</p> </li> <li> <p>Update project dependencies.</p> </li> <li> <p>Change default value of <code>n_bins</code> when initializing <code>ForecasterAutoreg</code> from 15 to 10.</p> </li> <li> <p>Refactor <code>_recursive_predict</code> in all recursive forecasters.</p> </li> <li> <p>Change default value of <code>transformer_series</code> when initializing <code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> from <code>StandardScaler()</code> to <code>None</code>.</p> </li> <li> <p>Function <code>_get_metric</code> moved from <code>model_selection</code> to <code>metrics</code>.</p> </li> <li> <p>Change information message when <code>verbose</code> is <code>True</code> in <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code>.</p> </li> <li> <p><code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit</code> in <code>utils</code> return <code>n_jobs = 1</code> if regressor is <code>LGBMRegressor</code>. This is because <code>lightgbm</code> is highly optimized for gradient boosting and parallelizes operations at a very fine-grained level, making additional parallelization unnecessary and potentially harmful due to resource contention.</p> </li> <li> <p><code>metric_values</code> returned by <code>backtesting_forecaster</code> and <code>backtesting_sarimax</code> is a <code>pandas DataFrame</code> with one column per metric instead of a <code>list</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li>Bug fix in <code>backtesting_forecaster_multiseries</code> using a <code>ForecasterAutoregMultiSeries</code> or <code>ForecasterAutoregMultiSeriesCustom</code> that includes differentiation.</li> </ul>"},{"location":"releases/releases.html#0.12.1","title":"0.12.1 May 20, 2024","text":"<p>Fix This is a minor release to fix a bug.</p> <p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li>Bug fix when storing <code>last_window</code> using a [<code>ForecasterAutoregMultiSeries</code>] that includes differentiation.</li> </ul>"},{"location":"releases/releases.html#0.12.0","title":"0.12.0 May 05, 2024","text":"<p>The main changes in this release are:</p> <ul> <li> <p>Feature Multiseries forecaster (Global Models) can be trained using series of different lengths and with different exogenous variables per series.</p> </li> <li> <p>Feature New functionality to select features using scikit-learn selectors (<code>select_features</code> and <code>select_features_multiseries</code>).</p> </li> <li> <p>Feature Added new forecaster <code>ForecasterRnn</code> to create forecasting models based on deep learning (RNN and LSTM).</p> </li> <li> <p>Feature New method to predict intervals conditioned on the range of the predicted values. This is can help to improve the interval coverage when the residuals are not homoscedastic (<code>ForecasterAutoreg</code>).</p> </li> <li> <p>Enhancement Bayesian hyperparameter search is now available for all multiseries forecasters using <code>optuna</code> as the search engine.</p> </li> <li> <p>Enhancement All Recursive Forecasters are now able to differentiate the time series before modeling it.</p> </li> <li> <p>API Change Changed the default value of the <code>transformer_series</code> argument to use a <code>StandardScaler()</code> in the Global Forecasters (<code>ForecasterAutoregMultiSeries</code>, <code>ForecasterAutoregMultiSeriesCustom</code> and <code>ForecasterAutoregMultiVariate</code>).</p> </li> </ul> <p>Added</p> <ul> <li> <p>Added <code>bayesian_search_forecaster_multiseries</code> function to <code>model_selection_multiseries</code> module. This function performs a Bayesian hyperparameter search for the <code>ForecasterAutoregMultiSeries</code>, <code>ForecasterAutoregMultiSeriesCustom</code>, and <code>ForecasterAutoregMultiVariate</code> using <code>optuna</code> as the search engine.</p> </li> <li> <p><code>ForecasterAutoregMultiVariate</code> allows to include None when lags is a dict so that a series does not participate in the construction of X_train.</p> </li> <li> <p>The <code>output_file</code> argument has been added to the hyperparameter search functions in the <code>model_selection</code>, <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> modules to save the results of the hyperparameter search in a tab-separated values (TSV) file.</p> </li> <li> <p>New argument <code>binned_residuals</code> in method <code>predict_interval</code> allows to condition the bootstrapped residuals on range of the predicted values. </p> </li> <li> <p>Added <code>save_custom_functions</code> argument to the <code>save_forecaster</code> function in the <code>utils</code> module. If <code>True</code>, save custom functions used in the forecaster (<code>fun_predictors</code> and <code>weight_func</code>) as .py files. Custom functions must be available in the environment where the forecaster is loaded.</p> </li> <li> <p>Added <code>select_features</code> and <code>select_features_multiseries</code> functions to the <code>model_selection</code> and <code>model_selection_multiseries</code> modules to perform feature selection using scikit-learn selectors.</p> </li> <li> <p>Added <code>sort_importance</code> argument to <code>get_feature_importances</code> method in all Forecasters. If <code>True</code>, sort the feature importances in descending order.</p> </li> <li> <p>Added <code>initialize_lags_grid</code> function to <code>model_selection</code> module. This function initializes the lags to be used in the hyperparameter search functions in <code>model_selection</code> and <code>model_selection_multiseries</code>.</p> </li> <li> <p>Added <code>_initialize_levels_model_selection_multiseries</code> function to <code>model_selection_multiseries</code> module. This function initializes the levels of the series to be used in the model selection functions.</p> </li> <li> <p>Added <code>set_dark_theme</code> function to the <code>plot</code> module to set a dark theme for matplotlib plots.</p> </li> <li> <p>Allow tuple type for <code>lags</code> argument in all Forecasters.</p> </li> <li> <p>Argument <code>differentiation</code> in all Forecasters to model the n-order differentiated time series.</p> </li> <li> <p>Added <code>window_size_diff</code> attribute to all Forecasters. It stores the size of the window (<code>window_size</code>) extended by the order of differentiation. Added  to all Forecasters for API consistency.</p> </li> <li> <p>Added <code>store_last_window</code> parameter to <code>fit</code> method in Forecasters. If <code>True</code>, store the last window of the training data.</p> </li> <li> <p>Added <code>utils.set_skforecast_warnings</code> function to set the warnings of the skforecast package.</p> </li> <li> <p>Added new forecaster <code>ForecasterRnn</code> to create forecasting models based on deep learning (RNN and LSTM).</p> </li> <li> <p>Added new function <code>create_and_compile_model</code> to module <code>skforecast.ForecasterRnn.utils</code> to help to create and compile a RNN or LSTM models to be used in <code>ForecasterRnn</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Deprecated argument <code>lags_grid</code> in <code>bayesian_search_forecaster</code>. Use <code>search_space</code> to define the candidate values for the lags. This allows the lags to be optimized along with the other hyperparameters of the regressor in the bayesian search.</p> </li> <li> <p><code>n_boot</code> argument in <code>predict_interval</code>changed from 500 to 250.</p> </li> <li> <p>Changed the default value of the <code>transformer_series</code> argument to use a <code>StandardScaler()</code> in the Global Forecasters (<code>ForecasterAutoregMultiSeries</code>, <code>ForecasterAutoregMultiSeriesCustom</code> and <code>ForecasterAutoregMultiVariate</code>).</p> </li> <li> <p>Refactor <code>utils.select_n_jobs_backtesting</code> to use the forecaster directly instead of <code>forecaster_name</code> and <code>regressor_name</code>.</p> </li> <li> <p>Remove <code>_backtesting_forecaster_verbose</code> in model_selection in favor of <code>_create_backtesting_folds</code>, (deprecated since 0.8.0).</p> </li> </ul> <p>Fixed</p> <ul> <li>Small bug in <code>utils.select_n_jobs_backtesting</code>, rename <code>ForecasterAutoregMultiseries</code> to <code>ForecasterAutoregMultiSeries</code>.</li> </ul>"},{"location":"releases/releases.html#0.11.0","title":"0.11.0 Nov 16, 2023","text":"<p>The main changes in this release are:</p> <ul> <li> <p>New <code>predict_quantiles</code> method in all Autoreg Forecasters to calculate the specified quantiles for each step.</p> </li> <li> <p>Create <code>ForecasterBaseline.ForecasterEquivalentDate</code>, a Forecaster to create simple model that serves as a basic reference for evaluating the performance of more complex models.</p> </li> </ul> <p>Added</p> <ul> <li> <p>Added <code>skforecast.datasets</code> module. It contains functions to load data for our examples and user guides.</p> </li> <li> <p>Added <code>predict_quantiles</code> method to all Autoreg Forecasters.</p> </li> <li> <p>Added <code>SkforecastVersionWarning</code> to the <code>exception</code> module. This warning notify that the skforecast version installed in the environment differs from the version used to initialize the forecaster when using <code>load_forecaster</code>.</p> </li> <li> <p>Create <code>ForecasterBaseline.ForecasterEquivalentDate</code>, a Forecaster to create simple model that serves as a basic reference for evaluating the performance of more complex models.</p> </li> </ul> <p>Changed</p> <ul> <li>Enhance the management of internal copying in skforecast to minimize the number of copies, thereby accelerating data processing.</li> </ul> <p>Fixed</p> <ul> <li> <p>Rename <code>self.skforecast_version</code> attribute to <code>self.skforecast_version</code> in all Forecasters.</p> </li> <li> <p>Fixed a bug where the <code>create_train_X_y</code> method did not correctly align lags and exogenous variables when the index was not a Pandas index in all Forecasters.</p> </li> </ul>"},{"location":"releases/releases.html#0.10.1","title":"0.10.1 Sep 26, 2023","text":"<p>This is a minor release to fix a bug when using <code>grid_search_forecaster</code>, <code>random_search_forecaster</code> or <code>bayesian_search_forecaster</code> with a Forecaster that includes differentiation.</p> <p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li>Bug fix <code>grid_search_forecaster</code>, <code>random_search_forecaster</code> or <code>bayesian_search_forecaster</code> with a Forecaster that includes differentiation.</li> </ul>"},{"location":"releases/releases.html#0.10.0","title":"0.10.0 Sep 07, 2023","text":"<p>The main changes in this release are:</p> <ul> <li> <p>New <code>Sarimax.Sarimax</code> model. A wrapper of <code>statsmodels.SARIMAX</code> that follows the scikit-learn API and can be used with the <code>ForecasterSarimax</code>.</p> </li> <li> <p>Added <code>differentiation</code> argument to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code> to model the n-order differentiated time series using the new skforecast preprocessor <code>TimeSeriesDifferentiator</code>.</p> </li> </ul> <p>Added</p> <ul> <li> <p>New <code>Sarimax.Sarimax</code> model. A wrapper of <code>statsmodels.SARIMAX</code> that follows the scikit-learn API.</p> </li> <li> <p>Added <code>skforecast.preprocessing.TimeSeriesDifferentiator</code> to preprocess time series by differentiating or integrating them (reverse differentiation).</p> </li> <li> <p>Added <code>differentiation</code> argument to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code> to model the n-order differentiated time series.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Refactor <code>ForecasterSarimax</code> to work with both skforecast Sarimax and pmdarima ARIMA models.</p> </li> <li> <p>Replace <code>setup.py</code> with <code>pyproject.toml</code>.</p> </li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.9.1","title":"0.9.1 Jul 14, 2023","text":"<p>The main changes in this release are:</p> <ul> <li>Fix imports in <code>skforecast.utils</code> module to correctly import <code>sklearn.linear_model</code> into the <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code> functions.</li> </ul> <p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li>Fix imports in <code>skforecast.utils</code> module to correctly import <code>sklearn.linear_model</code> into the <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code> functions.</li> </ul>"},{"location":"releases/releases.html#0.9.0","title":"0.9.0 Jul 09, 2023","text":"<p>The main changes in this release are:</p> <ul> <li> <p><code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code> include the <code>n_jobs</code> argument in their <code>fit</code> method, allowing multi-process parallelization for improved performance.</p> </li> <li> <p>All backtesting and grid search functions have been extended to include the <code>n_jobs</code> argument, allowing multi-process parallelization for improved performance.</p> </li> <li> <p>Argument <code>refit</code> now can be also an <code>integer</code> in all backtesting dependent functions in modules <code>model_selection</code>, <code>model_selection_multiseries</code>, and <code>model_selection_sarimax</code>. This allows the Forecaster to be trained every this number of iterations.</p> </li> <li> <p><code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> can be trained using series of different lengths. This means that the model can handle datasets with different numbers of data points in each series.</p> </li> </ul> <p>Added</p> <ul> <li> <p>Support for <code>scikit-learn 1.3.x</code>.</p> </li> <li> <p>Argument <code>n_jobs='auto'</code> to <code>fit</code> method in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code> to allow multi-process parallelization.</p> </li> <li> <p>Argument <code>n_jobs='auto'</code> to all backtesting dependent functions in modules <code>model_selection</code>, <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> to allow multi-process parallelization.</p> </li> <li> <p>Argument <code>refit</code> now can be also an <code>integer</code> in all backtesting dependent functions in modules <code>model_selection</code>, <code>model_selection_multiseries</code>, and <code>model_selection_sarimax</code>. This allows the Forecaster to be trained every this number of iterations.</p> </li> <li> <p><code>ForecasterAutoregMultiSeries</code> and <code>ForecasterAutoregMultiSeriesCustom</code> allow to use series of different lengths for training.</p> </li> <li> <p>Added <code>show_progress</code> to grid search functions.</p> </li> <li> <p>Added functions <code>select_n_jobs_backtesting</code> and <code>select_n_jobs_fit_forecaster</code> to <code>utils</code> to select the number of jobs to use during multi-process parallelization.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Remove <code>get_feature_importance</code> in favor of <code>get_feature_importances</code> in all Forecasters, (deprecated since 0.8.0).</p> </li> <li> <p>The <code>model_selection._create_backtesting_folds</code> function now also returns the last window indices and whether or not to train the forecaster.</p> </li> <li> <p>The <code>model_selection</code> functions <code>_backtesting_forecaster_refit</code> and <code>_backtesting_forecaster_no_refit</code> have been unified in <code>_backtesting_forecaster</code>.</p> </li> <li> <p>The <code>model_selection_multiseries</code> functions <code>_backtesting_forecaster_multiseries_refit</code> and <code>_backtesting_forecaster_multiseries_no_refit</code> have been unified in <code>_backtesting_forecaster_multiseries</code>.</p> </li> <li> <p>The <code>model_selection_sarimax</code> functions <code>_backtesting_refit_sarimax</code> and <code>_backtesting_no_refit_sarimax</code> have been unified in <code>_backtesting_sarimax</code>.</p> </li> <li> <p><code>utils.preprocess_y</code> allows a pandas DataFrame as input.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Ensure reproducibility of Direct Forecasters when using <code>predict_bootstrapping</code>, <code>predict_dist</code> and <code>predict_interval</code> with a <code>list</code> of steps.</p> </li> <li> <p>The <code>create_train_X_y</code> method returns a dict of pandas Series as <code>y_train</code> in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>. This ensures that each series has the appropriate index according to the step to be trained.</p> </li> <li> <p>The <code>filter_train_X_y_for_step</code> method in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code> now updates the index of <code>X_train_step</code> to ensure correct alignment with <code>y_train_step</code>.</p> </li> </ul>"},{"location":"releases/releases.html#0.8.1","title":"0.8.1 May 27, 2023","text":"<p>Added</p> <ul> <li>Argument <code>store_in_sample_residuals=True</code> in <code>fit</code> method added to all forecasters to speed up functions such as backtesting.</li> </ul> <p>Changed</p> <ul> <li>Refactor <code>utils.exog_to_direct</code> and <code>utils.exog_to_direct_numpy</code> to increase performance.</li> </ul> <p>Fixed</p> <ul> <li><code>utils.check_exog_dtypes</code> now compares the <code>dtype.name</code> instead of the <code>dtype</code>. (suggested by Metaming https://github.com/Metaming)</li> </ul>"},{"location":"releases/releases.html#0.8.0","title":"0.8.0 May 16, 2023","text":"<p>Added</p> <ul> <li> <p>Added the <code>fit_kwargs</code> argument to all forecasters to allow the inclusion of additional keyword arguments passed to the regressor's <code>fit</code> method.</p> </li> <li> <p>Added the <code>set_fit_kwargs</code> method to set the <code>fit_kwargs</code> attribute.</p> </li> <li> <p>Support for <code>pandas 2.0.x</code>.</p> </li> <li> <p>Added <code>exceptions</code> module with custom warnings.</p> </li> <li> <p>Added function <code>utils.check_exog_dtypes</code> to issue a warning if exogenous variables are one of type <code>init</code>, <code>float</code>, or <code>category</code>. Raise Exception if <code>exog</code> has categorical columns with non integer values.</p> </li> <li> <p>Added function <code>utils.get_exog_dtypes</code> to get the data types of the exogenous variables included during the training of the forecaster model. </p> </li> <li> <p>Added function <code>utils.cast_exog_dtypes</code> to cast data types of the exogenous variables using a dictionary as a mapping.</p> </li> <li> <p>Added function <code>utils.check_select_fit_kwargs</code> to check if the argument <code>fit_kwargs</code> is a dictionary and select only the keys used by the <code>fit</code> method of the regressor.</p> </li> <li> <p>Added function <code>model_selection._create_backtesting_folds</code> to provide train/test indices (position) for backtesting functions.</p> </li> <li> <p>Added argument <code>gap</code> to functions in <code>model_selection</code>, <code>model_selection_multiseries</code> and <code>model_selection_sarimax</code> to omit observations between training and prediction.</p> </li> <li> <p>Added argument <code>show_progress</code> to functions <code>model_selection.backtesting_forecaster</code>, <code>model_selection_multiseries.backtesting_forecaster_multiseries</code> and <code>model_selection_sarimax.backtesting_forecaster_sarimax</code> to indicate weather to show a progress bar.</p> </li> <li> <p>Added argument <code>remove_suffix</code>, default <code>False</code>, to the method <code>filter_train_X_y_for_step()</code> in <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>. If <code>remove_suffix=True</code> the suffix \"_step_i\" will be removed from the column names of the training matrices.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Rename optional dependency package <code>statsmodels</code> to <code>sarimax</code>. Now only <code>pmdarima</code> will be installed, <code>statsmodels</code> is no longer needed.</p> </li> <li> <p>Rename <code>get_feature_importance()</code> to <code>get_feature_importances()</code> in all Forecasters. <code>get_feature_importance()</code> method will me removed in skforecast 0.9.0.</p> </li> <li> <p>Refactor <code>get_feature_importances()</code> in all Forecasters.</p> </li> <li> <p>Remove <code>model_selection_statsmodels</code> in favor of <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code>, (deprecated since 0.7.0).</p> </li> <li> <p>Remove attributes <code>create_predictors</code> and <code>source_code_create_predictors</code> in favor of <code>fun_predictors</code> and <code>source_code_fun_predictors</code> in <code>ForecasterAutoregCustom</code>, (deprecated since 0.7.0).</p> </li> <li> <p>The <code>utils.check_exog</code> function now includes a new optional parameter, <code>allow_nan</code>, that controls whether a warning should be issued if the input <code>exog</code> contains NaN values. </p> </li> <li> <p><code>utils.check_exog</code> is applied before and after <code>exog</code> transformations.</p> </li> <li> <p>The <code>utils.preprocess_y</code> function now includes a new optional parameter, <code>return_values</code>, that controls whether to return a numpy ndarray with the values of y or not. This new option is intended to avoid copying data when it is not necessary.</p> </li> <li> <p>The <code>utils.preprocess_exog</code> function now includes a new optional parameter, <code>return_values</code>, that controls whether to return a numpy ndarray with the values of y or not. This new option is intended to avoid copying data when it is not necessary.</p> </li> <li> <p>Replaced <code>tqdm.tqdm</code> by <code>tqdm.auto.tqdm</code>.</p> </li> <li> <p>Refactor <code>utils.exog_to_direct</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li>The dtypes of exogenous variables are maintained when generating the training matrices with the <code>create_train_X_y</code> method in all the Forecasters.</li> </ul>"},{"location":"releases/releases.html#0.7.0","title":"0.7.0 Mar 21, 2023","text":"<p>Added</p> <ul> <li> <p>Class <code>ForecasterAutoregMultiSeriesCustom</code>.</p> </li> <li> <p>Class <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code> (wrapper of pmdarima).</p> </li> <li> <p>Method <code>predict_interval()</code> to <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiVariate</code>.</p> </li> <li> <p>Method <code>predict_bootstrapping()</code> to all forecasters, generate multiple forecasting predictions using a bootstrapping process.</p> </li> <li> <p>Method <code>predict_dist()</code> to all forecasters, fit a given probability distribution for each step using a bootstrapping process.</p> </li> <li> <p>Function <code>plot_prediction_distribution</code> in module <code>plot</code>.</p> </li> <li> <p>Alias <code>backtesting_forecaster_multivariate</code> for <code>backtesting_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Alias <code>grid_search_forecaster_multivariate</code> for <code>grid_search_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Alias <code>random_search_forecaster_multivariate</code> for <code>random_search_forecaster_multiseries</code> in <code>model_selection_multiseries</code> module.</p> </li> <li> <p>Attribute <code>forecaster_id</code> to all Forecasters.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Deprecated <code>python 3.7</code> compatibility.</p> </li> <li> <p>Added <code>python 3.11</code> compatibility.</p> </li> <li> <p><code>model_selection_statsmodels</code> is deprecated in favor of <code>ForecasterSarimax</code> and <code>model_selection_sarimax</code>. It will be removed in version 0.8.0.</p> </li> <li> <p>Remove <code>levels_weights</code> argument in <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code>, deprecated since version 0.6.0. Use <code>series_weights</code> and <code>weights_func</code> when creating the forecaster instead.</p> </li> <li> <p>Attributes <code>create_predictors</code> and <code>source_code_create_predictors</code> renamed to <code>fun_predictors</code> and <code>source_code_fun_predictors</code> in <code>ForecasterAutoregCustom</code>. Old names will be removed in version 0.8.0.</p> </li> <li> <p>Remove engine <code>'skopt'</code> in <code>bayesian_search_forecaster</code> in favor of engine <code>'optuna'</code>. To continue using it, use skforecast 0.6.0.</p> </li> <li> <p><code>in_sample_residuals</code> and <code>out_sample_residuals</code> are stored as numpy ndarrays instead of pandas series.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>set_out_sample_residuals()</code> is now expecting a <code>dict</code> for the <code>residuals</code> argument instead of a <code>pandas DataFrame</code>.</p> </li> <li> <p>Remove the <code>scikit-optimize</code> dependency.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Remove operator <code>**</code> in <code>set_params()</code> method for all forecasters.</p> </li> <li> <p>Replace <code>getfullargspec</code> in favor of <code>inspect.signature</code> (contribution by @jordisilv).</p> </li> </ul>"},{"location":"releases/releases.html#0.6.0","title":"0.6.0 Nov 30, 2022","text":"<p>Added</p> <ul> <li> <p>Class <code>ForecasterAutoregMultivariate</code>.</p> </li> <li> <p>Function <code>initialize_lags</code> in <code>utils</code> module  to create lags values in the initialization of forecasters (applies to all forecasters).</p> </li> <li> <p>Function <code>initialize_weights</code> in <code>utils</code> module to check and initialize arguments <code>series_weights</code>and <code>weight_func</code> (applies to all forecasters).</p> </li> <li> <p>Argument <code>weights_func</code> in all Forecasters to allow weighted time series forecasting. Individual time based weights can be assigned to each value of the series during the model training.</p> </li> <li> <p>Argument <code>series_weights</code> in <code>ForecasterAutoregMultiSeries</code> to define individual weights each series.</p> </li> <li> <p>Include argument <code>random_state</code> in all Forecasters <code>set_out_sample_residuals</code> methods for random sampling with reproducible output.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>predict</code> and <code>predict_interval</code> methods allow the simultaneous prediction of multiple levels.</p> </li> <li> <p><code>backtesting_forecaster_multiseries</code> allows backtesting multiple levels simultaneously.</p> </li> <li> <p><code>metric</code> argument can be a list in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code>. If <code>metric</code> is a <code>list</code>, multiple metrics will be calculated. (suggested by Pablo D\u00e1vila Herrero https://github.com/Pablo-Davila)</p> </li> <li> <p>Function <code>multivariate_time_series_corr</code> in module <code>utils</code>.</p> </li> <li> <p>Function <code>plot_multivariate_time_series_corr</code> in module <code>plot</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>ForecasterAutoregDirect</code> allows to predict specific steps.</p> </li> <li> <p>Remove <code>ForecasterAutoregMultiOutput</code> in favor of <code>ForecasterAutoregDirect</code>, (deprecated since 0.5.0).</p> </li> <li> <p>Rename function <code>exog_to_multi_output</code> to <code>exog_to_direct</code> in <code>utils</code> module.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, rename parameter <code>series_levels</code> to <code>series_col_names</code>.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code> change type of <code>out_sample_residuals</code> to a <code>dict</code> of numpy ndarrays.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, delete argument <code>level</code> from method <code>set_out_sample_residuals</code>.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, <code>level</code> argument of <code>predict</code> and <code>predict_interval</code> renamed to <code>levels</code>.</p> </li> <li> <p>In <code>backtesting_forecaster_multiseries</code>, <code>level</code> argument of <code>predict</code> and <code>predict_interval</code> renamed to <code>levels</code>.</p> </li> <li> <p>In <code>check_predict_input</code> function, argument <code>level</code> renamed to <code>levels</code> and <code>series_levels</code> renamed to <code>series_col_names</code>.</p> </li> <li> <p>In <code>backtesting_forecaster_multiseries</code>, <code>metrics_levels</code> output is now a pandas DataFrame.</p> </li> <li> <p>In <code>grid_search_forecaster_multiseries</code> and <code>random_search_forecaster_multiseries</code>, argument <code>levels_weights</code> is deprecated since version 0.6.0, and will be removed in version 0.7.0. Use <code>series_weights</code> and <code>weights_func</code> when creating the forecaster instead.</p> </li> <li> <p>Refactor <code>_create_lags_</code> in <code>ForecasterAutoreg</code>, <code>ForecasterAutoregDirect</code> and <code>ForecasterAutoregMultiSeries</code>. (suggested by Bennett https://github.com/Bennett561)</p> </li> <li> <p>Refactor <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code>.</p> </li> <li> <p>In <code>ForecasterAutoregDirect</code>, <code>filter_train_X_y_for_step</code> now starts at 1 (before 0).</p> </li> <li> <p>In <code>ForecasterAutoregDirect</code>, DataFrame <code>y_train</code> now start with 1, <code>y_step_1</code> (before <code>y_step_0</code>).</p> </li> <li> <p>Remove <code>cv_forecaster</code> from module <code>model_selection</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, argument <code>last_window</code> predict method now works when it is a pandas DataFrame.</p> </li> <li> <p>In <code>ForecasterAutoregMultiSeries</code>, fix bug transformers initialization.</p> </li> </ul>"},{"location":"releases/releases.html#0.5.1","title":"0.5.1 Oct 05, 2022","text":"<p>Added</p> <ul> <li> <p>Check that <code>exog</code> and <code>y</code> have the same length in <code>_evaluate_grid_hyperparameters</code> and <code>bayesian_search_forecaster</code> to avoid fit exception when <code>return_best</code>.</p> </li> <li> <p>Check that <code>exog</code> and <code>series</code> have the same length in <code>_evaluate_grid_hyperparameters_multiseries</code> to avoid fit exception when <code>return_best</code>.</p> </li> </ul> <p>Changed</p> <ul> <li>Argument <code>levels_list</code> in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code> renamed to <code>levels</code>.</li> </ul> <p>Fixed</p> <ul> <li> <p><code>ForecasterAutoregMultiOutput</code> updated to match <code>ForecasterAutoregDirect</code>.</p> </li> <li> <p>Fix Exception to raise when <code>level_weights</code> does not add up to a number close to 1.0 (before was exactly 1.0) in <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code>.</p> </li> <li> <p><code>Create_train_X_y</code> in <code>ForecasterAutoregMultiSeries</code> now works when the forecaster is not fitted.</p> </li> </ul>"},{"location":"releases/releases.html#0.5.0","title":"0.5.0 Sep 23, 2022","text":"<p>Added</p> <ul> <li> <p>New arguments <code>transformer_y</code> (<code>transformer_series</code> for multiseries) and <code>transformer_exog</code> in all forecaster classes. It is for transforming (scaling, max-min, ...) the modeled time series and exogenous variables inside the forecaster.</p> </li> <li> <p>Functions in utils <code>transform_series</code> and <code>transform_dataframe</code> to carry out the transformation of the modeled time series and exogenous variables.</p> </li> <li> <p>Functions <code>_backtesting_forecaster_verbose</code>, <code>random_search_forecaster</code>, <code>_evaluate_grid_hyperparameters</code>, <code>bayesian_search_forecaster</code>, <code>_bayesian_search_optuna</code> and <code>_bayesian_search_skopt</code> in model_selection.</p> </li> <li> <p>Created <code>ForecasterAutoregMultiSeries</code> class for modeling multiple time series simultaneously.</p> </li> <li> <p>Created module <code>model_selection_multiseries</code>. Functions: <code>_backtesting_forecaster_multiseries_refit</code>, <code>_backtesting_forecaster_multiseries_no_refit</code>, <code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>_evaluate_grid_hyperparameters_multiseries</code>.</p> </li> <li> <p>Function <code>_check_interval</code> in utils. (suggested by Thomas Karaouzene https://github.com/tkaraouzene)</p> </li> <li> <p><code>metric</code> can be a list in <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, <code>backtesting_forecaster_multiseries</code>. If <code>metric</code> is a <code>list</code>, multiple metrics will be calculated. (suggested by Pablo D\u00e1vila Herrero https://github.com/Pablo-Davila)</p> </li> <li> <p>Skforecast works with python 3.10.</p> </li> <li> <p>Functions <code>save_forecaster</code> and <code>load_forecaster</code> to module utils.</p> </li> <li> <p><code>get_feature_importance()</code> method checks if the forecast is fitted.</p> </li> </ul> <p>Changed</p> <ul> <li> <p><code>backtesting_forecaster</code> change default value of argument <code>fixed_train_size: bool=True</code>.</p> </li> <li> <p>Remove argument <code>set_out_sample_residuals</code> in function <code>backtesting_forecaster</code> (deprecated since 0.4.2).</p> </li> <li> <p><code>backtesting_forecaster</code> verbose now includes fold size.</p> </li> <li> <p><code>grid_search_forecaster</code> results include the name of the used metric as column name.</p> </li> <li> <p>Remove <code>get_coef</code> method from <code>ForecasterAutoreg</code>, <code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiOutput</code> (deprecated since 0.4.3).</p> </li> <li> <p><code>_get_metric</code> now allows <code>mean_squared_log_error</code>.</p> </li> <li> <p><code>ForecasterAutoregMultiOutput</code> has been renamed to <code>ForecasterAutoregDirect</code>. <code>ForecasterAutoregMultiOutput</code> will be removed in version 0.6.0.</p> </li> <li> <p><code>check_predict_input</code> updated to check <code>ForecasterAutoregMultiSeries</code> inputs.</p> </li> <li> <p><code>set_out_sample_residuals</code> has a new argument <code>transform</code> to transform the residuals before being stored.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p><code>fit</code> now stores <code>last_window</code> values with len = forecaster.max_lag in ForecasterAutoreg and ForecasterAutoregCustom.</p> </li> <li> <p><code>in_sample_residuals</code> stored as a <code>pd.Series</code> when <code>len(residuals) &gt; 1000</code>.</p> </li> </ul>"},{"location":"releases/releases.html#0.4.3","title":"0.4.3 Mar 18, 2022","text":"<p>Added</p> <ul> <li> <p>Checks if all elements in lags are <code>int</code> when creating ForecasterAutoreg and ForecasterAutoregMultiOutput.</p> </li> <li> <p>Add <code>fixed_train_size: bool=False</code> argument to <code>backtesting_forecaster</code> and <code>backtesting_sarimax</code></p> </li> </ul> <p>Changed</p> <ul> <li> <p>Rename <code>get_metric</code> to <code>_get_metric</code>.</p> </li> <li> <p>Functions in model_selection module allow custom metrics.</p> </li> <li> <p>Functions in model_selection_statsmodels module allow custom metrics.</p> </li> <li> <p>Change function <code>set_out_sample_residuals</code> (ForecasterAutoreg and ForecasterAutoregCustom), <code>residuals</code> argument must be a <code>pandas Series</code> (was <code>numpy ndarray</code>).</p> </li> <li> <p>Returned value of backtesting functions (model_selection and model_selection_statsmodels) is now a <code>float</code> (was <code>numpy ndarray</code>).</p> </li> <li> <p><code>get_coef</code> and <code>get_feature_importance</code> methods unified in <code>get_feature_importance</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Requirements versions.</p> </li> <li> <p>Method <code>fit</code> doesn't remove <code>out_sample_residuals</code> each time the forecaster is fitted.</p> </li> <li> <p>Added random seed to residuals downsampling (ForecasterAutoreg and ForecasterAutoregCustom)</p> </li> </ul>"},{"location":"releases/releases.html#0.4.2","title":"0.4.2 Jan 08, 2022","text":"<p>Added</p> <ul> <li> <p>Increased verbosity of function <code>backtesting_forecaster()</code>.</p> </li> <li> <p>Random state argument in <code>backtesting_forecaster()</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>Function <code>backtesting_forecaster()</code> do not modify the original forecaster.</p> </li> <li> <p>Deprecated argument <code>set_out_sample_residuals</code> in function <code>backtesting_forecaster()</code>.</p> </li> <li> <p>Function <code>model_selection.time_series_spliter</code> renamed to <code>model_selection.time_series_splitter</code></p> </li> </ul> <p>Fixed</p> <ul> <li>Methods <code>get_coef</code> and <code>get_feature_importance</code> of <code>ForecasterAutoregMultiOutput</code> class return proper feature names.</li> </ul>"},{"location":"releases/releases.html#0.4.1","title":"0.4.1 Dec 13, 2021","text":"<p>Added</p> <p>Changed</p> <p>Fixed</p> <ul> <li><code>fit</code> and <code>predict</code> transform pandas Series and DataFrames to numpy arrays if regressor is XGBoost.</li> </ul>"},{"location":"releases/releases.html#0.4.0","title":"0.4.0 Dec 10, 2021","text":"<p>Version 0.4 has undergone a huge code refactoring. Main changes are related to input-output formats (only pandas Series and DataFrames are allowed although internally numpy arrays are used for performance) and model validation methods (unified into backtesting with and without refit).</p> <p>Added</p> <ul> <li><code>ForecasterBase</code> as parent class</li> </ul> <p>Changed</p> <ul> <li> <p>Argument <code>y</code> must be pandas Series. Numpy ndarrays are not allowed anymore.</p> </li> <li> <p>Argument <code>exog</code> must be pandas Series or pandas DataFrame. Numpy ndarrays are not allowed anymore.</p> </li> <li> <p>Output of <code>predict</code> is a pandas Series with index according to the steps predicted.</p> </li> <li> <p>Scikit-learn pipelines are allowed as regressors.</p> </li> <li> <p><code>backtesting_forecaster</code> and <code>backtesting_forecaster_intervals</code> have been combined in a single function.</p> <ul> <li>It is possible to backtest forecasters already trained.</li> <li><code>ForecasterAutoregMultiOutput</code> allows incomplete folds.</li> <li>It is possible to update <code>out_sample_residuals</code> with backtesting residuals.</li> </ul> </li> <li> <p><code>cv_forecaster</code> has the option to update <code>out_sample_residuals</code> with backtesting residuals.</p> </li> <li> <p><code>backtesting_sarimax_statsmodels</code> and <code>cv_sarimax_statsmodels</code> have been combined in a single function.</p> </li> <li> <p><code>gridsearch_forecaster</code> use backtesting as validation strategy with the option of refit.</p> </li> <li> <p>Extended information when printing <code>Forecaster</code> object.</p> </li> <li> <p>All static methods for checking and preprocessing inputs moved to module utils.</p> </li> <li> <p>Remove deprecated class <code>ForecasterCustom</code>.</p> </li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.3.0","title":"0.3.0 Sep 01, 2021","text":"<p>Added</p> <ul> <li> <p>New module model_selection_statsmodels to cross-validate, backtesting and grid search AutoReg and SARIMAX models from statsmodels library:</p> <ul> <li><code>backtesting_autoreg_statsmodels</code></li> <li><code>cv_autoreg_statsmodels</code></li> <li><code>backtesting_sarimax_statsmodels</code></li> <li><code>cv_sarimax_statsmodels</code></li> <li><code>grid_search_sarimax_statsmodels</code></li> </ul> </li> <li> <p>Added attribute window_size to <code>ForecasterAutoreg</code> and <code>ForecasterAutoregCustom</code>. It is equal to <code>max_lag</code>.</p> </li> </ul> <p>Changed</p> <ul> <li><code>cv_forecaster</code> returns cross-validation metrics and cross-validation predictions.</li> <li>Added an extra column for each parameter in the dataframe returned by <code>grid_search_forecaster</code>.</li> <li>statsmodels 0.12.2 added to requirements</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.2.0","title":"0.2.0 Aug 26, 2021","text":"<p>Added</p> <ul> <li> <p>Multiple exogenous variables can be passed as pandas DataFrame.</p> </li> <li> <p>Documentation at https://skforecast.org</p> </li> <li> <p>New unit test</p> </li> <li> <p>Increased typing</p> </li> </ul> <p>Changed</p> <ul> <li>New implementation of <code>ForecasterAutoregMultiOutput</code>. The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old version can be access with <code>skforecast.deprecated.ForecasterAutoregMultiOutput</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.1.9","title":"0.1.9 Jul 27, 2021","text":"<p>Added</p> <ul> <li> <p>Logging total number of models to fit in <code>grid_search_forecaster</code>.</p> </li> <li> <p>Class <code>ForecasterAutoregCustom</code>.</p> </li> <li> <p>Method <code>create_train_X_y</code> to facilitate access to the training data matrix created from <code>y</code> and <code>exog</code>.</p> </li> </ul> <p>Changed</p> <ul> <li> <p>New implementation of <code>ForecasterAutoregMultiOutput</code>. The training process in the new version creates a different X_train for each step. See Direct multi-step forecasting for more details. Old version can be accessed with <code>skforecast.deprecated.ForecasterAutoregMultiOutput</code>.</p> </li> <li> <p>Class <code>ForecasterCustom</code> has been renamed to <code>ForecasterAutoregCustom</code>. However, <code>ForecasterCustom</code> will still remain to keep backward compatibility.</p> </li> <li> <p>Argument <code>metric</code> in <code>cv_forecaster</code>, <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code> and <code>backtesting_forecaster_intervals</code> changed from 'neg_mean_squared_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error' to 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'.</p> </li> <li> <p>Check if argument <code>metric</code> in <code>cv_forecaster</code>, <code>backtesting_forecaster</code>, <code>grid_search_forecaster</code> and <code>backtesting_forecaster_intervals</code> is one of 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'.</p> </li> <li> <p><code>time_series_spliter</code> doesn't include the remaining observations in the last complete fold but in a new one when <code>allow_incomplete_fold=True</code>. Take in consideration that incomplete folds with few observations could overestimate or underestimate the validation metric.</p> </li> </ul> <p>Fixed</p> <ul> <li>Update lags of  <code>ForecasterAutoregMultiOutput</code> after <code>grid_search_forecaster</code>.</li> </ul>"},{"location":"releases/releases.html#0.1.8.1","title":"0.1.8.1 May 17, 2021","text":"<p>Added</p> <ul> <li><code>set_out_sample_residuals</code> method to store or update out of sample residuals used by <code>predict_interval</code>.</li> </ul> <p>Changed</p> <ul> <li> <p><code>backtesting_forecaster_intervals</code> and <code>backtesting_forecaster</code> print number of steps per fold.</p> </li> <li> <p>Only stored up to 1000 residuals.</p> </li> <li> <p>Improved verbose in <code>backtesting_forecaster_intervals</code>.</p> </li> </ul> <p>Fixed</p> <ul> <li> <p>Warning of incomplete folds when using <code>backtesting_forecast</code> with a  <code>ForecasterAutoregMultiOutput</code>.</p> </li> <li> <p><code>ForecasterAutoregMultiOutput.predict</code> allow exog data longer than needed (steps).</p> </li> <li> <p><code>backtesting_forecast</code> prints correctly the number of folds when remainder observations are cero.</p> </li> <li> <p>Removed named argument X in <code>self.regressor.predict(X)</code> to allow using XGBoost regressor.</p> </li> <li> <p>Values stored in <code>self.last_window</code> when training <code>ForecasterAutoregMultiOutput</code>. </p> </li> </ul>"},{"location":"releases/releases.html#0.1.8","title":"0.1.8 Apr 02, 2021","text":"<p>Added</p> <ul> <li>Class <code>ForecasterAutoregMultiOutput.py</code>: forecaster with direct multi-step predictions.</li> <li>Method <code>ForecasterCustom.predict_interval</code> and  <code>ForecasterAutoreg.predict_interval</code>: estimate prediction interval using bootstrapping.</li> <li><code>skforecast.model_selection.backtesting_forecaster_intervals</code> perform backtesting and return prediction intervals.</li> </ul> <p>Changed</p> <p>Fixed</p>"},{"location":"releases/releases.html#0.1.7","title":"0.1.7 Mar 19, 2021","text":"<p>Added</p> <ul> <li>Class <code>ForecasterCustom</code>: same functionalities as <code>ForecasterAutoreg</code> but allows custom definition of predictors.</li> </ul> <p>Changed</p> <ul> <li><code>grid_search forecaster</code> adapted to work with objects <code>ForecasterCustom</code> in addition to <code>ForecasterAutoreg</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.1.6","title":"0.1.6 Mar 14, 2021","text":"<p>Added</p> <ul> <li>Method <code>get_feature_importances</code> to <code>skforecast.ForecasterAutoreg</code>.</li> <li>Added backtesting strategy in <code>grid_search_forecaster</code>.</li> <li>Added <code>backtesting_forecast</code> to <code>skforecast.model_selection</code>.</li> </ul> <p>Changed</p> <ul> <li>Method <code>create_lags</code> return a matrix where the order of columns match the ascending order of lags. For example, column 0 contains the values of the minimum lag used as predictor.</li> <li>Renamed argument <code>X</code> to <code>last_window</code> in method <code>predict</code>.</li> <li>Renamed <code>ts_cv_forecaster</code> to <code>cv_forecaster</code>.</li> </ul> <p>Fixed</p>"},{"location":"releases/releases.html#0.1.4","title":"0.1.4 Feb 15, 2021","text":"<p>Added</p> <ul> <li>Method <code>get_coef</code> to <code>skforecast.ForecasterAutoreg</code>.</li> </ul> <p>Changed</p> <p>Fixed</p>"},{"location":"user_guides/autoregresive-forecaster.html","title":"Recursive multi-step forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.plot import plot_prediction_intervals, set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error from skforecast.datasets import fetch_dataset from skforecast.preprocessing import RollingFeatures from skforecast.recursive import ForecasterRecursive from skforecast.plot import plot_prediction_intervals, set_dark_theme In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=10),\n                 transformer_y   = None, \n             )\n\nforecaster.fit(y=data_train, store_in_sample_residuals=True)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=10),                  transformer_y   = None,               )  forecaster.fit(y=data_train, store_in_sample_residuals=True) forecaster Out[3]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]</li> <li>Window features: ['roll_mean_10']</li> <li>Window size: 15</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:28:04</li> <li>Last fit date: 2025-08-06 13:28:07</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[4]: <pre>2005-07-01    1.026507\n2005-08-01    1.042429\n2005-09-01    1.116730\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[6]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             ) print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.006632513357651682\n</pre> In\u00a0[7]: Copied! <pre># Predict intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(\n                    steps    = 36,\n                    interval = [5, 95],\n                    method   = 'bootstrapping',\n                    n_boot   = 100\n             )\npredictions.head(3)\n</pre> # Predict intervals # ============================================================================== predictions = forecaster.predict_interval(                     steps    = 36,                     interval = [5, 95],                     method   = 'bootstrapping',                     n_boot   = 100              ) predictions.head(3) Out[7]: pred lower_bound upper_bound 2005-07-01 1.026507 0.992734 1.104815 2005-08-01 1.042429 1.021760 1.120737 2005-09-01 1.116730 1.061465 1.196657 In\u00a0[8]: Copied! <pre># Plot prediction interval\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_test,\n    target_variable     = 'y',\n    title               = \"Prediction intervals\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1},\n    ax                  = ax\n)\nax.legend(loc='upper left');\n</pre> # Plot prediction interval # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) plot_prediction_intervals(     predictions         = predictions,     y_true              = data_test,     target_variable     = 'y',     title               = \"Prediction intervals\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1},     ax                  = ax ) ax.legend(loc='upper left'); In\u00a0[9]: Copied! <pre>forecaster.get_feature_importances()\n</pre> forecaster.get_feature_importances() Out[9]: feature importance 11 lag_12 88 10 lag_11 47 1 lag_2 45 0 lag_1 35 12 lag_13 35 15 roll_mean_10 30 8 lag_9 29 6 lag_7 28 5 lag_6 26 13 lag_14 24 2 lag_3 23 14 lag_15 20 4 lag_5 19 9 lag_10 19 3 lag_4 17 7 lag_8 15 <p> \ud83d\udca1 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[10]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 differentiation = 1\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  differentiation = 1              )  forecaster.fit(y=data_train) forecaster Out[10]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]</li> <li>Window features: None</li> <li>Window size: 16</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: 1</li> <li>Creation date: 2025-08-06 13:28:16</li> <li>Last fit date: 2025-08-06 13:28:16</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[11]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[11]: <pre>2005-07-01    0.909529\n2005-08-01    0.941633\n2005-09-01    1.007650\nFreq: MS, Name: pred, dtype: float64</pre>"},{"location":"user_guides/autoregresive-forecaster.html#recursive-multi-step-forecasting","title":"Recursive multi-step forecasting\u00b6","text":"<p>Recursive multi-step forecasting involves using the predicted values from previous time steps as input to forecast the values for the subsequent time steps. The model initially predicts one time step ahead and then uses that forecast as an input for the next time step, continuing this recursive process until the desired forecast horizon is reached.</p> <p>To clarify, since the value of $t_{n-1}$ is required to predict $t_{n}$, and $t_{n-1}$ is unknown, a recursive process is applied in which, each new prediction is based on the previous one.</p> <p> Diagram of recursive multi-step forecasting </p> <p> Recursive forecasting </p> <p>When using machine learning models for recursive multi-step forecasting, one of the major challenges is transforming the time series data into a matrix format that relates each value of the series to the preceding time window (lags). This process can be complex and time-consuming, requiring careful feature engineering to ensure the model can accurately capture the underlying patterns in the data.</p> <p> Time series transformation including an exogenous variable </p> <p>By using the classes <code>ForecasterRecursive</code>, it is possible to build machine learning models for recursive multi-step forecasting with ease. These classes can automatically transform the time series data into a matrix format suitable for input to a machine learning algorithm, and provide a range of options for tuning the model parameters to achieve the best possible performance.</p>"},{"location":"user_guides/autoregresive-forecaster.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#prediction-intervals","title":"Prediction intervals\u00b6","text":"<p>A prediction interval defines the interval within which the true value of <code>y</code> is expected to be found with a given probability. For example, the prediction interval (5, 95) can be expected to contain the true prediction value with 90% probability. Skforecast implemets several methods to estimate prediction intervals, for a detailed explanation of the different methods visit Probabilistic forecasting.</p>"},{"location":"user_guides/autoregresive-forecaster.html#feature-importances","title":"Feature importances\u00b6","text":""},{"location":"user_guides/autoregresive-forecaster.html#training-and-prediction-matrices","title":"Training and prediction matrices\u00b6","text":"<p>While the primary goal of building forecasting models is to predict future values, it is equally important to evaluate if the model is effectively learning from the training data. Analyzing predictions on the training data or exploring the prediction matrices is crucial for assessing model performance and understanding areas for optimization. This process can help identify issues like overfitting or underfitting, as well as provide deeper insights into the model\u2019s decision-making process. Check the How to Extract Training and Prediction Matrices user guide for more information.</p>"},{"location":"user_guides/autoregresive-forecaster.html#differentiation","title":"Differentiation\u00b6","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p>"},{"location":"user_guides/backtesting.html","title":"Backtesting forecaster","text":"<p> \u270e Note </p> <p>All backtesting and grid search functions have been extended to include the <code>n_jobs</code> argument, allowing multi-process parallelization for improved performance. This applies to all functions of the different <code>model_selection</code> modules.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.plot import plot_prediction_intervals, set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error from skforecast.datasets import fetch_dataset from skforecast.preprocessing import RollingFeatures from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster from skforecast.plot import plot_prediction_intervals, set_dark_theme In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data[['y']]\ndata = data.sort_index()\ndata.head(3)\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data[['y']] data = data.sort_index() data.head(3) <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> Out[2]: y datetime 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 In\u00a0[3]: Copied! <pre># Train-validation dates\n# ==============================================================================\nend_train = '2002-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[:end_train, 'y'].plot(ax=ax, label='train')\ndata.loc[end_train:, 'y'].plot(ax=ax, label='validation')\nax.legend()\nplt.show();\n</pre> # Train-validation dates # ============================================================================== end_train = '2002-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(6, 3)) data.loc[:end_train, 'y'].plot(ax=ax, label='train') data.loc[end_train:, 'y'].plot(ax=ax, label='validation') ax.legend() plt.show(); <pre>Train dates      : 1991-07-01 00:00:00 --- 2002-01-01 00:00:00  (n=127)\nValidation dates : 2002-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=77)\n</pre> In\u00a0[4]: Copied! <pre># Create TimeSeriesFold\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = len(data.loc[:end_train]),\n         window_size           = 10,\n         refit                 = True,\n         fixed_train_size      = False,\n         gap                   = 0,\n         allow_incomplete_fold = True,\n         verbose               = False\n     )\n\ncv.split(X=data, as_pandas=True)\n</pre> # Create TimeSeriesFold # ============================================================================== cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = len(data.loc[:end_train]),          window_size           = 10,          refit                 = True,          fixed_train_size      = False,          gap                   = 0,          allow_incomplete_fold = True,          verbose               = False      )  cv.split(X=data, as_pandas=True) Out[4]: fold train_start train_end last_window_start last_window_end test_start test_end test_start_with_gap test_end_with_gap fit_forecaster 0 0 0 127 117 127 127 137 127 137 True 1 1 0 137 127 137 137 147 137 147 True 2 2 0 147 137 147 147 157 147 157 True 3 3 0 157 147 157 157 167 157 167 True 4 4 0 167 157 167 167 177 167 177 True 5 5 0 177 167 177 177 187 177 187 True 6 6 0 187 177 187 187 197 187 197 True 7 7 0 197 187 197 197 204 197 204 True <p>It is posible to use a datetime (string compatible with pandas or a pandas Timestamp) as <code>initial_train_size</code>. In this case, the initial training set will include all observations up to the specified date (inclusive).</p> In\u00a0[5]: Copied! <pre># Create TimeSeriesFold\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = '2002-01-01 00:00:00',  # end_train\n         window_size           = 10,\n         refit                 = True,\n         fixed_train_size      = False,\n         gap                   = 0,\n         allow_incomplete_fold = True,\n         verbose               = False\n     )\n\ncv.split(X=data, as_pandas=True)\n</pre> # Create TimeSeriesFold # ============================================================================== cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = '2002-01-01 00:00:00',  # end_train          window_size           = 10,          refit                 = True,          fixed_train_size      = False,          gap                   = 0,          allow_incomplete_fold = True,          verbose               = False      )  cv.split(X=data, as_pandas=True) Out[5]: fold train_start train_end last_window_start last_window_end test_start test_end test_start_with_gap test_end_with_gap fit_forecaster 0 0 0 127 117 127 127 137 127 137 True 1 1 0 137 127 137 137 147 137 147 True 2 2 0 147 137 147 147 157 147 157 True 3 3 0 157 147 157 157 167 157 167 True 4 4 0 167 157 167 167 177 167 177 True 5 5 0 177 167 177 177 187 177 187 True 6 6 0 187 177 187 187 197 187 197 True 7 7 0 197 187 197 197 204 197 204 True <p>When used in combination with <code>backtesting_forecaster</code>, the <code>window_size</code> argument doesn't need to be specified. This is because <code>backtesting_forecaster</code> automatically sets this value based on the configuration of the forecaster. This ensures that the backtesting process is perfectly tailored to the needs of the forecaster.</p> In\u00a0[6]: Copied! <pre># Backtesting forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[10])\n             )\n\ncv = TimeSeriesFold(\n        steps                 = 10,\n        initial_train_size    = len(data.loc[:end_train]),\n        refit                 = True,\n        fixed_train_size      = False,\n        gap                   = 0,\n        allow_incomplete_fold = True\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster    = forecaster,\n                          y             = data['y'],\n                          cv            = cv,\n                          metric        = 'mean_squared_error',\n                          n_jobs        = 'auto',\n                          verbose       = True,\n                          show_progress = True\n                      )\n</pre> # Backtesting forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[10])              )  cv = TimeSeriesFold(         steps                 = 10,         initial_train_size    = len(data.loc[:end_train]),         refit                 = True,         fixed_train_size      = False,         gap                   = 0,         allow_incomplete_fold = True      )  metric, predictions = backtesting_forecaster(                           forecaster    = forecaster,                           y             = data['y'],                           cv            = cv,                           metric        = 'mean_squared_error',                           n_jobs        = 'auto',                           verbose       = True,                           show_progress = True                       ) <pre>Information of folds\n--------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude between last observed data (last window) and predictions (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   1991-07-01 00:00:00 -- 2007-11-01 00:00:00  (n=197)\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre># Backtest error and predictions\n# ==============================================================================\ndisplay(metric)\npredictions.head(4)\n</pre> # Backtest error and predictions # ============================================================================== display(metric) predictions.head(4) mean_squared_error 0 0.008131 Out[7]: pred 2002-02-01 0.570082 2002-03-01 0.731883 2002-04-01 0.697942 2002-05-01 0.752874 In\u00a0[8]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:, 'y'].plot(ax=ax)\npredictions.plot(ax=ax)\nax.legend()\nplt.show();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:, 'y'].plot(ax=ax) predictions.plot(ax=ax) ax.legend() plt.show(); <p> \u270e Note </p> <p>With an intermittent refit, the model is trained every <code>refit</code> * <code>steps</code> observations. In this case, 3 * 10 = 30 observations. This can be easily observed in the <code>verbose</code> logs.</p> In\u00a0[9]: Copied! <pre># Backtesting with intermittent refit\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[10])\n             )\n\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = len(data.loc[:end_train]),\n         refit                 = 3,\n         fixed_train_size      = False,\n         gap                   = 0,\n         allow_incomplete_fold = True\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster    = forecaster,\n                          y             = data['y'],\n                          cv            = cv,\n                          metric        = 'mean_squared_error',\n                          verbose       = True,\n                          show_progress = True\n                      )\n</pre> # Backtesting with intermittent refit # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[10])              )  cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = len(data.loc[:end_train]),          refit                 = 3,          fixed_train_size      = False,          gap                   = 0,          allow_incomplete_fold = True      )  metric, predictions = backtesting_forecaster(                           forecaster    = forecaster,                           y             = data['y'],                           cv            = cv,                           metric        = 'mean_squared_error',                           verbose       = True,                           show_progress = True                       ) <pre>Information of folds\n--------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude between last observed data (last window) and predictions (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Training:   No training in this fold\n    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\nFold: 2\n    Training:   No training in this fold\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\nFold: 4\n    Training:   No training in this fold\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Training:   No training in this fold\n    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Training:   No training in this fold\n    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[10]: Copied! <pre># Backtest error and predictions\n# ==============================================================================\ndisplay(metric)\npredictions.head(4)\n</pre> # Backtest error and predictions # ============================================================================== display(metric) predictions.head(4) mean_squared_error 0 0.00863 Out[10]: pred 2002-02-01 0.570082 2002-03-01 0.731883 2002-04-01 0.697942 2002-05-01 0.752874 <p> \u270e Note </p> <p>In this example, although only the last 10 predictions (steps) are stored for model evaluation, the total number of predicted steps in each fold is 15 (steps + gap).</p> In\u00a0[11]: Copied! <pre># Backtesting forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[10])\n             )\n\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = len(data.loc[:end_train]),\n         refit                 = True,\n         fixed_train_size      = False,\n         gap                   = 5,\n         allow_incomplete_fold = False\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster    = forecaster,\n                          y             = data['y'],\n                          cv            = cv,\n                          metric        = 'mean_squared_error',\n                          verbose       = True,\n                          show_progress = True\n                      )\n</pre> # Backtesting forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[10])              )  cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = len(data.loc[:end_train]),          refit                 = True,          fixed_train_size      = False,          gap                   = 5,          allow_incomplete_fold = False      )  metric, predictions = backtesting_forecaster(                           forecaster    = forecaster,                           y             = data['y'],                           cv            = cv,                           metric        = 'mean_squared_error',                           verbose       = True,                           show_progress = True                       ) <pre>Information of folds\n--------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 7\n    Number skipped folds: 0 \n    Number of steps per fold: 10\n    Number of steps to exclude between last observed data (last window) and predictions (gap): 5\n    Last fold has been excluded because it was incomplete.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-07-01 00:00:00 -- 2003-04-01 00:00:00  (n=10)\nFold: 1\n    Training:   1991-07-01 00:00:00 -- 2002-11-01 00:00:00  (n=137)\n    Validation: 2003-05-01 00:00:00 -- 2004-02-01 00:00:00  (n=10)\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2004-03-01 00:00:00 -- 2004-12-01 00:00:00  (n=10)\nFold: 3\n    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n    Validation: 2005-01-01 00:00:00 -- 2005-10-01 00:00:00  (n=10)\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-11-01 00:00:00 -- 2006-08-01 00:00:00  (n=10)\nFold: 5\n    Training:   1991-07-01 00:00:00 -- 2006-03-01 00:00:00  (n=177)\n    Validation: 2006-09-01 00:00:00 -- 2007-06-01 00:00:00  (n=10)\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-07-01 00:00:00 -- 2008-04-01 00:00:00  (n=10)\n\n</pre> <pre>  0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> In\u00a0[12]: Copied! <pre># Backtest error and predictions\n# ==============================================================================\ndisplay(metric)\npredictions.head(4)\n</pre> # Backtest error and predictions # ============================================================================== display(metric) predictions.head(4) mean_squared_error 0 0.009746 Out[12]: pred 2002-07-01 0.847688 2002-08-01 0.907662 2002-09-01 0.932360 2002-10-01 0.988048 In\u00a0[13]: Copied! <pre># Backtesting forecaster with refit and skip folds\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[10])\n             )\n\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = len(data.loc[:end_train]),\n         refit                 = True,\n         fixed_train_size      = False,\n         gap                   = 0,\n         skip_folds            = 2,\n         allow_incomplete_fold = True\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster    = forecaster,\n                          y             = data['y'],\n                          cv            = cv,\n                          metric        = 'mean_squared_error',\n                          n_jobs        = 'auto',\n                          verbose       = True,\n                          show_progress = True\n                      )\n</pre> # Backtesting forecaster with refit and skip folds # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[10])              )  cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = len(data.loc[:end_train]),          refit                 = True,          fixed_train_size      = False,          gap                   = 0,          skip_folds            = 2,          allow_incomplete_fold = True      )  metric, predictions = backtesting_forecaster(                           forecaster    = forecaster,                           y             = data['y'],                           cv            = cv,                           metric        = 'mean_squared_error',                           n_jobs        = 'auto',                           verbose       = True,                           show_progress = True                       ) <pre>Information of folds\n--------------------\nNumber of observations used for initial training: 127\nNumber of observations used for backtesting: 77\n    Number of folds: 8\n    Number skipped folds: 4 [1, 3, 5, 7]\n    Number of steps per fold: 10\n    Number of steps to exclude between last observed data (last window) and predictions (gap): 0\n    Last fold only includes 7 observations.\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\nFold: 1\n    Fold skipped\nFold: 2\n    Training:   1991-07-01 00:00:00 -- 2003-09-01 00:00:00  (n=147)\n    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\nFold: 3\n    Fold skipped\nFold: 4\n    Training:   1991-07-01 00:00:00 -- 2005-05-01 00:00:00  (n=167)\n    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\nFold: 5\n    Fold skipped\nFold: 6\n    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\nFold: 7\n    Fold skipped\n\n</pre> <pre>  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> In\u00a0[14]: Copied! <pre># Backtest error and predictions\n# ==============================================================================\ndisplay(metric)\npredictions.head(4)\n</pre> # Backtest error and predictions # ============================================================================== display(metric) predictions.head(4) mean_squared_error 0 0.00537 Out[14]: pred 2002-02-01 0.570082 2002-03-01 0.731883 2002-04-01 0.697942 2002-05-01 0.752874 In\u00a0[15]: Copied! <pre># Backtesting forecaster with prediction intervals\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = Ridge(random_state=123),\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[10])\n             )\n\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = len(data.loc[:end_train]),\n         refit                 = True,\n         fixed_train_size      = False,\n         gap                   = 0,\n         skip_folds            = None,\n         allow_incomplete_fold = True\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster              = forecaster,\n                          y                       = data['y'],\n                          cv                      = cv,\n                          metric                  = 'mean_squared_error',\n                          interval_method         = 'bootstrapping',\n                          interval                = [5, 95],\n                          n_boot                  = 150,\n                          use_in_sample_residuals = True,\n                          verbose                 = False,\n                          show_progress           = True\n                      )\n</pre> # Backtesting forecaster with prediction intervals # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = Ridge(random_state=123),                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[10])              )  cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = len(data.loc[:end_train]),          refit                 = True,          fixed_train_size      = False,          gap                   = 0,          skip_folds            = None,          allow_incomplete_fold = True      )  metric, predictions = backtesting_forecaster(                           forecaster              = forecaster,                           y                       = data['y'],                           cv                      = cv,                           metric                  = 'mean_squared_error',                           interval_method         = 'bootstrapping',                           interval                = [5, 95],                           n_boot                  = 150,                           use_in_sample_residuals = True,                           verbose                 = False,                           show_progress           = True                       ) <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[16]: Copied! <pre># Interval predictions\n# ==============================================================================\npredictions.head()\n</pre> # Interval predictions # ============================================================================== predictions.head() Out[16]: pred lower_bound upper_bound 2002-02-01 0.704045 0.517564 0.798605 2002-03-01 0.674050 0.574914 0.781793 2002-04-01 0.698856 0.522447 0.792217 2002-05-01 0.703732 0.532736 0.806036 2002-06-01 0.734222 0.553879 0.825653 In\u00a0[17]: Copied! <pre># Plot prediction interval\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data['y'],\n    target_variable     = 'y',\n    title               = \"Prediction intervals\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1},\n    ax                  = ax\n)\nax.legend(loc='upper left');\n</pre> # Plot prediction interval # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) plot_prediction_intervals(     predictions         = predictions,     y_true              = data['y'],     target_variable     = 'y',     title               = \"Prediction intervals\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1},     ax                  = ax ) ax.legend(loc='upper left'); In\u00a0[18]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name='h2o_exog', raw=False)\ndata.index.name = 'datetime'\n</pre> # Download data # ============================================================================== data = fetch_dataset(name='h2o_exog', raw=False) data.index.name = 'datetime' <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[19]: Copied! <pre># Train-validation dates\n# ==============================================================================\nend_train = '2002-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"\n    f\"  (n={len(data.loc[end_train:])})\"\n)\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.plot(ax=ax);\n</pre> # Train-validation dates # ============================================================================== end_train = '2002-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.index.max()}\"     f\"  (n={len(data.loc[end_train:])})\" )  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.plot(ax=ax); <pre>Train dates      : 1992-04-01 00:00:00 --- 2002-01-01 00:00:00  (n=118)\nValidation dates : 2002-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=77)\n</pre> In\u00a0[20]: Copied! <pre># Backtest forecaster exogenous variables\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[10])\n             )\n\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = len(data.loc[:end_train]),\n         refit                 = True,\n         fixed_train_size      = False,\n         gap                   = 0,\n         skip_folds            = None,\n         allow_incomplete_fold = True\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data['y'],\n                          exog       = data[['exog_1', 'exog_2']],\n                          cv         = cv,\n                          metric     = 'mean_squared_error'\n                      )\n</pre> # Backtest forecaster exogenous variables # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[10])              )  cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = len(data.loc[:end_train]),          refit                 = True,          fixed_train_size      = False,          gap                   = 0,          skip_folds            = None,          allow_incomplete_fold = True      )  metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data['y'],                           exog       = data[['exog_1', 'exog_2']],                           cv         = cv,                           metric     = 'mean_squared_error'                       ) <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> In\u00a0[21]: Copied! <pre># Backtest error\n# ==============================================================================\nprint(\"Backtest error with exogenous variables:\")\nmetric\n</pre> # Backtest error # ============================================================================== print(\"Backtest error with exogenous variables:\") metric <pre>Backtest error with exogenous variables:\n</pre> Out[21]: mean_squared_error 0 0.007147 In\u00a0[22]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata.loc[end_train:].plot(ax=ax)\npredictions.plot(ax=ax)\nax.legend(loc=\"upper left\")\nplt.show();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data.loc[end_train:].plot(ax=ax) predictions.plot(ax=ax) ax.legend(loc=\"upper left\") plt.show(); In\u00a0[23]: Copied! <pre># Create TimeSeriesFold\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = '2002-01-01 00:00:00',\n         window_size           = 10,\n         refit                 = True,\n         fixed_train_size      = False,\n         gap                   = 0,\n         allow_incomplete_fold = True,\n         verbose               = False\n     )\n\ncv.split(X=data, as_pandas=True)\n</pre> # Create TimeSeriesFold # ============================================================================== cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = '2002-01-01 00:00:00',          window_size           = 10,          refit                 = True,          fixed_train_size      = False,          gap                   = 0,          allow_incomplete_fold = True,          verbose               = False      )  cv.split(X=data, as_pandas=True) Out[23]: fold train_start train_end last_window_start last_window_end test_start test_end test_start_with_gap test_end_with_gap fit_forecaster 0 0 0 118 108 118 118 128 118 128 True 1 1 0 128 118 128 128 138 128 138 True 2 2 0 138 128 138 138 148 138 148 True 3 3 0 148 138 148 148 158 148 158 True 4 4 0 158 148 158 158 168 158 168 True 5 5 0 168 158 168 168 178 168 178 True 6 6 0 178 168 178 178 188 178 188 True 7 7 0 188 178 188 188 195 188 195 True In\u00a0[24]: Copied! <pre># Return predictors values used in backtesting\n# ==============================================================================\nmetric, predictions = backtesting_forecaster(\n                          forecaster        = forecaster,\n                          y                 = data['y'],\n                          exog              = data[['exog_1', 'exog_2']],\n                          cv                = cv,\n                          metric            = 'mean_squared_error',\n                          return_predictors = True\n                      )\npredictions.head(4)\n</pre> # Return predictors values used in backtesting # ============================================================================== metric, predictions = backtesting_forecaster(                           forecaster        = forecaster,                           y                 = data['y'],                           exog              = data[['exog_1', 'exog_2']],                           cv                = cv,                           metric            = 'mean_squared_error',                           return_predictors = True                       ) predictions.head(4) <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> Out[24]: pred fold lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 lag_13 lag_14 lag_15 roll_mean_10 exog_1 exog_2 2002-02-01 0.618160 0 1.145868 1.012313 1.109590 1.024293 0.867444 0.907705 0.851926 0.699431 0.704115 0.648470 0.672569 0.510647 1.043805 1.013024 0.938596 0.897115 1.389827 1.673530 2002-03-01 0.742523 0 0.618160 1.145868 1.012313 1.109590 1.024293 0.867444 0.907705 0.851926 0.699431 0.704115 0.648470 0.672569 0.510647 1.043805 1.013024 0.894085 1.383532 1.596904 2002-04-01 0.685457 0 0.742523 0.618160 1.145868 1.012313 1.109590 1.024293 0.867444 0.907705 0.851926 0.699431 0.704115 0.648470 0.672569 0.510647 1.043805 0.897925 1.381575 1.510958 2002-05-01 0.775086 0 0.685457 0.742523 0.618160 1.145868 1.012313 1.109590 1.024293 0.867444 0.907705 0.851926 0.699431 0.704115 0.648470 0.672569 0.510647 0.896528 1.373176 1.462083 In\u00a0[25]: Copied! <pre># Backtesting with custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n    \"\"\"\n    Calculate the mean squared error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_squared_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\n\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = len(data.loc[:end_train]),\n         refit                 = True,\n         fixed_train_size      = False,\n         gap                   = 0,\n         skip_folds            = None,\n         allow_incomplete_fold = True\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data['y'],\n                          cv         = cv,\n                          metric     = custom_metric\n                      )\n\nmetric\n</pre> # Backtesting with custom metric # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean squared error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_squared_error(y_true[mask], y_pred[mask])          return metric   cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = len(data.loc[:end_train]),          refit                 = True,          fixed_train_size      = False,          gap                   = 0,          skip_folds            = None,          allow_incomplete_fold = True      )  metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data['y'],                           cv         = cv,                           metric     = custom_metric                       )  metric <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> Out[25]: custom_metric 0 0.007179 In\u00a0[26]: Copied! <pre># Backtesting with multiple metric\n# ==============================================================================\nmultiple_metrics = ['mean_squared_error', 'mean_absolute_error', custom_metric]\n\ncv = TimeSeriesFold(\n         steps                 = 10,\n         initial_train_size    = len(data.loc[:end_train]),\n         refit                 = True,\n         fixed_train_size      = False,\n         gap                   = 0,\n         skip_folds            = None,\n         allow_incomplete_fold = True\n     )\n\nmetrics, predictions = backtesting_forecaster(\n                           forecaster = forecaster,\n                           y          = data['y'],\n                           cv         = cv,\n                           metric     = multiple_metrics\n                       )\n\nmetrics\n</pre> # Backtesting with multiple metric # ============================================================================== multiple_metrics = ['mean_squared_error', 'mean_absolute_error', custom_metric]  cv = TimeSeriesFold(          steps                 = 10,          initial_train_size    = len(data.loc[:end_train]),          refit                 = True,          fixed_train_size      = False,          gap                   = 0,          skip_folds            = None,          allow_incomplete_fold = True      )  metrics, predictions = backtesting_forecaster(                            forecaster = forecaster,                            y          = data['y'],                            cv         = cv,                            metric     = multiple_metrics                        )  metrics <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> Out[26]: mean_squared_error mean_absolute_error custom_metric 0 0.007042 0.067956 0.007179 <p> \ud83d\udca1 Tip </p> <p>To learn more about how to extract the training and prediction matrices, visit the following link: How to Extract Training and Prediction Matrices.</p> In\u00a0[27]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[10])\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[10])              )  forecaster.fit(y=data['y']) In\u00a0[28]: Copied! <pre># Backtesting on training data\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 1,\n         initial_train_size = None,\n         refit              = False\n     )\n\nmetric, predictions_training = backtesting_forecaster(\n                                   forecaster = forecaster,\n                                   y          = data['y'],\n                                   cv         = cv,\n                                   metric     = 'mean_squared_error'\n                               )\n\nmetric\n</pre> # Backtesting on training data # ============================================================================== cv = TimeSeriesFold(          steps              = 1,          initial_train_size = None,          refit              = False      )  metric, predictions_training = backtesting_forecaster(                                    forecaster = forecaster,                                    y          = data['y'],                                    cv         = cv,                                    metric     = 'mean_squared_error'                                )  metric <pre>  0%|          | 0/180 [00:00&lt;?, ?it/s]</pre> Out[28]: mean_squared_error 0 0.000709 In\u00a0[29]: Copied! <pre># Training predictions\n# ==============================================================================\npredictions_training.head(4)\n</pre> # Training predictions # ============================================================================== predictions_training.head(4) Out[29]: pred 1993-07-01 0.525134 1993-08-01 0.569934 1993-09-01 0.601695 1993-10-01 0.641750 <p>It is important to note that the first 15 observations are excluded from the predictions since they are required to generate the lags that serve as predictors in the model.</p> In\u00a0[30]: Copied! <pre># Plot training predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata['y'].plot(ax=ax)\npredictions_training.plot(ax=ax)\nax.set_title(\"Backtesting on training data\")\nax.legend()\nplt.show();\n</pre> # Plot training predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data['y'].plot(ax=ax) predictions_training.plot(ax=ax) ax.set_title(\"Backtesting on training data\") ax.legend() plt.show();"},{"location":"user_guides/backtesting.html#backtesting","title":"Backtesting\u00b6","text":"<p>In time series forecasting, the process of backtesting consists of evaluating the performance of a predictive model by applying it retrospectively to historical data. Therefore, it is a special type of cross-validation applied to the previous period(s).</p> <p>The purpose of backtesting is to evaluate the accuracy and effectiveness of a model and identify any potential issues or areas of improvement. By testing the model on historical data, one can assess how well it performs on data that it has not seen before. This is an important step in the modeling process, as it helps to ensure that the model is robust and reliable.</p> <p>Backtesting can be done using a variety of techniques, such as simple train-test splits or more sophisticated methods like rolling windows or expanding windows. The choice of method depends on the specific needs of the analysis and the characteristics of the time series data.</p> <p>Overall, backtesting is an essential step in the development of a time series forecasting model. By rigorously testing the model on historical data, one can improve its accuracy and ensure that it is effective at predicting future values of the time series.</p>"},{"location":"user_guides/backtesting.html#backtesting-without-refit","title":"Backtesting without refit\u00b6","text":"<p>Backtesting without refit is a strategy where the model is trained only once and used sequentially without updating it, following the temporal order of the data. This approach is advantageous as it is much faster than other methods that require retraining the model each time. However, the model may lose its predictive power over time as it does not incorporate the latest information available.</p> <p> </p> <p> Backtesting without refit. </p> <p>Note: The argument needed to achieve this configuration is <code>refit=False</code>.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-refit-and-increasing-training-size","title":"Backtesting with refit and increasing training size\u00b6","text":"<p>In this strategy, the model is trained before making predictions each time, and all available data up to that point is used in the training process. This differs from standard cross-validation, where the data is randomly distributed between training and validation sets.</p> <p>Instead of randomizing the data, this backtesting sequentially increases the size of the training set while maintaining the temporal order of the data. By doing this, the model can be tested on progressively larger amounts of historical data, providing a more accurate assessment of its predictive capabilities.</p> <p> </p> <p> Backtesting with refit and increasing training size (fixed origin). </p> <p>Note: The arguments needed to achieve this configuration are <code>refit=True</code> and <code>fixed_train_size=False</code>.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-refit-and-fixed-training-size","title":"Backtesting with refit and fixed training size\u00b6","text":"<p>In this approach, the model is trained using a fixed window of past observations, and the testing is performed on a rolling basis, where the training window is moved forward in time. The size of the training window is kept constant, allowing for the model to be tested on different sections of the data. This technique is particularly useful when there is a limited amount of data available, or when the data is non-stationary, and the model's performance may vary over time. Is also known as time series cross-validation or walk-forward validation.</p> <p> </p> <p> Backtesting with refit and fixed training size (rolling origin). </p> <p>Note: The arguments needed to achieve this configuration are <code>refit=True</code> and <code>fixed_train_size=True</code>.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-intermittent-refit","title":"Backtesting with intermittent refit\u00b6","text":"<p>The model is retrained every $n$ iterations of prediction. This method is often used when the frequency of retraining and prediction is different. It can be implemented using either a fixed or rolling origin, providing flexibility in adapting the model to new data.</p> <p> \ud83d\udca1 Tip </p> <p>This strategy usually achieves a good balance between the computational cost of retraining and avoiding model degradation.</p> <p> Backtesting with intermittent refit. </p> <p>Note: The argument needed to achieve this configuration is <code>refit=n</code>, where $n$ is an integer. This configuration also allows the use of <code>fixed_train_size</code>.</p>"},{"location":"user_guides/backtesting.html#backtesting-including-gap","title":"Backtesting including gap\u00b6","text":"<p>This approach introduces a time gap between the training and test sets, replicating a scenario where predictions cannot be made immediately after the end of the training data.</p> <p>For example, consider the goal of predicting the 24 hours of day D+1, but the predictions need to be made at 11:00 to allow sufficient flexibility. At 11:00 on day D, the task is to forecast hours [12 - 23] of the same day and hours [0 - 23] of day D+1. Thus, a total of 36 hours into the future must be predicted, with only the last 24 hours to be stored.</p> <p> Backtesting with refit and gap. </p> <p>Note: After setting the desired values for <code>refit</code> and <code>fixed_train_size</code>. The argument needed to achieve this configuration is <code>gap=n</code>, where $n$ is an integer.</p>"},{"location":"user_guides/backtesting.html#skip-folds","title":"Skip folds\u00b6","text":"<p>All of the above backtesting strategies can be combined with the option to skip a certain number of folds by setting the <code>skip_folds</code> argument. Since the model predicts fewer points in time, the computational cost is reduced and the backtesting process is faster. This is particularly useful when one is interested in an approximate estimate of the model's performance, but does not require an exact evaluation, for example, when searching for hyperparameters. If <code>skip_folds</code> is an integer, every 'skip_folds'-th is returned. If <code>skip_folds</code> is a list, the folds in the list are skipped. For example, if <code>skip_folds = 3</code>, and there are 10 folds, the returned folds will be [0, 3, 6, 9]. If <code>skip_folds</code> is a list [1, 2, 3], the returned folds will be [0, 4, 5, 6, 7, 8, 9].</p>"},{"location":"user_guides/backtesting.html#which-strategy-should-i-use","title":"Which strategy should I use?\u00b6","text":"<p>To ensure an accurate evaluation of your model and gain confidence in its predictive performance on new data, it is critical to employ an appropriate backtesting strategy. Factors such as use case characteristics, available computing resources and time intervals between predictions need to be considered to determine which strategy to use. These factors determine when the model should be refitted and the prediction horizon that should be used.</p> <ul> <li><p>Prediction horizon (<code>steps</code>): suppose you need to predict the users of an application every Monday for the whole week. In this case, each iteration of backtesting would be a seven-step prediction, representing the seven days of the week.</p> </li> <li><p>Refit strategy (<code>refit</code>): Continuing with the example above, at the end of the week you need to decide whether or not to update the model. Training the model with additional data can improve its predictive ability, but it requires more time and computational resources, which may not always be readily available. A reasonable approach is to compare different retrain frequencies and select the one for which the error metric shows a consistent upward trend. This behaviour can be effectively simulated using the backtesting framework.</p> </li> </ul> <p>As an example, backtesting is performed using the data from this skforecast example. The same backtest is run with <code>steps=24</code> (predict 24 hours) and different <code>refit</code> strategies: <code>False</code> (no re-fitting between predictions), re-fitting every <code>30</code> days, every <code>14</code> days, every <code>7</code> days and <code>True</code> (re-fitting after every prediction). Notice that the significant increase in time does not correspond to a decrease in error.</p> refit value execution time (s) metric False 1.4 262.5 30 4.0 263.4 14 6.3 262.5 7 11.1 261.4 True 69.1 258.3 <p>In general, the more closely the backtesting process resembles the actual scenario in which the model is used, the more reliable the estimated metric will be.</p>"},{"location":"user_guides/backtesting.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/backtesting.html#timeseriesfold","title":"TimeSeriesFold\u00b6","text":"<p>The <code>TimeSeriesFold</code> class is designed to generate the partitions used in the backtesting process to train and evaluate the model. By leveraging its various arguments, it provides extensive flexibility, enabling the simulation of scenarios such as refit, no refit, rolling origin, and others. The <code>split</code> method returns the index positions of the time series corresponding to each partition. When <code>as_pandas=True</code> is specified, the output is a DataFrame with detailed information, including descriptive column names.</p>"},{"location":"user_guides/backtesting.html#backtest","title":"Backtest\u00b6","text":"<p>An example of a backtesting with refit consists of the following steps:</p> <ol> <li><p>Train the model using an initial training set, the length of which is specified by <code>initial_train_size</code>.</p> </li> <li><p>Once the model is trained, it is used to make predictions for the next 10 steps (<code>steps=10</code>) in the data. These predictions are saved for later evaluation.</p> </li> <li><p>As <code>refit</code> is set to <code>True</code>, the size of the training set is increased by adding the lats 10 data points (the previously predicted 10 steps), while the next 10 steps are used as test data.</p> </li> <li><p>After expanding the training set, the model is retrained using the updated training data and then used to predict the next 10 steps.</p> </li> <li><p>Repeat steps 3 and 4 until the entire series has been tested.</p> </li> </ol> <p>By following these steps, you can ensure that the model is evaluated on multiple sets of test data, thereby providing a more accurate assessment of its predictive power.</p>"},{"location":"user_guides/backtesting.html#backtesting-intermittent-refit","title":"Backtesting intermittent refit\u00b6","text":"<p>The same backtesting as above is repeated, but this time with <code>refit = 3</code>. The training of the model is done every 3 folds instead of every fold.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-gap","title":"Backtesting with gap\u00b6","text":"<p>The gap size can be adjusted with the <code>gap</code> argument. In addition, the <code>allow_incomplete_fold</code> parameter allows the last fold to be excluded from the analysis if it doesn't have the same size as the required number of <code>steps</code>.</p> <p>These arguments can be used in conjunction with either refit set to <code>True</code> or <code>False</code>, depending on the needs and objectives of the use case.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-skip-folds","title":"Backtesting with skip folds\u00b6","text":"<ul> <li><p>If <code>skip_folds</code> is an integer, every 'skip_folds'-th is returned. For example, if <code>skip_folds = 3</code> and there are 10 folds, the returned folds will be [0, 3, 6, 9].</p> </li> <li><p>If <code>skip_folds</code> is a list, the folds in the list are skipped. If <code>skip_folds = [1, 2, 3]</code> and there are 10 folds, the returned folds will be [0, 4, 5, 6, 7, 8, 9].</p> </li> </ul>"},{"location":"user_guides/backtesting.html#backtesting-with-prediction-intervals","title":"Backtesting with prediction intervals\u00b6","text":"<p>Backtesting can be used not only to obtain point estimate predictions but also to obtain prediction intervals. Prediction intervals provide a range of values within which the actual values are expected to fall with a certain level of confidence. By estimating prediction intervals during backtesting, one can get a better understanding of the uncertainty associated with your model's predictions. This information can be used to evaluate the reliability of the model's predictions and to make more informed decisions.</p> <p>To learn more about probabilistic forecasting features available in skforecast, see Probabilistic forecasting.</p> <p>The <code>interval</code> argument can be specified as:</p> <ul> <li><p>If <code>list</code> or <code>tuple</code>: Sequence of percentiles to compute, each value must be between 0 and 100 inclusive. For example, a 95% confidence interval can be specified as <code>interval = [2.5, 97.5]</code> or multiple percentiles (e.g. 10, 50 and 90) as <code>interval = [10, 50, 90]</code>.</p> </li> <li><p>If <code>'bootstrapping'</code> (str): <code>n_boot</code> bootstrapping predictions will be generated.</p> </li> <li><p>If <code>scipy.stats distribution object</code>, the distribution parameters will be estimated for each prediction.</p> </li> <li><p>If <code>None</code>, no probabilistic predictions are estimated.</p> </li> </ul>"},{"location":"user_guides/backtesting.html#backtesting-with-exogenous-variables","title":"Backtesting with exogenous variables\u00b6","text":"<p>All the backtesting strategies discussed in this document can also be applied when incorporating exogenous variables in the forecasting model.</p> <p>Exogenous variables are additional independent variables that can impact the value of the target variable being forecasted. These variables can provide valuable information to the model and improve its forecasting accuracy.</p>"},{"location":"user_guides/backtesting.html#return-predictors-values","title":"Return predictor's values\u00b6","text":"<p>In addition to the forecasted values, the backtesting process can also return the predictor values used to generate each prediction. This can be enabled by setting the argument <code>return_predictors=True</code>. Also, a <code>fold</code> column is added to the returned DataFrame, indicating the fold to which each observation belongs. This allows for a more detailed analysis of the model's performance and the factors that influenced its predictions.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-custom-metric","title":"Backtesting with custom metric\u00b6","text":"<p>In addition to the commonly used metrics such as mean_squared_error, mean_absolute_error, and mean_absolute_percentage_error, users have the flexibility to define their own custom metric function, provided that it includes the arguments <code>y_true</code> (the true values of the series) and <code>y_pred</code> (the predicted values), and returns a numeric value (either a <code>float</code> or an <code>int</code>).</p> <p>This customizability enables users to evaluate the model's predictive performance in a wide range of scenarios, such as considering only certain months, days, non holiday; or focusing only on the last step of the predicted horizon.</p> <p>To illustrate this, consider the following example: a 12-month horizon is forecasted, but the interest metric is calculated by considering only the last three months of each year. This is achieved by defining a custom metric function that takes into account only the relevant months, which is then passed as an argument to the backtesting function.</p>"},{"location":"user_guides/backtesting.html#backtesting-with-multiple-metrics","title":"Backtesting with multiple metrics\u00b6","text":"<p>The <code>backtesting_forecaster</code> function provides a convenient way to estimate multiple metrics simultaneously by accepting a list of metrics as an input argument. This list can include any combination of built-in metrics, such as mean_squared_error, mean_absolute_error, and mean_absolute_percentage_error, as well as user-defined custom metrics.</p> <p>By specifying multiple metrics, users can obtain a more comprehensive evaluation of the model's predictive performance, which can help in selecting the best model for a particular task. Additionally, the ability to include custom metrics allows users to tailor the evaluation to specific use cases and domain-specific requirements.</p>"},{"location":"user_guides/backtesting.html#backtesting-on-training-data","title":"Backtesting on training data\u00b6","text":"<p>While the primary goal of building forecasting models is to predict future values, it is equally important to evaluate if the model is effectively learning from the training data. Analyzing predictions on the training data or exploring the prediction matrices is crucial for assessing model performance and understanding areas for optimization. This process can help identify issues like overfitting or underfitting, as well as provide deeper insights into the model\u2019s decision-making process.</p> <p>To obtain predictions on the training data,the forecaster must first be fitted using the training dataset. Then, backtesting can be performed using  <code>backtesting_forecaster</code> and specifying the arguments <code>initial_train_size=None</code> and <code>refit=False</code>. This configuration enables backtesting on the same data that was used to train the model.</p>"},{"location":"user_guides/calendar-features.html","title":"Calendars features","text":"<p>Calendar features serve as key elements in time series forecasting. These features decompose date and time into basic units such as year, month, day, weekday, etc., allowing models to identify recurring patterns, understand seasonal variations, and identify trends. Calendar features can be used as exogenous variables because they are known for the period for which predictions are being made (the forecast horizon).</p> <p>Dates and time in Pandas</p> <p>Pandas provides a comprehensive set of capabilities tailored for handling time series data in various domains. Using the NumPy <code>datetime64</code> and <code>timedelta64</code> data types, Pandas combines a wide range of functionality from various Python libraries while introducing a wealth of novel tools to effectively manipulate time series data. This includes:</p> <ul> <li><p>Easily parse date and time data from multiple sources and formats.</p> </li> <li><p>Generate sequences of fixed-frequency dates and time spans.</p> </li> <li><p>Streamline the manipulation and conversion of date-time information, including time zones.</p> </li> <li><p>Facilitate the resampling or conversion of time series data to specific frequencies.</p> </li> </ul> <p>For an in-depth exploration of Pandas' comprehensive time series and date capabilities, please refer to this resource.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skforecast.datasets import fetch_dataset\nfrom feature_engine.datetime import DatetimeFeatures\nfrom feature_engine.creation import CyclicalFeatures\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from skforecast.datasets import fetch_dataset from feature_engine.datetime import DatetimeFeatures from feature_engine.creation import CyclicalFeatures In\u00a0[2]: Copied! <pre># Downloading data\n# ==============================================================================\ndata = fetch_dataset(name=\"bike_sharing\", raw=True)\ndata = data[['date_time', 'users']]\ndata.head()\n</pre> # Downloading data # ============================================================================== data = fetch_dataset(name=\"bike_sharing\", raw=True) data = data[['date_time', 'users']] data.head() <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 12)\n</pre> Out[2]: date_time users 0 2011-01-01 00:00:00 16.0 1 2011-01-01 01:00:00 40.0 2 2011-01-01 02:00:00 32.0 3 2011-01-01 03:00:00 13.0 4 2011-01-01 04:00:00 1.0 <p>To take advantage of the date-time functionality offered by Pandas, the column of interest must be stored as <code>datetime</code>. Although not required, it is recommended to set it as an index for further integration with skforecast.</p> In\u00a0[3]: Copied! <pre># Preprocess data\n# ==============================================================================\ndata['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\ndata = data.set_index('date_time')\ndata = data.asfreq('h')\ndata = data.sort_index()\ndata.head()\n</pre> # Preprocess data # ============================================================================== data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S') data = data.set_index('date_time') data = data.asfreq('h') data = data.sort_index() data.head() Out[3]: users date_time 2011-01-01 00:00:00 16.0 2011-01-01 01:00:00 40.0 2011-01-01 02:00:00 32.0 2011-01-01 03:00:00 13.0 2011-01-01 04:00:00 1.0 <p>Next, several features are created from the date and time information: year, month, day of the week, and hour.</p> In\u00a0[4]: Copied! <pre># Create calendar features\n# ==============================================================================\ndata['year'] = data.index.year\ndata['month'] = data.index.month\ndata['day_of_week'] = data.index.dayofweek\ndata['hour'] = data.index.hour\ndata.head()\n</pre> # Create calendar features # ============================================================================== data['year'] = data.index.year data['month'] = data.index.month data['day_of_week'] = data.index.dayofweek data['hour'] = data.index.hour data.head() Out[4]: users year month day_of_week hour date_time 2011-01-01 00:00:00 16.0 2011 1 5 0 2011-01-01 01:00:00 40.0 2011 1 5 1 2011-01-01 02:00:00 32.0 2011 1 5 2 2011-01-01 03:00:00 13.0 2011 1 5 3 2011-01-01 04:00:00 1.0 2011 1 5 4 <p> \ud83d\udca1 Tip </p> <p>Numerous calendar-related features can be generated, including day of the year, week of the year, hour of the day, and others. An easy approach to automate their extraction is to use the <code>DatetimeFeatures</code> transformer within the Feature-engine Python library. This class integrates seamlessly into the scikit-learn pipeline, making it compatible with skforecast as well. For a deeper understanding and detailed information, please refer to DatetimeFeatures.</p> In\u00a0[5]: Copied! <pre># Create calendar features with Feature-engine\n# ==============================================================================\nfeatures_to_extract = ['month', 'week', 'day_of_week', 'hour']\ncalendar_transformer = DatetimeFeatures(\n                           variables           = 'index',\n                           features_to_extract = features_to_extract,\n                           drop_original       = True,\n                       )\n\ncalendar_features = calendar_transformer.fit_transform(data)\ncalendar_features.head()\n</pre> # Create calendar features with Feature-engine # ============================================================================== features_to_extract = ['month', 'week', 'day_of_week', 'hour'] calendar_transformer = DatetimeFeatures(                            variables           = 'index',                            features_to_extract = features_to_extract,                            drop_original       = True,                        )  calendar_features = calendar_transformer.fit_transform(data) calendar_features.head() Out[5]: users year month day_of_week hour week date_time 2011-01-01 00:00:00 16.0 2011 1 5 0 52 2011-01-01 01:00:00 40.0 2011 1 5 1 52 2011-01-01 02:00:00 32.0 2011 1 5 2 52 2011-01-01 03:00:00 13.0 2011 1 5 3 52 2011-01-01 04:00:00 1.0 2011 1 5 4 52 <p>Sunlight often plays a key role in time series patterns. For example, a household's hourly electricity consumption may correlate significantly with whether it's nighttime, as more electricity is typically used for lighting during those hours. Understanding and incorporating sunlight-related characteristics into analyses can provide valuable insights into consumption patterns and behavioral trends. In addition, factors such as sunrise/sunset times, seasonal changes affecting daylight, and their influence on different data sets can provide deeper context and help predict consumption fluctuations. There are several Python libraries available for extracting sunrise and sunset times. Two of the most commonly used are <code>pvlib</code> and <code>astral</code>.</p> In\u00a0[6]: Copied! <pre># Features based on the sunligth\n# ==============================================================================\nfrom astral.sun import sun\nfrom astral import LocationInfo\n\nlocation = LocationInfo(\"Washington, D.C.\", \"USA\")\nsunrise_hour = [sun(location.observer, date=date)['sunrise'] for date in data.index]\nsunset_hour = [sun(location.observer, date=date)['sunset'] for date in data.index]\n\n# Round to the nearest hour\nsunrise_hour = pd.Series(sunrise_hour, index=data.index).dt.round(\"h\").dt.hour\nsunset_hour = pd.Series(sunset_hour, index=data.index).dt.round(\"h\").dt.hour\n\nsun_light_features = pd.DataFrame({\n                         'sunrise_hour': sunrise_hour,\n                         'sunset_hour': sunset_hour}\n                     )\nsun_light_features['daylight_hours'] = sun_light_features['sunset_hour'] - sun_light_features['sunrise_hour']\nsun_light_features.tail()\n</pre> # Features based on the sunligth # ============================================================================== from astral.sun import sun from astral import LocationInfo  location = LocationInfo(\"Washington, D.C.\", \"USA\") sunrise_hour = [sun(location.observer, date=date)['sunrise'] for date in data.index] sunset_hour = [sun(location.observer, date=date)['sunset'] for date in data.index]  # Round to the nearest hour sunrise_hour = pd.Series(sunrise_hour, index=data.index).dt.round(\"h\").dt.hour sunset_hour = pd.Series(sunset_hour, index=data.index).dt.round(\"h\").dt.hour  sun_light_features = pd.DataFrame({                          'sunrise_hour': sunrise_hour,                          'sunset_hour': sunset_hour}                      ) sun_light_features['daylight_hours'] = sun_light_features['sunset_hour'] - sun_light_features['sunrise_hour'] sun_light_features.tail() Out[6]: sunrise_hour sunset_hour daylight_hours date_time 2012-12-31 19:00:00 8 16 8 2012-12-31 20:00:00 8 16 8 2012-12-31 21:00:00 8 16 8 2012-12-31 22:00:00 8 16 8 2012-12-31 23:00:00 8 16 8 <p>Certain aspects of the calendar, such as hours of the day or days of the week, behave in cycles. For example, the hours of a day range from 0 to 23. If interpreted as a continuous variable, the hour of 23:00 would be 23 units away from the hour of 00:00. However, this is not true because 23:00 is only one hour away from 00:00. The same is true for the months of the year, since December is only one month away from January. Using techniques such as trigonometric functions - sine and cosine transformations - makes it possible to represent cyclic patterns and avoid inconsistencies in data representation. This technique is called cyclic coding and can significantly improve the predictive power of models.</p> In\u00a0[7]: Copied! <pre># Cyclical encoding\n# ==============================================================================\nfeatures_to_encode = [\n    \"month\",\n    \"week\",\n    \"day_of_week\",\n    \"hour\",\n]\nmax_values = {\n    \"month\": 12,\n    \"week\": 52,\n    \"day_of_week\": 7,\n    \"hour\": 24,\n}\ncyclical_encoder = CyclicalFeatures(\n    variables     = features_to_encode,\n    max_values    = max_values,\n    drop_original = True\n)\n\ncyclical_features = cyclical_encoder.fit_transform(calendar_features)\ncyclical_features.head(3)\n</pre> # Cyclical encoding # ============================================================================== features_to_encode = [     \"month\",     \"week\",     \"day_of_week\",     \"hour\", ] max_values = {     \"month\": 12,     \"week\": 52,     \"day_of_week\": 7,     \"hour\": 24, } cyclical_encoder = CyclicalFeatures(     variables     = features_to_encode,     max_values    = max_values,     drop_original = True )  cyclical_features = cyclical_encoder.fit_transform(calendar_features) cyclical_features.head(3) Out[7]: users year month_sin month_cos week_sin week_cos day_of_week_sin day_of_week_cos hour_sin hour_cos date_time 2011-01-01 00:00:00 16.0 2011 0.5 0.866025 -2.449294e-16 1.0 -0.974928 -0.222521 0.000000 1.000000 2011-01-01 01:00:00 40.0 2011 0.5 0.866025 -2.449294e-16 1.0 -0.974928 -0.222521 0.258819 0.965926 2011-01-01 02:00:00 32.0 2011 0.5 0.866025 -2.449294e-16 1.0 -0.974928 -0.222521 0.500000 0.866025 In\u00a0[8]: Copied! <pre># Plot value of sin and cos for each day_of_week\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(3.5, 3))\nsp = ax.scatter(\n         cyclical_features[\"day_of_week_sin\"],\n         cyclical_features[\"day_of_week_cos\"],\n         c    = calendar_features['day_of_week'],\n         cmap = 'viridis'\n     )\nax.set(\n    xlabel=\"sin(day_of_week)\",\n    ylabel=\"cos(day_of_week)\",\n)\n_ = fig.colorbar(sp)\nplt.show();\n</pre> # Plot value of sin and cos for each day_of_week # ============================================================================== fig, ax = plt.subplots(figsize=(3.5, 3)) sp = ax.scatter(          cyclical_features[\"day_of_week_sin\"],          cyclical_features[\"day_of_week_cos\"],          c    = calendar_features['day_of_week'],          cmap = 'viridis'      ) ax.set(     xlabel=\"sin(day_of_week)\",     ylabel=\"cos(day_of_week)\", ) _ = fig.colorbar(sp) plt.show(); In\u00a0[9]: Copied! <pre># Plot value of sin and cos for each hour\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(3.5, 3))\nsp = ax.scatter(\n         cyclical_features[\"hour_sin\"],\n         cyclical_features[\"hour_cos\"],\n         c    = calendar_features['hour'],\n         cmap = 'viridis'\n     )\nax.set(\n    xlabel=\"sin(hour)\",\n    ylabel=\"cos(hour)\",\n)\n_ = fig.colorbar(sp)\nplt.show();\n</pre> # Plot value of sin and cos for each hour # ============================================================================== fig, ax = plt.subplots(figsize=(3.5, 3)) sp = ax.scatter(          cyclical_features[\"hour_sin\"],          cyclical_features[\"hour_cos\"],          c    = calendar_features['hour'],          cmap = 'viridis'      ) ax.set(     xlabel=\"sin(hour)\",     ylabel=\"cos(hour)\", ) _ = fig.colorbar(sp) plt.show(); <p> \u270e Note </p> <p>See Cyclical features in time series forecasting for a more detailed description of strategies for encoding cyclic features.</p>"},{"location":"user_guides/calendar-features.html#calendar-features","title":"Calendar features\u00b6","text":""},{"location":"user_guides/calendar-features.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/calendar-features.html#extract-calendar-features","title":"Extract calendar features\u00b6","text":""},{"location":"user_guides/calendar-features.html#sunlight-related-features","title":"Sunlight-Related Features\u00b6","text":""},{"location":"user_guides/calendar-features.html#cyclical-encoding","title":"Cyclical encoding\u00b6","text":""},{"location":"user_guides/categorical-features.html","title":"Categorical features","text":"<p>In the field of machine learning, categorical features play a crucial role in determining the predictive ability of a model. Categorical features are features that can take a limited number of values, such as color, gender or location. While these features can provide useful insights into patterns and relationships within data, they also present unique challenges for machine learning models.</p> <p>One of these challenges is the need to transform categorical features before they can be used by most models. This transformation involves converting categorical values into numerical values that can be processed by machine learning algorithms.</p> <p>Another challenge is dealing with infrequent categories, which can lead to biased models. If a categorical feature has a large number of categories, but some of them are rare or appear infrequently in the data, the model may not be able to learn accurately from these categories, resulting in biased predictions and inaccurate results.</p> <p>Despite these difficulties, categorical features are still an essential component in many use cases. When properly encoded and handled, machine learning models can effectively learn from patterns and relationships in categorical data, leading to better predictions.</p> <p>This document provides an overview of three of the most commonly used transformations: one-hot encoding, ordinal encoding, and target encoding. It explains how to apply them in the skforecast package using scikit-learn encoders, which provide a convenient and flexible way to pre-process data. It also shows how to use the native implementation of four popular gradient boosting frameworks \u2013 XGBoost, LightGBM, scikit-learn's HistogramGradientBoosting and CatBoost \u2013 to handle categorical features directly in the model.</p> <p>For a comprehensive demonstration of the use of categorical features in time series forecasting, check out the article Forecasting time series with gradient boosting: Skforecast, XGBoost, LightGBM and CatBoost.</p> <p> \u270e Note </p> <p>All of the transformations described in this document can be applied to the entire dataset, regardless of the forecaster. However, it is important to ensure that the transformations are learned only from the training data to avoid information leakage. Furthermore, the same transformation should be applied to the input data during prediction. To reduce the likelihood of errors and to ensure consistent application of the transformations, it is advisable to include the transformation within the forecaster object, so that it is handled internally.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nimport lightgbm\nimport xgboost\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import TargetEncoder\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.compose import make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nplt.style.use('fivethirtyeight')\nplt.rcParams['lines.linewidth'] = 1.5\ncolor = '\\033[1m\\033[38;5;208m' \nprint(f\"{color}Version scikit-learn: {sklearn.__version__}\")\nprint(f\"{color}Version lightgbm: {lightgbm.__version__}\")\nprint(f\"{color}Version xgboost: {xgboost.__version__}\")\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt import sklearn import lightgbm import xgboost from lightgbm import LGBMRegressor from sklearn.ensemble import HistGradientBoostingRegressor from xgboost import XGBRegressor from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.preprocessing import TargetEncoder from sklearn.preprocessing import FunctionTransformer from sklearn.compose import make_column_transformer from sklearn.compose import make_column_selector from sklearn.pipeline import make_pipeline from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive plt.style.use('fivethirtyeight') plt.rcParams['lines.linewidth'] = 1.5 color = '\\033[1m\\033[38;5;208m'  print(f\"{color}Version scikit-learn: {sklearn.__version__}\") print(f\"{color}Version lightgbm: {lightgbm.__version__}\") print(f\"{color}Version xgboost: {xgboost.__version__}\") <pre>Version scikit-learn: 1.7.1\nVersion lightgbm: 4.6.0\nVersion xgboost: 3.0.2\n</pre> In\u00a0[2]: Copied! <pre># Downloading data\n# ==============================================================================\ndata = fetch_dataset(name='bike_sharing', raw=True)\n</pre> # Downloading data # ============================================================================== data = fetch_dataset(name='bike_sharing', raw=True) <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 12)\n</pre> In\u00a0[3]: Copied! <pre># Preprocess data\n# ==============================================================================\ndata['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\ndata = data.set_index('date_time')\ndata = data.asfreq('h')\ndata = data.sort_index()\ndata['holiday'] = data['holiday'].astype(int)\ndata = data[['holiday', 'weather', 'temp', 'hum', 'users']]\ndata[['holiday', 'weather']] = data[['holiday', 'weather']].astype(str)\nprint(data.dtypes)\ndata.head(3)\n</pre>  # Preprocess data # ============================================================================== data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S') data = data.set_index('date_time') data = data.asfreq('h') data = data.sort_index() data['holiday'] = data['holiday'].astype(int) data = data[['holiday', 'weather', 'temp', 'hum', 'users']] data[['holiday', 'weather']] = data[['holiday', 'weather']].astype(str) print(data.dtypes) data.head(3) <pre>holiday     object\nweather     object\ntemp       float64\nhum        float64\nusers      float64\ndtype: object\n</pre> Out[3]: holiday weather temp hum users date_time 2011-01-01 00:00:00 0 clear 9.84 81.0 16.0 2011-01-01 01:00:00 0 clear 9.02 80.0 40.0 2011-01-01 02:00:00 0 clear 9.02 80.0 32.0 <p>Only part of the data is used to simplify the example.</p> In\u00a0[4]: Copied! <pre># Split train-test\n# ==============================================================================\nstart_train = '2012-06-01 00:00:00'\nend_train = '2012-07-31 23:59:00'\nend_test = '2012-08-15 23:59:00'\ndata_train = data.loc[start_train:end_train, :]\ndata_test  = data.loc[end_train:end_test, :]\n\nprint(\n    f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}\"\n    f\"  (n={len(data_test)})\"\n)\n</pre> # Split train-test # ============================================================================== start_train = '2012-06-01 00:00:00' end_train = '2012-07-31 23:59:00' end_test = '2012-08-15 23:59:00' data_train = data.loc[start_train:end_train, :] data_test  = data.loc[end_train:end_test, :]  print(     f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}\"     f\"  (n={len(data_test)})\" ) <pre>Dates train : 2012-06-01 00:00:00 --- 2012-07-31 23:00:00  (n=1464)\nDates test  : 2012-08-01 00:00:00 --- 2012-08-15 23:00:00  (n=360)\n</pre> In\u00a0[5]: Copied! <pre># ColumnTransformer with one-hot encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical features (no numerical)\n# using one-hot encoding. Numeric features are left untouched. For binary\n# features, only one column is created.\none_hot_encoder = make_column_transformer(\n                      (\n                          OneHotEncoder(sparse_output=False, drop='if_binary'),\n                          make_column_selector(dtype_exclude=np.number)\n                      ),\n                      remainder=\"passthrough\",\n                      verbose_feature_names_out=False,\n                  ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with one-hot encoding # ============================================================================== # A ColumnTransformer is used to transform categorical features (no numerical) # using one-hot encoding. Numeric features are left untouched. For binary # features, only one column is created. one_hot_encoder = make_column_transformer(                       (                           OneHotEncoder(sparse_output=False, drop='if_binary'),                           make_column_selector(dtype_exclude=np.number)                       ),                       remainder=\"passthrough\",                       verbose_feature_names_out=False,                   ).set_output(transform=\"pandas\") In\u00a0[6]: Copied! <pre># Create and fit forecaster with a transformer for exogenous features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(random_state=123, verbose=-1),\n                 lags             = 5,\n                 transformer_exog = one_hot_encoder\n             )\n\nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n\nforecaster\n</pre> # Create and fit forecaster with a transformer for exogenous features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterRecursive(                  regressor        = LGBMRegressor(random_state=123, verbose=-1),                  lags             = 5,                  transformer_exog = one_hot_encoder              )  forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] )  forecaster Out[6]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3 4 5]</li> <li>Window features: None</li> <li>Window size: 5</li> <li>Series name: users</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:30:47</li> <li>Last fit date: 2025-08-06 13:30:49</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     holiday, weather, temp, hum                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: ColumnTransformer(remainder='passthrough',                   transformers=[('onehotencoder',                                  OneHotEncoder(drop='if_binary',                                                sparse_output=False),                                  )],                   verbose_feature_names_out=False)</li> </ul> Training Information <ul> <li>Training range: [Timestamp('2011-01-01 00:00:00'), Timestamp('2012-07-31 23:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: h</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>Once the forecaster has been trained, the transformer can be inspected (feature_names_in, feature_names_out, ...) by accessing the <code>transformer_exog</code> attribute.</p> In\u00a0[7]: Copied! <pre># Access to the transformer used for exogenous features\n# ==============================================================================\nprint(forecaster.transformer_exog.get_feature_names_out())\nforecaster.transformer_exog\n</pre> # Access to the transformer used for exogenous features # ============================================================================== print(forecaster.transformer_exog.get_feature_names_out()) forecaster.transformer_exog <pre>['holiday_1' 'weather_clear' 'weather_mist' 'weather_rain' 'temp' 'hum']\n</pre> Out[7]: <pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder',\n                                 OneHotEncoder(drop='if_binary',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001FAAC90A8A0&gt;)],\n                  verbose_feature_names_out=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriFitted Parameters transformers\u00a0 [('onehotencoder', ...)] remainder\u00a0 'passthrough' sparse_threshold\u00a0 0.3 n_jobs\u00a0 None transformer_weights\u00a0 None verbose\u00a0 False verbose_feature_names_out\u00a0 False force_int_remainder_cols\u00a0 'deprecated' onehotencoder<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001FAAC90A8A0&gt;</pre>OneHotEncoder?Documentation for OneHotEncoder Parameters categories\u00a0 'auto' drop\u00a0 'if_binary' sparse_output\u00a0 False dtype\u00a0 &lt;class 'numpy.float64'&gt; handle_unknown\u00a0 'error' min_frequency\u00a0 None max_categories\u00a0 None feature_name_combiner\u00a0 'concat' remainder<pre>['temp', 'hum']</pre>passthrough<pre>passthrough</pre> In\u00a0[8]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[8]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: h, Name: pred, dtype: float64</pre> <p> \u270e Note </p> <p>It is possible to apply a transformation to the entire dataset independent of the forecaster. However, it is crucial to ensure that the transformations are only learned from the training data to avoid information leakage. In addition, the same transformation should be applied to the input data during prediction. It is therefore advisable to incorporate the transformation into the forecaster, so that it is handled internally. This approach ensures consistency in the application of transformations and reduces the likelihood of errors.</p> <p>To examine how data is being transformed, it is possible to use the <code>create_train_X_y()</code> method to generate the matrices used by the forecaster to train the model. This approach enables gaining insight into the specific data manipulations that occur during the training process.</p> In\u00a0[9]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data.loc[:end_train, 'users'],\n                       exog = data.loc[:end_train, exog_features]\n                   )\n\nprint(X_train.dtypes)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data.loc[:end_train, 'users'],                        exog = data.loc[:end_train, exog_features]                    )  print(X_train.dtypes) X_train.head() <pre>lag_1            float64\nlag_2            float64\nlag_3            float64\nlag_4            float64\nlag_5            float64\nholiday_1        float64\nweather_clear    float64\nweather_mist     float64\nweather_rain     float64\ntemp             float64\nhum              float64\ndtype: object\n</pre> Out[9]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday_1 weather_clear weather_mist weather_rain temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0.0 0.0 1.0 0.0 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0.0 1.0 0.0 0.0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0.0 1.0 0.0 0.0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0.0 1.0 0.0 0.0 13.12 76.0 In\u00a0[10]: Copied! <pre># Transform exogenous features using the transformer outside the forecaster\n# ==============================================================================\nexog_transformed = one_hot_encoder.fit_transform(data.loc[:end_train, exog_features])\nexog_transformed.head()\n</pre> # Transform exogenous features using the transformer outside the forecaster # ============================================================================== exog_transformed = one_hot_encoder.fit_transform(data.loc[:end_train, exog_features]) exog_transformed.head() Out[10]: holiday_1 weather_clear weather_mist weather_rain temp hum date_time 2011-01-01 00:00:00 0.0 1.0 0.0 0.0 9.84 81.0 2011-01-01 01:00:00 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 02:00:00 0.0 1.0 0.0 0.0 9.02 80.0 2011-01-01 03:00:00 0.0 1.0 0.0 0.0 9.84 75.0 2011-01-01 04:00:00 0.0 1.0 0.0 0.0 9.84 75.0 In\u00a0[11]: Copied! <pre># ColumnTransformer with ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\nordinal_encoder = make_column_transformer(\n                      (\n                          OrdinalEncoder(\n                              handle_unknown='use_encoded_value',\n                              unknown_value=-1,\n                              encoded_missing_value=-1\n                          ),\n                          make_column_selector(dtype_exclude=np.number)\n                      ),\n                      remainder=\"passthrough\",\n                      verbose_feature_names_out=False,\n                  ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. ordinal_encoder = make_column_transformer(                       (                           OrdinalEncoder(                               handle_unknown='use_encoded_value',                               unknown_value=-1,                               encoded_missing_value=-1                           ),                           make_column_selector(dtype_exclude=np.number)                       ),                       remainder=\"passthrough\",                       verbose_feature_names_out=False,                   ).set_output(transform=\"pandas\") In\u00a0[12]: Copied! <pre># Create and fit a forecaster with a transformer for exogenous features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\n\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(random_state=123, verbose=-1),\n                 lags             = 5,\n                 transformer_exog = ordinal_encoder\n             )\n\nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n\nforecaster\n</pre> # Create and fit a forecaster with a transformer for exogenous features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum']  forecaster = ForecasterRecursive(                  regressor        = LGBMRegressor(random_state=123, verbose=-1),                  lags             = 5,                  transformer_exog = ordinal_encoder              )  forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] )  forecaster Out[12]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3 4 5]</li> <li>Window features: None</li> <li>Window size: 5</li> <li>Series name: users</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:30:49</li> <li>Last fit date: 2025-08-06 13:30:49</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     holiday, weather, temp, hum                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: ColumnTransformer(remainder='passthrough',                   transformers=[('ordinalencoder',                                  OrdinalEncoder(encoded_missing_value=-1,                                                 handle_unknown='use_encoded_value',                                                 unknown_value=-1),                                  )],                   verbose_feature_names_out=False)</li> </ul> Training Information <ul> <li>Training range: [Timestamp('2011-01-01 00:00:00'), Timestamp('2012-07-31 23:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: h</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[13]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data.loc[:end_train, 'users'],\n                       exog = data.loc[:end_train, exog_features]\n                   )\n\nprint(X_train.dtypes)\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data.loc[:end_train, 'users'],                        exog = data.loc[:end_train, exog_features]                    )  print(X_train.dtypes) X_train.head() <pre>lag_1      float64\nlag_2      float64\nlag_3      float64\nlag_4      float64\nlag_5      float64\nholiday    float64\nweather    float64\ntemp       float64\nhum        float64\ndtype: object\n</pre> Out[13]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0.0 1.0 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0.0 0.0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0.0 0.0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0.0 0.0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0.0 0.0 13.12 76.0 <p>Once the forecaster has been trained, the transformer can be inspected by accessing the <code>transformer_exog</code> attribute.</p> In\u00a0[14]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[14]: <pre>2012-08-01 00:00:00    89.096098\n2012-08-01 01:00:00    57.749964\n2012-08-01 02:00:00    29.263922\nFreq: h, Name: pred, dtype: float64</pre> <p> \u26a0 Warning </p> <p>TargetEncoder differs from the other transformers in scikit-learn in that it requires not only the features to be transformed but also the response variable (target), in the context of prediction, this is the time series. Currently, the only transformers allowed in the prediction classes are those that do not require the target variable to be fitted. Therefore, to use target encoding, transformations must be applied outside the Forecaster object.</p> In\u00a0[15]: Copied! <pre># ColumnTransformer with target encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using target encoding. Numeric features are left untouched. TargetEncoder\n# considers missing values, such as np.nan or None, as another category and\n# encodes them like any other category. Categories that are not seen during fit\n# are encoded with the target mean\n\ntarget_encoder = make_column_transformer(\n                     (\n                         TargetEncoder(\n                             categories   = 'auto',\n                             target_type  = 'continuous',\n                             smooth       = 'auto',\n                             random_state = 9874\n                         ),\n                         make_column_selector(dtype_exclude=np.number)\n                     ),\n                     remainder=\"passthrough\",\n                     verbose_feature_names_out=False,\n                 ).set_output(transform=\"pandas\")\n</pre> # ColumnTransformer with target encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using target encoding. Numeric features are left untouched. TargetEncoder # considers missing values, such as np.nan or None, as another category and # encodes them like any other category. Categories that are not seen during fit # are encoded with the target mean  target_encoder = make_column_transformer(                      (                          TargetEncoder(                              categories   = 'auto',                              target_type  = 'continuous',                              smooth       = 'auto',                              random_state = 9874                          ),                          make_column_selector(dtype_exclude=np.number)                      ),                      remainder=\"passthrough\",                      verbose_feature_names_out=False,                  ).set_output(transform=\"pandas\") In\u00a0[16]: Copied! <pre># Transform the exogenous features using the transformer outside the forecaster\n# ==============================================================================\nexog_transformed = target_encoder.fit_transform(\n                       X = data.loc[:end_train, exog_features],\n                       y = data.loc[:end_train, 'users']\n                   )\nexog_transformed.head()\n</pre> # Transform the exogenous features using the transformer outside the forecaster # ============================================================================== exog_transformed = target_encoder.fit_transform(                        X = data.loc[:end_train, exog_features],                        y = data.loc[:end_train, 'users']                    ) exog_transformed.head() Out[16]: holiday weather temp hum date_time 2011-01-01 00:00:00 172.823951 188.121327 9.84 81.0 2011-01-01 01:00:00 172.607889 187.330734 9.02 80.0 2011-01-01 02:00:00 173.476675 189.423278 9.02 80.0 2011-01-01 03:00:00 172.823951 188.121327 9.84 75.0 2011-01-01 04:00:00 172.823951 188.121327 9.84 75.0 <p> \u26a0 Warning </p> <p>When deploying models in production, it is strongly recommended to avoid using automatic detection based on pandas <code>category</code> type columns. Although pandas provides an internal coding for these columns, it is not consistent across different datasets and may vary depending on the categories present in each one. It is therefore crucial to be aware of this issue and to take appropriate measures to ensure consistency in the coding of categorical features when deploying models in production.</p> <p>At the time of writing, the authors have observed that <code>LightGBM</code> and <code>HistGradientBoostingRegressor</code> internally manage changes in the coding of categories to ensure consistency.</p> <p>If the user still wishes to rely on automatic detection of categorical features based on pandas data types, categorical variables must first be encoded as integers (ordinal encoding) and then stored as category type. This is necessary because skforecast uses a numeric numpy array internally to speed up the calculation.</p> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features</p> <p>When creating a forecaster with <code>LGBMRegressor</code>, it is necessary to specify the names of the categorical columns using the <code>fit_kwargs</code> argument. This is because the <code>categorical_feature</code> argument is only specified in the <code>fit</code> method of <code>LGBMRegressor</code>, and not during its initialization.</p> In\u00a0[17]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                       (\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           categorical_features\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                        (                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            categorical_features                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[18]: Copied! <pre># Create and fit forecaster indicating the categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(random_state=963, verbose=-1),\n                 lags             = 5,\n                 transformer_exog = transformer_exog,\n                 fit_kwargs       = {'categorical_feature': categorical_features}\n             )\n \nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster indicating the categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterRecursive(                  regressor        = LGBMRegressor(random_state=963, verbose=-1),                  lags             = 5,                  transformer_exog = transformer_exog,                  fit_kwargs       = {'categorical_feature': categorical_features}              )   forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[19]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[19]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: h, Name: pred, dtype: float64</pre> In\u00a0[20]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nregressor = forecaster.regressor\ncat_index = regressor.booster_.params.get('categorical_column')\nif cat_index is not None:\n    features_in_model = regressor.booster_.feature_name()\n    cat_features_in_model = [features_in_model[i] for i in cat_index]\n\ncat_features_in_model\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== regressor = forecaster.regressor cat_index = regressor.booster_.params.get('categorical_column') if cat_index is not None:     features_in_model = regressor.booster_.feature_name()     cat_features_in_model = [features_in_model[i] for i in cat_index]  cat_features_in_model Out[20]: <pre>['holiday', 'weather']</pre> In\u00a0[21]: Copied! <pre># Show the encoding applied to the categorical features\n# ==============================================================================\nordinal_encoder = transformer_exog.named_transformers_['ordinalencoder']\nfor feature, cats in zip(categorical_features, ordinal_encoder.categories_):\n    print(f\"Feature '{feature}' categories and codes:\")\n    for code, category in enumerate(cats):\n        print(f\"  {category}: {code}\")\n</pre> # Show the encoding applied to the categorical features # ============================================================================== ordinal_encoder = transformer_exog.named_transformers_['ordinalencoder'] for feature, cats in zip(categorical_features, ordinal_encoder.categories_):     print(f\"Feature '{feature}' categories and codes:\")     for code, category in enumerate(cats):         print(f\"  {category}: {code}\") <pre>Feature 'holiday' categories and codes:\n  0: 0\n  1: 1\nFeature 'weather' categories and codes:\n  clear: 0\n  mist: 1\n  rain: 2\n</pre> <p>Allow the model to automatically detect categorical features</p> In\u00a0[22]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After encoding, the features are converted back to category type so that \n# they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           FunctionTransformer(\n                               func=lambda x: x.astype('category'),\n                               feature_names_out= 'one-to-one'\n                           )\n                       )\n\ntransformer_exog = make_column_transformer(\n                       (\n                           pipeline_categorical,\n                           make_column_selector(dtype_exclude=np.number)\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After encoding, the features are converted back to category type so that  # they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            FunctionTransformer(                                func=lambda x: x.astype('category'),                                feature_names_out= 'one-to-one'                            )                        )  transformer_exog = make_column_transformer(                        (                            pipeline_categorical,                            make_column_selector(dtype_exclude=np.number)                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[23]: Copied! <pre># Create and fit forecaster with automatic detection of categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterRecursive(\n                 regressor        = LGBMRegressor(random_state=963, verbose=-1),\n                 lags             = 5,\n                 transformer_exog = transformer_exog,\n                 fit_kwargs       = {'categorical_feature': 'auto'}\n             )\n            \nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster with automatic detection of categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterRecursive(                  regressor        = LGBMRegressor(random_state=963, verbose=-1),                  lags             = 5,                  transformer_exog = transformer_exog,                  fit_kwargs       = {'categorical_feature': 'auto'}              )              forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[24]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[24]: <pre>2012-08-01 00:00:00    88.946940\n2012-08-01 01:00:00    59.848451\n2012-08-01 02:00:00    28.870817\nFreq: h, Name: pred, dtype: float64</pre> <p>As with any other forecaster, the matrices used during model training can be created with <code>create_train_X_y</code>.</p> In\u00a0[25]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data.loc[:end_train, 'users'],\n                       exog = data.loc[:end_train, exog_features]\n                   )\nX_train.head()\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data.loc[:end_train, 'users'],                        exog = data.loc[:end_train, exog_features]                    ) X_train.head() Out[25]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0 1 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0 0 9.02 80.0 2011-01-01 07:00:00 2.0 1.0 1.0 13.0 32.0 0 0 8.20 86.0 2011-01-01 08:00:00 3.0 2.0 1.0 1.0 13.0 0 0 9.84 75.0 2011-01-01 09:00:00 8.0 3.0 2.0 1.0 1.0 0 0 13.12 76.0 In\u00a0[26]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nregressor = forecaster.regressor\ncat_index = regressor.booster_.params.get('categorical_column')\nif cat_index is not None:\n    features_in_model = regressor.booster_.feature_name()\n    cat_features_in_model = [features_in_model[i] for i in cat_index]\n    \ncat_features_in_model\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== regressor = forecaster.regressor cat_index = regressor.booster_.params.get('categorical_column') if cat_index is not None:     features_in_model = regressor.booster_.feature_name()     cat_features_in_model = [features_in_model[i] for i in cat_index]      cat_features_in_model Out[26]: <pre>['holiday', 'weather']</pre> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features</p> <p>When creating a forecaster using <code>HistogramGradientBoosting</code>, the names of the categorical columns should be specified during the instantiation by passing them as a list to the <code>categorical_feature</code> argument.</p> In\u00a0[27]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                       (\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           categorical_features\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                        (                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            categorical_features                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[28]: Copied! <pre># Create and fit forecaster indicating the categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterRecursive(\n                 regressor = HistGradientBoostingRegressor(\n                                 categorical_features = categorical_features,\n                                 random_state = 963\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n            \nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster indicating the categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterRecursive(                  regressor = HistGradientBoostingRegressor(                                  categorical_features = categorical_features,                                  random_state = 963                              ),                  lags = 5,                  transformer_exog = transformer_exog              )              forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[29]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[29]: <pre>2012-08-01 00:00:00    99.185547\n2012-08-01 01:00:00    71.914255\n2012-08-01 02:00:00    43.342723\nFreq: h, Name: pred, dtype: float64</pre> <p><code>HistGradientBoostingRegressor</code> stores a boolean mask indicating which features were considered categorical. It will be <code>None</code> if there are no categorical features.</p> In\u00a0[30]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nforecaster.regressor.feature_names_in_[forecaster.regressor.is_categorical_]\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== forecaster.regressor.feature_names_in_[forecaster.regressor.is_categorical_] Out[30]: <pre>array(['holiday', 'weather'], dtype=object)</pre> In\u00a0[31]: Copied! <pre># Show the encoding applied to the categorical features\n# ==============================================================================\nordinal_encoder = transformer_exog.named_transformers_['ordinalencoder']\nfor feature, cats in zip(categorical_features, ordinal_encoder.categories_):\n    print(f\"Feature '{feature}' categories and codes:\")\n    for code, category in enumerate(cats):\n        print(f\"  {category}: {code}\")\n</pre> # Show the encoding applied to the categorical features # ============================================================================== ordinal_encoder = transformer_exog.named_transformers_['ordinalencoder'] for feature, cats in zip(categorical_features, ordinal_encoder.categories_):     print(f\"Feature '{feature}' categories and codes:\")     for code, category in enumerate(cats):         print(f\"  {category}: {code}\") <pre>Feature 'holiday' categories and codes:\n  0: 0\n  1: 1\nFeature 'weather' categories and codes:\n  clear: 0\n  mist: 1\n  rain: 2\n</pre> <p>Allow the model to automatically detect categorical features</p> In\u00a0[32]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (not numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After encoding, the features are converted back to category type so that \n# they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           FunctionTransformer(\n                               func=lambda x: x.astype('category'),\n                               feature_names_out= 'one-to-one'\n                           )\n                       )\n\ntransformer_exog = make_column_transformer(\n                       (\n                           pipeline_categorical,\n                           make_column_selector(dtype_exclude=np.number)\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (not numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After encoding, the features are converted back to category type so that  # they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            FunctionTransformer(                                func=lambda x: x.astype('category'),                                feature_names_out= 'one-to-one'                            )                        )  transformer_exog = make_column_transformer(                        (                            pipeline_categorical,                            make_column_selector(dtype_exclude=np.number)                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[33]: Copied! <pre># Create and fit forecaster with automatic detection of categorical features\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterRecursive(\n                 regressor        = HistGradientBoostingRegressor(random_state=963, categorical_features='from_dtype'),\n                 lags             = 5,\n                 transformer_exog = transformer_exog,\n             )\n            \nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster with automatic detection of categorical features # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterRecursive(                  regressor        = HistGradientBoostingRegressor(random_state=963, categorical_features='from_dtype'),                  lags             = 5,                  transformer_exog = transformer_exog,              )              forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[34]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nforecaster.regressor.feature_names_in_[forecaster.regressor.is_categorical_]\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== forecaster.regressor.feature_names_in_[forecaster.regressor.is_categorical_] Out[34]: <pre>array(['holiday', 'weather'], dtype=object)</pre> In\u00a0[35]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[35]: <pre>2012-08-01 00:00:00    99.185547\n2012-08-01 01:00:00    71.914255\n2012-08-01 02:00:00    43.342723\nFreq: h, Name: pred, dtype: float64</pre> <p>Encoding the categories as integers and explicitly specifying the names of the categorical features</p> <p>At the time of writing, the <code>XGBRegressor</code> module does not provide an option to specify the names of categorical features. Instead, the feature types are specified by passing a list of strings to the <code>feature_types</code> argument, where 'c' denotes categorical and 'q' numeric features. The <code>enable_categorical</code> argument must also be set to <code>True</code>.</p> <p>Determining the positions of each column to create a list of feature types can be a challenging task. The shape of the data matrix depends on two factors, the number of lags used and the transformations applied to the exogenous variables. However, there is a workaround to this problem. First, create a forecaster without specifying the <code>feature_types</code> argument. Next, the <code>create_train_X_y</code> method can be used with a small sample of data to determine the position of each feature. Once the position of each feature has been determined, the <code>set_params()</code> method can be used to specify the values of <code>feature_types</code>. By following this approach it is possible to ensure that the feature types are correctly specified, thus avoiding any errors that may occur due to incorrect specification.</p> In\u00a0[36]: Copied! <pre># Transformer: ordinal encoding\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (no numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1.\ncategorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist()\ntransformer_exog = make_column_transformer(\n                       (\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           categorical_features\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding # ============================================================================== # A ColumnTransformer is used to transform categorical (no numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. categorical_features = data.select_dtypes(exclude=[np.number]).columns.tolist() transformer_exog = make_column_transformer(                        (                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            categorical_features                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") <p>A forecaster is created without specifying the <code>feature_types</code> argument.</p> In\u00a0[37]: Copied! <pre># Create forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterRecursive(\n                 regressor = XGBRegressor(\n                                 tree_method        = 'hist',\n                                 random_state       = 12345,\n                                 enable_categorical = True,\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n</pre> # Create forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterRecursive(                  regressor = XGBRegressor(                                  tree_method        = 'hist',                                  random_state       = 12345,                                  enable_categorical = True,                              ),                  lags = 5,                  transformer_exog = transformer_exog              ) <p>Once the forecaster is instantiated, its <code>create_train_X_y()</code> method is used to generate the training matrices that allow the user to identify the positions of the variables.</p> In\u00a0[38]: Copied! <pre># Create training matrices using a sample of the training data\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data.loc[:end_train, 'users'][:10],\n                       exog = data.loc[:end_train, exog_features][:10]\n                   )\nX_train.head(2)\n</pre> # Create training matrices using a sample of the training data # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data.loc[:end_train, 'users'][:10],                        exog = data.loc[:end_train, exog_features][:10]                    ) X_train.head(2) Out[38]: lag_1 lag_2 lag_3 lag_4 lag_5 holiday weather temp hum date_time 2011-01-01 05:00:00 1.0 13.0 32.0 40.0 16.0 0.0 1.0 9.84 75.0 2011-01-01 06:00:00 1.0 1.0 13.0 32.0 40.0 0.0 0.0 9.02 80.0 <p>Create a list to identify which columns in the training matrix are numeric ('q') and categorical ('c').</p> In\u00a0[39]: Copied! <pre>feature_types = [\n    \"c\" if X_train[col].dtype.name in [\"object\", \"category\"] or col in categorical_features\n    else \"q\"\n    for col in X_train.columns\n]\nfeature_types\n</pre> feature_types = [     \"c\" if X_train[col].dtype.name in [\"object\", \"category\"] or col in categorical_features     else \"q\"     for col in X_train.columns ] feature_types Out[39]: <pre>['q', 'q', 'q', 'q', 'q', 'c', 'c', 'q', 'q']</pre> <p>Update the regressor parameters using the forecaster's <code>set_params</code> method and fit.</p> In\u00a0[40]: Copied! <pre># Update regressor parameters\n# ==============================================================================\nforecaster.set_params({'feature_types': feature_types})\n</pre> # Update regressor parameters # ============================================================================== forecaster.set_params({'feature_types': feature_types}) In\u00a0[41]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Fit forecaster # ============================================================================== forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[42]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[42]: <pre>2012-08-01 00:00:00    82.127357\n2012-08-01 01:00:00    45.740013\n2012-08-01 02:00:00    23.580956\nFreq: h, Name: pred, dtype: float64</pre> In\u00a0[43]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nfeature_types = np.array(forecaster.regressor.get_booster().feature_types)\nfeatures_in_model = np.array(forecaster.regressor.get_booster().feature_names)\nfeatures_in_model[feature_types == 'c']\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== feature_types = np.array(forecaster.regressor.get_booster().feature_types) features_in_model = np.array(forecaster.regressor.get_booster().feature_names) features_in_model[feature_types == 'c'] Out[43]: <pre>array(['holiday', 'weather'], dtype='&lt;U7')</pre> In\u00a0[44]: Copied! <pre># Show the encoding applied to the categorical features\n# ==============================================================================\nordinal_encoder = transformer_exog.named_transformers_['ordinalencoder']\nfor feature, cats in zip(categorical_features, ordinal_encoder.categories_):\n    print(f\"Feature '{feature}' categories and codes:\")\n    for code, category in enumerate(cats):\n        print(f\"  {category}: {code}\")\n</pre> # Show the encoding applied to the categorical features # ============================================================================== ordinal_encoder = transformer_exog.named_transformers_['ordinalencoder'] for feature, cats in zip(categorical_features, ordinal_encoder.categories_):     print(f\"Feature '{feature}' categories and codes:\")     for code, category in enumerate(cats):         print(f\"  {category}: {code}\") <pre>Feature 'holiday' categories and codes:\n  0: 0\n  1: 1\nFeature 'weather' categories and codes:\n  clear: 0\n  mist: 1\n  rain: 2\n</pre> <p>Allow the model to automatically detect categorical features</p> In\u00a0[45]: Copied! <pre># Transformer: ordinal encoding and cast to category type\n# ==============================================================================\n# A ColumnTransformer is used to transform categorical (no numerical) features\n# using ordinal encoding. Numeric features are left untouched. Missing values\n# are coded as -1. If a new category is found in the test set, it is encoded\n# as -1. After the encoding, the features are converted back to category type so \n# that they can be identified as categorical features by the regressor.\n\npipeline_categorical = make_pipeline(\n                           OrdinalEncoder(\n                               dtype=int,\n                               handle_unknown=\"use_encoded_value\",\n                               unknown_value=-1,\n                               encoded_missing_value=-1\n                           ),\n                           FunctionTransformer(\n                               func=lambda x: x.astype('category'),\n                               feature_names_out= 'one-to-one'\n                           )\n                       )\n\ntransformer_exog = make_column_transformer(\n                       (\n                           pipeline_categorical,\n                           make_column_selector(dtype_exclude=np.number)\n                       ),\n                       remainder=\"passthrough\",\n                       verbose_feature_names_out=False,\n                   ).set_output(transform=\"pandas\")\n</pre> # Transformer: ordinal encoding and cast to category type # ============================================================================== # A ColumnTransformer is used to transform categorical (no numerical) features # using ordinal encoding. Numeric features are left untouched. Missing values # are coded as -1. If a new category is found in the test set, it is encoded # as -1. After the encoding, the features are converted back to category type so  # that they can be identified as categorical features by the regressor.  pipeline_categorical = make_pipeline(                            OrdinalEncoder(                                dtype=int,                                handle_unknown=\"use_encoded_value\",                                unknown_value=-1,                                encoded_missing_value=-1                            ),                            FunctionTransformer(                                func=lambda x: x.astype('category'),                                feature_names_out= 'one-to-one'                            )                        )  transformer_exog = make_column_transformer(                        (                            pipeline_categorical,                            make_column_selector(dtype_exclude=np.number)                        ),                        remainder=\"passthrough\",                        verbose_feature_names_out=False,                    ).set_output(transform=\"pandas\") In\u00a0[46]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nexog_features = ['holiday', 'weather', 'temp', 'hum']\nforecaster = ForecasterRecursive(\n                 regressor = XGBRegressor(\n                                 enable_categorical=True,\n                                 tree_method='hist',\n                                 random_state=963\n                             ),\n                 lags = 5,\n                 transformer_exog = transformer_exog\n             )\n            \nforecaster.fit(\n    y    = data.loc[:end_train, 'users'],\n    exog = data.loc[:end_train, exog_features]\n)\n</pre> # Create and fit forecaster # ============================================================================== exog_features = ['holiday', 'weather', 'temp', 'hum'] forecaster = ForecasterRecursive(                  regressor = XGBRegressor(                                  enable_categorical=True,                                  tree_method='hist',                                  random_state=963                              ),                  lags = 5,                  transformer_exog = transformer_exog              )              forecaster.fit(     y    = data.loc[:end_train, 'users'],     exog = data.loc[:end_train, exog_features] ) In\u00a0[47]: Copied! <pre># Predictions\n# ==============================================================================\nforecaster.predict(steps=3, exog=data_test[exog_features])\n</pre> # Predictions # ============================================================================== forecaster.predict(steps=3, exog=data_test[exog_features]) Out[47]: <pre>2012-08-01 00:00:00    82.127357\n2012-08-01 01:00:00    45.740013\n2012-08-01 02:00:00    23.580956\nFreq: h, Name: pred, dtype: float64</pre> In\u00a0[48]: Copied! <pre># Print the features considered as categorical by the forecaster\n# ==============================================================================\nfeature_types = np.array(forecaster.regressor.get_booster().feature_types)\nfeatures_in_model = np.array(forecaster.regressor.get_booster().feature_names)\nfeatures_in_model[feature_types == 'c']\n</pre> # Print the features considered as categorical by the forecaster # ============================================================================== feature_types = np.array(forecaster.regressor.get_booster().feature_types) features_in_model = np.array(forecaster.regressor.get_booster().feature_names) features_in_model[feature_types == 'c'] Out[48]: <pre>array(['holiday', 'weather'], dtype='&lt;U7')</pre>"},{"location":"user_guides/categorical-features.html#categorical-features","title":"Categorical features\u00b6","text":""},{"location":"user_guides/categorical-features.html#libraries-and-data","title":"Libraries and data\u00b6","text":"<p>The dataset used in this user guide consists of information on the number of users of a bicycle rental service, in addition to weather variables and holiday data. Two of the variables in the dataset, <code>holiday</code> and <code>weather</code>, are categorical.</p>"},{"location":"user_guides/categorical-features.html#one-hot-encoding","title":"One Hot Encoding\u00b6","text":"<p>One hot encoding, also known as dummy encoding or one-of-K encoding, consists of replacing the categorical variable with a set of binary variables that take the value 0 or 1 to indicate whether a particular category is present in an observation. For example, suppose a dataset contains a categorical variable called \"color\" with the possible values of \"red,\" \"blue,\" and \"green\". Using one hot encoding, this variable is converted into three binary variables such as <code>color_red</code>, <code>color_blue</code>, and <code>color_green</code>, where each variable takes a value of 0 or 1 depending on the category.</p> <p>The OneHotEncoder class in scikit-learn can be used to transform any categorical feature with n possible values into n new binary features, where one of them takes the value 1, and all the others take the value 0. The <code>OneHotEncoder</code> can be configured to handle certain corner cases, including unknown categories, missing values, and infrequent categories.</p> <ul> <li><p>When <code>handle_unknown='ignore'</code> and <code>drop</code> is not <code>None</code>, unknown categories are encoded as zeros. Additionally, if a feature contains both <code>np.nan</code> and <code>None</code>, they are considered separate categories.</p> </li> <li><p>It supports the aggregation of infrequent categories into a single output for each feature. The parameters to enable the aggregation of infrequent categories are <code>min_frequency</code> and <code>max_categories</code>. By setting <code>handle_unknown</code> to 'infrequent_if_exist', unknown categories are considered infrequent.</p> </li> <li><p>To avoid collinearity between features, it is possible to drop one of the categories per feature using the <code>drop</code> argument. This is especially important when using linear models.</p> </li> </ul> <p>ColumnTransformers in scikit-learn provide a powerful way to define transformations and apply them to specific features. By encapsulating the <code>OneHotEncoder</code> in a <code>ColumnTransformer</code> object, it can be passed to a forecaster using the <code>transformer_exog</code> argument.</p>"},{"location":"user_guides/categorical-features.html#ordinal-encoding","title":"Ordinal encoding\u00b6","text":"<p>Ordinal encoding is a technique used to convert categorical variables into numerical variables. Each category is assigned a unique numerical value based on its order or rank, as determined by a chosen criterion such as frequency or importance. This encoding method is particularly useful when categories have a natural order or ranking, such as educational qualifications. However, it is important to note that the numerical values assigned to each category do not represent any inherent numerical difference between them, but simply provide a numerical representation.</p> <p>The scikit-learn library provides the OrdinalEncoder class, which allows users to replace categorical variables with ordinal numbers ranging from 0 to n_categories-1. In addition, this class includes the <code>encoded_missing_value</code> parameter, which allows for the encoding of missing values. It is important to note that this implementation arbitrarily assigns numbers to categories on a first-seen-first-served basis. Users should therefore exercise caution when interpreting the numerical values assigned to the categories. Other implementations, such as the Feature-engine, numbers can be ordered based on the mean of the target.</p>"},{"location":"user_guides/categorical-features.html#target-encoding","title":"Target encoding\u00b6","text":"<p>Target encoding is a technic that encodes categorical variables based on the relationship between the categories and the target variable. Each category is encoded based on a shrinked estimate of the average target values for observations belonging to the category. The encoding scheme mixes the global target mean with the target mean conditioned on the value of the category.</p> <p>For example, suppose a categorical variable \"City\" with categories \"New York,\" \"Los Angeles,\" and \"Chicago,\" and a target variable \"Salary.\" One can calculate the mean salary for each city based on the training data, and use these mean values to encode the categories.</p> <p>This encoding scheme is useful with categorical features with high cardinality, where one-hot encoding would inflate the feature space making it more expensive for a downstream model to process. A classical example of high cardinality categories is location-based such as zip code or region.</p> <p>The TargetEncoder class is available in Scikit-learn (since version 1.3). <code>TargetEncoder</code> considers missing values, such as <code>np.nan</code> or <code>None</code>, as another category and encodes them like any other category. Categories that are not seen during fit are encoded with the target mean, i.e. <code>target_mean_</code>. A more detailed description of target encoding can be found in the scikit-learn user guide.</p>"},{"location":"user_guides/categorical-features.html#native-implementation-for-categorical-features","title":"Native implementation for categorical features\u00b6","text":"<p>Some machine learning models, including XGBoost, LightGBM, CatBoost, and HistGradientBoostingRegressor, provide built-in methods to handle categorical features, but they assume that the input categories are integers starting from 0 up to the number of categories [0, 1, ..., n_categories-1]. In practice, categorical variables are not coded with numbers but with strings, so an intermediate transformation step is necessary. Two options are:</p> <ul> <li><p>Set columns with categorical variables to the type <code>category</code>. For each column, the data structure consists of an array of categories and an array of integer values (codes) that point to the actual value of the array of categories. That is, internally it is a numeric array with a mapping that relates each value to a category. Models are able to automatically identify the columns of type <code>category</code> and access their internal codes.</p> </li> <li><p>Preprocess the categorical columns with an <code>OrdinalEncoder</code> to transform their values to integers and explicitly indicate that the columns should be treated as categorical.</p> </li> </ul>"},{"location":"user_guides/categorical-features.html#lightgbm","title":"LightGBM\u00b6","text":""},{"location":"user_guides/categorical-features.html#scikit-learn-histogramgradientboosting","title":"Scikit-learn HistogramGradientBoosting\u00b6","text":""},{"location":"user_guides/categorical-features.html#xgboost","title":"XGBoost\u00b6","text":""},{"location":"user_guides/categorical-features.html#catboost","title":"CatBoost\u00b6","text":"<p>Unfortunately, the current version of skforecast is not compatible with CatBoost's built-in handling of categorical features. The issue arises because CatBoost only accepts categorical features as integers, while skforecast converts input data to floats for faster computation using numpy arrays in the internal prediction process. If a CatBoost model is required, an external encoder should be used for the categorical variables.</p>"},{"location":"user_guides/datasets.html","title":"Datasets","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nfrom skforecast.datasets import fetch_dataset\n</pre> # Libraries # ============================================================================== from skforecast.datasets import fetch_dataset <p>By default, the data is structured as a pandas dataframe with a datetime index and frequency. Additionally, a concise description is printed for quick reference.</p> In\u00a0[2]: Copied! <pre># Download data \n# ==============================================================================\ndata = fetch_dataset(name=\"bike_sharing\")\ndata.head()\n</pre> # Download data  # ============================================================================== data = fetch_dataset(name=\"bike_sharing\") data.head() <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 11)\n</pre> Out[2]: holiday workingday weather temp atemp hum windspeed users month hour weekday date_time 2011-01-01 00:00:00 0.0 0.0 clear 9.84 14.395 81.0 0.0 16.0 1 0 5 2011-01-01 01:00:00 0.0 0.0 clear 9.02 13.635 80.0 0.0 40.0 1 1 5 2011-01-01 02:00:00 0.0 0.0 clear 9.02 13.635 80.0 0.0 32.0 1 2 5 2011-01-01 03:00:00 0.0 0.0 clear 9.84 14.395 75.0 0.0 13.0 1 3 5 2011-01-01 04:00:00 0.0 0.0 clear 9.84 14.395 75.0 0.0 1.0 1 4 5 <p>Downloading raw data, without any preprocessing, is possible by specifying the <code>raw=True</code> argument.</p> In\u00a0[3]: Copied! <pre># Download raw data \n# ==============================================================================\ndata = fetch_dataset(name=\"bike_sharing\", raw=True)\ndata.head()\n</pre> # Download raw data  # ============================================================================== data = fetch_dataset(name=\"bike_sharing\", raw=True) data.head() <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 12)\n</pre> Out[3]: date_time holiday workingday weather temp atemp hum windspeed users month hour weekday 0 2011-01-01 00:00:00 0.0 0.0 clear 9.84 14.395 81.0 0.0 16.0 1 0 5 1 2011-01-01 01:00:00 0.0 0.0 clear 9.02 13.635 80.0 0.0 40.0 1 1 5 2 2011-01-01 02:00:00 0.0 0.0 clear 9.02 13.635 80.0 0.0 32.0 1 2 5 3 2011-01-01 03:00:00 0.0 0.0 clear 9.84 14.395 75.0 0.0 13.0 1 3 5 4 2011-01-01 04:00:00 0.0 0.0 clear 9.84 14.395 75.0 0.0 1.0 1 4 5"},{"location":"user_guides/datasets.html#data-sets","title":"Data sets\u00b6","text":"<p>All datasets used in the skforecast library and related tutorials are accessible using the <code>skforecast.datasets.fetch_dataset()</code> function. Each dataset in this collection comes with a description of its time series and a reference to its original source.</p> <p>Available data sets are stored at skforecast-datasets.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html","title":"Dependent multivariate series forecasting","text":"<p> \ud83d\udca1 Tip </p> <p>Why can only one series be predicted?</p> <p>Direct forecasting strategies face a scalability challenge.</p> <p>To predict multiple time series and multiple time steps, you would need a separate model for each series and each future time step. For example, with 1,000 series and a forecast horizon of 24, this would require training 24,000 individual models, a computationally impractical approach.</p> <p>That\u2019s why the <code>ForecasterDirectMultiVariate</code> can learn from multiple series, but can only predict one series at a time.</p> <p>Looking for true multivariate with multiple series?</p> <p>Try our <code>ForecasterRnn</code> class, which can handle multivariate forecasting with multiple series and multiple steps in a single deep learning model.</p> <p> \u270e Note </p> <p>Skforecast offers additional approaches to create Global Forecasting Models:</p> <ul> <li> Global Forecasting Models: Independent multi-series forecasting </li> <li> Global Forecasting Models: Time series with different lengths and different exogenous variables </li> <li> Global Forecasting Models: Forecasting with Deep Learning </li> </ul> <p>To learn more about global forecasting models visit our examples:</p> <ul> <li> Global Forecasting Models: Multi-series forecasting with Python and skforecast </li> <li> Scalable Forecasting: Modeling thousand time series with a single global model </li> <li> Global Forecasting Models: Comparative Analysis of Single and Multi-Series Forecasting Modeling </li> <li> Forecasting with Deep Learning </li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.direct import ForecasterDirectMultiVariate\nfrom skforecast.model_selection import (\n    OneStepAheadFold,\n    TimeSeriesFold,\n    backtesting_forecaster_multiseries,\n    grid_search_forecaster_multiseries,\n    random_search_forecaster_multiseries,\n    bayesian_search_forecaster_multiseries\n)\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from lightgbm import LGBMRegressor from sklearn.metrics import mean_absolute_error from skforecast.datasets import fetch_dataset from skforecast.preprocessing import RollingFeatures from skforecast.direct import ForecasterDirectMultiVariate from skforecast.model_selection import (     OneStepAheadFold,     TimeSeriesFold,     backtesting_forecaster_multiseries,     grid_search_forecaster_multiseries,     random_search_forecaster_multiseries,     bayesian_search_forecaster_multiseries ) from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name=\"air_quality_valencia_no_missing\")\n</pre> # Data download # ============================================================================== data = fetch_dataset(name=\"air_quality_valencia_no_missing\") <pre>air_quality_valencia_no_missing\n-------------------------------\nHourly measures of several air chemical pollutant at Valencia city (Avd.\nFrancia) from 2019-01-01 to 20213-12-31. Including the following variables:\npm2.5 (\u00b5g/m\u00b3), CO (mg/m\u00b3), NO (\u00b5g/m\u00b3), NO2 (\u00b5g/m\u00b3), PM10 (\u00b5g/m\u00b3), NOx (\u00b5g/m\u00b3),\nO3 (\u00b5g/m\u00b3), Veloc. (m/s), Direc. (degrees), SO2 (\u00b5g/m\u00b3). Missing values have\nbeen imputed using linear interpolation.\nRed de Vigilancia y Control de la Contaminaci\u00f3n Atmosf\u00e9rica, 46250047-Val\u00e8ncia -\nAv. Fran\u00e7a, https://mediambient.gva.es/es/web/calidad-ambiental/datos-\nhistoricos.\nShape of the dataset: (43824, 10)\n</pre> In\u00a0[3]: Copied! <pre># Aggregate at daily frequency to reduce dimensions\n# ==============================================================================\ndata = data.resample('D').mean()\nprint(\"Shape: \", data.shape)\ndata.head()\n</pre> # Aggregate at daily frequency to reduce dimensions # ============================================================================== data = data.resample('D').mean() print(\"Shape: \", data.shape) data.head() <pre>Shape:  (1826, 10)\n</pre> Out[3]: so2 co no no2 pm10 nox o3 veloc. direc. pm2.5 datetime 2019-01-01 6.000000 0.141667 17.375000 37.250000 21.458333 63.458333 20.291667 0.416667 207.416667 17.208333 2019-01-02 6.041667 0.170833 23.458333 49.333333 26.416667 85.041667 11.708333 0.579167 225.375000 17.375000 2019-01-03 5.916667 0.216667 41.291667 53.250000 36.166667 116.333333 9.833333 0.500000 211.833333 21.625000 2019-01-04 5.458333 0.204167 21.208333 45.750000 32.208333 77.958333 15.166667 0.675000 199.583333 22.166667 2019-01-05 4.541667 0.191667 10.291667 36.375000 32.875000 51.833333 21.083333 0.875000 254.208333 24.916667 In\u00a0[4]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2023-05-31 23:59:59'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2023-05-31 23:59:59' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  \"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  \"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2019-01-01 00:00:00 --- 2023-05-31 00:00:00  (n=1612)\nTest dates  : 2023-06-01 00:00:00 --- 2023-12-31 00:00:00  (n=214)\n</pre> In\u00a0[5]: Copied! <pre># Plot time series\n# ==============================================================================\nset_dark_theme()\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 5), sharex=True)\nfor i, col in enumerate(data.columns[:3]):\n    data_train[col].plot(ax=axes[i], label='train')\n    data_test[col].plot(ax=axes[i], label='test')\n    axes[i].set_ylabel('Concentration(ug/m^3)', fontsize=8)\n    axes[i].set_title(col)\n    axes[i].legend(loc='upper right')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== set_dark_theme() fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 5), sharex=True) for i, col in enumerate(data.columns[:3]):     data_train[col].plot(ax=axes[i], label='train')     data_test[col].plot(ax=axes[i], label='test')     axes[i].set_ylabel('Concentration(ug/m^3)', fontsize=8)     axes[i].set_title(col)     axes[i].legend(loc='upper right')  fig.tight_layout() plt.show(); <p> \u270e Note </p> <p><code>ForecasterDirectMultiVariate</code> includes the <code>n_jobs</code> argument, allowing multi-process parallelization to train regressors for all steps simultaneously, significantly reducing training time.</p> <p>Its effectiveness depends on factors like the regressor type, the number of model fits to perform, and the volume of data. When <code>n_jobs</code> is set to <code>'auto'</code>, the level of parallelization is automatically determined using heuristic rules designed to select the most efficient configuration for each scenario.</p> <p>For more information, see the guide Parallelization in skforecast.</p> In\u00a0[6]: Copied! <pre># Create and fit forecaster MultiVariate\n# ==============================================================================\nforecaster = ForecasterDirectMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'co',\n                 steps              = 7,\n                 lags               = 7,\n                 window_features    = RollingFeatures(stats=['mean'], window_sizes=[7]),\n                 transformer_series = None,\n                 transformer_exog   = None\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Create and fit forecaster MultiVariate # ============================================================================== forecaster = ForecasterDirectMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'co',                  steps              = 7,                  lags               = 7,                  window_features    = RollingFeatures(stats=['mean'], window_sizes=[7]),                  transformer_series = None,                  transformer_exog   = None              )  forecaster.fit(series=data_train) forecaster Out[6]: ForecasterDirectMultiVariate General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Target series (level): co</li> <li>Lags: [1 2 3 4 5 6 7]</li> <li>Window features: ['roll_mean_7']</li> <li>Window size: 7</li> <li>Maximum steps to predict: 7</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:39:31</li> <li>Last fit date: 2025-08-06 13:39:37</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Target series (level): co</li> <li>Multivariate series: so2, co, no, no2, pm10, nox, o3, veloc., direc., pm2.5</li> <li>Training range: [Timestamp('2019-01-01 00:00:00'), Timestamp('2023-05-31 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>When predicting, the value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li><p>If <code>int</code> only steps within the range of 1 to int are predicted.</p> </li> <li><p>If <code>list</code> of <code>int</code>. Only the steps contained in the list are predicted.</p> </li> <li><p>If <code>None</code> as many steps are predicted as were defined at initialization.</p> </li> </ul> In\u00a0[7]: Copied! <pre># Predict with forecaster MultiVariate\n# ==============================================================================\npredictions = forecaster.predict(steps=None)  # All steps\npredictions\n</pre> # Predict with forecaster MultiVariate # ============================================================================== predictions = forecaster.predict(steps=None)  # All steps predictions Out[7]: level pred 2023-06-01 co 0.100165 2023-06-02 co 0.108636 2023-06-03 co 0.113710 2023-06-04 co 0.103102 2023-06-05 co 0.105516 2023-06-06 co 0.114029 2023-06-07 co 0.110274 In\u00a0[8]: Copied! <pre># Predict only a subset of steps\n# ==============================================================================\npredictions = forecaster.predict(steps=[1, 5])\npredictions\n</pre> # Predict only a subset of steps # ============================================================================== predictions = forecaster.predict(steps=[1, 5]) predictions Out[8]: level pred 2023-06-01 co 0.100165 2023-06-05 co 0.105516 In\u00a0[9]: Copied! <pre># Predict intervals using in-sample residuals\n# ==============================================================================\nforecaster.set_in_sample_residuals(series=data_train)\npredictions = forecaster.predict_interval(random_state=9871)\npredictions\n</pre> # Predict intervals using in-sample residuals # ============================================================================== forecaster.set_in_sample_residuals(series=data_train) predictions = forecaster.predict_interval(random_state=9871) predictions Out[9]: level pred lower_bound upper_bound 2023-06-01 co 0.100165 0.096533 0.103797 2023-06-02 co 0.108636 0.099615 0.117657 2023-06-03 co 0.113710 0.101929 0.125490 2023-06-04 co 0.103102 0.099967 0.106237 2023-06-05 co 0.105516 0.099785 0.111247 2023-06-06 co 0.114029 0.102248 0.125810 2023-06-07 co 0.110274 0.098493 0.122054 <p>To learn more about probabilistic forecasting features available in skforecast, see the probabilistic forecasting user guides.</p> In\u00a0[10]: Copied! <pre># Backtesting MultiVariate\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps                 = 7,\n         initial_train_size    = len(data_train),\n         refit                 = False,\n         allow_incomplete_fold = True\n     )\n\nmetrics_levels, backtest_preds = backtesting_forecaster_multiseries(\n                                     forecaster = forecaster,\n                                     series     = data,\n                                     cv         = cv,\n                                     metric     = 'mean_absolute_error'\n                                 )\ndisplay(metrics_levels)\nbacktest_preds.head(4)\n</pre> # Backtesting MultiVariate # ============================================================================== cv = TimeSeriesFold(          steps                 = 7,          initial_train_size    = len(data_train),          refit                 = False,          allow_incomplete_fold = True      )  metrics_levels, backtest_preds = backtesting_forecaster_multiseries(                                      forecaster = forecaster,                                      series     = data,                                      cv         = cv,                                      metric     = 'mean_absolute_error'                                  ) display(metrics_levels) backtest_preds.head(4) <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> levels mean_absolute_error 0 co 0.016993 Out[10]: level pred 2023-06-01 co 0.100165 2023-06-02 co 0.108636 2023-06-03 co 0.113710 2023-06-04 co 0.103102 In\u00a0[11]: Copied! <pre># Create and forecaster MultiVariate\n# ==============================================================================\nforecaster = ForecasterDirectMultiVariate(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 level           = 'co',\n                 steps           = 7,\n                 lags            = 7,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[7])\n             )\n</pre> # Create and forecaster MultiVariate # ============================================================================== forecaster = ForecasterDirectMultiVariate(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  level           = 'co',                  steps           = 7,                  lags            = 7,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[7])              ) In\u00a0[12]: Copied! <pre># Random search MultiVariate\n# ==============================================================================\nlags_grid = [7, 14]\nparam_distributions = {\n    'n_estimators': np.arange(start=10, stop=20, step=1, dtype=int),\n    'max_depth': np.arange(start=3, stop=6, step=1, dtype=int)\n}\n\ncv = TimeSeriesFold(\n         steps              = 7,\n         initial_train_size = len(data_train),\n         refit              = False,\n     )\n\nresults = random_search_forecaster_multiseries(\n              forecaster          = forecaster,\n              series              = data,\n              lags_grid           = lags_grid,\n              param_distributions = param_distributions,\n              cv                  = cv,\n              metric              = 'mean_absolute_error',\n              n_iter              = 5,\n          )\n\nresults\n</pre> # Random search MultiVariate # ============================================================================== lags_grid = [7, 14] param_distributions = {     'n_estimators': np.arange(start=10, stop=20, step=1, dtype=int),     'max_depth': np.arange(start=3, stop=6, step=1, dtype=int) }  cv = TimeSeriesFold(          steps              = 7,          initial_train_size = len(data_train),          refit              = False,      )  results = random_search_forecaster_multiseries(               forecaster          = forecaster,               series              = data,               lags_grid           = lags_grid,               param_distributions = param_distributions,               cv                  = cv,               metric              = 'mean_absolute_error',               n_iter              = 5,           )  results <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n  Parameters: {'n_estimators': np.int64(19), 'max_depth': np.int64(5)}\n  Backtesting metric: 0.016567112132881933\n  Levels: ['co']\n\n</pre> Out[12]: levels lags lags_label params mean_absolute_error n_estimators max_depth 0 [co] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 19, 'max_depth': 5} 0.016567 19 5 1 [co] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 16, 'max_depth': 5} 0.017329 16 5 2 [co] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 18, 'max_depth': 3} 0.017705 18 3 3 [co] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 19, 'max_depth': 5} 0.017724 19 5 4 [co] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 18, 'max_depth': 3} 0.017854 18 3 5 [co] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 17, 'max_depth': 3} 0.017885 17 3 6 [co] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 16, 'max_depth': 5} 0.017923 16 5 7 [co] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 15, 'max_depth': 3} 0.018076 15 3 8 [co] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 17, 'max_depth': 3} 0.018101 17 3 9 [co] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 15, 'max_depth': 3} 0.018235 15 3 <p>It is also possible to perform a bayesian optimization with Optuna using the <code>bayesian_search_forecaster_multiseries</code> function. For more information about this type of optimization, visit the user guide.</p> In\u00a0[26]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterDirectMultiVariate(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 level           = 'co',\n                 steps           = 7,\n                 lags            = 7,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[7])\n             )\n\n# Search space\ndef search_space(trial):\n    search_space  = {\n        'lags'            : trial.suggest_categorical('lags', [7, 14]),\n        'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1., 10),\n        'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n    }\n\n    return search_space\n\ncv = OneStepAheadFold(initial_train_size = '2023-05-31 23:59:59')\n\nresults, best_trial = bayesian_search_forecaster_multiseries(\n                          forecaster            = forecaster,\n                          series                = data,\n                          search_space          = search_space,\n                          cv                    = cv,\n                          metric                = 'mean_absolute_error',\n                          n_trials              = 5,\n                          kwargs_create_study   = {},\n                          kwargs_study_optimize = {}\n                      )\n\nresults.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterDirectMultiVariate(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  level           = 'co',                  steps           = 7,                  lags            = 7,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[7])              )  # Search space def search_space(trial):     search_space  = {         'lags'            : trial.suggest_categorical('lags', [7, 14]),         'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1., 10),         'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])     }      return search_space  cv = OneStepAheadFold(initial_train_size = '2023-05-31 23:59:59')  results, best_trial = bayesian_search_forecaster_multiseries(                           forecaster            = forecaster,                           series                = data,                           search_space          = search_space,                           cv                    = cv,                           metric                = 'mean_absolute_error',                           n_trials              = 5,                           kwargs_create_study   = {},                           kwargs_study_optimize = {}                       )  results.head(4) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n  Parameters: {'n_estimators': 16, 'min_samples_leaf': 9, 'max_features': 'log2'}\n  Backtesting metric: 0.010437198123137972\n  Levels: ['co']\n\n</pre> <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\jupyter_client\\session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\nc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\jupyter_client\\session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n</pre> Out[26]: levels lags params mean_absolute_error n_estimators min_samples_leaf max_features 0 [co] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'n_estimators': 16, 'min_samples_leaf': 9, 'm... 0.010437 16 9 log2 1 [co] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 15, 'min_samples_leaf': 4, 'm... 0.010891 15 4 sqrt 2 [co] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 14, 'min_samples_leaf': 8, 'm... 0.011436 14 8 log2 3 [co] [1, 2, 3, 4, 5, 6, 7] {'n_estimators': 12, 'min_samples_leaf': 6, 'm... 0.012376 12 6 log2 <p>The <code>best_trial</code> return contains the details of the trial that achieved the best result during optimization. For more information, refer to the Study class.</p> In\u00a0[15]: Copied! <pre># Optuna best trial in the study\n# ==============================================================================\nbest_trial\n</pre> # Optuna best trial in the study # ============================================================================== best_trial Out[15]: <pre>FrozenTrial(number=3, state=1, values=[0.010437198123137972], datetime_start=datetime.datetime(2025, 8, 6, 13, 39, 52, 376342), datetime_complete=datetime.datetime(2025, 8, 6, 13, 39, 52, 540519), params={'lags': 14, 'n_estimators': 16, 'min_samples_leaf': 9, 'max_features': 'log2'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lags': CategoricalDistribution(choices=(7, 14)), 'n_estimators': IntDistribution(high=20, log=False, low=10, step=1), 'min_samples_leaf': IntDistribution(high=10, log=False, low=1, step=1), 'max_features': CategoricalDistribution(choices=('log2', 'sqrt'))}, trial_id=3, value=None)</pre> In\u00a0[28]: Copied! <pre># Create and fit forecaster MultiVariate Custom lags\n# ==============================================================================\nlags_dict = {\n    'so2': [7, 14], 'co': 7, 'no': [7, 14], 'no2': [7, 14],\n    'pm10': [7, 14], 'nox': [7, 14], 'o3': [7, 14], 'veloc.': 3,\n    'direc.': 3, 'pm2.5': [7, 14]\n}\nforecaster = ForecasterDirectMultiVariate(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 level     = 'co',\n                 steps     = 7,\n                 lags      = lags_dict,\n             )\n\nforecaster.fit(series=data_train)\n\n# Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=7)\npredictions\n</pre> # Create and fit forecaster MultiVariate Custom lags # ============================================================================== lags_dict = {     'so2': [7, 14], 'co': 7, 'no': [7, 14], 'no2': [7, 14],     'pm10': [7, 14], 'nox': [7, 14], 'o3': [7, 14], 'veloc.': 3,     'direc.': 3, 'pm2.5': [7, 14] } forecaster = ForecasterDirectMultiVariate(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  level     = 'co',                  steps     = 7,                  lags      = lags_dict,              )  forecaster.fit(series=data_train)  # Predict # ============================================================================== predictions = forecaster.predict(steps=7) predictions Out[28]: level pred 2023-06-01 co 0.098798 2023-06-02 co 0.107185 2023-06-03 co 0.112225 2023-06-04 co 0.107208 2023-06-05 co 0.098643 2023-06-06 co 0.100375 2023-06-07 co 0.099513 <p>If <code>None</code> is assigned to any key in the <code>lags</code> dictionary, that series will be excluded from the creation of the <code>X</code> training matrix.</p> <p>In the following example, no lags are generated for the <code>'co'</code> series. However, since <code>'co'</code> is the specified target <code>level</code>, its values are still used to construct the <code>y</code> training matrix.</p> In\u00a0[29]: Copied! <pre># Create and fit forecaster MultiVariate Custom lags with None\n# ==============================================================================\nlags_dict = {\n    'so2': [7, 14], 'co': None, 'no': [7, 14], 'no2': [7, 14],\n    'pm10': [7, 14], 'nox': [7, 14], 'o3': [7, 14], 'veloc.': 3,\n    'direc.': 3, 'pm2.5': [7, 14]\n}\nforecaster = ForecasterDirectMultiVariate(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 level     = 'co',\n                 lags      = lags_dict,\n                 steps     = 7,\n             )\n\nforecaster.fit(series=data_train)\n\n# Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=7)\npredictions\n</pre> # Create and fit forecaster MultiVariate Custom lags with None # ============================================================================== lags_dict = {     'so2': [7, 14], 'co': None, 'no': [7, 14], 'no2': [7, 14],     'pm10': [7, 14], 'nox': [7, 14], 'o3': [7, 14], 'veloc.': 3,     'direc.': 3, 'pm2.5': [7, 14] } forecaster = ForecasterDirectMultiVariate(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  level     = 'co',                  lags      = lags_dict,                  steps     = 7,              )  forecaster.fit(series=data_train)  # Predict # ============================================================================== predictions = forecaster.predict(steps=7) predictions Out[29]: level pred 2023-06-01 co 0.107966 2023-06-02 co 0.118692 2023-06-03 co 0.125480 2023-06-04 co 0.132908 2023-06-05 co 0.118237 2023-06-06 co 0.121638 2023-06-07 co 0.120753 <p>It is possible to use the <code>create_train_X_y</code> method to generate the matrices that the forecaster is using to train the model. This approach enables gaining insight into the specific lags that have been created.</p> In\u00a0[30]: Copied! <pre># Extract training matrix\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(series=data_train)\n\n# X and y to train model for step 1\nX_train_step_1, y_train_step_1 = forecaster.filter_train_X_y_for_step(\n                                     step    = 1,\n                                     X_train = X_train,\n                                     y_train = y_train,\n                                 )\n\nX_train_step_1.head(4)\n</pre> # Extract training matrix # ============================================================================== X_train, y_train = forecaster.create_train_X_y(series=data_train)  # X and y to train model for step 1 X_train_step_1, y_train_step_1 = forecaster.filter_train_X_y_for_step(                                      step    = 1,                                      X_train = X_train,                                      y_train = y_train,                                  )  X_train_step_1.head(4) Out[30]: so2_lag_7 so2_lag_14 no_lag_7 no_lag_14 no2_lag_7 no2_lag_14 pm10_lag_7 pm10_lag_14 nox_lag_7 nox_lag_14 o3_lag_7 o3_lag_14 veloc._lag_1 veloc._lag_2 veloc._lag_3 direc._lag_1 direc._lag_2 direc._lag_3 pm2.5_lag_7 pm2.5_lag_14 datetime 2019-01-15 1.061795 2.362414 3.462136 2.232968 2.971237 2.098158 1.175259 0.229121 3.419881 2.298657 -1.871669 -1.800847 -0.726869 -0.979544 -0.792953 0.673274 1.019948 1.581166 2.104497 1.084751 2019-01-16 1.805006 2.408865 6.247303 3.270300 2.622404 3.254290 1.156585 0.599485 4.605306 3.486375 -1.754371 -2.256761 -1.150585 -0.726869 -0.979544 0.171311 0.673274 1.019948 1.782163 1.108193 2019-01-17 -0.982034 2.269513 0.826174 6.311248 2.596491 3.629037 -0.119456 1.327762 1.927209 5.208336 -0.331300 -2.356354 -0.944558 -1.150585 -0.726869 0.511174 0.171311 0.673274 -0.374541 1.705975 2019-01-18 1.479852 1.758556 8.194077 2.886629 4.617729 2.911437 1.221944 1.032094 6.726230 3.096583 -1.433461 -2.073068 -1.115600 -0.944558 -1.150585 1.436094 0.511174 0.171311 0.815163 1.782163 In\u00a0[31]: Copied! <pre># Extract training matrix\n# ==============================================================================\ny_train_step_1.head(4)\n</pre> # Extract training matrix # ============================================================================== y_train_step_1.head(4) Out[31]: <pre>datetime\n2019-01-15    3.149604\n2019-01-16    1.554001\n2019-01-17   -0.323179\n2019-01-18   -0.417038\nFreq: D, Name: co_step_1, dtype: float64</pre> In\u00a0[32]: Copied! <pre># Transformers in MultiVariate\n# ==============================================================================\nforecaster = ForecasterDirectMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'co',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = {'co': StandardScaler(), 'no2': StandardScaler()}\n             )\n\nforecaster.fit(series=data_train)\nforecaster\n</pre> # Transformers in MultiVariate # ============================================================================== forecaster = ForecasterDirectMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'co',                  lags               = 7,                  steps              = 7,                  transformer_series = {'co': StandardScaler(), 'no2': StandardScaler()}              )  forecaster.fit(series=data_train) forecaster <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 IgnoredArgumentWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 {'pm2.5', 'pm10', 'no', 'so2', 'veloc.', 'o3', 'direc.', 'nox'} not present in       \u2502\n\u2502 `transformer_series`. No transformation is applied to these series.                  \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : IgnoredArgumentWarning                                                    \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:372                                                                          \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=IgnoredArgumentWarning)          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[32]: ForecasterDirectMultiVariate General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Target series (level): co</li> <li>Lags: [1 2 3 4 5 6 7]</li> <li>Window features: None</li> <li>Window size: 7</li> <li>Maximum steps to predict: 7</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:40:51</li> <li>Last fit date: 2025-08-06 13:40:52</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: 'co': StandardScaler(), 'no2': StandardScaler()</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Target series (level): co</li> <li>Multivariate series: so2, co, no, no2, pm10, nox, o3, veloc., direc., pm2.5</li> <li>Training range: [Timestamp('2019-01-01 00:00:00'), Timestamp('2023-05-31 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[33]: Copied! <pre># Weights in MultiVariate\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\n\nforecaster = ForecasterDirectMultiVariate(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 level              = 'co',\n                 lags               = 7,\n                 steps              = 7,\n                 transformer_series = StandardScaler(),\n                 weight_func        = custom_weights\n             )\n\nforecaster.fit(series=data_train)\nforecaster.predict(steps=7).head(3)\n</pre> # Weights in MultiVariate # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.     \"\"\"     weights = np.where(                   (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),                    0,                    1               )          return weights   forecaster = ForecasterDirectMultiVariate(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  level              = 'co',                  lags               = 7,                  steps              = 7,                  transformer_series = StandardScaler(),                  weight_func        = custom_weights              )  forecaster.fit(series=data_train) forecaster.predict(steps=7).head(3) Out[33]: level pred 2023-06-01 co 0.106243 2023-06-02 co 0.103617 2023-06-03 co 0.102537 <p> \u26a0 Warning </p> <p>The <code>weight_func</code> argument will be ignored if the regressor does not accept <code>sample_weight</code> in its <code>fit</code> method.</p> <p>The source code of the <code>weight_func</code> added to the forecaster is stored in the argument <code>source_code_weight_func</code>.</p> In\u00a0[34]: Copied! <pre># Source code weight function\n# ==============================================================================\nprint(forecaster.source_code_weight_func)\n</pre> # Source code weight function # ============================================================================== print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n\n    return weights\n\n</pre> <p> \ud83d\udca1 Tip </p> <p>More information about time series forecasting metrics can be found in the Metrics guide.</p> In\u00a0[35]: Copied! <pre># Grid search MultiVariate with multiple metrics\n# ==============================================================================\nforecaster = ForecasterDirectMultiVariate(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 level     = 'co',\n                 lags      = 7,\n                 steps     = 7\n             )    \n\ndef custom_metric(y_true, y_pred):\n    \"\"\"\n    Calculate the mean absolute error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\ncv = TimeSeriesFold(\n         steps              = 7,\n         initial_train_size = len(data_train),\n         refit              = False,\n     )\n\nlags_grid = [7, 14]\nparam_grid = {'alpha': [0.01, 0.1, 1]}\n\nresults = grid_search_forecaster_multiseries(\n              forecaster = forecaster,\n              series     = data,\n              lags_grid  = lags_grid,\n              param_grid = param_grid,\n              cv         = cv,\n              metric     = [mean_absolute_error, custom_metric, 'mean_squared_error']\n          )\n\nresults\n</pre> # Grid search MultiVariate with multiple metrics # ============================================================================== forecaster = ForecasterDirectMultiVariate(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  level     = 'co',                  lags      = 7,                  steps     = 7              )      def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_absolute_error(y_true[mask], y_pred[mask])          return metric  cv = TimeSeriesFold(          steps              = 7,          initial_train_size = len(data_train),          refit              = False,      )  lags_grid = [7, 14] param_grid = {'alpha': [0.01, 0.1, 1]}  results = grid_search_forecaster_multiseries(               forecaster = forecaster,               series     = data,               lags_grid  = lags_grid,               param_grid = param_grid,               cv         = cv,               metric     = [mean_absolute_error, custom_metric, 'mean_squared_error']           )  results <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n  Parameters: {'alpha': 0.01}\n  Backtesting metric: 0.014486558378954958\n  Levels: ['co']\n\n</pre> Out[35]: levels lags lags_label params mean_absolute_error custom_metric mean_squared_error alpha 0 [co] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 0.01} 0.014487 0.021981 0.000439 0.01 1 [co] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 0.1} 0.014487 0.021981 0.000439 0.10 2 [co] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'alpha': 1} 0.014487 0.021981 0.000439 1.00 3 [co] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'alpha': 0.01} 0.014773 0.022789 0.000511 0.01 4 [co] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'alpha': 1} 0.014773 0.022789 0.000511 1.00 5 [co] [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] {'alpha': 0.1} 0.014773 0.022789 0.000511 0.10 In\u00a0[36]: Copied! <pre># Feature importances for step 1\n# ==============================================================================\nforecaster.get_feature_importances(step=1)\n</pre> # Feature importances for step 1 # ============================================================================== forecaster.get_feature_importances(step=1) Out[36]: feature importance 14 co_lag_1 146 31 no_lag_4 84 61 pm10_lag_6 64 1 so2_lag_2 52 0 so2_lag_1 50 ... ... ... 52 no2_lag_11 8 131 pm2.5_lag_6 8 132 pm2.5_lag_7 7 82 nox_lag_13 5 54 no2_lag_13 3 <p>140 rows \u00d7 2 columns</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#global-forecasting-models-dependent-multi-series-forecasting-multivariate-forecasting","title":"Global Forecasting Models: Dependent multi-series forecasting (Multivariate forecasting)\u00b6","text":"<p>Univariate time series forecasting focuses on modeling a single time series as a linear or nonlinear function of its own past values (lags), using historical observations to predict future ones.</p> <p>Global forecasting builds a single predictive model that considers all time series simultaneously. This approach seeks to learn the shared patterns that underlie the different series, helping to reduce the influence of noise present in individual time series. It is computationally efficient, easier to maintain, and often yields more robust generalization across series.</p> <p>In dependent multi-series forecasting (also known as multivariate time series forecasting), all series are modeled jointly under the assumption that each series depends not only on its own past values, but also on the past values of the other series. The forecaster is expected to learn both the individual dynamics of each series and the relationships between them.</p> <p>A typical example is the set of sensor readings (such as flow, temperature, and pressure) collected from an industrial machine like a compressor, where the variables influence each other over time.</p> <p> Internal Forecaster time series transformation to train a forecaster with multiple dependent time series. </p> <p>Since a separate training matrix is created for each series in the dataset, it is necessary to define the target level on which forecasting will be performed. To predict the next n steps, one model is trained for each forecast horizon step. In the example shown, the selected level is <code>Series 1</code>.</p> <p>This approach corresponds to a direct multi-step forecasting strategy, where each step is predicted independently using a separate model.</p> <p> Diagram of direct forecasting with multiple dependent time series. </p> <p>Using the <code>ForecasterDirectMultiVariate</code> class, it is possible to easily build machine learning models for multivariate forecasting.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#train-and-predict-forecasterdirectmultivariate","title":"Train and predict ForecasterDirectMultiVariate\u00b6","text":"<p>When initializing the forecaster, the series (<code>level</code>) to be predicted and the maximum number of <code>steps</code> must be indicated since a different model will be created for each step.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#backtesting-multivariate","title":"Backtesting MultiVariate\u00b6","text":"<p>See the backtesting user guide to learn more about backtesting.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#hyperparameter-tuning-and-lags-selection-multivariate","title":"Hyperparameter tuning and lags selection MultiVariate\u00b6","text":"<p>Hyperparameter tuning consists of systematically evaluating combinations of hyperparameters (including lags) to find the configuration that yields the best predictive performance. The skforecast library supports several tuning strategies: grid search, random search, and Bayesian search. These strategies can be used with either backtesting or one-step-ahead validation to determine the optimal parameter set for a given forecasting task.</p> <p>The functions <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>bayesian_search_forecaster_multiseries</code> from the <code>model_selection</code> module allow for lags and hyperparameter optimization.</p> <p>The following example shows how to use <code>random_search_forecaster_multiseries</code> to find the best lags and model hyperparameters.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#different-lags-for-each-time-series","title":"Different lags for each time series\u00b6","text":"<p>Passing a <code>dict</code> to the <code>lags</code> argument allows specifying different lags for each time series. The dictionary keys must correspond to the names of the series used during training, and the values define the lags to be applied to each one individually.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#exogenous-variables-in-multivariate","title":"Exogenous variables in MultiVariate\u00b6","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process.</p> <p>In the <code>ForecasterDirectMultiVariate</code>, as in the other forecasters, exogenous variables can be easily included as predictors using the <code>exog</code> argument.</p> <p>To learn more about exogenous variables in skforecast visit the exogenous variables user guide.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#scikit-learn-transformers-in-multivariate","title":"Scikit-learn transformers in MultiVariate\u00b6","text":"<p>By default, the <code>ForecasterDirectMultiVariate</code> class uses the scikit-learn <code>StandardScaler</code> transformer to scale the data. This transformer is applied to all series. However, it is possible to use different transformers for each series or not to apply any transformation at all:</p> <ul> <li><p>If <code>transformer_series</code> is a <code>transformer</code> the same transformation will be applied to all series.</p> </li> <li><p>If <code>transformer_series</code> is a <code>dict</code> a different transformation can be set for each series. Series not present in the dict will not have any transformation applied to them (check warning message).</p> </li> </ul> <p>Learn more about using scikit-learn transformers with skforecast.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#weights-in-multivariate","title":"Weights in MultiVariate\u00b6","text":"<p>The weights are used to control the influence that each observation has on the training of the model.</p> <p>Learn more about weighted time series forecasting with skforecast.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>The functions <code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code>, and <code>bayesian_search_forecaster_multiseries</code> support the evaluation of multiple metrics by passing a <code>list</code> of metric functions. This list can include both built-in metrics (e.g. <code>mean_squared_error</code>, <code>mean_absolute_error</code>) and custom-defined ones.</p> <p>When multiple metrics are provided, the first metric in the list is used to select the best model.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Since <code>ForecasterDirectMultiVariate</code> fits one model per step, it is necessary to specify from which model retrieves its feature importances.</p>"},{"location":"user_guides/dependent-multi-series-multivariate-forecasting.html#training-and-prediction-matrices","title":"Training and prediction matrices\u00b6","text":"<p>While the primary goal of building forecasting models is to predict future values, it is equally important to evaluate if the model is effectively learning from the training data. Analyzing predictions on the training data or exploring the prediction matrices is crucial for assessing model performance and understanding areas for optimization. This process can help identify issues like overfitting or underfitting, as well as provide deeper insights into the model\u2019s decision-making process. Check the How to Extract Training and Prediction Matrices user guide for more information.</p>"},{"location":"user_guides/direct-multi-step-forecasting.html","title":"Direct multi-step forecasting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.direct import ForecasterDirect\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from skforecast.datasets import fetch_dataset from skforecast.preprocessing import RollingFeatures from skforecast.direct import ForecasterDirect from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data['y']\ndata = data.sort_index()\n\n# Split train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data['y'] data = data.sort_index()  # Split train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:]  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.legend(); <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> <p> \u270e Note </p> <p><code>ForecasterDirect</code> includes the <code>n_jobs</code> parameter, allowing multi-process parallelization. This allows to train regressors for all steps simultaneously.</p> <p>The benefits of parallelization depend on several factors, including the regressor used, the number of fits to be performed, and the volume of data involved. When the <code>n_jobs</code> parameter is set to <code>'auto'</code>, the level of parallelization is automatically selected based on heuristic rules that aim to choose the best option for each scenario.</p> <p>For a more detailed look at parallelization, visit Parallelization in skforecast.</p> In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterDirect(\n                 regressor       = Ridge(),\n                 steps           = 36,\n                 lags            = 15,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=10)\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterDirect(                  regressor       = Ridge(),                  steps           = 36,                  lags            = 15,                  window_features = RollingFeatures(stats=['mean'], window_sizes=10)              )  forecaster.fit(y=data_train) forecaster Out[3]: ForecasterDirect General Information <ul> <li>Regressor: Ridge</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]</li> <li>Window features: ['roll_mean_10']</li> <li>Window size: 15</li> <li>Maximum steps to predict: 36</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:36:13</li> <li>Last fit date: 2025-08-06 13:36:14</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': None, 'solver': 'auto', 'tol': 0.0001}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\n# Predict only a subset of steps\npredictions = forecaster.predict(steps=[1, 5])\ndisplay(predictions)\n</pre> # Predict # ============================================================================== # Predict only a subset of steps predictions = forecaster.predict(steps=[1, 5]) display(predictions) <pre>2005-07-01    0.952188\n2005-11-01    1.180630\nName: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Predict all steps defined in the initialization.\npredictions = forecaster.predict()\ndisplay(predictions.head(3))\n</pre> # Predict all steps defined in the initialization. predictions = forecaster.predict() display(predictions.head(3)) <pre>2005-07-01    0.952188\n2005-08-01    1.004327\n2005-09-01    1.115190\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== predictions = forecaster.predict(steps=36) error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             ) print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.008447518677148016\n</pre> In\u00a0[8]: Copied! <pre>forecaster.get_feature_importances(step=1)\n</pre> forecaster.get_feature_importances(step=1) Out[8]: feature importance 11 lag_12 0.551517 10 lag_11 0.153757 0 lag_1 0.138408 12 lag_13 0.056914 1 lag_2 0.050145 2 lag_3 0.043166 9 lag_10 0.019264 15 roll_mean_10 0.016897 8 lag_9 0.010420 7 lag_8 -0.014137 5 lag_6 -0.014835 4 lag_5 -0.019494 3 lag_4 -0.021308 6 lag_7 -0.022662 14 lag_15 -0.035939 13 lag_14 -0.071710"},{"location":"user_guides/direct-multi-step-forecasting.html#direct-multi-step-forecaster","title":"Direct multi-step forecaster\u00b6","text":"<p>This strategy, commonly known as direct multistep forecasting, is computationally more expensive than the recursive since it requires training several models. However, in some scenarios, it achieves better results. This type of model can be obtained with the <code>ForecasterDirect</code> class and can also include one or multiple exogenous variables.</p> <p>Direct multi-step forecasting is a time series forecasting strategy in which a separate model is trained to predict each step in the forecast horizon. This is in contrast to recursive multi-step forecasting, where a single model is used to make predictions for all future time steps by recursively using its own output as input.</p> <p>Direct multi-step forecasting can be more computationally expensive than recursive forecasting since it requires training multiple models. However, it can often achieve better accuracy in certain scenarios, particularly when there are complex patterns and dependencies in the data that are difficult to capture with a single model.</p> <p>This approach can be performed using the <code>ForecasterDirect</code> class, which can also incorporate one or multiple exogenous variables to improve the accuracy of the forecasts.</p> <p> Diagram of direct multi-step forecasting. </p> <p> Direct forecasting </p> <p>To train a <code>ForecasterDirect</code> a different training matrix is created for each model.</p> <p> Transformation of a time series into matrices to train a direct multi-step forecasting model. </p>"},{"location":"user_guides/direct-multi-step-forecasting.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/direct-multi-step-forecasting.html#prediction","title":"Prediction\u00b6","text":"<p>When predicting, the value of <code>steps</code> must be less than or equal to the value of steps defined when initializing the forecaster. Starts at 1.</p> <ul> <li><p>If <code>int</code> only steps within the range of 1 to int are predicted.</p> </li> <li><p>If <code>list</code> of <code>int</code>. Only the steps contained in the list are predicted.</p> </li> <li><p>If <code>None</code> as many steps are predicted as were defined at initialization.</p> </li> </ul>"},{"location":"user_guides/direct-multi-step-forecasting.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Since <code>ForecasterDirect</code> fits one model per step, it is necessary to specify from which model retrieves its feature importances.</p>"},{"location":"user_guides/direct-multi-step-forecasting.html#training-and-prediction-matrices","title":"Training and prediction matrices\u00b6","text":"<p>While the primary goal of building forecasting models is to predict future values, it is equally important to evaluate if the model is effectively learning from the training data. Analyzing predictions on the training data or exploring the prediction matrices is crucial for assessing model performance and understanding areas for optimization. This process can help identify issues like overfitting or underfitting, as well as provide deeper insights into the model\u2019s decision-making process. Check the How to Extract Training and Prediction Matrices user guide for more information.</p>"},{"location":"user_guides/exogenous-variables.html","title":"Exogenous variables","text":"<p> Time series transformation including an exogenous variable. </p> <p> \u26a0 Warning </p> <p>When exogenous variables are included in a forecasting model, it is assumed that all exogenous inputs are known in the future. Do not include exogenous variables as predictors if their future value will not be known when making predictions.</p> <p> \u270e Note </p> <p>For a detailed guide on how to include categorical exogenous variables, please visit Categorical Features.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom feature_engine.timeseries.forecasting import WindowFeatures\nfrom feature_engine.timeseries.forecasting import LagFeatures\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error from sklearn.pipeline import make_pipeline from feature_engine.timeseries.forecasting import WindowFeatures from feature_engine.timeseries.forecasting import LagFeatures from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name='h2o_exog', raw=False)\ndata.index.name = 'datetime'\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3.5))\ndata.plot(ax=ax)\nplt.show()\n</pre> # Download data # ============================================================================== data = fetch_dataset(name='h2o_exog', raw=False) data.index.name = 'datetime'  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3.5)) data.plot(ax=ax) plt.show() <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[3]: Copied! <pre># Split data in train and test\n# ==============================================================================\nsteps = 36\ndata_train = data.iloc[:-steps, :]\ndata_test  = data.iloc[-steps:, :]\n</pre> # Split data in train and test # ============================================================================== steps = 36 data_train = data.iloc[:-steps, :] data_test  = data.iloc[-steps:, :] In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15\n             )\nforecaster.fit(\n    y    = data_train['y'],\n    exog = data_train[['exog_1', 'exog_2']]\n)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15              ) forecaster.fit(     y    = data_train['y'],     exog = data_train[['exog_1', 'exog_2']] ) forecaster Out[4]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]</li> <li>Window features: None</li> <li>Window size: 15</li> <li>Series name: y</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:39:38</li> <li>Last fit date: 2025-08-06 13:39:40</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     exog_1, exog_2                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(\n                  steps = 36,\n                  exog  = data_test[['exog_1', 'exog_2']]\n              )\n\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(                   steps = 36,                   exog  = data_test[['exog_1', 'exog_2']]               )  predictions.head(3) Out[5]: <pre>2005-07-01    1.023969\n2005-08-01    1.044023\n2005-09-01    1.110078\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3.5))\ndata_train['y'].plot(ax=ax, label='train')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend()\nplt.show()\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3.5)) data_train['y'].plot(ax=ax, label='train') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend() plt.show() In\u00a0[7]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (MSE): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (MSE): {error_mse}\") <pre>Test error (MSE): 0.005576949968874203\n</pre> In\u00a0[8]: Copied! <pre># Feature importances with exogenous variables\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances with exogenous variables # ============================================================================== forecaster.get_feature_importances() Out[8]: feature importance 11 lag_12 66 15 exog_1 49 16 exog_2 37 10 lag_11 36 5 lag_6 31 13 lag_14 26 4 lag_5 26 2 lag_3 25 14 lag_15 24 12 lag_13 23 3 lag_4 23 1 lag_2 22 9 lag_10 18 0 lag_1 16 7 lag_8 16 6 lag_7 15 8 lag_9 12 In\u00a0[9]: Copied! <pre># Window required by the Forecaster to create predictors\n# ==============================================================================\nwindow_size = forecaster.window_size\nprint(\"Window size required by the Forecaster:\", window_size)\n</pre> # Window required by the Forecaster to create predictors # ============================================================================== window_size = forecaster.window_size print(\"Window size required by the Forecaster:\", window_size) <pre>Window size required by the Forecaster: 15\n</pre> <p>A exogenous variable which skips the first <code>window_size</code> observations of the time series is simulated.</p> In\u00a0[10]: Copied! <pre># Simulate data\n# ==============================================================================\nexog_no_first_window_size = data_train[['exog_1', 'exog_2']].copy()\nexog_no_first_window_size = exog_no_first_window_size.iloc[window_size:, :]\n\n# Plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3.5))\ndata_train[['y']].plot(ax=ax)\nexog_no_first_window_size.plot(ax=ax)\nplt.show()\n</pre> # Simulate data # ============================================================================== exog_no_first_window_size = data_train[['exog_1', 'exog_2']].copy() exog_no_first_window_size = exog_no_first_window_size.iloc[window_size:, :]  # Plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3.5)) data_train[['y']].plot(ax=ax) exog_no_first_window_size.plot(ax=ax) plt.show() In\u00a0[11]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15\n             )\n\nforecaster.fit(\n    y    = data_train['y'],\n    exog = exog_no_first_window_size[['exog_1', 'exog_2']]\n)\n\n# Predict\n# ==============================================================================\npredictions = forecaster.predict(\n                  steps = 36,\n                  exog  = data_test[['exog_1', 'exog_2']]\n              )\npredictions.head(3)\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15              )  forecaster.fit(     y    = data_train['y'],     exog = exog_no_first_window_size[['exog_1', 'exog_2']] )  # Predict # ============================================================================== predictions = forecaster.predict(                   steps = 36,                   exog  = data_test[['exog_1', 'exog_2']]               ) predictions.head(3) Out[11]: <pre>2005-07-01    1.023969\n2005-08-01    1.044023\n2005-09-01    1.110078\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (MSE): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (MSE): {error_mse}\") <pre>Test error (MSE): 0.005576949968874203\n</pre> <p>Since the training matrices are the same as those used with the full exogenous variables, the resulting model is the same and the predictions are identical.</p> In\u00a0[13]: Copied! <pre># Check training matrices are the same with both methods\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 15\n             )\n\nX_train_full_exog, y_train_full_exog = forecaster.create_train_X_y(\n    y    = data_train['y'],\n    exog = data_train[['exog_1', 'exog_2']]\n)\n\nX_train_no_full_exog, y_train_no_full_exog = forecaster.create_train_X_y(\n    y    = data_train['y'],\n    exog = exog_no_first_window_size[['exog_1', 'exog_2']]\n)\n\npd.testing.assert_frame_equal(X_train_full_exog, X_train_no_full_exog)\npd.testing.assert_series_equal(y_train_full_exog, y_train_no_full_exog)\n</pre> # Check training matrices are the same with both methods # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 15              )  X_train_full_exog, y_train_full_exog = forecaster.create_train_X_y(     y    = data_train['y'],     exog = data_train[['exog_1', 'exog_2']] )  X_train_no_full_exog, y_train_no_full_exog = forecaster.create_train_X_y(     y    = data_train['y'],     exog = exog_no_first_window_size[['exog_1', 'exog_2']] )  pd.testing.assert_frame_equal(X_train_full_exog, X_train_no_full_exog) pd.testing.assert_series_equal(y_train_full_exog, y_train_no_full_exog) <p> \u26a0 Warning </p> <p>   This section focuses on lagged values and window features derived from past values of the exogenous variables. These features are different from the window features derived from series being forecasted. See the    Window and custom features for more information on the latter. </p> In\u00a0[14]: Copied! <pre># Downloading data\n# ==============================================================================\ndata = fetch_dataset(name='bike_sharing', raw=False)\ndata = data.loc[:, ['users', 'holiday', 'temp', 'windspeed']]\ndata.head(3)\n</pre> # Downloading data # ============================================================================== data = fetch_dataset(name='bike_sharing', raw=False) data = data.loc[:, ['users', 'holiday', 'temp', 'windspeed']] data.head(3) <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 11)\n</pre> Out[14]: users holiday temp windspeed date_time 2011-01-01 00:00:00 16.0 0.0 9.84 0.0 2011-01-01 01:00:00 40.0 0.0 9.02 0.0 2011-01-01 02:00:00 32.0 0.0 9.02 0.0 <p>Combining the <code>LagFeatures</code> and <code>WindowFeatures</code> from feature-engine library, it is straightforward to create lagged values and window features from exogenous variables. In this example, the last 3 lagged values aswell as the mean and standard deviation of the last 24 values are extracted from the exogenous variable <code>temp</code> and <code>windspeed</code>.</p> In\u00a0[15]: Copied! <pre># Create lagged features and rolling windows features from exogenous variables\n# ==============================================================================\nlag_transformer = LagFeatures(\n                    variables = [\"temp\", \"windspeed\"],\n                    periods   = [1, 2, 3],\n                  )\n\nwf_transformer = WindowFeatures(\n                    variables      = [\"temp\", \"windspeed\"],\n                    window         = [\"24h\"],\n                    functions      = [\"mean\"],\n                    freq           = \"h\",\n                    missing_values = \"ignore\",\n                    drop_na        = False,\n                )\n\nexog_transformer = make_pipeline(\n                        wf_transformer,\n                        lag_transformer\n                   )\n\nexog_transformer\n</pre> # Create lagged features and rolling windows features from exogenous variables # ============================================================================== lag_transformer = LagFeatures(                     variables = [\"temp\", \"windspeed\"],                     periods   = [1, 2, 3],                   )  wf_transformer = WindowFeatures(                     variables      = [\"temp\", \"windspeed\"],                     window         = [\"24h\"],                     functions      = [\"mean\"],                     freq           = \"h\",                     missing_values = \"ignore\",                     drop_na        = False,                 )  exog_transformer = make_pipeline(                         wf_transformer,                         lag_transformer                    )  exog_transformer Out[15]: <pre>Pipeline(steps=[('windowfeatures',\n                 WindowFeatures(freq='h', functions=['mean'],\n                                missing_values='ignore',\n                                variables=['temp', 'windspeed'],\n                                window=['24h'])),\n                ('lagfeatures',\n                 LagFeatures(periods=[1, 2, 3],\n                             variables=['temp', 'windspeed']))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted Parameters steps\u00a0 [('windowfeatures', ...), ('lagfeatures', ...)] transform_input\u00a0 None memory\u00a0 None verbose\u00a0 False WindowFeatures Parameters variables\u00a0 ['temp', 'windspeed'] window\u00a0 ['24h'] min_periods\u00a0 None functions\u00a0 ['mean'] periods\u00a0 1 freq\u00a0 'h' sort_index\u00a0 True missing_values\u00a0 'ignore' drop_original\u00a0 False drop_na\u00a0 False LagFeatures Parameters variables\u00a0 ['temp', 'windspeed'] periods\u00a0 [1, 2, ...] freq\u00a0 None fill_value\u00a0 None sort_index\u00a0 True missing_values\u00a0 'raise' drop_original\u00a0 False drop_na\u00a0 False In\u00a0[16]: Copied! <pre>data = exog_transformer.fit_transform(data)\ndata.head(5)\n</pre> data = exog_transformer.fit_transform(data) data.head(5) Out[16]: users holiday temp windspeed temp_window_24h_mean windspeed_window_24h_mean temp_lag_1 windspeed_lag_1 temp_lag_2 windspeed_lag_2 temp_lag_3 windspeed_lag_3 date_time 2011-01-01 00:00:00 16.0 0.0 9.84 0.0 NaN NaN NaN NaN NaN NaN NaN NaN 2011-01-01 01:00:00 40.0 0.0 9.02 0.0 9.840000 0.0 9.84 0.0 NaN NaN NaN NaN 2011-01-01 02:00:00 32.0 0.0 9.02 0.0 9.430000 0.0 9.02 0.0 9.84 0.0 NaN NaN 2011-01-01 03:00:00 13.0 0.0 9.84 0.0 9.293333 0.0 9.02 0.0 9.02 0.0 9.84 0.0 2011-01-01 04:00:00 1.0 0.0 9.84 0.0 9.430000 0.0 9.84 0.0 9.02 0.0 9.02 0.0"},{"location":"user_guides/exogenous-variables.html#exogenous-variables-features","title":"Exogenous variables (features)\u00b6","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process. The inclusion of exogenous variables can enhance the accuracy of forecasts.</p> <p>In skforecast, exogenous variables can be easily included as predictors in all forecasting models. To ensure that their effects are accurately accounted for, it is crucial to include these variables during both the training and prediction phases. This will help to optimize the accuracy of forecasts and provide more reliable predictions.</p>"},{"location":"user_guides/exogenous-variables.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#train-forecaster-with-exogenous-variables","title":"Train forecaster with exogenous variables\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#prediction","title":"Prediction\u00b6","text":"<p>If the <code>Forecaster</code> has been trained using exogenous variables, they should be provided during the prediction phase.</p>"},{"location":"user_guides/exogenous-variables.html#feature-importances","title":"Feature importances\u00b6","text":"<p>If exogenous variables are included as predictors, they have a value of feature importances.</p>"},{"location":"user_guides/exogenous-variables.html#handling-missing-exogenous-data-in-initial-training-periods","title":"Handling missing exogenous data in initial training periods\u00b6","text":"<p>When working with time series models that incorporate exogenous variables, it\u2019s common to encounter cases where exogenous data isn't available for the very first part of the historical dataset. This can raise concerns, especially since these initial observations are essential for creating predictors and training matrices. However, full alignment between the exogenous variables and the time series data is only necessary after this initial window period.</p> <p>In practical terms, this means that if you have missing exogenous values in the early part of your data, they won't prevent model training as long as your exogenous variables are aligned from the point where predictors are created (after the first <code>window_size</code> observations).</p>"},{"location":"user_guides/exogenous-variables.html#lagged-values-and-window-features-from-exogenous-variables","title":"Lagged values and window features from exogenous variables\u00b6","text":""},{"location":"user_guides/exogenous-variables.html#backtesting-with-exogenous-variables","title":"Backtesting with exogenous variables\u00b6","text":"<p>All the backtesting strategies available in skforecast can also be applied when incorporating exogenous variables in the forecasting model. Visit the Backtesting section for more information.</p>"},{"location":"user_guides/explainability.html","title":"Model Explainability","text":"<p>Machine learning explainability, also known as interpretability, refers to the ability to understand, interpret, and explain the decisions or predictions made by machine learning models in a human-understandable way. It aims to shed light on how a model arrives at a particular result or decision.</p> <p>Due to the complex nature of many modern machine learning models, such as ensemble methods, they often function as black boxes, making it difficult to understand why a particular prediction was made. Explainability techniques aim to demystify these models, providing insight into their inner workings and helping to build trust, improve transparency, and meet regulatory requirements in various domains. Enhancing model explainability not only aids in understanding model behavior but also helps detect biases, improve model performance, and enables stakeholders to make more informed decisions based on machine learning insights.</p> <p>skforecast is compatible with some of the most used interpretability methods: Shap values, Permutation importance, Partial Dependency Plots and Model-specific methods.</p> <p> \ud83d\udca1 Tip </p> <p>To learn more about explainability, visit: Interpretable forecasting models</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport shap\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt import shap from sklearn.inspection import permutation_importance from sklearn.inspection import PartialDependenceDisplay from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name=\"vic_electricity\")\ndata.head(3)\n</pre> # Download data # ============================================================================== data = fetch_dataset(name=\"vic_electricity\") data.head(3) <pre>vic_electricity\n---------------\nHalf-hourly electricity demand for Victoria, Australia\nO'Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022).tsibbledata: Diverse\nDatasets for 'tsibble'. https://tsibbledata.tidyverts.org/,\nhttps://github.com/tidyverts/tsibbledata/.\nhttps://tsibbledata.tidyverts.org/reference/vic_elec.html\nShape of the dataset: (52608, 4)\n</pre> Out[2]: Demand Temperature Date Holiday Time 2011-12-31 13:00:00 4382.825174 21.40 2012-01-01 True 2011-12-31 13:30:00 4263.365526 21.05 2012-01-01 True 2011-12-31 14:00:00 4048.966046 20.70 2012-01-01 True In\u00a0[3]: Copied! <pre># Aggregation to daily frequency\n# ==============================================================================\ndata = data.resample('D').agg({'Demand': 'sum', 'Temperature': 'mean'})\ndata.head(3)\n</pre> # Aggregation to daily frequency # ============================================================================== data = data.resample('D').agg({'Demand': 'sum', 'Temperature': 'mean'}) data.head(3) Out[3]: Demand Temperature Time 2011-12-31 82531.745918 21.047727 2012-01-01 227778.257304 26.578125 2012-01-02 275490.988882 31.751042 In\u00a0[4]: Copied! <pre># Split train-test\n# ==============================================================================\ndata_train = data.loc[: '2014-12-21']\ndata_test = data.loc['2014-12-22':]\n</pre> # Split train-test # ============================================================================== data_train = data.loc[: '2014-12-21'] data_test = data.loc['2014-12-22':] <p>A forecasting model is created to predict the energy demand using the past 7 values (last week) and the temperature as an exogenous variable.</p> In\u00a0[5]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterRecursive)\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 7\n             )\n\nforecaster.fit(\n    y    = data_train['Demand'],\n    exog = data_train['Temperature']\n)\nforecaster\n</pre> # Create a recursive multi-step forecaster (ForecasterRecursive) # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 7              )  forecaster.fit(     y    = data_train['Demand'],     exog = data_train['Temperature'] ) forecaster Out[5]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3 4 5 6 7]</li> <li>Window features: None</li> <li>Window size: 7</li> <li>Series name: Demand</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:42:00</li> <li>Last fit date: 2025-08-06 13:42:03</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     Temperature                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('2011-12-31 00:00:00'), Timestamp('2014-12-21 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>Feature importance is a technique used in machine learning to determine the relevance or importance of each feature (or variable) in a model's prediction. In other words, it measures how much each feature contributes to the model's output.</p> <p>Feature importance can be used for several purposes, such as identifying the most relevant features for a given prediction, understanding the behavior of a model, and selecting the best set of features for a given task. It can also help to identify potential biases or errors in the data used to train the model. It is important to note that feature importance is not a definitive measure of causality. Just because a feature is identified as important does not necessarily mean that it causes the outcome. Other factors, such as confounding variables, may also be at play.</p> <p>The method used to calculate feature importance may vary depending on the type of machine learning model being used. Different machine learning models may have different assumptions and characteristics that affect the calculation of feature importance. For example, decision tree-based models such as Random Forest and Gradient Boosting typically use mean decrease impurity or permutation feature importance methods to calculate feature importance.</p> <p>Linear regression models typically use coefficients or standardized coefficients to determine the importance of a feature. The magnitude of the coefficient reflects the strength and direction of the relationship between the feature and the target variable.</p> <p>The importance of the predictors included in a forecaster can be obtained using the method <code>get_feature_importances()</code>. This method accesses the <code>coef_</code> and <code>feature_importances_</code> attributes of the internal regressor.</p> <p> \u26a0 Warning </p> <p>The <code>get_feature_importances()</code> method will only return values if the forecaster's regressor has either the <code>coef_</code> or <code>feature_importances_</code> attribute, which is the default in scikit-learn. If your regressor does not follow this naming convention, please consider opening an issue on GitHub and we will strive to include it in future updates.</p> In\u00a0[6]: Copied! <pre># Predictors importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Predictors importances # ============================================================================== forecaster.get_feature_importances() Out[6]: feature importance 7 Temperature 570 0 lag_1 470 2 lag_3 387 1 lag_2 362 6 lag_7 325 5 lag_6 313 4 lag_5 298 3 lag_4 275 <p>To properly retrieve the feature importances in the <code>ForecasterDirect</code> and <code>ForecasterDirectMultiVariate</code>, it is essential to specify the model from which to extract the feature importances are to be extracted. This is because Direct Strategy Forecasters fit one model per step, and each model may have different important features. Therefore, the user must explicitly specify which model's feature importances wish to extract to ensure that the correct features are used.</p> <p>SHAP (SHapley Additive exPlanations) values are a widely adopted method for explaining machine learning models. They provide both visual and quantitative insights into how features and their values impact the model. SHAP values serve two primary purposes:</p> <ul> <li><p>Global Interpretability: SHAP values help identify how each feature influenced the model during training. By averaging SHAP values across the dataset, one can rank features by their overall importance and gain insight into the model\u2019s decision-making process.</p> </li> <li><p>Local Interpretability: SHAP values also explain individual predictions by indicating how much each feature contributed to a specific output. This enables a breakdown of single predictions to understand the role each feature played in the outcome.</p> </li> </ul> <p>SHAP value explanations can be generated for skforecast models using two essential components:</p> <ul> <li><p>The internal regressor of the forecaster, accessible via <code>forecaster.regressor</code>.</p> </li> <li><p>The internal matrices used for fitting, backtesting, and predicting with the forecaster. These matrices are accessible through the methods <code>create_train_X_y()</code> and <code>create_predict_X()</code>, and by setting the argument <code>return_predictors = True</code> in the <code>backtesting_forecaster()</code> function.</p> </li> </ul> <p>By leveraging these elements, users can produce clear and interpretable explanations for their forecasting models. These explanations can be used to assess model reliability, identify the most influential features, and better understand the relationships between input variables and the target variable.</p> <p>Averaging the shap values across the data set used to train the model, it is possible to obtain an estimation of the contribution (magnitude and direction) of each feature in the model. The higher the absolute value of the SHAP value, the more important the feature is for the model. The sign of the SHAP value indicates whether the feature has a positive or negative impact on the prediction.</p> <p>First, the training matrices used to fit the model are created with the method <code>create_train_X_y()</code>.</p> In\u00a0[7]: Copied! <pre># Training matrices used by the forecaster to fit the internal regressor\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data_train['Demand'],\n                       exog = data_train['Temperature']\n                   )\n\ndisplay(X_train.head(3))  # Features\ndisplay(y_train.head(3))  # Target\n</pre> # Training matrices used by the forecaster to fit the internal regressor # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data_train['Demand'],                        exog = data_train['Temperature']                    )  display(X_train.head(3))  # Features display(y_train.head(3))  # Target lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 Temperature Time 2012-01-07 205338.714620 211066.426550 213792.376946 258955.329422 275490.988882 227778.257304 82531.745918 24.098958 2012-01-08 200693.270298 205338.714620 211066.426550 213792.376946 258955.329422 275490.988882 227778.257304 20.223958 2012-01-09 200061.614738 200693.270298 205338.714620 211066.426550 213792.376946 258955.329422 275490.988882 19.161458 <pre>Time\n2012-01-07    200693.270298\n2012-01-08    200061.614738\n2012-01-09    216201.836844\nFreq: D, Name: y, dtype: float64</pre> <p>Then, the SHAP values are calculated using the <code>shap</code> library. The <code>shap_values()</code> method is used to calculate the SHAP values for the training data. If the data set is large, it is recommended to use only a random sample.</p> In\u00a0[8]: Copied! <pre># Create SHAP explainer (for three base models)\n# ==============================================================================\nexplainer = shap.TreeExplainer(forecaster.regressor)\n\n# Sample 50% of the data to speed up the calculation\nrng = np.random.default_rng(seed=785412)\nsample = rng.choice(X_train.index, size=int(len(X_train) * 0.5), replace=False)\nX_train_sample = X_train.loc[sample, :]\nshap_values = explainer.shap_values(X_train_sample)\n</pre> # Create SHAP explainer (for three base models) # ============================================================================== explainer = shap.TreeExplainer(forecaster.regressor)  # Sample 50% of the data to speed up the calculation rng = np.random.default_rng(seed=785412) sample = rng.choice(X_train.index, size=int(len(X_train) * 0.5), replace=False) X_train_sample = X_train.loc[sample, :] shap_values = explainer.shap_values(X_train_sample) <p>Once the SHAP values are calculated, several plots can be generated to visualize the results.</p> <p>The SHAP summary plot typically displays the feature importance or contribution of each feature to the model's output across multiple data points. It shows how much each feature contributes to pushing the model's prediction away from a base value (often the model's average prediction). By examining a SHAP summary plot, one can gain insights into which features have the most significant impact on predictions, whether they positively or negatively influence the outcome, and how different feature values contribute to specific predictions.</p> In\u00a0[9]: Copied! <pre># Shap summary plot (top 10)\n# ==============================================================================\nshap.initjs()\nshap.summary_plot(shap_values, X_train_sample, max_display=10, show=False)\nfig, ax = plt.gcf(), plt.gca()\nax.set_title(\"SHAP Summary plot\")\nax.tick_params(labelsize=8)\nfig.set_size_inches(6, 3)\n</pre> # Shap summary plot (top 10) # ============================================================================== shap.initjs() shap.summary_plot(shap_values, X_train_sample, max_display=10, show=False) fig, ax = plt.gcf(), plt.gca() ax.set_title(\"SHAP Summary plot\") ax.tick_params(labelsize=8) fig.set_size_inches(6, 3) In\u00a0[10]: Copied! <pre>shap.summary_plot(shap_values, X_train_sample, plot_type=\"bar\", plot_size=(6, 3))\n</pre> shap.summary_plot(shap_values, X_train_sample, plot_type=\"bar\", plot_size=(6, 3)) <p>SHAP dependence plots are visualizations used to understand the relationship between a feature and the model output by displaying how the value of a single feature affects predictions made by the model while considering interactions with other features. These plots are particularly useful for examining how a certain feature impacts the model's predictions across its range of values while considering interactions with other variables.</p> In\u00a0[11]: Copied! <pre># Dependence plot for Temperature\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 4))\nshap.dependence_plot(\"Temperature\", shap_values, X_train_sample, ax=ax)\n</pre> # Dependence plot for Temperature # ============================================================================== fig, ax = plt.subplots(figsize=(7, 4)) shap.dependence_plot(\"Temperature\", shap_values, X_train_sample, ax=ax) <p>SHAP values not only allow for interpreting the general behavior of the model (Global Interpretability) but also serve as a powerful tool for analyzing individual predictions (Local Interpretability). This is especially useful when trying to understand why a model made a specific prediction for a given instance.</p> <p>To carry out this analysis, it is necessary to access the predictor values \u2014 lags and exogenous variables \u2014 at the time of the prediction. This can be achieved by using the <code>create_predict_X</code> method or by enabling the <code>return_predictors = True</code> argument in the <code>backtesting_forecaster</code> function.</p> <p>Suppose the forecaster is employed to predict the next 10 values of the series, and a specific prediction corresponding to the date '2014-12-28' requires explanation.</p> In\u00a0[12]: Copied! <pre># Forecasting next 10 days\n# ==============================================================================\nset_dark_theme()\n\npredictions = forecaster.predict(steps=10, exog=data_test['Temperature'])\n\nfig, ax = plt.subplots(figsize=(6, 2.5))\ndata_test['Demand'].plot(ax=ax, label='Test')\npredictions.plot(ax=ax, label='Predictions', linestyle='--')\nax.set_xlabel(None)\nax.legend();\n</pre> # Forecasting next 10 days # ============================================================================== set_dark_theme()  predictions = forecaster.predict(steps=10, exog=data_test['Temperature'])  fig, ax = plt.subplots(figsize=(6, 2.5)) data_test['Demand'].plot(ax=ax, label='Test') predictions.plot(ax=ax, label='Predictions', linestyle='--') ax.set_xlabel(None) ax.legend(); <p>The method <code>create_predict_X</code> is used to create the input matrix used internally by the forecaster's <code>predict</code> method. This matrix is then used to generate SHAP values for the forecasted values.</p> In\u00a0[13]: Copied! <pre># Create input matrix used to forecast the next 10 steps\n# ==============================================================================\nX_predict = forecaster.create_predict_X(steps=10, exog=data_test['Temperature'])\nX_predict.head(3)\n</pre> # Create input matrix used to forecast the next 10 steps # ============================================================================== X_predict = forecaster.create_predict_X(steps=10, exog=data_test['Temperature']) X_predict.head(3) Out[13]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 Temperature 2014-12-22 216483.631690 186486.896670 197129.766534 214934.022460 215507.677076 226093.767670 231923.044018 22.950000 2014-12-23 241514.532543 216483.631690 186486.896670 197129.766534 214934.022460 215507.677076 226093.767670 18.829167 2014-12-24 226165.936559 241514.532543 216483.631690 186486.896670 197129.766534 214934.022460 215507.677076 18.312500 <p> \u26a0 Warning </p> <p>If transformations or differentiation are included in the Forecaster, the output matrix will be in the transformed scale. To obtain the SHAP values in the original scale, it is necessary to reverse the transformations or differentiation. For more information, visit: Extract training and prediction matrices.</p> In\u00a0[14]: Copied! <pre># SHAP values for the predictions\n# ==============================================================================\nshap_values = explainer.shap_values(X_predict)\n</pre> # SHAP values for the predictions # ============================================================================== shap_values = explainer.shap_values(X_predict) In\u00a0[15]: Copied! <pre># Waterfall plot for a single prediction\n# ==============================================================================\npredicted_date = '2014-12-28'\niloc_predicted_date = X_predict.index.get_loc(predicted_date)\n\nshap_values_single = explainer(X_predict)\nshap.plots.waterfall(shap_values_single[iloc_predicted_date], show=False)\n\nfig, ax = plt.gcf(), plt.gca()\nfig.set_size_inches(6, 3.5)\nax_list = fig.axes\nax = ax_list[0]\nax.tick_params(labelsize=10)\nax.set_title(\"Waterfall plot for a single prediction\")\nplt.show()\n</pre> # Waterfall plot for a single prediction # ============================================================================== predicted_date = '2014-12-28' iloc_predicted_date = X_predict.index.get_loc(predicted_date)  shap_values_single = explainer(X_predict) shap.plots.waterfall(shap_values_single[iloc_predicted_date], show=False)  fig, ax = plt.gcf(), plt.gca() fig.set_size_inches(6, 3.5) ax_list = fig.axes ax = ax_list[0] ax.tick_params(labelsize=10) ax.set_title(\"Waterfall plot for a single prediction\") plt.show() <p>The waterfall plot illustrates how different features pushed the model\u2019s output higher (shown in red) or lower (shown in blue), relative to the average model prediction.</p> <ul> <li><p><code>lag_1</code> had the largest negative impact, reducing the prediction by over 16000 units.</p> </li> <li><p><code>Temperature</code> contributed positively, increasing the prediction by around 7,214 units.</p> </li> </ul> <p>The model prediction (f(x)) was 222766.341, while the expected value (E[f(x)]) across all predictions is 224,215.914. This means the specific inputs for this prediction led the model to forecast a value lower than average, largely due to the strong negative impact of <code>lag_1</code>.</p> <p>Same insights can be obtained using the <code>shap.force_plot method</code>.</p> In\u00a0[16]: Copied! <pre># Forceplot for a single prediction \n# ==============================================================================\nshap.force_plot(\n    base_value  = explainer.expected_value,\n    shap_values = shap_values_single.values[iloc_predicted_date],\n    features    = X_predict.iloc[iloc_predicted_date, :]\n)\n</pre> # Forceplot for a single prediction  # ============================================================================== shap.force_plot(     base_value  = explainer.expected_value,     shap_values = shap_values_single.values[iloc_predicted_date],     features    = X_predict.iloc[iloc_predicted_date, :] ) Out[16]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  In\u00a0[17]: Copied! <pre># Force plot for the 10 predictions\n# ==============================================================================\nshap.force_plot(\n    base_value  = explainer.expected_value,\n    shap_values = shap_values,\n    features    = X_predict\n)\n</pre> # Force plot for the 10 predictions # ============================================================================== shap.force_plot(     base_value  = explainer.expected_value,     shap_values = shap_values,     features    = X_predict ) Out[17]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>The analysis of individual predictions using SHAP values can be also apllied to predictions made in a backtesting process. For that, the <code>return_predictors=True</code> argument must be set in the <code>backtesting_forecaster</code> method. This will return a DataFrame with the predicted value ('pred'), the partition it belongs to ('fold'), and the value of the lags and exogenous variables used to make each prediction.</p> <p>In this scenario, a backtesting process is employed to train the model using data up to '2014-12-01 23:59:00'. The model then generates predictions in folds of 24 steps. SHAP values are subsequently computed for the forecast corresponding to the date '2014-12-16'.</p> In\u00a0[18]: Copied! <pre># Backtesting returning the predictors\n# ==============================================================================\ncv = TimeSeriesFold(steps= 24, initial_train_size = len(data.loc[:'2014-12-01 23:59:00']))\n_, predictions = backtesting_forecaster(\n                     forecaster        = forecaster,\n                     y                 = data['Demand'],\n                     exog              = data['Temperature'],\n                     cv                = cv,\n                     metric            = 'mean_absolute_error',\n                     return_predictors = True,\n                 )\npredictions.head(3)\n</pre> # Backtesting returning the predictors # ============================================================================== cv = TimeSeriesFold(steps= 24, initial_train_size = len(data.loc[:'2014-12-01 23:59:00'])) _, predictions = backtesting_forecaster(                      forecaster        = forecaster,                      y                 = data['Demand'],                      exog              = data['Temperature'],                      cv                = cv,                      metric            = 'mean_absolute_error',                      return_predictors = True,                  ) predictions.head(3) <pre>  0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> Out[18]: pred fold lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 Temperature 2014-12-02 231237.145266 0 237812.592388 234970.336660 189653.758108 202017.012448 214602.854760 218321.456402 214318.765210 19.833333 2014-12-03 227614.717135 0 231237.145266 237812.592388 234970.336660 189653.758108 202017.012448 214602.854760 218321.456402 19.616667 2014-12-04 229619.129116 0 227614.717135 231237.145266 237812.592388 234970.336660 189653.758108 202017.012448 214602.854760 21.702083 In\u00a0[19]: Copied! <pre># Waterfall for a single prediction generated during backtesting\n# ==============================================================================\npredictions = predictions.astype(data['Temperature'].dtypes)  # Ensure that the types are the same\niloc_predicted_date = predictions.index.get_loc('2014-12-16')\nshap_values_single = explainer(predictions.iloc[:, 2:])\nshap.plots.waterfall(shap_values_single[iloc_predicted_date], show=False)\n\nfig, ax = plt.gcf(), plt.gca()\nfig.set_size_inches(6, 3.5)\nax_list = fig.axes\nax = ax_list[0]\nax.tick_params(labelsize=8)\nax.set_title(\"Waterfall plot for a single backtesting prediction\")\nplt.show()\n</pre> # Waterfall for a single prediction generated during backtesting # ============================================================================== predictions = predictions.astype(data['Temperature'].dtypes)  # Ensure that the types are the same iloc_predicted_date = predictions.index.get_loc('2014-12-16') shap_values_single = explainer(predictions.iloc[:, 2:]) shap.plots.waterfall(shap_values_single[iloc_predicted_date], show=False)  fig, ax = plt.gcf(), plt.gca() fig.set_size_inches(6, 3.5) ax_list = fig.axes ax = ax_list[0] ax.tick_params(labelsize=8) ax.set_title(\"Waterfall plot for a single backtesting prediction\") plt.show() <p>Permutation feature importance is a model inspection technique that measures the contribution of each feature to the statistical performance of a fitted model on a given tabular dataset. This technique is particularly useful for non-linear or opaque estimators, and involves randomly shuffling the values of a single feature and observing the resulting degradation of the model's score. By breaking the relationship between the feature and the target variable, we determine how much the model relies on that particular feature.</p> In\u00a0[20]: Copied! <pre># Training matrices used by the forecaster to fit the internal regressor\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data_train['Demand'],\n                       exog = data_train['Temperature']\n                   )\n\n# Permutation importances\n# ==============================================================================\nr = permutation_importance(\n        estimator    = forecaster.regressor,\n        X            = X_train,\n        y            = y_train,\n        n_repeats    = 3,\n        max_samples  = 0.5,\n        random_state = 123\n    )\n\nimportances = pd.DataFrame({\n                  'feature': X_train.columns,\n                  'mean_importance': r.importances_mean,\n                  'std_importance': r.importances_std\n              }).sort_values('mean_importance', ascending=False)\nimportances\n</pre> # Training matrices used by the forecaster to fit the internal regressor # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data_train['Demand'],                        exog = data_train['Temperature']                    )  # Permutation importances # ============================================================================== r = permutation_importance(         estimator    = forecaster.regressor,         X            = X_train,         y            = y_train,         n_repeats    = 3,         max_samples  = 0.5,         random_state = 123     )  importances = pd.DataFrame({                   'feature': X_train.columns,                   'mean_importance': r.importances_mean,                   'std_importance': r.importances_std               }).sort_values('mean_importance', ascending=False) importances Out[20]: feature mean_importance std_importance 0 lag_1 0.617276 0.014583 7 Temperature 0.411240 0.014405 6 lag_7 0.196190 0.001865 1 lag_2 0.122398 0.007803 5 lag_6 0.083912 0.003637 2 lag_3 0.041294 0.002019 4 lag_5 0.030787 0.001079 3 lag_4 0.024816 0.001021 <p>Partial dependence plots (PDPs) are a useful tool for understanding the relationship between a feature and the target outcome in a machine learning model. In scikit-learn, you can create partial dependence plots using the <code>plot_partial_dependence</code> function. This function visualizes the effect of one or two features on the predicted outcome, while marginalizing the effect of all other features.</p> <p>The resulting plots show how changes in the selected feature(s) affect the predicted outcome while holding other features constant on average. Remember that these plots should be interpreted in the context of your model and data. They provide insight into the relationship between specific features and the model's predictions.</p> <p>A more detailed description of the Partial Dependency Plot can be found in Scikitlearn's User Guides.</p> In\u00a0[21]: Copied! <pre># Scikit-learn partial dependence plots\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(9, 4))\nax.set_title(\"Decision Tree\")\npd.plots = PartialDependenceDisplay.from_estimator(\n    estimator = forecaster.regressor,\n    X         = X_train,\n    features  = [\"Temperature\", \"lag_1\"],\n    kind      = 'both',\n    ax        = ax,\n)\nax.set_title(\"Partial Dependence Plot\")\nfig.tight_layout()\nplt.show()\n</pre> # Scikit-learn partial dependence plots # ============================================================================== fig, ax = plt.subplots(figsize=(9, 4)) ax.set_title(\"Decision Tree\") pd.plots = PartialDependenceDisplay.from_estimator(     estimator = forecaster.regressor,     X         = X_train,     features  = [\"Temperature\", \"lag_1\"],     kind      = 'both',     ax        = ax, ) ax.set_title(\"Partial Dependence Plot\") fig.tight_layout() plt.show()"},{"location":"user_guides/explainability.html#forecaster-explainability-feature-importance-shap-values-and-partial-dependence-plots","title":"Forecaster explainability: Feature importance, SHAP Values and Partial Dependence Plots\u00b6","text":""},{"location":"user_guides/explainability.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/explainability.html#forecasting-model","title":"Forecasting model\u00b6","text":""},{"location":"user_guides/explainability.html#model-specific-feature-importances","title":"Model-specific feature importances\u00b6","text":""},{"location":"user_guides/explainability.html#shap-explanations-for-skforecast-models","title":"SHAP explanations for skforecast models\u00b6","text":""},{"location":"user_guides/explainability.html#shap-feature-importance-in-the-overall-model","title":"SHAP feature importance in the overall model\u00b6","text":""},{"location":"user_guides/explainability.html#shap-summary-plot","title":"SHAP Summary Plot\u00b6","text":""},{"location":"user_guides/explainability.html#shap-dependence-plots","title":"SHAP Dependence Plots\u00b6","text":""},{"location":"user_guides/explainability.html#shap-explanations-for-individual-predictions","title":"SHAP Explanations for Individual Predictions\u00b6","text":""},{"location":"user_guides/explainability.html#shap-values-of-predict-output","title":"SHAP values of predict output\u00b6","text":""},{"location":"user_guides/explainability.html#shap-values-of-backtesting_forecaster-output","title":"SHAP values of backtesting_forecaster() output\u00b6","text":""},{"location":"user_guides/explainability.html#permutation-feature-importance","title":"Permutation feature importance\u00b6","text":""},{"location":"user_guides/explainability.html#partial-dependence-plots","title":"Partial dependence plots\u00b6","text":""},{"location":"user_guides/feature-selection.html","title":"Feature selection","text":"<p>Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons: to simplify models to make them easier to interpret, to reduce training time, to avoid the curse of dimensionality, to improve generalization by reducing overfitting (formally, variance reduction), and others.</p> <p>Skforecast is compatible with the feature selection methods implemented in scikit-learn and feature-engine libraries. There are several methods for feature selection, but the most common are:</p> <p>Recursive feature elimination</p> <p>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features, and the importance of each feature is obtained either by a specific attribute (such as <code>coef_</code>, <code>feature_importances_</code>) or by a <code>callable</code>. Then, the least important features are pruned from the current set of features. This procedure is repeated recursively on the pruned set until the desired number of features to select is eventually reached. <code>RFECV</code> performs RFE in a cross-validation loop to find the optimal number of features.</p> <p>Sequential Feature Selection</p> <p>Sequential Feature Selection (<code>SFS</code>) can be either forward or backward, with the <code>direction</code> parameter controlling whether forward or backward SFS is used.</p> <ul> <li><p>Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. It starts with zero features and finds the one that maximizes a cross-validated score when an estimator is trained on that single feature. Once this first feature is selected, the procedure is repeated, adding one new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the <code>n_features_to_select</code> parameter.</p> </li> <li><p>Backward-SFS follows the same idea, but works in the opposite direction. Instead of starting with no features and greedily adding features, it starts with all features and greedily removes features from the set.</p> </li> </ul> <p>In general, forward and backward selection do not produce equivalent results. Also, one can be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3.</p> <p><code>SFS</code> differs does not require the underlying model to expose a <code>coef_</code> or <code>feature_importances_</code> attribute. However, it may be slower compared to the other approaches, considering that more models have to be evaluated. For example in backward selection, the iteration going from $m$ features to $m - 1$ features using k-fold cross-validation requires fitting $m * k$ models to be evaluated.</p> <p>Feature selection based on threshold (SelectFromModel)</p> <p><code>SelectFromModel</code> can be used along with any estimator that has a <code>coef_</code> or <code>feature_importances_</code> attribute after fitting. Features are considered unimportant and removed, if the corresponding <code>coef_</code> or <code>feature_importances_</code> values are below the given <code>threshold</code> parameter. In addition to specifying the <code>threshold</code> numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are <code>'mean'</code>, <code>'median'</code> and float multiples of these, such as <code>'0.1*mean'</code>.</p> <p>This method is very fast compared to the others because it does not require any additional model training. However, it does not evaluate the impact of feature removal on the model. It is often used for an initial selection before applying another more computationally expensive feature selection method.</p> <p>Minimum Redundancy Maximum Relevance (MRMR)</p> <p>Minimum Redundancy Maximum Relevance (MRMR) is a filter-based feature selection method that aims to identify a subset of features that are both highly relevant to the target variable and minimally redundant with respect to each other. Relevance is typically measured using mutual information between each feature and the target, while redundancy is assessed via the mutual information between pairs of features. By optimizing both criteria, mRMR helps reduce overfitting and improve model interpretability, especially in high-dimensional settings. The <code>MRMR</code> class from feature-engine can be used to implement this method.</p> <p> \ud83d\udca1 Tip </p> <p>Feature selection is a powerful tool for improving the performance of machine learning models. However, it is computationally expensive and can be time-consuming. Since the goal is to find the best subset of features, not the best model, it is not necessary to use the entire data set or a highly complex model. Instead, it is recommended to use a small subset of the data and a simple model. Once the best subset of features has been identified, the model can then be trained using the entire dataset and a more complex configuration.  For example, in this use case, the model is an <code>LGMBRegressor</code> with 900 trees and a maximum depth of 7. However, to find the best subset of features, only 100 trees and a maximum depth of 5 are used.</p> <p>The <code>select_features</code> and <code>select_features_multiseries</code> functions can be used to select the best subset of features (autoregressive and exogenous variables). These functions are compatible with the feature selection methods implemented in the scikit-learn library. The available parameters are:</p> <ul> <li><p><code>forecaster</code>: Forecaster of type <code>ForecasterRecursive</code>,  <code>ForecasterDirect</code>, <code>ForecasterRecursiveMultiSeries</code> or <code>ForecasterDirectMultiVariate</code>.</p> </li> <li><p><code>selector</code>: Feature selector from <code>sklearn.feature_selection</code>. For example, <code>RFE</code> or <code>RFECV</code>.</p> </li> <li><p><code>y</code> or <code>series</code>: Target time series to which the feature selection will be applied.</p> </li> <li><p><code>exog</code>: Exogenous variables.</p> </li> <li><p><code>select_only</code>: Decide what type of features to include in the selection process.</p> <ul> <li><p>If <code>'autoreg'</code>, only autoregressive features (lags and window features) are evaluated by the selector. All exogenous features are included in the output <code>selected_exog</code>.</p> </li> <li><p>If <code>'exog'</code>, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included in the outputs <code>selected_lags</code> and <code>selected_window_features</code>.</p> </li> <li><p>If <code>None</code>, all features are evaluated by the selector.</p> </li> </ul> </li> <li><p><code>force_inclusion</code>: Features to force include in the final list of selected features.</p> <ul> <li><p>If <code>list</code>, list of feature names to force include.</p> </li> <li><p>If <code>str</code>, regular expression to identify features to force include. For example, if <code>force_inclusion=\"^sun_\"</code>, all features that begin with \"sun_\" will be included in the final list of selected features.</p> </li> </ul> </li> <li><p><code>subsample</code>: Proportion of records to use for feature selection.</p> </li> <li><p><code>random_state</code>: Sets a seed for the random subsample so that the subsampling process is always deterministic.</p> </li> <li><p><code>verbose</code>: Print information about feature selection process.</p> </li> </ul> <p>These functions return three <code>list</code>:</p> <ul> <li><p><code>selected_lags</code>: List of selected lags.</p> </li> <li><p><code>selected_window_features</code>: List of selected window features.</p> </li> <li><p><code>selected_exog</code>: List of selected exogenous features.</p> </li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.selection import MRMR\n\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\nfrom skforecast.feature_selection import select_features\nfrom skforecast.feature_selection import select_features_multiseries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from lightgbm import LGBMRegressor from sklearn.feature_selection import RFECV from sklearn.feature_selection import SequentialFeatureSelector from sklearn.feature_selection import SelectFromModel from sklearn.model_selection import ShuffleSplit from sklearn.preprocessing import StandardScaler from feature_engine.selection import MRMR  from skforecast.datasets import fetch_dataset from skforecast.preprocessing import RollingFeatures from skforecast.recursive import ForecasterRecursive from skforecast.recursive import ForecasterRecursiveMultiSeries from skforecast.feature_selection import select_features from skforecast.feature_selection import select_features_multiseries In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name=\"bike_sharing_extended_features\")\ndata.head(3)\n</pre> # Download data # ============================================================================== data = fetch_dataset(name=\"bike_sharing_extended_features\") data.head(3) <pre>bike_sharing_extended_features\n------------------------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, the dataset\nwas enriched by introducing supplementary features. Addition includes calendar-\nbased variables (day of the week, hour of the day, month, etc.), indicators for\nsunlight, incorporation of rolling temperature averages, and the creation of\npolynomial features generated from variable pairs. All cyclic variables are\nencoded using sine and cosine functions to ensure accurate representation.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17352, 90)\n</pre> Out[2]: users weather month_sin month_cos week_of_year_sin week_of_year_cos week_day_sin week_day_cos hour_day_sin hour_day_cos ... temp_roll_mean_1_day temp_roll_mean_7_day temp_roll_max_1_day temp_roll_min_1_day temp_roll_max_7_day temp_roll_min_7_day holiday_previous_day holiday_next_day temp holiday date_time 2011-01-08 00:00:00 25.0 mist 0.5 0.866025 0.120537 0.992709 -0.781832 0.62349 0.258819 0.965926 ... 8.063334 10.127976 9.02 6.56 18.86 4.92 0.0 0.0 7.38 0.0 2011-01-08 01:00:00 16.0 mist 0.5 0.866025 0.120537 0.992709 -0.781832 0.62349 0.500000 0.866025 ... 8.029166 10.113334 9.02 6.56 18.86 4.92 0.0 0.0 7.38 0.0 2011-01-08 02:00:00 16.0 mist 0.5 0.866025 0.120537 0.992709 -0.781832 0.62349 0.707107 0.707107 ... 7.995000 10.103572 9.02 6.56 18.86 4.92 0.0 0.0 7.38 0.0 <p>3 rows \u00d7 90 columns</p> In\u00a0[3]: Copied! <pre># Data selection (reduce data size to speed up the example)\n# ==============================================================================\ndata = data.drop(columns=\"weather\")\ndata = data.loc[\"2012-01-01 00:00:00\":]\n</pre> # Data selection (reduce data size to speed up the example) # ============================================================================== data = data.drop(columns=\"weather\") data = data.loc[\"2012-01-01 00:00:00\":] In\u00a0[4]: Copied! <pre># Create forecaster\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'mean', 'sum'],\n                      window_sizes = [24, 48, 24]\n                  )\n\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(\n                                       n_estimators = 900,\n                                       random_state = 15926,\n                                       max_depth    = 7,\n                                       verbose      = -1\n                                   ),\n                 lags            = 48,\n                 window_features = window_features\n             )\n</pre> # Create forecaster # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'mean', 'sum'],                       window_sizes = [24, 48, 24]                   )  forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(                                        n_estimators = 900,                                        random_state = 15926,                                        max_depth    = 7,                                        verbose      = -1                                    ),                  lags            = 48,                  window_features = window_features              ) In\u00a0[5]: Copied! <pre># Feature selection (autoregressive and exog) with scikit-learn RFECV\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"X does not have valid feature names, but .* was fitted with feature names\"\n)\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\nselector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=25)\nselected_lags, selected_window_features, selected_exog = select_features(\n    forecaster      = forecaster,\n    selector        = selector,\n    y               = data[\"users\"],\n    exog            = data.drop(columns=\"users\"),\n    select_only     = None,\n    force_inclusion = None,\n    subsample       = 0.5,\n    random_state    = 123,\n    verbose         = True,\n)\n</pre> # Feature selection (autoregressive and exog) with scikit-learn RFECV # ============================================================================== import warnings warnings.filterwarnings(     \"ignore\",     message=\"X does not have valid feature names, but .* was fitted with feature names\" ) regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1) selector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=25) selected_lags, selected_window_features, selected_exog = select_features(     forecaster      = forecaster,     selector        = selector,     y               = data[\"users\"],     exog            = data.drop(columns=\"users\"),     select_only     = None,     force_inclusion = None,     subsample       = 0.5,     random_state    = 123,     verbose         = True, ) <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 4356\nNumber of features available: 139\n    Lags            (n=48)\n    Window features (n=3)\n    Exog            (n=88)\nNumber of features selected: 58\n    Lags            (n=36) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 19, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 34, 35, 36, 40, 41, 42, 44, 46, 47, 48]\n    Window features (n=1) : ['roll_mean_24']\n    Exog            (n=21) : ['hour_day_sin', 'hour_day_cos', 'poly_month_cos__week_of_year_sin', 'poly_month_cos__hour_day_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_cos__sunset_hour_sin', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp']\n</pre> <p>Then, the Forecaster model is trained with the selected features. As the window features are generated with the <code>RollingFeatures</code> class, the selected window features must be included manually creating a new object.</p> In\u00a0[6]: Copied! <pre># Train forecaster with selected features\n# ==============================================================================\nnew_window_features = RollingFeatures(\n                          stats        = ['mean'],\n                          window_sizes = 24\n                      )\n\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(\n                                       n_estimators = 900,\n                                       random_state = 15926,\n                                       max_depth    = 7,\n                                       verbose      = -1\n                                   ),\n                 lags            = selected_lags,\n                 window_features = new_window_features\n             )\n\nforecaster.fit(y=data[\"users\"], exog=data[selected_exog])\n</pre> # Train forecaster with selected features # ============================================================================== new_window_features = RollingFeatures(                           stats        = ['mean'],                           window_sizes = 24                       )  forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(                                        n_estimators = 900,                                        random_state = 15926,                                        max_depth    = 7,                                        verbose      = -1                                    ),                  lags            = selected_lags,                  window_features = new_window_features              )  forecaster.fit(y=data[\"users\"], exog=data[selected_exog]) In\u00a0[7]: Copied! <pre># Feature selection (only autoregressive) with scikit-learn RFECV\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\nselector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=25)\n\nselected_lags, selected_window_features, selected_exog = select_features(\n    forecaster  = forecaster,\n    selector    = selector,\n    y           = data[\"users\"],\n    exog        = data.drop(columns=\"users\"),\n    select_only = 'autoreg',\n    subsample   = 0.5,\n    verbose     = True,\n)\n</pre> # Feature selection (only autoregressive) with scikit-learn RFECV # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1) selector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=25)  selected_lags, selected_window_features, selected_exog = select_features(     forecaster  = forecaster,     selector    = selector,     y           = data[\"users\"],     exog        = data.drop(columns=\"users\"),     select_only = 'autoreg',     subsample   = 0.5,     verbose     = True, ) <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 4356\nNumber of features available: 125\n    Lags            (n=36)\n    Window features (n=1)\n    Exog            (n=88)\nNumber of features selected: 33\n    Lags            (n=33) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 19, 22, 23, 24, 25, 26, 28, 29, 30, 32, 34, 35, 36, 40, 41, 42, 44, 46, 48]\n    Window features (n=0) : []\n    Exog            (n=88) : ['month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos', 'sunset_hour_sin', 'sunset_hour_cos', 'poly_month_sin__month_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__week_day_cos', 'poly_month_sin__hour_day_sin', 'poly_month_sin__hour_day_cos', 'poly_month_sin__sunrise_hour_sin', 'poly_month_sin__sunrise_hour_cos', 'poly_month_sin__sunset_hour_sin', 'poly_month_sin__sunset_hour_cos', 'poly_month_cos__week_of_year_sin', 'poly_month_cos__week_of_year_cos', 'poly_month_cos__week_day_sin', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_sin', 'poly_month_cos__hour_day_cos', 'poly_month_cos__sunrise_hour_sin', 'poly_month_cos__sunrise_hour_cos', 'poly_month_cos__sunset_hour_sin', 'poly_month_cos__sunset_hour_cos', 'poly_week_of_year_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_sin', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_sin', 'poly_week_of_year_cos__sunset_hour_cos', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_sin__sunrise_hour_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_sin__sunset_hour_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_sin', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_week_day_cos__sunset_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos', 'poly_sunrise_hour_sin__sunrise_hour_cos', 'poly_sunrise_hour_sin__sunset_hour_sin', 'poly_sunrise_hour_sin__sunset_hour_cos', 'poly_sunrise_hour_cos__sunset_hour_sin', 'poly_sunrise_hour_cos__sunset_hour_cos', 'poly_sunset_hour_sin__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day', 'holiday_previous_day', 'holiday_next_day', 'temp', 'holiday']\n</pre> In\u00a0[8]: Copied! <pre># Check all exogenous features are selected\n# ==============================================================================\nlen(selected_exog) == data.drop(columns=\"users\").shape[1]\n</pre> # Check all exogenous features are selected # ============================================================================== len(selected_exog) == data.drop(columns=\"users\").shape[1] Out[8]: <pre>True</pre> In\u00a0[9]: Copied! <pre># Feature selection (only exog) with scikit-learn RFECV\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\nselector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=25)\n\nselected_lags, selected_window_features, selected_exog = select_features(\n    forecaster  = forecaster,\n    selector    = selector,\n    y           = data[\"users\"],\n    exog        = data.drop(columns=\"users\"),\n    select_only = 'exog',\n    subsample   = 0.5,\n    verbose     = True,\n)\n</pre> # Feature selection (only exog) with scikit-learn RFECV # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1) selector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=25)  selected_lags, selected_window_features, selected_exog = select_features(     forecaster  = forecaster,     selector    = selector,     y           = data[\"users\"],     exog        = data.drop(columns=\"users\"),     select_only = 'exog',     subsample   = 0.5,     verbose     = True, ) <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 4356\nNumber of features available: 125\n    Lags            (n=36)\n    Window features (n=1)\n    Exog            (n=88)\nNumber of features selected: 61\n    Lags            (n=36) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 19, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 34, 35, 36, 40, 41, 42, 44, 46, 47, 48]\n    Window features (n=1) : ['roll_mean_24']\n    Exog            (n=61) : ['week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__week_day_cos', 'poly_month_sin__hour_day_sin', 'poly_month_sin__hour_day_cos', 'poly_month_cos__week_of_year_sin', 'poly_month_cos__week_day_sin', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_sin', 'poly_month_cos__hour_day_cos', 'poly_week_of_year_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_sin', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_sin', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_sin__sunrise_hour_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_sin__sunset_hour_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_sin', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_week_day_cos__sunset_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day', 'temp', 'holiday']\n</pre> In\u00a0[10]: Copied! <pre># Check all autoregressive features are selected\n# ==============================================================================\nprint(\"Same lags :\", len(selected_lags) == len(forecaster.lags))\nprint(\"Same window features :\", len(selected_window_features) == len(forecaster.window_features))\n</pre> # Check all autoregressive features are selected # ============================================================================== print(\"Same lags :\", len(selected_lags) == len(forecaster.lags)) print(\"Same window features :\", len(selected_window_features) == len(forecaster.window_features)) <pre>Same lags : True\nSame window features : True\n</pre> <p>The <code>force_inclusion</code> argument can be used to force the selection of certain features. To illustrate this, a non-informative feature is added to the data set, <code>noise</code>. This feature contains no information about the target variable and therefore should not be selected by the feature selector. However, if we force the inclusion of this feature, it will be included in the final list of selected features.</p> In\u00a0[11]: Copied! <pre># Add non-informative feature\n# ==============================================================================\ndata['noise'] = np.random.normal(size=len(data))\n</pre> # Add non-informative feature # ============================================================================== data['noise'] = np.random.normal(size=len(data)) In\u00a0[12]: Copied! <pre># Feature selection (only exog) with scikit-learn RFECV\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\nselector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=10)\n\nselected_lags, selected_window_features, selected_exog = select_features(\n    forecaster      = forecaster,\n    selector        = selector,\n    y               = data[\"users\"],\n    exog            = data.drop(columns=\"users\"),\n    select_only     = 'exog',\n    force_inclusion = [\"noise\"],\n    subsample       = 0.5,\n    verbose         = True,\n)\n</pre> # Feature selection (only exog) with scikit-learn RFECV # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1) selector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=10)  selected_lags, selected_window_features, selected_exog = select_features(     forecaster      = forecaster,     selector        = selector,     y               = data[\"users\"],     exog            = data.drop(columns=\"users\"),     select_only     = 'exog',     force_inclusion = [\"noise\"],     subsample       = 0.5,     verbose         = True, ) <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 4356\nNumber of features available: 126\n    Lags            (n=36)\n    Window features (n=1)\n    Exog            (n=89)\nNumber of features selected: 70\n    Lags            (n=36) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 19, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 34, 35, 36, 40, 41, 42, 44, 46, 47, 48]\n    Window features (n=1) : ['roll_mean_24']\n    Exog            (n=70) : ['month_sin', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__week_day_cos', 'poly_month_sin__hour_day_sin', 'poly_month_sin__hour_day_cos', 'poly_month_sin__sunrise_hour_cos', 'poly_month_sin__sunset_hour_sin', 'poly_month_sin__sunset_hour_cos', 'poly_month_cos__week_of_year_sin', 'poly_month_cos__week_day_sin', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_sin', 'poly_month_cos__hour_day_cos', 'poly_week_of_year_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_sin', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_sin', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_sin__sunrise_hour_cos', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_sin__sunset_hour_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_sin', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_week_day_cos__sunset_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'temp_roll_max_7_day', 'temp_roll_min_7_day', 'holiday_previous_day', 'holiday_next_day', 'temp', 'holiday', 'noise']\n</pre> In\u00a0[13]: Copied! <pre># Check if \"noise\" is in selected_exog\n# ==============================================================================\n\"noise\" in selected_exog\n</pre> # Check if \"noise\" is in selected_exog # ============================================================================== \"noise\" in selected_exog Out[13]: <pre>True</pre> In\u00a0[14]: Copied! <pre># Feature selection (only exog) with scikit-learn SequentialFeatureSelector\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=50, max_depth=3, random_state=15926, verbose=-1)\nselector = SequentialFeatureSelector(\n               estimator            = forecaster.regressor,\n               n_features_to_select = 25,\n               direction            = \"forward\",\n               cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),\n               scoring              = \"neg_mean_absolute_error\",\n           )\n\nselected_lags, selected_window_features, selected_exog = select_features(\n    forecaster   = forecaster,\n    selector     = selector,\n    y            = data[\"users\"],\n    exog         = data.drop(columns=\"users\"),\n    select_only  = 'exog',\n    subsample    = 0.2,\n    random_state = 123,\n    verbose      = True,\n)\n</pre> # Feature selection (only exog) with scikit-learn SequentialFeatureSelector # ============================================================================== regressor = LGBMRegressor(n_estimators=50, max_depth=3, random_state=15926, verbose=-1) selector = SequentialFeatureSelector(                estimator            = forecaster.regressor,                n_features_to_select = 25,                direction            = \"forward\",                cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),                scoring              = \"neg_mean_absolute_error\",            )  selected_lags, selected_window_features, selected_exog = select_features(     forecaster   = forecaster,     selector     = selector,     y            = data[\"users\"],     exog         = data.drop(columns=\"users\"),     select_only  = 'exog',     subsample    = 0.2,     random_state = 123,     verbose      = True, ) <pre>Recursive feature elimination (SequentialFeatureSelector)\n---------------------------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 1742\nNumber of features available: 126\n    Lags            (n=36)\n    Window features (n=1)\n    Exog            (n=89)\nNumber of features selected: 25\n    Lags            (n=36) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 19, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 34, 35, 36, 40, 41, 42, 44, 46, 47, 48]\n    Window features (n=1) : ['roll_mean_24']\n    Exog            (n=25) : ['week_of_year_sin', 'week_day_sin', 'hour_day_sin', 'hour_day_cos', 'sunset_hour_sin', 'poly_month_sin__week_day_cos', 'poly_month_cos__week_of_year_cos', 'poly_month_cos__sunset_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_cos', 'poly_week_day_sin__week_day_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_cos__hour_day_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_sunrise_hour_sin__sunset_hour_sin', 'poly_sunrise_hour_cos__sunset_hour_sin', 'poly_sunrise_hour_cos__sunset_hour_cos', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'holiday_next_day', 'temp', 'holiday']\n</pre> In\u00a0[15]: Copied! <pre># Feature selection with feature-engine MRMR\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\nselector = MRMR(method=\"MIQ\", max_features=25, regression=True, cv=3)\n\nselected_lags, selected_window_features, selected_exog = select_features(\n    forecaster   = forecaster,\n    selector     = selector,\n    y            = data[\"users\"],\n    exog         = data.drop(columns=\"users\"),\n    select_only  = None,\n    subsample    = 0.5,\n    random_state = 123,\n    verbose      = True,\n)\n</pre> # Feature selection with feature-engine MRMR # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1) selector = MRMR(method=\"MIQ\", max_features=25, regression=True, cv=3)  selected_lags, selected_window_features, selected_exog = select_features(     forecaster   = forecaster,     selector     = selector,     y            = data[\"users\"],     exog         = data.drop(columns=\"users\"),     select_only  = None,     subsample    = 0.5,     random_state = 123,     verbose      = True, ) <pre>Recursive feature elimination (MRMR)\n------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 4356\nNumber of features available: 126\n    Lags            (n=36)\n    Window features (n=1)\n    Exog            (n=89)\nNumber of features selected: 25\n    Lags            (n=19) : [1, 2, 3, 4, 5, 9, 10, 11, 14, 16, 22, 23, 24, 25, 26, 34, 46, 47, 48]\n    Window features (n=0) : []\n    Exog            (n=6) : ['hour_day_cos', 'poly_month_sin__month_cos', 'poly_week_day_cos__hour_day_sin', 'poly_week_day_cos__sunrise_hour_cos', 'temp', 'holiday']\n</pre> <p>Combining feature selection methods can help speed up the process. An effective approach is to first use <code>SelectFromModel</code> to eliminate the less important features, and then use <code>SequentialFeatureSelector</code> to determine the best subset of features from this reduced list. This two-step method often improves efficiency by focusing on the most important features.</p> In\u00a0[16]: Copied! <pre># Feature selection (autoregressive and exog) with SelectFromModel + SequentialFeatureSelector\n# ==============================================================================\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\n\n# Step 1: Select the 70% most important features with SelectFromModel\nselector_1 = SelectFromModel(\n                 estimator    = regressor,\n                 max_features = int(data.shape[1] * 0.7),\n                 threshold    = -np.inf\n             )\nselected_lags_1, selected_window_features_1, selected_exog_1 = select_features(\n    forecaster  = forecaster,\n    selector    = selector_1,\n    y           = data[\"users\"],\n    exog        = data.drop(columns=\"users\"),\n    select_only = None,\n    subsample   = 0.2,\n    verbose     = True,\n)\n</pre> # Feature selection (autoregressive and exog) with SelectFromModel + SequentialFeatureSelector # ============================================================================== regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)  # Step 1: Select the 70% most important features with SelectFromModel selector_1 = SelectFromModel(                  estimator    = regressor,                  max_features = int(data.shape[1] * 0.7),                  threshold    = -np.inf              ) selected_lags_1, selected_window_features_1, selected_exog_1 = select_features(     forecaster  = forecaster,     selector    = selector_1,     y           = data[\"users\"],     exog        = data.drop(columns=\"users\"),     select_only = None,     subsample   = 0.2,     verbose     = True, ) <pre>Recursive feature elimination (SelectFromModel)\n-----------------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 1742\nNumber of features available: 126\n    Lags            (n=36)\n    Window features (n=1)\n    Exog            (n=89)\nNumber of features selected: 62\n    Lags            (n=36) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 19, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 34, 35, 36, 40, 41, 42, 44, 46, 47, 48]\n    Window features (n=1) : [np.str_('roll_mean_24')]\n    Exog            (n=25) : [np.str_('week_day_cos'), np.str_('hour_day_sin'), np.str_('hour_day_cos'), np.str_('poly_month_sin__week_day_sin'), np.str_('poly_month_sin__week_day_cos'), np.str_('poly_month_sin__hour_day_sin'), np.str_('poly_month_cos__week_of_year_sin'), np.str_('poly_month_cos__week_day_sin'), np.str_('poly_month_cos__week_day_cos'), np.str_('poly_week_of_year_sin__hour_day_sin'), np.str_('poly_week_of_year_sin__hour_day_cos'), np.str_('poly_week_of_year_cos__week_day_cos'), np.str_('poly_week_of_year_cos__hour_day_sin'), np.str_('poly_week_day_sin__hour_day_sin'), np.str_('poly_week_day_sin__hour_day_cos'), np.str_('poly_week_day_cos__hour_day_sin'), np.str_('poly_week_day_cos__hour_day_cos'), np.str_('poly_hour_day_sin__hour_day_cos'), np.str_('poly_hour_day_sin__sunrise_hour_sin'), np.str_('poly_hour_day_sin__sunset_hour_sin'), np.str_('poly_hour_day_cos__sunset_hour_sin'), np.str_('temp_roll_mean_7_day'), np.str_('temp_roll_min_1_day'), np.str_('temp'), np.str_('noise')]\n</pre> In\u00a0[17]: Copied! <pre># Step 2: Select the 25 most important features with SequentialFeatureSelector\nwindow_features_1 = RollingFeatures(stats=['mean'], window_sizes=24)\nforecaster.set_lags(lags=selected_lags_1)\nforecaster.set_window_features(window_features=window_features_1)\n\nselector_2 = SequentialFeatureSelector(\n                 estimator            = regressor,\n                 n_features_to_select = 25,\n                 direction            = \"forward\",\n                 cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),\n                 scoring              = \"neg_mean_absolute_error\",\n             )\n\nselected_lags, selected_window_features, selected_exog = select_features(\n    forecaster  = forecaster,\n    selector    = selector_2,\n    y           = data[\"users\"],\n    exog        = data[selected_exog_1],\n    select_only = None,\n    subsample   = 0.2,\n    verbose     = True,\n)\n</pre> # Step 2: Select the 25 most important features with SequentialFeatureSelector window_features_1 = RollingFeatures(stats=['mean'], window_sizes=24) forecaster.set_lags(lags=selected_lags_1) forecaster.set_window_features(window_features=window_features_1)  selector_2 = SequentialFeatureSelector(                  estimator            = regressor,                  n_features_to_select = 25,                  direction            = \"forward\",                  cv                   = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),                  scoring              = \"neg_mean_absolute_error\",              )  selected_lags, selected_window_features, selected_exog = select_features(     forecaster  = forecaster,     selector    = selector_2,     y           = data[\"users\"],     exog        = data[selected_exog_1],     select_only = None,     subsample   = 0.2,     verbose     = True, ) <pre>Recursive feature elimination (SequentialFeatureSelector)\n---------------------------------------------------------\nTotal number of records available: 8712\nTotal number of records used for feature selection: 1742\nNumber of features available: 62\n    Lags            (n=36)\n    Window features (n=1)\n    Exog            (n=25)\nNumber of features selected: 25\n    Lags            (n=13) : [1, 6, 7, 8, 16, 24, 25, 28, 29, 30, 32, 41, 48]\n    Window features (n=0) : []\n    Exog            (n=12) : ['hour_day_sin', 'hour_day_cos', 'poly_month_cos__week_of_year_sin', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunset_hour_sin', 'temp_roll_mean_7_day', 'temp_roll_min_1_day', 'temp']\n</pre> <p>As with univariate forecasting models, feature selection can be applied to global forecasting models (multi-series). In this case, the <code>select_features_multiseries</code> function is used. This function has the same parameters as <code>select_features</code>, but the <code>y</code> parameter is replaced by <code>series</code>.</p> <ul> <li><p><code>forecaster</code>: Forecaster of type <code>ForecasterRecursiveMultiSeries</code> or <code>ForecasterDirectMultiVariate</code>.</p> </li> <li><p><code>selector</code>: Feature selector from <code>sklearn.feature_selection</code>. For example, <code>RFE</code> or <code>RFECV</code>.</p> </li> <li><p><code>series</code>: Target time series to which the feature selection will be applied.</p> </li> <li><p><code>exog</code>: Exogenous variables.</p> </li> <li><p><code>select_only</code>: Decide what type of features to include in the selection process.</p> <ul> <li><p>If <code>'autoreg'</code>, only autoregressive features (lags or custom predictors) are evaluated by the selector. All exogenous features are included in the output <code>selected_exog</code>.</p> </li> <li><p>If <code>'exog'</code>, only exogenous features are evaluated without the presence of autoregressive features. All autoregressive features are included in the outputs <code>selected_lags</code> and <code>selected_window_features</code>.</p> </li> <li><p>If <code>None</code>, all features are evaluated by the selector.</p> </li> </ul> </li> <li><p><code>force_inclusion</code>: Features to force include in the final list of selected features.</p> <ul> <li><p>If <code>list</code>, list of feature names to force include.</p> </li> <li><p>If <code>str</code>, regular expression to identify features to force include. For example, if <code>force_inclusion=\"^sun_\"</code>, all features that begin with \"sun_\" will be included in the final list of selected features.</p> </li> </ul> </li> <li><p><code>subsample</code>: Proportion of records to use for feature selection.</p> </li> <li><p><code>random_state</code>: Sets a seed for the random subsample so that the subsampling process is always deterministic.</p> </li> <li><p><code>verbose</code>: Print information about feature selection process.</p> </li> </ul> In\u00a0[18]: Copied! <pre># Data\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\n</pre> # Data # ============================================================================== data = fetch_dataset(name=\"items_sales\") <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> In\u00a0[19]: Copied! <pre># Create exogenous features based on the calendar\n# ==============================================================================\ndata[\"month\"] = data.index.month\ndata[\"day_of_week\"] = data.index.dayofweek\ndata[\"day_of_month\"] = data.index.day\ndata[\"week_of_year\"] = data.index.isocalendar().week\ndata[\"quarter\"] = data.index.quarter\ndata[\"is_month_start\"] = data.index.is_month_start.astype(int)\ndata[\"is_month_end\"] = data.index.is_month_end.astype(int)\ndata[\"is_quarter_start\"] = data.index.is_quarter_start.astype(int)\ndata[\"is_quarter_end\"] = data.index.is_quarter_end.astype(int)\ndata[\"is_year_start\"] = data.index.is_year_start.astype(int)\ndata[\"is_year_end\"] = data.index.is_year_end.astype(int)\ndata.head()\n</pre> # Create exogenous features based on the calendar # ============================================================================== data[\"month\"] = data.index.month data[\"day_of_week\"] = data.index.dayofweek data[\"day_of_month\"] = data.index.day data[\"week_of_year\"] = data.index.isocalendar().week data[\"quarter\"] = data.index.quarter data[\"is_month_start\"] = data.index.is_month_start.astype(int) data[\"is_month_end\"] = data.index.is_month_end.astype(int) data[\"is_quarter_start\"] = data.index.is_quarter_start.astype(int) data[\"is_quarter_end\"] = data.index.is_quarter_end.astype(int) data[\"is_year_start\"] = data.index.is_year_start.astype(int) data[\"is_year_end\"] = data.index.is_year_end.astype(int) data.head() Out[19]: item_1 item_2 item_3 month day_of_week day_of_month week_of_year quarter is_month_start is_month_end is_quarter_start is_quarter_end is_year_start is_year_end date 2012-01-01 8.253175 21.047727 19.429739 1 6 1 52 1 1 0 1 0 1 0 2012-01-02 22.777826 26.578125 28.009863 1 0 2 1 1 0 0 0 0 0 0 2012-01-03 27.549099 31.751042 32.078922 1 1 3 1 1 0 0 0 0 0 0 2012-01-04 25.895533 24.567708 27.252276 1 2 4 1 1 0 0 0 0 0 0 2012-01-05 21.379238 18.191667 20.357737 1 3 5 1 1 0 0 0 0 0 0 In\u00a0[20]: Copied! <pre># Create forecaster\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n    regressor       = LGBMRegressor(n_estimators=900, random_state=159, max_depth=7, verbose=-1),\n    lags            = 24,\n    window_features = RollingFeatures(stats=['mean', 'mean', 'mean'], window_sizes=[24, 48, 72])\n)\n</pre> # Create forecaster # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(     regressor       = LGBMRegressor(n_estimators=900, random_state=159, max_depth=7, verbose=-1),     lags            = 24,     window_features = RollingFeatures(stats=['mean', 'mean', 'mean'], window_sizes=[24, 48, 72]) ) In\u00a0[21]: Copied! <pre># Feature selection (autoregressive and exog) with scikit-learn RFECV\n# ==============================================================================\nseries_columns = [\"item_1\", \"item_2\", \"item_3\"]\nexog_columns = [col for col in data.columns if col not in series_columns]\nregressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1)\nselector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=25)\n\nselected_lags, selected_window_features, selected_exog = select_features_multiseries(\n    forecaster      = forecaster,\n    selector        = selector,\n    series          = data[series_columns],\n    exog            = data[exog_columns],\n    select_only     = None,\n    force_inclusion = None,\n    subsample       = 0.5,\n    random_state    = 123,\n    verbose         = True,\n)\n</pre> # Feature selection (autoregressive and exog) with scikit-learn RFECV # ============================================================================== series_columns = [\"item_1\", \"item_2\", \"item_3\"] exog_columns = [col for col in data.columns if col not in series_columns] regressor = LGBMRegressor(n_estimators=100, max_depth=5, random_state=15926, verbose=-1) selector = RFECV(estimator=regressor, step=1, cv=3, min_features_to_select=25)  selected_lags, selected_window_features, selected_exog = select_features_multiseries(     forecaster      = forecaster,     selector        = selector,     series          = data[series_columns],     exog            = data[exog_columns],     select_only     = None,     force_inclusion = None,     subsample       = 0.5,     random_state    = 123,     verbose         = True, ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DataTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `exog` may contain only `int`, `float` or `category` dtypes. Most machine learning   \u2502\n\u2502 models do not allow other types of values. Fitting the forecaster may fail.          \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : DataTypeWarning                                                           \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:641                                                                          \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=DataTypeWarning)                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>Recursive feature elimination (RFECV)\n-------------------------------------\nTotal number of records available: 3075\nTotal number of records used for feature selection: 1537\nNumber of features available: 38\n    Lags            (n=24)\n    Window features (n=3)\n    Exog            (n=11)\nNumber of features selected: 28\n    Lags            (n=24) : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n    Window features (n=2) : ['roll_mean_24', 'roll_mean_72']\n    Exog            (n=2) : ['day_of_week', 'week_of_year']\n</pre> <p>Once the best subset of features has been selected, the global forecasting model is trained with the selected features.</p> In\u00a0[22]: Copied! <pre># Train forecaster with selected features\n# ==============================================================================\nnew_window_features = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 72])\nforecaster.set_lags(lags=selected_lags)\nforecaster.set_window_features(window_features=new_window_features)\n\nforecaster.fit(series=data[series_columns], exog=data[selected_exog])\nforecaster\n</pre> # Train forecaster with selected features # ============================================================================== new_window_features = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 72]) forecaster.set_lags(lags=selected_lags) forecaster.set_window_features(window_features=new_window_features)  forecaster.fit(series=data[series_columns], exog=data[selected_exog]) forecaster <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DataTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `exog` may contain only `int`, `float` or `category` dtypes. Most machine learning   \u2502\n\u2502 models do not allow other types of values. Fitting the forecaster may fail.          \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : DataTypeWarning                                                           \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:641                                                                          \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=DataTypeWarning)                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[22]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]</li> <li>Window features: ['roll_mean_24', 'roll_mean_72']</li> <li>Window size: 72</li> <li>Series encoding: ordinal</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Series weights: None</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:58:49</li> <li>Last fit date: 2025-08-06 13:58:53</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     day_of_week, week_of_year                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): item_1, item_2, item_3</li> <li>Training range: 'item_1': ['2012-01-01', '2015-01-01'], 'item_2': ['2012-01-01', '2015-01-01'], 'item_3': ['2012-01-01', '2015-01-01']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 7, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 900, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 159, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p>"},{"location":"user_guides/feature-selection.html#feature-selection","title":"Feature selection\u00b6","text":""},{"location":"user_guides/feature-selection.html#feature-selection-with-skforecast","title":"Feature selection with skforecast\u00b6","text":""},{"location":"user_guides/feature-selection.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/feature-selection.html#create-forecaster","title":"Create forecaster\u00b6","text":"<p>A forecasting model is created to predict the number of users using the last 48 values (last two days) and the exogenous features available in the dataset.</p>"},{"location":"user_guides/feature-selection.html#feature-selection-with-recursive-feature-elimination-rfecv","title":"Feature selection with Recursive Feature Elimination (RFECV)\u00b6","text":""},{"location":"user_guides/feature-selection.html#selection-of-autoregressive-and-exogenous-features","title":"Selection of autoregressive and exogenous features\u00b6","text":"<p>By default, the <code>select_features</code> function selects the best subset of autoregressive and exogenous features.</p>"},{"location":"user_guides/feature-selection.html#selection-on-a-subset-of-features","title":"Selection on a subset of features\u00b6","text":"<ul> <li><p>If <code>select_only = 'autoreg'</code>, only autoregressive features (lags or custom predictors) are evaluated by the selector. All exogenous features are included in the output <code>selected_exog</code>.</p> </li> <li><p>If <code>select_only = 'exog'</code>, exogenous features are evaluated by the selector in the absence of autoregressive features. All autoregressive features are included in the outputs <code>selected_lags</code> and <code>selected_window_features</code>.</p> </li> </ul>"},{"location":"user_guides/feature-selection.html#force-selection-of-specific-features","title":"Force selection of specific features\u00b6","text":""},{"location":"user_guides/feature-selection.html#feature-selection-with-sequential-feature-selection-sfs","title":"Feature selection with Sequential Feature Selection (SFS)\u00b6","text":"<p>Sequential Feature Selection is a robust method for selecting features, but it is computationally expensive. When the data set is very large, one way to reduce the computational cost is to use a single validation split to evaluate each candidate model instead of cross-validation (default).</p>"},{"location":"user_guides/feature-selection.html#feature-selection-with-minimum-redundancy-maximum-relevance-mrmr","title":"Feature selection with Minimum Redundancy Maximum Relevance (MRMR)\u00b6","text":"<p>Minimum Redundancy Maximum Relevance (MRMR) is a filter-based feature selection method that is model-agnostic and fast to compute, making it suitable for high-dimensional datasets. However, it relies on statistical criteria like mutual information rather than evaluating model performance directly, so it may be less tailored to a specific estimator compared to wrapper methods.</p>"},{"location":"user_guides/feature-selection.html#combination-of-feature-selection-methods","title":"Combination of feature selection methods\u00b6","text":""},{"location":"user_guides/feature-selection.html#feature-selection-in-global-forecasting-models","title":"Feature Selection in Global Forecasting Models\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html","title":"Forecaster in production","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.utils import save_forecaster\nfrom skforecast.utils import load_forecaster\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestRegressor from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.utils import save_forecaster from skforecast.utils import load_forecaster from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0}\n)\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0} ) <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> In\u00a0[3]: Copied! <pre># Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\ndata_train = data.loc[:'2005-01-01']\ndata_train.tail()\n</pre> # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') data_train = data.loc[:'2005-01-01'] data_train.tail() Out[3]: y date 2004-09-01 1.134432 2004-10-01 1.181011 2004-11-01 1.216037 2004-12-01 1.257238 2005-01-01 1.170690 In\u00a0[4]: Copied! <pre># Data preprocessing\n# ==============================================================================\ndata_train = data.loc[:'2005-01-01']\n# data_train\n</pre> # Data preprocessing # ============================================================================== data_train = data.loc[:'2005-01-01'] # data_train In\u00a0[5]: Copied! <pre># Train forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = 'forecasting_series_y'\n             )\n\nforecaster.fit(y=data_train['y'])\n</pre> # Train forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = 'forecasting_series_y'              )  forecaster.fit(y=data_train['y']) In\u00a0[6]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=3)\n</pre> # Predict # ============================================================================== forecaster.predict(steps=3) Out[6]: <pre>2005-02-01    0.927480\n2005-03-01    0.756215\n2005-04-01    0.692595\nFreq: MS, Name: pred, dtype: float64</pre> <p>As expected, predictions follow directly from the end of training data.</p> <p>When the <code>last_window</code> argument is provided, the forecaster uses this data to generate the necessary lags as predictors and starts the prediction thereafter.</p> In\u00a0[7]: Copied! <pre># Predict with last_window\n# ==============================================================================\nlast_window = data['y'].tail(5)\nforecaster.predict(steps=3, last_window=last_window)\n</pre> # Predict with last_window # ============================================================================== last_window = data['y'].tail(5) forecaster.predict(steps=3, last_window=last_window) Out[7]: <pre>2008-07-01    0.803853\n2008-08-01    0.870858\n2008-09-01    0.905003\nFreq: MS, Name: pred, dtype: float64</pre> <p>Since the provided <code>last_window</code> contains values from 2008-02-01 to 2008-06-01, the forecaster can create the needed lags and predict the next 5 steps.</p> <p> \u26a0 Warning </p> <p>When using the <code>last_window</code> argument, it is crucial to ensure that the length of <code>last_window</code> is sufficient to include the maximum lag (or custom predictor) used by the forecaster. For instance, if the forecaster employs lags 1, 24, and 48, <code>last_window</code> must include the most recent 48 values of the series. Failing to include the required number of past observations may result in an error or incorrect predictions.</p> In\u00a0[8]: Copied! <pre># Split data\n# ==============================================================================\ndata_train = data.loc[:'2005-01-01'].copy()\ndata_last_window = data.loc['2005-08-01':'2005-12-01'].copy()\n\nprint(\n    f\"Train dates       : {data_train.index.min()} --- {data_train.index.max()}\"\n    f\"  (n={len(data_train)})\"\n)\nprint(\n    f\"Last window dates : {data_last_window.index.min()} --- {data_last_window.index.max()}\"\n    f\"  (n={len(last_window)})\"\n)\n</pre> # Split data # ============================================================================== data_train = data.loc[:'2005-01-01'].copy() data_last_window = data.loc['2005-08-01':'2005-12-01'].copy()  print(     f\"Train dates       : {data_train.index.min()} --- {data_train.index.max()}\"     f\"  (n={len(data_train)})\" ) print(     f\"Last window dates : {data_last_window.index.min()} --- {data_last_window.index.max()}\"     f\"  (n={len(last_window)})\" ) <pre>Train dates       : 1991-07-01 00:00:00 --- 2005-01-01 00:00:00  (n=163)\nLast window dates : 2005-08-01 00:00:00 --- 2005-12-01 00:00:00  (n=5)\n</pre> In\u00a0[9]: Copied! <pre># Plot time series partition\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(label='train', ax=ax)\ndata_last_window['y'].plot(label='last window', ax=ax)\nax.set_xlabel('')\nax.legend();\n</pre> # Plot time series partition # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) data_train['y'].plot(label='train', ax=ax) data_last_window['y'].plot(label='last window', ax=ax) ax.set_xlabel('') ax.legend(); In\u00a0[10]: Copied! <pre># Train forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = 'forecasting_series_y'\n             )\n\nforecaster.fit(y=data_train['y'])\n</pre> # Train forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = 'forecasting_series_y'              )  forecaster.fit(y=data_train['y']) <p>The size of the window needed to make predictions and the last window stored in the forecaster to make predictions immediately after the training data can be observed with the <code>window_size</code> and <code>last_window</code> attributes.</p> In\u00a0[11]: Copied! <pre># Forecaster Attributes\n# ==============================================================================\nprint('Window size:', forecaster.window_size)\nprint(\"Forecaster last window:\")\nprint(forecaster.last_window_)\n</pre> # Forecaster Attributes # ============================================================================== print('Window size:', forecaster.window_size) print(\"Forecaster last window:\") print(forecaster.last_window_) <pre>Window size: 5\nForecaster last window:\n                   y\ndate                \n2004-09-01  1.134432\n2004-10-01  1.181011\n2004-11-01  1.216037\n2004-12-01  1.257238\n2005-01-01  1.170690\n</pre> <p>The model is saved for future use.</p> <p> \u270e Note </p> <p>Learn more about how to Save and load forecasters.</p> In\u00a0[12]: Copied! <pre># Save Forecaster\n# ==============================================================================\nsave_forecaster(forecaster, file_name='forecaster_001.joblib', verbose=False)\n</pre> # Save Forecaster # ============================================================================== save_forecaster(forecaster, file_name='forecaster_001.joblib', verbose=False) <p> \ud83d\udca1 Tip </p> <p>Since the Forecaster has already been trained, there is no need to re-fit the model.</p> In\u00a0[13]: Copied! <pre># Load Forecaster\n# ==============================================================================\nforecaster_loaded = load_forecaster('forecaster_001.joblib', verbose=True)\n</pre> # Load Forecaster # ============================================================================== forecaster_loaded = load_forecaster('forecaster_001.joblib', verbose=True) <pre>=================== \nForecasterRecursive \n=================== \nRegressor: RandomForestRegressor \nLags: [1 2 3 4 5] \nWindow features: None \nWindow size: 5 \nSeries name: y \nExogenous included: False \nExogenous names: None \nTransformer for y: None \nTransformer for exog: None \nWeight function included: False \nDifferentiation order: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2005-01-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: \n    {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth':\n    None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None,\n    'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2,\n    'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100,\n    'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0,\n    'warm_start': False} \nfit_kwargs: {} \nCreation date: 2025-08-06 13:42:42 \nLast fit date: 2025-08-06 13:42:42 \nSkforecast version: 0.17.0 \nPython version: 3.12.11 \nForecaster id: forecasting_series_y \n\n</pre> <p>The forecaster's training range ends at '2005-01-01'. Using a <code>last_window</code>, the forecaster will be able to make predictions for '2006-01-01', 1 year later, without having to re-fit the model.</p> In\u00a0[14]: Copied! <pre># 1 year later last window\n# ==============================================================================\ndata_last_window\n</pre> # 1 year later last window # ============================================================================== data_last_window Out[14]: y date 2005-08-01 1.006497 2005-09-01 1.094736 2005-10-01 1.027043 2005-11-01 1.149232 2005-12-01 1.160712 In\u00a0[15]: Copied! <pre># Predict with last_window\n# ==============================================================================\nforecaster.predict(steps=3, last_window=data_last_window['y'])\n</pre> # Predict with last_window # ============================================================================== forecaster.predict(steps=3, last_window=data_last_window['y']) Out[15]: <pre>2006-01-01    0.979303\n2006-02-01    0.760421\n2006-03-01    0.634806\nFreq: MS, Name: pred, dtype: float64</pre>"},{"location":"user_guides/forecaster-in-production.html#using-forecaster-models-in-production","title":"Using forecaster models in production\u00b6","text":"<p>When using a trained model in production, regular predictions need to be generated, for example, on a weekly basis every Monday. By default, the <code>predict</code> method on a trained forecaster object generates predictions starting right after the last training observation. Therefore, the model could be retrained weekly, just before the first prediction is needed, and its predict method called. However, this approach may not be practical due to reasons such as: expensive model training, unavailability of the training data history, or high prediction frequency.</p> <p>In such scenarios, the model must be able to predict at any time, even if it has not been recently trained. Fortunately, every model generated using skforecast has the <code>last_window</code> argument in its <code>predict</code> method. This argument allows providing only the past values needed to create the autoregressive predictors (lags or custom predictors), enabling prediction without the need to retrain the model. This feature is particularly useful when there are limitations in retraining the model regularly or when dealing with high-frequency predictions.</p>"},{"location":"user_guides/forecaster-in-production.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#predicting-with-last-window","title":"Predicting with last window\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#real-use-case","title":"Real Use Case\u00b6","text":"<p>The main advantage of using the <code>last_window</code> argument is that it can be used to predict at any time, even if the Forecaster has not been trained recently.</p> <p>Imagine a use case where a model is trained, stored, and 1 year later the company wants to use it to make some predictions.</p>"},{"location":"user_guides/forecaster-in-production.html#data","title":"Data\u00b6","text":"<p>A gap is created between the end of the training data and the last window data to simulate this behavior.</p>"},{"location":"user_guides/forecaster-in-production.html#forecaster-initial-train","title":"Forecaster initial train\u00b6","text":""},{"location":"user_guides/forecaster-in-production.html#future-predictions","title":"Future predictions\u00b6","text":"<p>The model is loaded to make new predictions.</p>"},{"location":"user_guides/forecasting-baseline.html","title":"Forecasting baseline","text":"<p>In forecasting modeling, a baseline serves as a basic, often simplistic model that acts as a fundamental reference for evaluating the performance of more complex models. It provides a baseline forecast using simple techniques that do not rely on sophisticated algorithms or extensive data analysis. Common examples of baseline strategies include:</p> <ul> <li><p>Last Observed Value: Uses the last observed value as a forecast for all future periods.</p> </li> <li><p>Moving Average: In this technique, the average of the last few observations is calculated and used as a forecast for the next period. For example, a 3-period moving average would use the average of the last three observations.</p> </li> <li><p>Last Equivalent Date (Seasonal Naive Forecasting): Extends the concept of the last observed value by considering the corresponding period in the previous season (e.g., the same working day from the previous week) as the forecast for the current period.</p> </li> </ul> <p>The primary goal of establishing a baseline is to provide a benchmark against which the performance of more advanced predictive models can be evaluated. If the model does not outperform the baseline, it may indicate that there is a fundamental problem with the approach or that the added complexity is not justified by the available data. This underscores the importance of carefully evaluating the appropriateness of complex models relative to the simplicity and effectiveness of baseline models.</p> <p>The <code>ForecasterEquivalentDate</code> class from <code>skforecast.recursive</code> allows the creation of a baseline forecast based on the concept of equivalent dates. In this context, an equivalent date is a historical date that has similar characteristics to the target date. The forecast for a given date is based on the value observed on the last n equivalent dates.</p> <p>The behavior of the forecast is primarily controlled by two arguments:</p> <ul> <li><p><code>offset</code>: This parameter determines how many steps back in time to go to find the most recent equivalent date for the target period. When given as an integer, <code>offset</code> represents the number of steps to go back in time. For example, if the frequency of the time series is daily, <code>offset = 7</code> means that the most recent data similar to the target period is the value observed 7 days ago. Additionally, it is possible to use Pandas DateOffsets to move forward a given number of valid dates. For example, <code>Bday(2)</code> can be used to move back two business days.</p> </li> <li><p><code>n_offsets</code>: This parameter determines the number of equivalent dates to use in the prediction. If <code>n_offsets</code> is greater than 1, the values at the equivalent dates are aggregated using the specified aggregation function, <code>agg_func</code>. For example, if the frequency of the time series is daily, <code>offset = 7</code>, <code>n_offsets = 2</code>, and <code>agg_func = np.mean</code>, the predicted value will be the mean of the values observed 7 and 14 days ago.</p> </li> </ul> <p> \u270e Note </p> <p><code>ForecasterEquivalentDate</code> is designed to integrate seamlessly with other functionality offered by skforecast, such as backtesting. This makes it easy to obtain the baseline for a given period.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterEquivalentDate\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.plot import plot_prediction_intervals\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterEquivalentDate from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster from skforecast.plot import set_dark_theme from skforecast.plot import plot_prediction_intervals In\u00a0[2]: Copied! <pre># Download data\n# ======================================================================================\ndata = fetch_dataset('fuel_consumption')\ndata = data.rename(columns={'Gasolinas': 'litters'})\ndata.index.name = 'date'\ndata = data.loc[:'1985-01-01 00:00:00', 'litters']\ndisplay(data.head(4))\n\n# Train-test dates\n# ======================================================================================\nend_train = '1980-01-01 23:59:59'\ndata_train = data.loc[:end_train]\ndata_test  = data.loc[end_train:]\n\nprint(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n\n# Plot\n# ======================================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\nax.set_title('Monthly fuel consumption in Spain')\nax.legend();\n</pre> # Download data # ====================================================================================== data = fetch_dataset('fuel_consumption') data = data.rename(columns={'Gasolinas': 'litters'}) data.index.name = 'date' data = data.loc[:'1985-01-01 00:00:00', 'litters'] display(data.head(4))  # Train-test dates # ====================================================================================== end_train = '1980-01-01 23:59:59' data_train = data.loc[:end_train] data_test  = data.loc[end_train:]  print(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")  # Plot # ====================================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') ax.set_title('Monthly fuel consumption in Spain') ax.legend(); <pre>fuel_consumption\n----------------\nMonthly fuel consumption in Spain from 1969-01-01 to 2022-08-01.\nObtained from Corporaci\u00f3n de Reservas Estrat\u00e9gicas de Productos Petrol\u00edferos and\nCorporaci\u00f3n de Derecho P\u00fablico tutelada por el Ministerio para la Transici\u00f3n\nEcol\u00f3gica y el Reto Demogr\u00e1fico. https://www.cores.es/es/estadisticas\nShape of the dataset: (644, 5)\n</pre> <pre>date\n1969-01-01    166875.2129\n1969-02-01    155466.8105\n1969-03-01    184983.6699\n1969-04-01    202319.8164\nFreq: MS, Name: litters, dtype: float64</pre> <pre>Train dates : 1969-01-01 00:00:00 --- 1980-01-01 00:00:00  (n=133)\nTest dates  : 1980-02-01 00:00:00 --- 1985-01-01 00:00:00  (n=60)\n</pre> In\u00a0[3]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterEquivalentDate(\n                 offset    = pd.DateOffset(months=12),\n                 n_offsets = 2,\n                 agg_func  = np.mean\n             )\n\nforecaster.fit(y=data_train)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterEquivalentDate(                  offset    = pd.DateOffset(months=12),                  n_offsets = 2,                  agg_func  = np.mean              )  forecaster.fit(y=data_train) forecaster Out[3]: ForecasterEquivalentDate General Information <ul> <li>Regressor: NoneType</li> <li>Offset: </li> <li>Number of offsets: 2</li> <li>Aggregation function: mean</li> <li>Window size: 24</li> <li>Creation date: 2025-08-06 13:47:41</li> <li>Last fit date: 2025-08-06 13:47:41</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1969-01-01 00:00:00'), Timestamp('1980-01-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[4]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=len(data_test))\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=len(data_test)) predictions.head(3) Out[4]: <pre>1980-02-01    385298.35315\n1980-03-01    472815.89325\n1980-04-01    462944.81705\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_train.plot(ax=ax, label='train')\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_train.plot(ax=ax, label='train') data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[6]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_squared_error(\n                y_true = data_test,\n                y_pred = predictions\n            )\n            \nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_squared_error(                 y_true = data_test,                 y_pred = predictions             )              print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 446931887.96547604\n</pre> In\u00a0[7]: Copied! <pre># Store in-sample residuals\n# ==============================================================================\nforecaster.set_in_sample_residuals(y=data_train)\n</pre> # Store in-sample residuals # ============================================================================== forecaster.set_in_sample_residuals(y=data_train) In\u00a0[8]: Copied! <pre># Prediction intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(\n    steps                   = len(data_test),\n    interval                = [10, 90],  # 80% prediction interval\n    method                  = 'conformal',\n    use_in_sample_residuals = True\n)\n\npredictions.head(4)\n</pre> # Prediction intervals # ============================================================================== predictions = forecaster.predict_interval(     steps                   = len(data_test),     interval                = [10, 90],  # 80% prediction interval     method                  = 'conformal',     use_in_sample_residuals = True )  predictions.head(4) Out[8]: pred lower_bound upper_bound 1980-02-01 385298.35315 334204.43875 436392.26755 1980-03-01 472815.89325 413163.96550 532467.82100 1980-04-01 462944.81705 403292.88930 522596.74480 1980-05-01 477889.17740 418237.24965 537541.10515 In\u00a0[9]: Copied! <pre># Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_test,\n    target_variable     = \"litters\",\n    title               = \"Predicted intervals\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.4, 'zorder': 1}\n)\n</pre> # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions         = predictions,     y_true              = data_test,     target_variable     = \"litters\",     title               = \"Predicted intervals\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.4, 'zorder': 1} ) In\u00a0[10]: Copied! <pre># Backtesting\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 15,\n         initial_train_size = len(data_train),\n         refit              = True\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data,\n                          cv         = cv,\n                          metric     = 'mean_absolute_error'\n                      )\n\nprint(\"Backtest error:\")\nmetric\n</pre> # Backtesting # ============================================================================== cv = TimeSeriesFold(          steps              = 15,          initial_train_size = len(data_train),          refit              = True      )  metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data,                           cv         = cv,                           metric     = 'mean_absolute_error'                       )  print(\"Backtest error:\") metric <pre>  0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Backtest error:\n</pre> Out[10]: mean_absolute_error 0 18575.076906 In\u00a0[11]: Copied! <pre># Backtesting predictions\n# ==============================================================================\npredictions.head(4)\n</pre> # Backtesting predictions # ============================================================================== predictions.head(4) Out[11]: pred 1980-02-01 385298.35315 1980-03-01 472815.89325 1980-04-01 462944.81705 1980-05-01 477889.17740 In\u00a0[12]: Copied! <pre># Plot backtesting predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(6, 3))\ndata_test.plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot backtesting predictions # ============================================================================== fig, ax = plt.subplots(figsize=(6, 3)) data_test.plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend();"},{"location":"user_guides/forecasting-baseline.html#baseline-forecaster","title":"Baseline forecaster\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#forecasterequivalentdate","title":"ForecasterEquivalentDate\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#create-and-train-forecaster","title":"Create and train forecaster\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/forecasting-baseline.html#probabilistic-forecasting","title":"Probabilistic forecasting\u00b6","text":"<p>Conformal prediction is a framework for constructing prediction intervals that are guaranteed to contain the true value with a specified probability (coverage probability). It works by combining the predictions of a point-forecasting model with its past residuals, differences between previous predictions and actual values. These residuals help estimate the uncertainty in the forecast and determine the width of the prediction interval that is then added to the point forecast.</p> <p>To learn more about conformal predictions in skforecast, visit the Probabilistic Forecasting: Conformal Prediction user guide.</p>"},{"location":"user_guides/forecasting-baseline.html#backtesting","title":"Backtesting\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html","title":"ARIMA and SARIMAX forecasting","text":"<p> \ud83d\udca1 Tip </p> <p>To learn more about modeling time series with ARIMA models, visit our example: ARIMA and SARIMAX models with Python.</p> <p> \u26a0 Warning </p> <p>The <code>pmdarima.ARIMA</code> regressor is no longer supported by the <code>ForecasterSarimax</code>. You can use the <code>skforecast.Sarimax.Sarimax</code> model or, to continue using it, use skforecast 0.13.0 or lower.  User guide: https://skforecast.org/0.13.0/user_guides/forecasting-sarimax-arima#forecastersarimax</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.sarimax import Sarimax\nfrom skforecast.recursive import ForecasterSarimax\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_sarimax, grid_search_sarimax\nfrom skforecast.plot import set_dark_theme\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Libraries # ============================================================================== import matplotlib.pyplot as plt from sklearn.metrics import mean_absolute_error from skforecast.datasets import fetch_dataset from skforecast.sarimax import Sarimax from skforecast.recursive import ForecasterSarimax from skforecast.model_selection import TimeSeriesFold, backtesting_sarimax, grid_search_sarimax from skforecast.plot import set_dark_theme from statsmodels.tsa.statespace.sarimax import SARIMAX import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(name=\"h2o_exog\")\ndata.index.name = 'datetime'\n</pre> # Download data # ============================================================================== data = fetch_dataset(name=\"h2o_exog\") data.index.name = 'datetime' <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[3]: Copied! <pre># Train-test dates\n# ==============================================================================\nend_train = '2005-06-01 23:59:59'\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"\n    f\"(n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.loc[:].index.max()}  \"\n    f\"(n={len(data.loc[end_train:])})\"\n)\ndata_train = data.loc[:end_train]\ndata_test  = data.loc[end_train:]\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.plot(ax=ax)\nax.legend();\n</pre> # Train-test dates # ============================================================================== end_train = '2005-06-01 23:59:59' print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"     f\"(n={len(data.loc[:end_train])})\" ) print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.loc[:].index.max()}  \"     f\"(n={len(data.loc[end_train:])})\" ) data_train = data.loc[:end_train] data_test  = data.loc[end_train:]  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) data.plot(ax=ax) ax.legend(); <pre>Train dates : 1992-04-01 00:00:00 --- 2005-06-01 00:00:00  (n=159)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> <p>The following section focus on how to train an ARIMA model and forecast future values with each of the three libraries.</p> <p>statsmodels</p> In\u00a0[4]: Copied! <pre># ARIMA model with statsmodels.Sarimax\n# ==============================================================================\narima = SARIMAX(endog = data_train['y'], order = (1, 1, 1))\narima_res = arima.fit(disp=0)\narima_res.summary()\n</pre> # ARIMA model with statsmodels.Sarimax # ============================================================================== arima = SARIMAX(endog = data_train['y'], order = (1, 1, 1)) arima_res = arima.fit(disp=0) arima_res.summary() Out[4]: SARIMAX Results Dep. Variable: y   No. Observations:   159 Model: SARIMAX(1, 1, 1)   Log Likelihood      89.934 Date: Tue, 12 Aug 2025   AIC                 -173.869 Time: 14:47:39   BIC                 -164.681 Sample: 04-01-1992   HQIC                -170.137 - 06-01-2005 Covariance Type: opg coef std err z P&gt;|z| [0.025 0.975] ar.L1     0.6316     0.143     4.420  0.000     0.352     0.912 ma.L1    -0.9534     0.054   -17.814  0.000    -1.058    -0.849 sigma2     0.0186     0.002     8.619  0.000     0.014     0.023 Ljung-Box (L1) (Q): 0.75   Jarque-Bera (JB):   167.05 Prob(Q): 0.39   Prob(JB):           0.00 Heteroskedasticity (H): 2.13   Skew:               -1.66 Prob(H) (two-sided): 0.01   Kurtosis:           6.78 Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).  In\u00a0[5]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = arima_res.get_forecast(steps=12)\npredictions.predicted_mean.head(4)\n</pre> # Prediction # ============================================================================== predictions = arima_res.get_forecast(steps=12) predictions.predicted_mean.head(4) Out[5]: <pre>2005-07-01    0.859455\n2005-08-01    0.870313\n2005-09-01    0.877172\n2005-10-01    0.881503\nFreq: MS, Name: predicted_mean, dtype: float64</pre> <p>skforecast</p> In\u00a0[6]: Copied! <pre># ARIMA model with skforecast.Sarimax\n# ==============================================================================\narima = Sarimax(order=(1, 1, 1))\narima.fit(y=data_train['y'])\narima.summary()\n</pre> # ARIMA model with skforecast.Sarimax # ============================================================================== arima = Sarimax(order=(1, 1, 1)) arima.fit(y=data_train['y']) arima.summary() Out[6]: SARIMAX Results Dep. Variable: y   No. Observations:   159 Model: SARIMAX(1, 1, 1)   Log Likelihood      89.934 Date: Tue, 12 Aug 2025   AIC                 -173.869 Time: 14:47:39   BIC                 -164.681 Sample: 04-01-1992   HQIC                -170.137 - 06-01-2005 Covariance Type: opg coef std err z P&gt;|z| [0.025 0.975] ar.L1     0.6316     0.143     4.420  0.000     0.352     0.912 ma.L1    -0.9534     0.054   -17.814  0.000    -1.058    -0.849 sigma2     0.0186     0.002     8.619  0.000     0.014     0.023 Ljung-Box (L1) (Q): 0.75   Jarque-Bera (JB):   167.05 Prob(Q): 0.39   Prob(JB):           0.00 Heteroskedasticity (H): 2.13   Skew:               -1.66 Prob(H) (two-sided): 0.01   Kurtosis:           6.78 Warnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).  In\u00a0[7]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = arima.predict(steps=12)\npredictions.head(4)\n</pre> # Prediction # ============================================================================== predictions = arima.predict(steps=12) predictions.head(4) Out[7]: pred 2005-07-01 0.859455 2005-08-01 0.870313 2005-09-01 0.877172 2005-10-01 0.881503 <p>The previous section introduced the construction of ARIMA-SARIMAX models using three different implementations. In order to seamlessly integrate these models with the various functionalities provided by skforecast, the next step is to encapsulate the skforecast <code>Sarimax</code> model within a <code>ForecasterSarimax</code> object. This encapsulation harmonizes the intricacies of the model and allows for the coherent use of skforecast's extensive capabilities.</p> In\u00a0[8]: Copied! <pre># Create and fit ForecasterSarimax\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(y=data_train['y'], suppress_warnings=True)\nforecaster\n</pre> # Create and fit ForecasterSarimax # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(y=data_train['y'], suppress_warnings=True) forecaster Out[8]: ForecasterSarimax General Information <ul> <li>Regressor: Sarimax</li> <li>Order: (12, 1, 1)</li> <li>Seasonal order: (0, 0, 0, 0)</li> <li>Trend: None</li> <li>Window size: 1</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Creation date: 2025-08-12 14:47:39</li> <li>Last fit date: 2025-08-12 14:47:44</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                          {'concentrate_scale': False, 'dates': None, 'disp': False,     'enforce_invertibility': True, 'enforce_stationarity': True, 'freq': None,     'hamilton_representation': False, 'maxiter': 200, 'measurement_error':     False, 'method': 'lbfgs', 'missing': 'none', 'mle_regression': True,     'order': (12, 1, 1), 'seasonal_order': (0, 0, 0, 0), 'simple_differencing':     False, 'sm_fit_kwargs': {}, 'sm_init_kwargs': {}, 'sm_predict_kwargs': {},     'start_params': None, 'time_varying_regression': False, 'trend': None,     'trend_offset': 1, 'use_exact_diffuse': False, 'validate_specification':     True}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[9]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=36)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=36) predictions.head(3) Out[9]: <pre>2005-07-01    0.957860\n2005-08-01    0.960092\n2005-09-01    1.108625\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[10]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(ax=ax, label='train')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['y'].plot(ax=ax, label='train') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[11]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_absolute_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_absolute_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.07125639765150053\n</pre> In\u00a0[12]: Copied! <pre># Predict intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(steps=36, alpha=0.05)\npredictions.head(3)\n</pre> # Predict intervals # ============================================================================== predictions = forecaster.predict_interval(steps=36, alpha=0.05) predictions.head(3) Out[12]: pred lower_bound upper_bound 2005-07-01 0.957860 0.857820 1.057900 2005-08-01 0.960092 0.853863 1.066320 2005-09-01 1.108625 0.998232 1.219018 <p> \ud83d\udca1 Tip </p> <p>To learn more about exogenous variables and how to correctly manage them with skforecast visit: Exogenous variables (features) user guide.</p> In\u00a0[13]: Copied! <pre># Create and fit ForecasterSarimax with exogenous variables\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(\n    y                 = data_train['y'], \n    exog              = data_train[['exog_1', 'exog_2']],\n    suppress_warnings = True\n)\n\n# Predict with exog\n# ==============================================================================\npredictions = forecaster.predict(\n                  steps = 36,\n                  exog  = data_test[['exog_1', 'exog_2']]\n              )\npredictions.head(3)\n</pre> # Create and fit ForecasterSarimax with exogenous variables # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(     y                 = data_train['y'],      exog              = data_train[['exog_1', 'exog_2']],     suppress_warnings = True )  # Predict with exog # ============================================================================== predictions = forecaster.predict(                   steps = 36,                   exog  = data_test[['exog_1', 'exog_2']]               ) predictions.head(3) Out[13]: <pre>2005-07-01    0.904512\n2005-08-01    0.932239\n2005-09-01    1.089341\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[14]: Copied! <pre># Split data Train - Last window - Test\n# ==============================================================================\nend_train = '2005-06-01 23:59:59'\nend_last_window = '2007-06-01 23:59:59'\n\nprint(\n    f\"Train dates       : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"\n    f\"(n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Last window dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_last_window].index.max()}  \"\n    f\"(n={len(data.loc[end_train:end_last_window])})\"\n)\nprint(\n    f\"Test dates        : {data.loc[end_last_window:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_last_window:])})\"\n)\ndata_train       = data.loc[:end_train]\ndata_last_window = data.loc[end_train:end_last_window]\ndata_test        = data.loc[end_last_window:]\n\n# Plot\n# ======================================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(ax=ax, label='train')\ndata_last_window['y'].plot(ax=ax, label='last window')\ndata_test['y'].plot(ax=ax, label='test')\nax.legend();\n</pre> # Split data Train - Last window - Test # ============================================================================== end_train = '2005-06-01 23:59:59' end_last_window = '2007-06-01 23:59:59'  print(     f\"Train dates       : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"     f\"(n={len(data.loc[:end_train])})\" ) print(     f\"Last window dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_last_window].index.max()}  \"     f\"(n={len(data.loc[end_train:end_last_window])})\" ) print(     f\"Test dates        : {data.loc[end_last_window:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_last_window:])})\" ) data_train       = data.loc[:end_train] data_last_window = data.loc[end_train:end_last_window] data_test        = data.loc[end_last_window:]  # Plot # ====================================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['y'].plot(ax=ax, label='train') data_last_window['y'].plot(ax=ax, label='last window') data_test['y'].plot(ax=ax, label='test') ax.legend(); <pre>Train dates       : 1992-04-01 00:00:00 --- 2005-06-01 00:00:00  (n=159)\nLast window dates : 2005-07-01 00:00:00 --- 2007-06-01 00:00:00  (n=24)\nTest dates        : 2007-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=12)\n</pre> <p>Since exogenous variables have been included in the Forecaster tuning, it is necessary to pass both past values and their future values to the <code>predict</code> method using the <code>last_window_exog</code> and <code>exog</code> parameters when making predictions.</p> In\u00a0[15]: Copied! <pre># Create and fit ForecasterSarimax with exogenous variables\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\nforecaster.fit(\n    y                 = data_train['y'], \n    exog              = data_train[['exog_1', 'exog_2']],\n    suppress_warnings = True\n)\n\n# Predict with exog and last window\n# ==============================================================================\npredictions = forecaster.predict(\n                  steps            = 12,\n                  exog             = data_test[['exog_1', 'exog_2']],\n                  last_window      = data_last_window['y'],\n                  last_window_exog = data_last_window[['exog_1', 'exog_2']]\n              )\npredictions.head(3)\n</pre> # Create and fit ForecasterSarimax with exogenous variables # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  forecaster.fit(     y                 = data_train['y'],      exog              = data_train[['exog_1', 'exog_2']],     suppress_warnings = True )  # Predict with exog and last window # ============================================================================== predictions = forecaster.predict(                   steps            = 12,                   exog             = data_test[['exog_1', 'exog_2']],                   last_window      = data_last_window['y'],                   last_window_exog = data_last_window[['exog_1', 'exog_2']]               ) predictions.head(3) Out[15]: <pre>2007-07-01    0.883929\n2007-08-01    1.041001\n2007-09-01    1.071707\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[16]: Copied! <pre># Prediction error\n# ==============================================================================\nerror_mse = mean_absolute_error(\n                y_true = data_test['y'],\n                y_pred = predictions\n            )\n\nprint(f\"Test error (mse): {error_mse}\")\n</pre> # Prediction error # ============================================================================== error_mse = mean_absolute_error(                 y_true = data_test['y'],                 y_pred = predictions             )  print(f\"Test error (mse): {error_mse}\") <pre>Test error (mse): 0.06267633382927788\n</pre> In\u00a0[17]: Copied! <pre># Plot predictions\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['y'].plot(ax=ax, label='train')\ndata_last_window['y'].plot(ax=ax, label='last window')\ndata_test['y'].plot(ax=ax, label='test')\npredictions.plot(ax=ax, label='predictions')\nax.legend();\n</pre> # Plot predictions # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_train['y'].plot(ax=ax, label='train') data_last_window['y'].plot(ax=ax, label='last window') data_test['y'].plot(ax=ax, label='test') predictions.plot(ax=ax, label='predictions') ax.legend(); In\u00a0[18]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[18]: feature importance 1 exog_2 1.520490 13 ar.L12 0.668892 15 sigma2 0.001597 12 ar.L11 -0.152130 2 ar.L1 -0.163030 6 ar.L5 -0.192486 4 ar.L3 -0.195244 10 ar.L9 -0.212033 7 ar.L6 -0.212089 9 ar.L8 -0.215658 11 ar.L10 -0.230837 3 ar.L2 -0.239330 8 ar.L7 -0.249651 5 ar.L4 -0.285973 0 exog_1 -0.536553 14 ma.L1 -0.977181 In\u00a0[19]: Copied! <pre># Backtest forecaster\n# ======================================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),\n             )\n\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data_train),\n         refit              = True,\n         fixed_train_size   = False,\n     )\n\nmetric, predictions = backtesting_sarimax(\n                          forecaster            = forecaster,\n                          y                     = data['y'],\n                          exog                  = data[['exog_1', 'exog_2']],\n                          cv                    = cv,\n                          metric                = 'mean_absolute_error',\n                          n_jobs                = 'auto',\n                          suppress_warnings_fit = True,\n                          verbose               = True,\n                          show_progress         = True\n                      )\n\nmetric\n</pre> # Backtest forecaster # ====================================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), seasonal_order=(0, 0, 0, 0), maxiter=200),              )  cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data_train),          refit              = True,          fixed_train_size   = False,      )  metric, predictions = backtesting_sarimax(                           forecaster            = forecaster,                           y                     = data['y'],                           exog                  = data[['exog_1', 'exog_2']],                           cv                    = cv,                           metric                = 'mean_absolute_error',                           n_jobs                = 'auto',                           suppress_warnings_fit = True,                           verbose               = True,                           show_progress         = True                       )  metric <pre>Information of folds\n--------------------\nNumber of observations used for initial training: 159\nNumber of observations used for backtesting: 36\n    Number of folds: 3\n    Number skipped folds: 0 \n    Number of steps per fold: 12\n    Number of steps to exclude between last observed data (last window) and predictions (gap): 0\n\nFold: 0\n    Training:   1992-04-01 00:00:00 -- 2005-06-01 00:00:00  (n=159)\n    Validation: 2005-07-01 00:00:00 -- 2006-06-01 00:00:00  (n=12)\nFold: 1\n    Training:   1992-04-01 00:00:00 -- 2006-06-01 00:00:00  (n=171)\n    Validation: 2006-07-01 00:00:00 -- 2007-06-01 00:00:00  (n=12)\nFold: 2\n    Training:   1992-04-01 00:00:00 -- 2007-06-01 00:00:00  (n=183)\n    Validation: 2007-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=12)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[19]: mean_absolute_error 0 0.056244 In\u00a0[20]: Copied! <pre># Backtest predictions\n# ======================================================================================\npredictions.head(4)\n</pre> # Backtest predictions # ====================================================================================== predictions.head(4) Out[20]: pred 2005-07-01 0.904512 2005-08-01 0.932239 2005-09-01 1.089341 2005-10-01 1.113679 <p> \ud83d\udca1 Tip </p> <p>In summary, while the statistical criteria approach offers speed and efficiency, validation techniques provide a more comprehensive and insightful evaluation, albeit at a slower pace due to their reliance on new data for testing. Fortunately, for sufficiently large data sets, they all lead to the same model.</p> <p> \u26a0 Warning </p> <p>When evaluating ARIMA-SARIMAX models, it is important to note that AIC assumes that all models are trained on the same data. Thus, using AIC to decide between different orders of differencing is technically invalid, since one data point is lost with each order of differencing. Therefore, the Auto Arima algorithm uses a unit root test to select the order of differencing, and only uses the AIC to select the order of the AR and MA components.</p> <p> \u270e Note </p> <p>For a detailed explanation of Akaike's Information Criterion (AIC) see Rob J Hyndman's blog and AIC Myths and Misunderstandings by Anderson and Burnham.</p> In\u00a0[21]: Copied! <pre># Train-validation-test data\n# ======================================================================================\nend_train = '2001-01-01 23:59:00'\nend_val = '2006-01-01 23:59:00'\n\nprint(f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\")\nprint(f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}  (n={len(data.loc[end_train:end_val])})\")\nprint(f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}  (n={len(data.loc[end_val:])})\")\n\n# Plot\n# ======================================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train, 'y'].plot(ax=ax, label='train')\ndata.loc[end_train:end_val, 'y'].plot(ax=ax, label='validation')\ndata.loc[end_val:, 'y'].plot(ax=ax, label='test')\nax.legend();\n</pre> # Train-validation-test data # ====================================================================================== end_train = '2001-01-01 23:59:00' end_val = '2006-01-01 23:59:00'  print(f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}  (n={len(data.loc[:end_train])})\") print(f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}  (n={len(data.loc[end_train:end_val])})\") print(f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}  (n={len(data.loc[end_val:])})\")  # Plot # ====================================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train, 'y'].plot(ax=ax, label='train') data.loc[end_train:end_val, 'y'].plot(ax=ax, label='validation') data.loc[end_val:, 'y'].plot(ax=ax, label='test') ax.legend(); <pre>Train dates      : 1992-04-01 00:00:00 --- 2001-01-01 00:00:00  (n=106)\nValidation dates : 2001-02-01 00:00:00 --- 2006-01-01 00:00:00  (n=60)\nTest dates       : 2006-02-01 00:00:00 --- 2008-06-01 00:00:00  (n=29)\n</pre> In\u00a0[22]: Copied! <pre># Grid search hyperparameter\n# ======================================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), maxiter=200)\n             )\n\nparam_grid = {\n    'order': [(12, 0, 0), (12, 1, 0), (12, 1, 1)],\n    'trend': [None, 'n', 'c']\n}\n\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data.loc[:end_train, 'y']),\n         refit              = False\n     )\n\nresults_grid = grid_search_sarimax(\n                   forecaster            = forecaster,\n                   y                     = data.loc[:end_val, 'y'],\n                   param_grid            = param_grid,\n                   cv                    = cv,\n                   metric                = 'mean_absolute_error',\n                   return_best           = True,\n                   n_jobs                = 'auto',\n                   suppress_warnings_fit = True,\n                   verbose               = False,\n                   show_progress         = True\n               )\n\nresults_grid.head(5)\n</pre> # Grid search hyperparameter # ====================================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), maxiter=200)              )  param_grid = {     'order': [(12, 0, 0), (12, 1, 0), (12, 1, 1)],     'trend': [None, 'n', 'c'] }  cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data.loc[:end_train, 'y']),          refit              = False      )  results_grid = grid_search_sarimax(                    forecaster            = forecaster,                    y                     = data.loc[:end_val, 'y'],                    param_grid            = param_grid,                    cv                    = cv,                    metric                = 'mean_absolute_error',                    return_best           = True,                    n_jobs                = 'auto',                    suppress_warnings_fit = True,                    verbose               = False,                    show_progress         = True                )  results_grid.head(5) <pre>Number of models compared: 9.\n</pre> <pre>params grid:   0%|          | 0/9 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found parameters, and the whole data set: \n  Parameters: {'order': (12, 1, 0), 'trend': 'c'}\n  Backtesting metric: 0.058789455086324986\n\n</pre> Out[22]: params mean_absolute_error order trend 0 {'order': (12, 1, 0), 'trend': 'c'} 0.058789 (12, 1, 0) c 1 {'order': (12, 1, 1), 'trend': 'c'} 0.058871 (12, 1, 1) c 2 {'order': (12, 1, 1), 'trend': 'n'} 0.059517 (12, 1, 1) n 3 {'order': (12, 1, 1), 'trend': None} 0.059517 (12, 1, 1) None 4 {'order': (12, 1, 0), 'trend': 'n'} 0.061720 (12, 1, 0) n <p>Since <code>return_best = True</code>, the Forecaster object is updated with the most optimal configuration found and trained with the entire dataset. This means that the grid search will yield the lowest error model with the best hyperparameters that lead to the highest performance metric. This last model can subsequently be utilized for forecasts on new data.</p> In\u00a0[23]: Copied! <pre>forecaster\n</pre> forecaster Out[23]: ForecasterSarimax General Information <ul> <li>Regressor: Sarimax</li> <li>Order: (12, 1, 0)</li> <li>Seasonal order: (0, 0, 0, 0)</li> <li>Trend: c</li> <li>Window size: 1</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Creation date: 2025-08-12 14:48:32</li> <li>Last fit date: 2025-08-12 14:49:16</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2006-01-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                          {'concentrate_scale': False, 'dates': None, 'disp': False,     'enforce_invertibility': True, 'enforce_stationarity': True, 'freq': None,     'hamilton_representation': False, 'maxiter': 200, 'measurement_error':     False, 'method': 'lbfgs', 'missing': 'none', 'mle_regression': True,     'order': (12, 1, 0), 'seasonal_order': (0, 0, 0, 0), 'simple_differencing':     False, 'sm_fit_kwargs': {}, 'sm_init_kwargs': {}, 'sm_predict_kwargs': {},     'start_params': None, 'time_varying_regression': False, 'trend': 'c',     'trend_offset': 1, 'use_exact_diffuse': False, 'validate_specification':     True}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[24]: Copied! <pre># Create and fit ForecasterSarimax (skforecast)\n# ==============================================================================\nforecaster = ForecasterSarimax(\n                 regressor=Sarimax(order=(12, 1, 1), maxiter=200),\n             )\nforecaster.fit(y=data_train['y'], suppress_warnings=True)\n\n# In-sample Predictions\n# ==============================================================================\nforecaster.regressor.sarimax_res.fittedvalues\n</pre> # Create and fit ForecasterSarimax (skforecast) # ============================================================================== forecaster = ForecasterSarimax(                  regressor=Sarimax(order=(12, 1, 1), maxiter=200),              ) forecaster.fit(y=data_train['y'], suppress_warnings=True)  # In-sample Predictions # ============================================================================== forecaster.regressor.sarimax_res.fittedvalues Out[24]: <pre>datetime\n1992-04-01    0.000000\n1992-05-01    0.379808\n1992-06-01    0.361364\n1992-07-01    0.413265\n1992-08-01    0.481084\n                ...   \n2005-02-01    0.739315\n2005-03-01    0.751082\n2005-04-01    0.716096\n2005-05-01    0.749931\n2005-06-01    0.817595\nFreq: MS, Length: 159, dtype: float64</pre>"},{"location":"user_guides/forecasting-sarimax-arima.html#arima-and-sarimax","title":"ARIMA and SARIMAX\u00b6","text":"<p>ARIMA (AutoRegressive Integrated Moving Average) and SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors) are prominent and widely used statistical forecasting models. While ARIMA models are more widely known, SARIMAX models extend the ARIMA framework by seamlessly integrating seasonal patterns and exogenous variables.</p> <p>In the ARIMA-SARIMAX model notation, the parameters $p$, $d$, and $q$ represent the autoregressive, differencing, and moving-average components, respectively. $P$, $D$, and $Q$ denote the same components for the seasonal part of the model, with $m$ representing the number of periods in each season.</p> <ul> <li><p>$p$ is the order (number of time lags) of the autoregressive part of the model.</p> </li> <li><p>$d$ is the degree of differencing (the number of times that past values have been subtracted from the data).</p> </li> <li><p>$q$ is the order of the moving average part of the model.</p> </li> <li><p>$P$ is the order (number of time lags) of the seasonal part of the model.</p> </li> <li><p>$D$ is the degree of differencing (the number of times the data have had past values subtracted) of the seasonal part of the model.</p> </li> <li><p>$Q$ is the order of the moving average of the seasonal part of the model.</p> </li> <li><p>$m$ refers to the number of periods in each season.</p> </li> </ul> <p>When the terms $P$, $D$, $Q$, and $m$ are zero and no exogenous variables are included in the model, the SARIMAX model is equivalent to an ARIMA.</p> <p>When two out of the three terms are zero, the model can be referred to based on the non-zero parameter, dropping \"AR\", \"I\" or \"MA\" from the acronym describing the model. For example, $ARIMA(1,0,0)$ is $AR(1)$, $ARIMA(0,1,0)$ is $I(1)$, and $ARIMA(0,0,1)$ is $MA(1)$.</p> <p>ARIMA implementations</p> <p>Several Python libraries implement ARIMA-SARIMAX models. Three of them are:</p> <ul> <li><p>statsmodels: this is one of the most complete libraries for statistical modeling. While the functional paradigm may be intuitive for those coming from the R environment, those accustomed to the object-oriented API of scikit-learn may need a short period of adaptation.</p> </li> <li><p>pmdarima: This is a wrapper for <code>statsmodels SARIMAX</code>. Its distinguishing feature is its seamless integration with the scikit-learn API, allowing users familiar with scikit-learn's conventions to seamlessly dive into time series modeling.</p> </li> <li><p>skforecast: a novel wrapper for <code>statsmodels SARIMAX</code> that also follows the scikit-learn API. This implementation is very similar to that of pmdarima, but has been streamlined to include only the essential elements for skforecast, resulting in significant speed improvements.</p> </li> </ul> <p>ForecasterSarimax</p> <p>The <code>ForecasterSarimax</code> class allows training and validation of ARIMA and SARIMAX models using the skforecast API. <code>ForecasterSarimax</code> is compatible with the <code>Sarimax</code> from skforecast implementation, a novel wrapper for statsmodels SARIMAX that also follows the sklearn API. This implementation is very similar to pmdarima, but has been streamlined to include only the essential elements for skforecast, resulting in significant speed improvements.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#statsmodels-and-skforecast","title":"Statsmodels and skforecast\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#forecastersarimax","title":"ForecasterSarimax\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#training","title":"Training\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#prediction","title":"Prediction\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#interval-prediction","title":"Interval prediction\u00b6","text":"<p>Either <code>alpha</code> or <code>interval</code> can be used to indicate the confidence of the estimated prediction interval.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#exogenous-variables","title":"Exogenous variables\u00b6","text":"<p>The addition of exogenous variables is done using the <code>exog</code> argument. The only requirement for including an exogenous variable is the need to know the value of the variable also during the forecast period.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#using-an-already-trained-arima","title":"Using an already trained ARIMA\u00b6","text":"<p>Forecasting with an ARIMA model becomes challenging when the forecast horizon data does not immediately follow the last observed value during the training phase. This complexity is due to the moving average (MA) component, which relies on past forecast errors as predictors. Thus, to predict at time 't', the error of the 't-1' prediction becomes a necessity. In situations where this prediction isn't available, the corresponding error remains unavailable.</p> <p>For this reason, in most cases, ARIMA models are retrained each time predictions need to be made. Despite considerable efforts and advances to speed up the training process for these models, it is not always feasible to retrain the model between predictions, either due to time constraints or insufficient computational resources for repeated access to historical data. An intermediate approach is to feed the model with data from the last training observation to the start of the prediction phase. This technique enables the estimation of intermediate predictions and, as a result, the necessary errors.</p> <p>For example, imagine a situation where a model was trained 20 days ago with daily data from the past three years. When generating new predictions, only the 20 most recent values would be needed, rather than the complete historical dataset (365 * 3 + 20).</p> <p>Integrating new data into the model can be complex, but the <code>ForecasterSarimax</code> class simplifies this considerably by automating the process through the <code>last_window</code> argument in its <code>predict</code> method.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#feature-importances","title":"Feature importances\u00b6","text":"<p>Returns the parameters of the model.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#backtesting","title":"Backtesting\u00b6","text":"<p>SARIMAX models can be evaluated using any of the backtesting strategies implemented in skforecast.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#model-tunning","title":"Model tunning\u00b6","text":"<p>To find the optimal hyperparameters for the SARIMAX model, the use of strategic search methods is essential. Among these methods, two widely used approaches are:</p> <ul> <li><p>Statistical Criteria: Information criterion metrics, such as Akaike's Information Criterion (AIC) or Bayesian Information Criterion (BIC), use different penalties on the maximum likelihood (log-likelihood) estimate of the model as a measure of fit. The advantage of using such criteria is that they are computed only on the training data, eliminating the need for predictions on new data. As a result, the optimization process is greatly accelerated. The well-known Auto Arima algorithm uses this approach.</p> </li> <li><p>Validation Techniques: The use of validation techniques, especially backtesting, is another effective strategy. Backtesting involves evaluating the performance of the model using historical data to simulate real-world conditions. This helps to validate the effectiveness of the hyperparameters under different scenarios, providing a practical assessment of their viability.</p> </li> </ul> <p>In the first approach, calculations are based solely on training data, eliminating the need for predictions on new data. This makes the optimization process very fast. However, it is important to note that information criteria metrics only measure the relative quality of models. This means that all tested models could still be poor fits. Therefore, the final selected model must undergo a backtesting phase. This phase calculates a metric (such as MAE, MSE, MAPE, etc.) that validates its performance on a meaningful scale.</p> <p>On the other hand, the second approach - validation techniques - tends to be more time-consuming, since the model must be trained and then evaluated on new data. However, the results generated are often more robust, and the metrics derived can provide deeper insights.</p>"},{"location":"user_guides/forecasting-sarimax-arima.html#grid-search-with-backtesting","title":"Grid search with backtesting\u00b6","text":""},{"location":"user_guides/forecasting-sarimax-arima.html#prediction-on-training-data-in-sample-predictions","title":"Prediction on training data (In-sample Predictions)\u00b6","text":"<p>Predictions on the training data are crucial for evaluating the accuracy and effectiveness of the model. By comparing the predicted values wtih the actual observed values in the training dataset, you can assess how well the model has learned the underlying patterns and trends in the data. This comparison helps in understanding the model's performance and identify areas where it may need improvement or adjustment. In essence, they act as a mirror, reflecting how the model interprets and reconstructs the historical data on which it was trained.</p> <p>The predictions of the fitted values are stored in the <code>fittedvalues</code> attribute of the <code>SARIMAXResults</code> object. This object is stored within the <code>sarimax_res</code> attribute of the skforecast <code>Sarimax</code> model:</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html","title":"Deep learning Recurrent Neural Networks","text":"<p>Recurrent Neural Networks (RNN) are a family of models specifically designed to work with sequential data, such as time series. Unlike traditional feedforward neural networks, which treat each input independently, RNNs introduce an internal memory that allows them to capture dependencies between elements of a sequence. This enables the model to leverage information from previous steps to improve future predictions.</p> <p>The fundamental building block of an RNN is the recurrent cell, which receives two inputs at each time step: the current data point and the previous hidden state (the \"memory\" of the network). At every step, the hidden state is updated, storing relevant information about the sequence up to that point. This architecture allows RNNs to \u201cremember\u201d trends and patterns over time.</p> <p>However, simple RNNs face difficulties when learning long-term dependencies due to issues like the vanishing or exploding gradient problem. To address these limitations, more advanced architectures such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) were developed. These variants are better at capturing complex and long-range patterns in time series data.</p> <p> Basic RNN diagram. Source: James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning (1st ed.) [PDF]. Springer. </p> <p>Types of Recurrent Layers in skforecast</p> <p>With skforecast, you can use three main types of recurrent cells:</p> <ul> <li><p>Simple RNN: Suitable for problems with short-term dependencies or when a simple model is sufficient. Less effective for capturing long-range patterns.</p> </li> <li><p>LSTM (Long Short-Term Memory): Adds gating mechanisms that allow the network to learn and retain information over longer periods. LSTMs are a popular choice for many complex forecasting problems.</p> </li> <li><p>GRU (Gated Recurrent Unit): Offers a simpler structure than LSTM, using fewer parameters while achieving comparable performance in many scenarios. Useful when computational efficiency is important.</p> </li> </ul> <p> \u270e Note </p> <p>Guidelines for choosing a recurrent layer:</p> <ul> <li>     Use LSTM if your time series contains long-term patterns or complex dependencies.     </li> <li>     Try GRU as a lighter alternative to LSTM.     </li> <li>     Use Simple RNN only for straightforward tasks or as a baseline.     </li> </ul> <p> \ud83d\udca1 Tip </p> <p>To learn more about forecasting with deep learning  models visit our examples:</p> <ul> <li> Deep Learning for time series forecasting: Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM).     </li> </ul> <p> \u26a0 Warning </p> <p>skforecast supports multiple Keras backends: TensorFlow, JAX, and PyTorch (torch). You can select the backend using the <code>KERAS_BACKEND</code> environment variable, or by editing your local configuration file at <code>~/.keras/keras.json</code>.</p> <pre>import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # Options: \"tensorflow\", \"jax\", or \"torch\"\nimport keras\n</pre> <p>The backend must be set before importing Keras in your Python session. Once Keras is imported, the backend cannot be changed without restarting your Python process.</p> <p>Alternatively, you can set the backend in your configuration file at <code>~/.keras/keras.json</code>:</p> <pre>{\n    \"backend\": \"tensorflow\"  # Options: \"tensorflow\", \"jax\", or \"torch\"\n}\n</pre> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # 'tensorflow', 'jax\u00b4 or 'torch'\nimport keras\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom feature_engine.datetime import DatetimeFeatures\nfrom feature_engine.creation import CyclicalFeatures\n\nimport skforecast\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.deep_learning import create_and_compile_model\nfrom skforecast.deep_learning import ForecasterRnn\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster_multiseries\nfrom skforecast.plot import plot_prediction_intervals\n\nfrom keras.optimizers import Adam\nfrom keras.losses import MeanSquaredError\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nprint(f\"skforecast version: {skforecast.__version__}\")\nprint(f\"keras version: {keras.__version__}\")\nprint(f\"Using backend: {keras.backend.backend()}\")\nif keras.backend.backend() == \"tensorflow\":\n    import tensorflow as tf\n    print(f\"tensorflow version: {tf.__version__}\")\nelif keras.backend.backend() == \"torch\":\n    import torch\n    print(f\"torch version: {torch.__version__}\")\nelse:\n    print(\"Backend not recognized. Please use 'tensorflow' or 'torch'.\")\n</pre> # Libraries # ============================================================================== import os os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # 'tensorflow', 'jax\u00b4 or 'torch' import keras  import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import make_pipeline from feature_engine.datetime import DatetimeFeatures from feature_engine.creation import CyclicalFeatures  import skforecast from skforecast.plot import set_dark_theme from skforecast.datasets import fetch_dataset from skforecast.deep_learning import create_and_compile_model from skforecast.deep_learning import ForecasterRnn from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster_multiseries from skforecast.plot import plot_prediction_intervals  from keras.optimizers import Adam from keras.losses import MeanSquaredError from keras.callbacks import EarlyStopping, ReduceLROnPlateau  import warnings warnings.filterwarnings('ignore', category=DeprecationWarning)  print(f\"skforecast version: {skforecast.__version__}\") print(f\"keras version: {keras.__version__}\") print(f\"Using backend: {keras.backend.backend()}\") if keras.backend.backend() == \"tensorflow\":     import tensorflow as tf     print(f\"tensorflow version: {tf.__version__}\") elif keras.backend.backend() == \"torch\":     import torch     print(f\"torch version: {torch.__version__}\") else:     print(\"Backend not recognized. Please use 'tensorflow' or 'torch'.\") <pre>skforecast version: 0.17.0\nkeras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n</pre> In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name=\"air_quality_valencia_no_missing\")\ndata.head()\n</pre> # Data download # ============================================================================== data = fetch_dataset(name=\"air_quality_valencia_no_missing\") data.head() <pre>air_quality_valencia_no_missing\n-------------------------------\nHourly measures of several air chemical pollutant at Valencia city (Avd.\nFrancia) from 2019-01-01 to 20213-12-31. Including the following variables:\npm2.5 (\u00b5g/m\u00b3), CO (mg/m\u00b3), NO (\u00b5g/m\u00b3), NO2 (\u00b5g/m\u00b3), PM10 (\u00b5g/m\u00b3), NOx (\u00b5g/m\u00b3),\nO3 (\u00b5g/m\u00b3), Veloc. (m/s), Direc. (degrees), SO2 (\u00b5g/m\u00b3). Missing values have\nbeen imputed using linear interpolation.\nRed de Vigilancia y Control de la Contaminaci\u00f3n Atmosf\u00e9rica, 46250047-Val\u00e8ncia -\nAv. Fran\u00e7a, https://mediambient.gva.es/es/web/calidad-ambiental/datos-\nhistoricos.\nShape of the dataset: (43824, 10)\n</pre> Out[2]: so2 co no no2 pm10 nox o3 veloc. direc. pm2.5 datetime 2019-01-01 00:00:00 8.0 0.2 3.0 36.0 22.0 40.0 16.0 0.5 262.0 19.0 2019-01-01 01:00:00 8.0 0.1 2.0 40.0 32.0 44.0 6.0 0.6 248.0 26.0 2019-01-01 02:00:00 8.0 0.1 11.0 42.0 36.0 58.0 3.0 0.3 224.0 31.0 2019-01-01 03:00:00 10.0 0.1 15.0 41.0 35.0 63.0 3.0 0.2 220.0 30.0 2019-01-01 04:00:00 11.0 0.1 16.0 39.0 36.0 63.0 3.0 0.4 221.0 30.0 In\u00a0[3]: Copied! <pre># Checking the frequency of the time series\n# ==============================================================================\nprint(f\"Index     : {data.index.dtype}\")\nprint(f\"Frequency : {data.index.freqstr}\")\n</pre> # Checking the frequency of the time series # ============================================================================== print(f\"Index     : {data.index.dtype}\") print(f\"Frequency : {data.index.freqstr}\") <pre>Index     : datetime64[ns]\nFrequency : h\n</pre> In\u00a0[4]: Copied! <pre># Split train-validation-test\n# ==============================================================================\ndata = data.loc[\"2019-01-01 00:00:00\":\"2021-12-31 23:59:59\", :].copy()\n\nend_train = \"2021-03-31 23:59:00\"\nend_validation = \"2021-09-30 23:59:00\"\ndata_train = data.loc[:end_train, :].copy()\ndata_val = data.loc[end_train:end_validation, :].copy()\ndata_test = data.loc[end_validation:, :].copy()\n\nprint(\n    f\"Dates train      : {data_train.index.min()} --- \" \n    f\"{data_train.index.max()}  (n={len(data_train)})\"\n)\nprint(\n    f\"Dates validation : {data_val.index.min()} --- \" \n    f\"{data_val.index.max()}  (n={len(data_val)})\"\n)\nprint(\n    f\"Dates test       : {data_test.index.min()} --- \" \n    f\"{data_test.index.max()}  (n={len(data_test)})\"\n)\n</pre> # Split train-validation-test # ============================================================================== data = data.loc[\"2019-01-01 00:00:00\":\"2021-12-31 23:59:59\", :].copy()  end_train = \"2021-03-31 23:59:00\" end_validation = \"2021-09-30 23:59:00\" data_train = data.loc[:end_train, :].copy() data_val = data.loc[end_train:end_validation, :].copy() data_test = data.loc[end_validation:, :].copy()  print(     f\"Dates train      : {data_train.index.min()} --- \"      f\"{data_train.index.max()}  (n={len(data_train)})\" ) print(     f\"Dates validation : {data_val.index.min()} --- \"      f\"{data_val.index.max()}  (n={len(data_val)})\" ) print(     f\"Dates test       : {data_test.index.min()} --- \"      f\"{data_test.index.max()}  (n={len(data_test)})\" ) <pre>Dates train      : 2019-01-01 00:00:00 --- 2021-03-31 23:00:00  (n=19704)\nDates validation : 2021-04-01 00:00:00 --- 2021-09-30 23:00:00  (n=4392)\nDates test       : 2021-10-01 00:00:00 --- 2021-12-31 23:00:00  (n=2208)\n</pre> In\u00a0[5]: Copied! <pre># Plot series\n# ==============================================================================\nset_dark_theme()\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color'] * 2\nfig, axes = plt.subplots(len(data.columns), 1, figsize=(8, 8), sharex=True)\n\nfor i, col in enumerate(data.columns):\n    axes[i].plot(data[col], label=col, color=colors[i])\n    axes[i].legend(loc='upper right', fontsize=8)\n    axes[i].tick_params(axis='both', labelsize=8)\n    axes[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train\n    axes[i].axvline(pd.to_datetime(end_validation), color='white', linestyle='--', linewidth=1)  # End validation\n\nfig.suptitle(\"Air Quality Valencia\", fontsize=16)\nplt.tight_layout()\n</pre> # Plot series # ============================================================================== set_dark_theme() colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] * 2 fig, axes = plt.subplots(len(data.columns), 1, figsize=(8, 8), sharex=True)  for i, col in enumerate(data.columns):     axes[i].plot(data[col], label=col, color=colors[i])     axes[i].legend(loc='upper right', fontsize=8)     axes[i].tick_params(axis='both', labelsize=8)     axes[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train     axes[i].axvline(pd.to_datetime(end_validation), color='white', linestyle='--', linewidth=1)  # End validation  fig.suptitle(\"Air Quality Valencia\", fontsize=16) plt.tight_layout() In\u00a0[6]: Copied! <pre># Basic usage of `create_and_compile_model`\n# ==============================================================================\nmodel = create_and_compile_model(\n            series          = data,    # All 10 series are used as predictors\n            levels          = [\"o3\"],  # Target series to predict\n            lags            = 32,      # Number of lags to use as predictors\n            steps           = 24,      # Number of steps to predict\n            recurrent_layer = \"LSTM\",  # Type of recurrent layer ('LSTM', 'GRU', or 'RNN')\n            recurrent_units = 100,     # Number of units in the recurrent layer\n            dense_units     = 64       # Number of units in the dense layer\n        )\n\nmodel.summary()\n</pre> # Basic usage of `create_and_compile_model` # ============================================================================== model = create_and_compile_model(             series          = data,    # All 10 series are used as predictors             levels          = [\"o3\"],  # Target series to predict             lags            = 32,      # Number of lags to use as predictors             steps           = 24,      # Number of steps to predict             recurrent_layer = \"LSTM\",  # Type of recurrent layer ('LSTM', 'GRU', or 'RNN')             recurrent_units = 100,     # Number of units in the recurrent layer             dense_units     = 64       # Number of units in the dense layer         )  model.summary() <pre>keras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"functional\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input (InputLayer)       \u2502 (None, 32, 10)         \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_1 (LSTM)                   \u2502 (None, 100)            \u2502        44,400 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 64)             \u2502         6,464 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_layer (Dense)   \u2502 (None, 24)             \u2502         1,560 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape (Reshape)               \u2502 (None, 24, 1)          \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 52,424 (204.78 KB)\n</pre> <pre> Trainable params: 52,424 (204.78 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <p>Advanced customization</p> <p>All arguments controlling layer types, units, activations, and other options can be customized. You may also pass your own Keras model if you need full flexibility beyond what the helper function provides.</p> <p>The arguments <code>recurrent_layers_kwargs</code> and <code>dense_layers_kwargs</code> allow you to specify the parameters for the recurrent and dense layers, respectively.</p> <ul> <li><p>When using a dictionary, the kwargs are replayed for each layer of the same type. For example, if you specify <code>recurrent_layers_kwargs = {'activation': 'tanh'}</code>, all recurrent layers will use the <code>tanh</code> activation function.</p> </li> <li><p>You can also pass a list of dictionaries to specify different parameters for each layer. For instance, <code>recurrent_layers_kwargs = [{'activation': 'tanh'}, {'activation': 'relu'}]</code> will specify that the first recurrent layer uses the <code>tanh</code> activation function and the second uses <code>relu</code>.</p> </li> </ul> In\u00a0[7]: Copied! <pre># Advance usage of `create_and_compile_model`\n# ==============================================================================\nmodel = create_and_compile_model(\n    series                    = data,\n    levels                    = [\"o3\"], \n    lags                      = 32,\n    steps                     = 24,\n    exog                      = None,  # No exogenous variables\n    recurrent_layer           = \"LSTM\",    \n    recurrent_units           = [128, 64],  \n    recurrent_layers_kwargs   = [{'activation': 'tanh'}, {'activation': 'relu'}],\n    dense_units               = [128, 64],\n    dense_layers_kwargs       = {'activation': 'relu'},\n    output_dense_layer_kwargs = {'activation': 'linear'},\n    compile_kwargs            = {'optimizer': Adam(learning_rate=0.001), 'loss': MeanSquaredError()},\n    model_name                = None\n)\n\nmodel.summary()\n</pre> # Advance usage of `create_and_compile_model` # ============================================================================== model = create_and_compile_model(     series                    = data,     levels                    = [\"o3\"],      lags                      = 32,     steps                     = 24,     exog                      = None,  # No exogenous variables     recurrent_layer           = \"LSTM\",         recurrent_units           = [128, 64],       recurrent_layers_kwargs   = [{'activation': 'tanh'}, {'activation': 'relu'}],     dense_units               = [128, 64],     dense_layers_kwargs       = {'activation': 'relu'},     output_dense_layer_kwargs = {'activation': 'linear'},     compile_kwargs            = {'optimizer': Adam(learning_rate=0.001), 'loss': MeanSquaredError()},     model_name                = None )  model.summary() <pre>keras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"functional_1\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input (InputLayer)       \u2502 (None, 32, 10)         \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_1 (LSTM)                   \u2502 (None, 32, 128)        \u2502        71,168 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_2 (LSTM)                   \u2502 (None, 64)             \u2502        49,408 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 128)            \u2502         8,320 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_2 (Dense)                 \u2502 (None, 64)             \u2502         8,256 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_layer (Dense)   \u2502 (None, 24)             \u2502         1,560 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape (Reshape)               \u2502 (None, 24, 1)          \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 138,712 (541.84 KB)\n</pre> <pre> Trainable params: 138,712 (541.84 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <p>To gain a deeper understanding of this function, refer to a later section of this guide: Understanding <code>create_and_compile_model</code> in depth.</p> <p>If you need to define a completely custom architecture, you can create your own Keras model and use it directly in skforecast workflows.</p> In\u00a0[8]: Copied! <pre># Plotting the model architecture (require `pydot` and `graphviz`)\n# ==============================================================================\n# from keras.utils import plot_model\n# plot_model(model, show_shapes=True, show_layer_names=True, to_file='model-architecture.png')\n</pre> # Plotting the model architecture (require `pydot` and `graphviz`) # ============================================================================== # from keras.utils import plot_model # plot_model(model, show_shapes=True, show_layer_names=True, to_file='model-architecture.png') <p> </p> In\u00a0[9]: Copied! <pre># Create model\n# ==============================================================================\nlags = 24\n\nmodel = create_and_compile_model(\n    series                  = data[[\"o3\"]],  # Only the 'o3' series is used as predictor\n    levels                  = [\"o3\"],        # Target series to predict\n    lags                    = lags,          # Number of lags to use as predictors\n    steps                   = 1,             # Single-step forecasting\n    recurrent_layer         = \"GRU\",\n    recurrent_units         = 64,\n    recurrent_layers_kwargs = {\"activation\": \"tanh\"},\n    dense_units             = 32,\n    compile_kwargs          = {'optimizer': Adam(), 'loss': MeanSquaredError()},\n    model_name              = \"Single-Series-Single-Step\" \n)\n\nmodel.summary()\n</pre> # Create model # ============================================================================== lags = 24  model = create_and_compile_model(     series                  = data[[\"o3\"]],  # Only the 'o3' series is used as predictor     levels                  = [\"o3\"],        # Target series to predict     lags                    = lags,          # Number of lags to use as predictors     steps                   = 1,             # Single-step forecasting     recurrent_layer         = \"GRU\",     recurrent_units         = 64,     recurrent_layers_kwargs = {\"activation\": \"tanh\"},     dense_units             = 32,     compile_kwargs          = {'optimizer': Adam(), 'loss': MeanSquaredError()},     model_name              = \"Single-Series-Single-Step\"  )  model.summary() <pre>keras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"Single-Series-Single-Step\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input (InputLayer)       \u2502 (None, 24, 1)          \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gru_1 (GRU)                     \u2502 (None, 64)             \u2502        12,864 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 32)             \u2502         2,080 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_layer (Dense)   \u2502 (None, 1)              \u2502            33 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape (Reshape)               \u2502 (None, 1, 1)           \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 14,977 (58.50 KB)\n</pre> <pre> Trainable params: 14,977 (58.50 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[10]: Copied! <pre># Forecaster Definition\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=[\"o3\"],\n    lags=lags,  # Must be same lags as used in create_and_compile_model\n    transformer_series=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 25,       # Number of epochs to train the model.\n        \"batch_size\": 512,  # Batch size to train the model.\n        \"callbacks\": [\n            EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n        ],  # Callback to stop training when it is no longer learning.\n        \"series_val\": data_val,  # Validation data for model training.\n    },\n)\n\n# Fit forecaster\n# ==============================================================================\nforecaster.fit(data_train[['o3']])\nforecaster\n</pre> # Forecaster Definition # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=[\"o3\"],     lags=lags,  # Must be same lags as used in create_and_compile_model     transformer_series=MinMaxScaler(),     fit_kwargs={         \"epochs\": 25,       # Number of epochs to train the model.         \"batch_size\": 512,  # Batch size to train the model.         \"callbacks\": [             EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)         ],  # Callback to stop training when it is no longer learning.         \"series_val\": data_val,  # Validation data for model training.     }, )  # Fit forecaster # ============================================================================== forecaster.fit(data_train[['o3']]) forecaster <pre>Epoch 1/25\n</pre> <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 16 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n</pre> <pre>39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 33ms/step - loss: 0.0720 - val_loss: 0.0117\nEpoch 2/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 29ms/step - loss: 0.0116 - val_loss: 0.0088\nEpoch 3/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 29ms/step - loss: 0.0084 - val_loss: 0.0067\nEpoch 4/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 28ms/step - loss: 0.0066 - val_loss: 0.0058\nEpoch 5/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 29ms/step - loss: 0.0058 - val_loss: 0.0057\nEpoch 6/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 29ms/step - loss: 0.0055 - val_loss: 0.0055\nEpoch 7/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 29ms/step - loss: 0.0053 - val_loss: 0.0056\nEpoch 8/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 29ms/step - loss: 0.0054 - val_loss: 0.0055\nEpoch 9/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 29ms/step - loss: 0.0052 - val_loss: 0.0054\nEpoch 10/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 32ms/step - loss: 0.0053 - val_loss: 0.0054\nEpoch 11/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 34ms/step - loss: 0.0055 - val_loss: 0.0054\nEpoch 12/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 33ms/step - loss: 0.0053 - val_loss: 0.0054\n</pre> Out[10]: ForecasterRnn General Information <ul> <li>Regressor: Functional</li> <li>Layers names: ['series_input', 'gru_1', 'dense_1', 'output_dense_td_layer', 'reshape']</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]</li> <li>Window size: 24</li> <li>Maximum steps to predict: [1]</li> <li>Exogenous included: False</li> <li>Creation date: 2025-08-20 11:24:33</li> <li>Last fit date: 2025-08-20 11:24:48</li> <li>Keras backend: tensorflow</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: MinMaxScaler()</li> <li>Transformer for exog: MinMaxScaler()</li> </ul> Training Information <ul> <li>Series names: o3</li> <li>Target series (levels): ['o3']</li> <li>Training range: [Timestamp('2019-01-01 00:00:00'), Timestamp('2021-03-31 23:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: h</li> </ul> Regressor Parameters <ul>                     {'name': 'Single-Series-Single-Step', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 24, 1), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'series_input'}, 'registered_name': None, 'name': 'series_input', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': False, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'Orthogonal', 'config': {'seed': None, 'gain': 1.0}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 24, 1]}, 'name': 'gru_1', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 24, 1), 'dtype': 'float32', 'keras_history': ['series_input', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 64]}, 'name': 'dense_1', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 64), 'dtype': 'float32', 'keras_history': ['gru_1', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'output_dense_td_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 32]}, 'name': 'output_dense_td_layer', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32), 'dtype': 'float32', 'keras_history': ['dense_1', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Reshape', 'config': {'name': 'reshape', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'target_shape': (1, 1)}, 'registered_name': None, 'build_config': {'input_shape': [None, 1]}, 'name': 'reshape', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 1), 'dtype': 'float32', 'keras_history': ['output_dense_td_layer', 0, 0]}},), 'kwargs': {}}]}], 'input_layers': [['series_input', 0, 0]], 'output_layers': [['reshape', 0, 0]]}                 </ul> Compile Parameters <ul>                     {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.0010000000474974513, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': None, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}                 </ul> Fit Kwargs <ul>                     {'epochs': 25, 'batch_size': 512, 'callbacks': []}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p> \u270e Note </p> <p>The skforecast library is fully compatible with GPUs. See the Running on GPU section below in this document for more information.</p> <p>In deep learning models, it\u2019s important to control overfitting, when a model performs well on training data but poorly on new, unseen data. One common approach is to use a Keras callback, such as <code>EarlyStopping</code>, which halts training if the validation loss stops improving.</p> <p>Another useful practice is to plot the training and validation loss after each epoch. This helps you visualize how the model is learning and spot signs of overfitting.</p> <p> Graphical explanation of overfitting. Source: https://datahacker.rs/018-pytorch-popular-techniques-to-prevent-the-overfitting-in-a-neural-networks/. </p> In\u00a0[11]: Copied! <pre># Track training and overfitting\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\n_ = forecaster.plot_history(ax=ax)\n</pre> # Track training and overfitting # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) _ = forecaster.plot_history(ax=ax) <p>In the plot above, the training loss (blue) decreases rapidly during the first two epochs, indicating the model is quickly capturing the main patterns in the data. The validation loss (red) starts low and remains stable throughout the training process, closely following the training loss. This suggests:</p> <ul> <li><p>The model is not overfitting, as the validation loss stays close to the training loss for all epochs.</p> </li> <li><p>Both losses decrease and stabilize together, indicating good generalization and effective learning.</p> </li> <li><p>No divergence is observed, which would appear as the validation loss increasing while training loss keeps decreasing.</p> </li> </ul> In\u00a0[12]: Copied! <pre># Predictions\n# ==============================================================================\npredictions = forecaster.predict()\npredictions\n</pre> # Predictions # ============================================================================== predictions = forecaster.predict() predictions Out[12]: level pred 2021-04-01 o3 48.646408 <p>In time series forecasting, the process of backtesting consists of evaluating the performance of a predictive model by applying it retrospectively to historical data. Therefore, it is a special type of cross-validation applied to the previous period(s). To learn more about backtesting, visit the backtesting user guide.</p> In\u00a0[13]: Copied! <pre># Backtesting with test data\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = forecaster.max_step,\n         initial_train_size = len(data.loc[:end_validation, :]),  # Training + Validation Data\n         refit              = False\n     )\n\nmetrics, predictions = backtesting_forecaster_multiseries(\n    forecaster  = forecaster,\n    series      = data[['o3']],\n    cv          = cv,\n    levels      = forecaster.levels,\n    metric      = \"mean_absolute_error\",\n    verbose     = False  # Set to True for detailed output\n)\n</pre> # Backtesting with test data # ============================================================================== cv = TimeSeriesFold(          steps              = forecaster.max_step,          initial_train_size = len(data.loc[:end_validation, :]),  # Training + Validation Data          refit              = False      )  metrics, predictions = backtesting_forecaster_multiseries(     forecaster  = forecaster,     series      = data[['o3']],     cv          = cv,     levels      = forecaster.levels,     metric      = \"mean_absolute_error\",     verbose     = False  # Set to True for detailed output ) <pre>Epoch 1/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 39ms/step - loss: 0.0054 - val_loss: 0.0056\nEpoch 2/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 39ms/step - loss: 0.0052 - val_loss: 0.0054\nEpoch 3/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 37ms/step - loss: 0.0053 - val_loss: 0.0053\nEpoch 4/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 34ms/step - loss: 0.0052 - val_loss: 0.0053\nEpoch 5/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 41ms/step - loss: 0.0052 - val_loss: 0.0053\nEpoch 6/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 33ms/step - loss: 0.0052 - val_loss: 0.0052\nEpoch 7/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 32ms/step - loss: 0.0052 - val_loss: 0.0055\nEpoch 8/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 33ms/step - loss: 0.0051 - val_loss: 0.0055\nEpoch 9/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 31ms/step - loss: 0.0051 - val_loss: 0.0050\nEpoch 10/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 31ms/step - loss: 0.0050 - val_loss: 0.0055\nEpoch 11/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 34ms/step - loss: 0.0049 - val_loss: 0.0050\nEpoch 12/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 30ms/step - loss: 0.0049 - val_loss: 0.0050\nEpoch 13/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 32ms/step - loss: 0.0050 - val_loss: 0.0048\nEpoch 14/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 32ms/step - loss: 0.0050 - val_loss: 0.0048\nEpoch 15/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 30ms/step - loss: 0.0048 - val_loss: 0.0053\nEpoch 16/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 32ms/step - loss: 0.0050 - val_loss: 0.0057\nEpoch 17/25\n48/48 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 34ms/step - loss: 0.0051 - val_loss: 0.0051\n</pre> <pre>  0%|          | 0/2208 [00:00&lt;?, ?it/s]</pre> In\u00a0[14]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nmetrics\n</pre> # Backtesting metrics # ============================================================================== metrics Out[14]: levels mean_absolute_error 0 o3 5.745553 In\u00a0[15]: Copied! <pre># Backtesting predictions\n# ==============================================================================\npredictions.head(4)\n</pre> # Backtesting predictions # ============================================================================== predictions.head(4) Out[15]: level pred 2021-10-01 00:00:00 o3 52.571770 2021-10-01 01:00:00 o3 56.919910 2021-10-01 02:00:00 o3 60.340588 2021-10-01 03:00:00 o3 60.829491 In\u00a0[16]: Copied! <pre># Plotting predictions vs real values in the test set\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\ndata_test[\"o3\"].plot(ax=ax, label=\"test\")\npredictions.loc[predictions[\"level\"] == \"o3\", \"pred\"].plot(ax=ax, label=\"predictions\")\nax.set_title(\"O3\")\nax.legend();\n</pre> # Plotting predictions vs real values in the test set # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) data_test[\"o3\"].plot(ax=ax, label=\"test\") predictions.loc[predictions[\"level\"] == \"o3\", \"pred\"].plot(ax=ax, label=\"predictions\") ax.set_title(\"O3\") ax.legend(); In\u00a0[17]: Copied! <pre># Create model\n# ==============================================================================\nlags = 24\n\nmodel = create_and_compile_model(\n    series                  = data[[\"o3\"]],  # Only the 'o3' series is used as predictor\n    levels                  = [\"o3\"],        # Target series to predict\n    lags                    = lags,          # Number of lags to use as predictors\n    steps                   = 24,            # Multi-step forecasting\n    recurrent_layer         = \"GRU\",\n    recurrent_units         = 128,\n    recurrent_layers_kwargs = {\"activation\": \"tanh\"},\n    dense_units             = 64,\n    compile_kwargs          = {'optimizer': 'adam', 'loss': 'mse'},\n    model_name              = \"Single-Series-Multi-Step\" \n)\n\nmodel.summary()\n</pre> # Create model # ============================================================================== lags = 24  model = create_and_compile_model(     series                  = data[[\"o3\"]],  # Only the 'o3' series is used as predictor     levels                  = [\"o3\"],        # Target series to predict     lags                    = lags,          # Number of lags to use as predictors     steps                   = 24,            # Multi-step forecasting     recurrent_layer         = \"GRU\",     recurrent_units         = 128,     recurrent_layers_kwargs = {\"activation\": \"tanh\"},     dense_units             = 64,     compile_kwargs          = {'optimizer': 'adam', 'loss': 'mse'},     model_name              = \"Single-Series-Multi-Step\"  )  model.summary() <pre>keras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"Single-Series-Multi-Step\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input (InputLayer)       \u2502 (None, 24, 1)          \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gru_1 (GRU)                     \u2502 (None, 128)            \u2502        50,304 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 64)             \u2502         8,256 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_layer (Dense)   \u2502 (None, 24)             \u2502         1,560 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape (Reshape)               \u2502 (None, 24, 1)          \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 60,120 (234.84 KB)\n</pre> <pre> Trainable params: 60,120 (234.84 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <p> \u270e Note </p> <p>The <code>fit_kwargs</code> parameter lets you customize any aspect of the model training process, passing arguments directly to the underlying Keras <code>Model.fit()</code> method. For example, you can specify the number of training epochs, batch size, and any callbacks you want to use.</p> <p>In the code example, the model is trained for 50 epochs with a batch size of 512. The <code>EarlyStopping</code> callback monitors the validation loss and automatically stops training if it does not improve for 3 consecutive epochs (<code>patience=3</code>). This helps prevent overfitting and saves computation time.</p> <p>You can also add other callbacks, such as <code>ModelCheckpoint</code> to save the model at each epoch, or <code>TensorBoard</code> for real-time visualization of training and validation metrics.</p> In\u00a0[18]: Copied! <pre># Forecaster Creation\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=[\"o3\"],\n    lags=lags,\n    transformer_series=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 25, \n        \"batch_size\": 512, \n        \"callbacks\": [\n            EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n        ],  # Callback to stop training when it is no longer learning.\n        \"series_val\": data_val,  # Validation data for model training.\n    },\n)\n\n# Fit forecaster\n# ==============================================================================\nforecaster.fit(data_train[['o3']])\n</pre> # Forecaster Creation # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=[\"o3\"],     lags=lags,     transformer_series=MinMaxScaler(),     fit_kwargs={         \"epochs\": 25,          \"batch_size\": 512,          \"callbacks\": [             EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)         ],  # Callback to stop training when it is no longer learning.         \"series_val\": data_val,  # Validation data for model training.     }, )  # Fit forecaster # ============================================================================== forecaster.fit(data_train[['o3']]) <pre>Epoch 1/25\n</pre> <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 16 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n</pre> <pre>39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 62ms/step - loss: 0.1136 - val_loss: 0.0312\nEpoch 2/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 60ms/step - loss: 0.0297 - val_loss: 0.0263\nEpoch 3/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 61ms/step - loss: 0.0265 - val_loss: 0.0237\nEpoch 4/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 58ms/step - loss: 0.0243 - val_loss: 0.0206\nEpoch 5/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0218 - val_loss: 0.0184\nEpoch 6/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0205 - val_loss: 0.0183\nEpoch 7/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0201 - val_loss: 0.0175\nEpoch 8/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0198 - val_loss: 0.0170\nEpoch 9/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 58ms/step - loss: 0.0193 - val_loss: 0.0168\nEpoch 10/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 58ms/step - loss: 0.0191 - val_loss: 0.0167\nEpoch 11/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 58ms/step - loss: 0.0189 - val_loss: 0.0167\nEpoch 12/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 58ms/step - loss: 0.0186 - val_loss: 0.0166\nEpoch 13/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0185 - val_loss: 0.0165\nEpoch 14/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0184 - val_loss: 0.0165\nEpoch 15/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 58ms/step - loss: 0.0182 - val_loss: 0.0173\nEpoch 16/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0182 - val_loss: 0.0163\nEpoch 17/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 59ms/step - loss: 0.0177 - val_loss: 0.0165\nEpoch 18/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0177 - val_loss: 0.0163\nEpoch 19/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0176 - val_loss: 0.0161\nEpoch 20/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0177 - val_loss: 0.0162\nEpoch 21/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 58ms/step - loss: 0.0175 - val_loss: 0.0166\nEpoch 22/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 59ms/step - loss: 0.0173 - val_loss: 0.0160\nEpoch 23/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 57ms/step - loss: 0.0172 - val_loss: 0.0164\nEpoch 24/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 58ms/step - loss: 0.0172 - val_loss: 0.0160\nEpoch 25/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 58ms/step - loss: 0.0173 - val_loss: 0.0160\n</pre> In\u00a0[19]: Copied! <pre># Train and overfitting tracking\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\n_ = forecaster.plot_history(ax=ax)\n</pre> # Train and overfitting tracking # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) _ = forecaster.plot_history(ax=ax) <p>In this case, the prediction quality is expected to be lower than in the previous example, as shown by the higher loss values across epochs. This is easily explained: the model now has to predict 24 values at each step instead of just 1. As a result, the validation loss is higher, since it reflects the combined error across all 24 predicted values, rather than the error for a single value.</p> In\u00a0[20]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = forecaster.predict()\npredictions.head(4)\n</pre> # Prediction # ============================================================================== predictions = forecaster.predict() predictions.head(4) Out[20]: level pred 2021-04-01 00:00:00 o3 48.992241 2021-04-01 01:00:00 o3 45.452671 2021-04-01 02:00:00 o3 40.783764 2021-04-01 03:00:00 o3 39.934483 In\u00a0[21]: Copied! <pre># Specific step predictions\n# ==============================================================================\npredictions = forecaster.predict(steps=[1, 3])\npredictions\n</pre> # Specific step predictions # ============================================================================== predictions = forecaster.predict(steps=[1, 3]) predictions Out[21]: level pred 2021-04-01 00:00:00 o3 48.992241 2021-04-01 02:00:00 o3 40.783764 In\u00a0[22]: Copied! <pre># Backtesting \n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = forecaster.max_step,\n         initial_train_size = len(data.loc[:end_validation, :]),  # Training + Validation Data\n         refit              = False\n     )\n\nmetrics, predictions = backtesting_forecaster_multiseries(\n    forecaster        = forecaster,\n    series            = data[['o3']],\n    cv                = cv,\n    levels            = forecaster.levels,\n    metric            = \"mean_absolute_error\",\n    verbose           = False,\n    suppress_warnings = True\n)\n</pre> # Backtesting  # ============================================================================== cv = TimeSeriesFold(          steps              = forecaster.max_step,          initial_train_size = len(data.loc[:end_validation, :]),  # Training + Validation Data          refit              = False      )  metrics, predictions = backtesting_forecaster_multiseries(     forecaster        = forecaster,     series            = data[['o3']],     cv                = cv,     levels            = forecaster.levels,     metric            = \"mean_absolute_error\",     verbose           = False,     suppress_warnings = True ) <pre>Epoch 1/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 62ms/step - loss: 0.0169 - val_loss: 0.0158\nEpoch 2/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 58ms/step - loss: 0.0170 - val_loss: 0.0157\nEpoch 3/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 56ms/step - loss: 0.0168 - val_loss: 0.0158\nEpoch 4/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 58ms/step - loss: 0.0167 - val_loss: 0.0158\nEpoch 5/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 56ms/step - loss: 0.0165 - val_loss: 0.0160\n</pre> <pre>  0%|          | 0/92 [00:00&lt;?, ?it/s]</pre> In\u00a0[23]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nmetric_single_series = metrics.loc[metrics[\"levels\"] == \"o3\", \"mean_absolute_error\"].iat[0]\nmetrics\n</pre> # Backtesting metrics # ============================================================================== metric_single_series = metrics.loc[metrics[\"levels\"] == \"o3\", \"mean_absolute_error\"].iat[0] metrics Out[23]: levels mean_absolute_error 0 o3 11.271916 In\u00a0[24]: Copied! <pre># Backtesting predictions\n# ==============================================================================\npredictions\n</pre> # Backtesting predictions # ============================================================================== predictions Out[24]: level pred 2021-10-01 00:00:00 o3 60.107918 2021-10-01 01:00:00 o3 58.672436 2021-10-01 02:00:00 o3 54.720192 2021-10-01 03:00:00 o3 50.960598 2021-10-01 04:00:00 o3 46.120068 ... ... ... 2021-12-31 19:00:00 o3 16.421032 2021-12-31 20:00:00 o3 17.846300 2021-12-31 21:00:00 o3 21.751144 2021-12-31 22:00:00 o3 24.250710 2021-12-31 23:00:00 o3 26.225739 <p>2208 rows \u00d7 2 columns</p> In\u00a0[25]: Copied! <pre># Plotting predictions vs real values in the test set\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\ndata_test[\"o3\"].plot(ax=ax, label=\"test\")\npredictions.loc[predictions[\"level\"] == \"o3\", \"pred\"].plot(ax=ax, label=\"predictions\")\nax.set_title(\"O3\")\nax.legend();\n</pre> # Plotting predictions vs real values in the test set # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) data_test[\"o3\"].plot(ax=ax, label=\"test\") predictions.loc[predictions[\"level\"] == \"o3\", \"pred\"].plot(ax=ax, label=\"predictions\") ax.set_title(\"O3\") ax.legend(); In\u00a0[26]: Copied! <pre># Create model\n# ==============================================================================\nlags = 24\n\nmodel = create_and_compile_model(\n    series                  = data,    # DataFrame with all series (predictors)\n    levels                  = [\"o3\"],  # Target series to predict\n    lags                    = lags,    # Number of lags to use as predictors\n    steps                   = 24,      # Multi-step forecasting\n    recurrent_layer         = \"GRU\",\n    recurrent_units         = [128, 64],\n    recurrent_layers_kwargs = {\"activation\": \"tanh\"},\n    dense_units             = [64, 32],\n    compile_kwargs          = {'optimizer': 'adam', 'loss': 'mse'},\n    model_name              = \"MultiVariate-Multi-Step\" \n)\n\nmodel.summary()\n</pre> # Create model # ============================================================================== lags = 24  model = create_and_compile_model(     series                  = data,    # DataFrame with all series (predictors)     levels                  = [\"o3\"],  # Target series to predict     lags                    = lags,    # Number of lags to use as predictors     steps                   = 24,      # Multi-step forecasting     recurrent_layer         = \"GRU\",     recurrent_units         = [128, 64],     recurrent_layers_kwargs = {\"activation\": \"tanh\"},     dense_units             = [64, 32],     compile_kwargs          = {'optimizer': 'adam', 'loss': 'mse'},     model_name              = \"MultiVariate-Multi-Step\"  )  model.summary() <pre>keras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"MultiVariate-Multi-Step\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input (InputLayer)       \u2502 (None, 24, 10)         \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gru_1 (GRU)                     \u2502 (None, 24, 128)        \u2502        53,760 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gru_2 (GRU)                     \u2502 (None, 64)             \u2502        37,248 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 64)             \u2502         4,160 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_2 (Dense)                 \u2502 (None, 32)             \u2502         2,080 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_layer (Dense)   \u2502 (None, 24)             \u2502           792 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape (Reshape)               \u2502 (None, 24, 1)          \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 98,040 (382.97 KB)\n</pre> <pre> Trainable params: 98,040 (382.97 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[27]: Copied! <pre># Forecaster Creation\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=[\"o3\"],\n    lags=lags,\n    transformer_series=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 25, \n        \"batch_size\": 512, \n        \"callbacks\": [\n            EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n        ],  # Callback to stop training when it is no longer learning.\n        \"series_val\": data_val,  # Validation data for model training.\n    },\n)\n\n# Fit forecaster\n# ==============================================================================\nforecaster.fit(data_train)\n</pre> # Forecaster Creation # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=[\"o3\"],     lags=lags,     transformer_series=MinMaxScaler(),     fit_kwargs={         \"epochs\": 25,          \"batch_size\": 512,          \"callbacks\": [             EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)         ],  # Callback to stop training when it is no longer learning.         \"series_val\": data_val,  # Validation data for model training.     }, )  # Fit forecaster # ============================================================================== forecaster.fit(data_train) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n</pre> <pre>Epoch 1/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 100ms/step - loss: 0.1185 - val_loss: 0.0470\nEpoch 2/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 92ms/step - loss: 0.0356 - val_loss: 0.0288\nEpoch 3/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 94ms/step - loss: 0.0276 - val_loss: 0.0264\nEpoch 4/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 92ms/step - loss: 0.0264 - val_loss: 0.0251\nEpoch 5/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0252 - val_loss: 0.0224\nEpoch 6/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 92ms/step - loss: 0.0227 - val_loss: 0.0193\nEpoch 7/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 92ms/step - loss: 0.0206 - val_loss: 0.0173\nEpoch 8/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 92ms/step - loss: 0.0191 - val_loss: 0.0163\nEpoch 9/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 107ms/step - loss: 0.0180 - val_loss: 0.0161\nEpoch 10/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 107ms/step - loss: 0.0175 - val_loss: 0.0157\nEpoch 11/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 107ms/step - loss: 0.0171 - val_loss: 0.0158\nEpoch 12/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 105ms/step - loss: 0.0167 - val_loss: 0.0161\nEpoch 13/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 107ms/step - loss: 0.0165 - val_loss: 0.0151\nEpoch 14/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 106ms/step - loss: 0.0163 - val_loss: 0.0151\nEpoch 15/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 107ms/step - loss: 0.0160 - val_loss: 0.0152\nEpoch 16/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 105ms/step - loss: 0.0160 - val_loss: 0.0150\nEpoch 17/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 106ms/step - loss: 0.0156 - val_loss: 0.0149\nEpoch 18/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 105ms/step - loss: 0.0158 - val_loss: 0.0153\nEpoch 19/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 106ms/step - loss: 0.0155 - val_loss: 0.0150\nEpoch 20/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 112ms/step - loss: 0.0153 - val_loss: 0.0150\n</pre> In\u00a0[28]: Copied! <pre># Training and overfitting tracking\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\n_ = forecaster.plot_history(ax=ax)\n</pre> # Training and overfitting tracking # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) _ = forecaster.plot_history(ax=ax) In\u00a0[29]: Copied! <pre># Prediction\n# ==============================================================================\npredictions = forecaster.predict()\npredictions.head(4)\n</pre> # Prediction # ============================================================================== predictions = forecaster.predict() predictions.head(4) Out[29]: level pred 2021-04-01 00:00:00 o3 52.557709 2021-04-01 01:00:00 o3 51.103519 2021-04-01 02:00:00 o3 47.209839 2021-04-01 03:00:00 o3 43.603031 In\u00a0[30]: Copied! <pre># Backtesting with test data\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = forecaster.max_step,\n         initial_train_size = len(data.loc[:end_validation, :]),  # Training + Validation Data\n         refit              = False\n     )\n\nmetrics, predictions = backtesting_forecaster_multiseries(\n    forecaster        = forecaster,\n    series            = data,\n    cv                = cv,\n    levels            = forecaster.levels,\n    metric            = \"mean_absolute_error\",\n    suppress_warnings = True,\n    verbose           = False\n)\n</pre> # Backtesting with test data # ============================================================================== cv = TimeSeriesFold(          steps              = forecaster.max_step,          initial_train_size = len(data.loc[:end_validation, :]),  # Training + Validation Data          refit              = False      )  metrics, predictions = backtesting_forecaster_multiseries(     forecaster        = forecaster,     series            = data,     cv                = cv,     levels            = forecaster.levels,     metric            = \"mean_absolute_error\",     suppress_warnings = True,     verbose           = False ) <pre>Epoch 1/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 114ms/step - loss: 0.0155 - val_loss: 0.0147\nEpoch 2/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 104ms/step - loss: 0.0153 - val_loss: 0.0145\nEpoch 3/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 105ms/step - loss: 0.0151 - val_loss: 0.0143\nEpoch 4/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 106ms/step - loss: 0.0150 - val_loss: 0.0143\nEpoch 5/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 104ms/step - loss: 0.0147 - val_loss: 0.0140\nEpoch 6/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 103ms/step - loss: 0.0148 - val_loss: 0.0141\nEpoch 7/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 105ms/step - loss: 0.0148 - val_loss: 0.0139\nEpoch 8/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 102ms/step - loss: 0.0147 - val_loss: 0.0138\nEpoch 9/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 104ms/step - loss: 0.0146 - val_loss: 0.0138\nEpoch 10/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 104ms/step - loss: 0.0143 - val_loss: 0.0138\nEpoch 11/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 102ms/step - loss: 0.0143 - val_loss: 0.0139\nEpoch 12/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 103ms/step - loss: 0.0143 - val_loss: 0.0138\nEpoch 13/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 102ms/step - loss: 0.0142 - val_loss: 0.0138\n</pre> <pre>  0%|          | 0/92 [00:00&lt;?, ?it/s]</pre> In\u00a0[31]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nmetric_multivariate = metrics.loc[metrics[\"levels\"] == \"o3\", \"mean_absolute_error\"].iat[0]\nmetrics\n</pre> # Backtesting metrics # ============================================================================== metric_multivariate = metrics.loc[metrics[\"levels\"] == \"o3\", \"mean_absolute_error\"].iat[0] metrics Out[31]: levels mean_absolute_error 0 o3 10.90494 In\u00a0[32]: Copied! <pre># Backtesting predictions\n# ==============================================================================\npredictions\n</pre> # Backtesting predictions # ============================================================================== predictions Out[32]: level pred 2021-10-01 00:00:00 o3 51.411842 2021-10-01 01:00:00 o3 50.147354 2021-10-01 02:00:00 o3 46.690861 2021-10-01 03:00:00 o3 40.983303 2021-10-01 04:00:00 o3 37.137405 ... ... ... 2021-12-31 19:00:00 o3 21.922625 2021-12-31 20:00:00 o3 20.657047 2021-12-31 21:00:00 o3 17.817327 2021-12-31 22:00:00 o3 15.759877 2021-12-31 23:00:00 o3 15.729614 <p>2208 rows \u00d7 2 columns</p> In\u00a0[33]: Copied! <pre># Plotting predictions vs real values in the test set\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\ndata_test[\"o3\"].plot(ax=ax, label=\"test\")\npredictions.loc[predictions[\"level\"] == \"o3\", \"pred\"].plot(ax=ax, label=\"predictions\")\nax.set_title(\"O3\")\nax.legend()\nplt.show()\n</pre> # Plotting predictions vs real values in the test set # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) data_test[\"o3\"].plot(ax=ax, label=\"test\") predictions.loc[predictions[\"level\"] == \"o3\", \"pred\"].plot(ax=ax, label=\"predictions\") ax.set_title(\"O3\") ax.legend() plt.show() <p>When using multiple time series as predictors, it is often expected that the model will produce more accurate forecasts for the target series. However, in this example, the predictions are actually worse than in the previous case where only a single series was used as input. This may happen if the additional time series used as predictors are not strongly related to the target series. As a result, the model is unable to learn meaningful relationships, and the extra information does not improve performance\u2014in fact, it may even introduce noise.</p> In\u00a0[34]: Copied! <pre># Create model\n# ==============================================================================\nlevels = ['o3', 'pm2.5', 'pm10']  # Multiple target series to predict\nlags = 24\n\nmodel = create_and_compile_model(\n    series                  = data,    # DataFrame with all series (predictors)\n    levels                  = levels, \n    lags                    = lags, \n    steps                   = 24, \n    recurrent_layer         = \"LSTM\",\n    recurrent_units         = [128, 64],\n    recurrent_layers_kwargs = {\"activation\": \"tanh\"},\n    dense_units             = [64, 32],\n    compile_kwargs          = {'optimizer': Adam(), 'loss': MeanSquaredError()},\n    model_name              = \"MultiVariate-MultiOutput-Multi-Step\"\n)\n\nmodel.summary()\n</pre> # Create model # ============================================================================== levels = ['o3', 'pm2.5', 'pm10']  # Multiple target series to predict lags = 24  model = create_and_compile_model(     series                  = data,    # DataFrame with all series (predictors)     levels                  = levels,      lags                    = lags,      steps                   = 24,      recurrent_layer         = \"LSTM\",     recurrent_units         = [128, 64],     recurrent_layers_kwargs = {\"activation\": \"tanh\"},     dense_units             = [64, 32],     compile_kwargs          = {'optimizer': Adam(), 'loss': MeanSquaredError()},     model_name              = \"MultiVariate-MultiOutput-Multi-Step\" )  model.summary() <pre>keras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"MultiVariate-MultiOutput-Multi-Step\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input (InputLayer)       \u2502 (None, 24, 10)         \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_1 (LSTM)                   \u2502 (None, 24, 128)        \u2502        71,168 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_2 (LSTM)                   \u2502 (None, 64)             \u2502        49,408 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 64)             \u2502         4,160 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_2 (Dense)                 \u2502 (None, 32)             \u2502         2,080 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_layer (Dense)   \u2502 (None, 72)             \u2502         2,376 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape (Reshape)               \u2502 (None, 24, 3)          \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 129,192 (504.66 KB)\n</pre> <pre> Trainable params: 129,192 (504.66 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[35]: Copied! <pre># Forecaster Creation\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=levels,\n    lags=lags,\n    transformer_series=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 25, \n        \"batch_size\": 512, \n        \"callbacks\": [\n            EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n        ],  # Callback to stop training when it is no longer learning.\n        \"series_val\": data_val,  # Validation data for model training.\n    },\n)\n\n# Fit forecaster\n# ==============================================================================\nforecaster.fit(data_train)\n</pre> # Forecaster Creation # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=levels,     lags=lags,     transformer_series=MinMaxScaler(),     fit_kwargs={         \"epochs\": 25,          \"batch_size\": 512,          \"callbacks\": [             EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)         ],  # Callback to stop training when it is no longer learning.         \"series_val\": data_val,  # Validation data for model training.     }, )  # Fit forecaster # ============================================================================== forecaster.fit(data_train) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n</pre> <pre>Epoch 1/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 90ms/step - loss: 0.0517 - val_loss: 0.0176\nEpoch 2/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0148 - val_loss: 0.0100\nEpoch 3/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 96ms/step - loss: 0.0116 - val_loss: 0.0091\nEpoch 4/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 101ms/step - loss: 0.0105 - val_loss: 0.0082\nEpoch 5/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 103ms/step - loss: 0.0095 - val_loss: 0.0073\nEpoch 6/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 100ms/step - loss: 0.0087 - val_loss: 0.0066\nEpoch 7/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 98ms/step - loss: 0.0082 - val_loss: 0.0063\nEpoch 8/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 98ms/step - loss: 0.0080 - val_loss: 0.0063\nEpoch 9/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0078 - val_loss: 0.0062\nEpoch 10/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 97ms/step - loss: 0.0077 - val_loss: 0.0061\nEpoch 11/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 100ms/step - loss: 0.0075 - val_loss: 0.0060\nEpoch 12/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 95ms/step - loss: 0.0073 - val_loss: 0.0061\nEpoch 13/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 94ms/step - loss: 0.0073 - val_loss: 0.0060\nEpoch 14/25\n39/39 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 94ms/step - loss: 0.0072 - val_loss: 0.0061\n</pre> In\u00a0[36]: Copied! <pre># Training and overfitting tracking\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\n_ = forecaster.plot_history(ax=ax)\n</pre> # Training and overfitting tracking # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) _ = forecaster.plot_history(ax=ax) <p>Predictions can be made for specific <code>steps</code> and <code>levels</code> as long as they are within the prediction horizon defined by the model. For example, you can predict ozone concentration (<code>levels = \"o3\"</code>) for the next one and five hours (<code>steps = [1, 5]</code>).</p> In\u00a0[37]: Copied! <pre># Specific steps and levels predictions\n# ==============================================================================\nforecaster.predict(steps=[1, 5], levels=\"o3\")\n</pre> # Specific steps and levels predictions # ============================================================================== forecaster.predict(steps=[1, 5], levels=\"o3\") Out[37]: level pred 2021-04-01 00:00:00 o3 51.330368 2021-04-01 04:00:00 o3 33.898960 In\u00a0[38]: Copied! <pre># Predictions for all steps and levels\n# ==============================================================================\npredictions = forecaster.predict()\npredictions\n</pre> # Predictions for all steps and levels # ============================================================================== predictions = forecaster.predict() predictions Out[38]: level pred 2021-04-01 00:00:00 o3 51.330368 2021-04-01 00:00:00 pm2.5 13.790528 2021-04-01 00:00:00 pm10 16.680853 2021-04-01 01:00:00 o3 46.169151 2021-04-01 01:00:00 pm2.5 14.281107 ... ... ... 2021-04-01 22:00:00 pm2.5 11.015145 2021-04-01 22:00:00 pm10 16.603825 2021-04-01 23:00:00 o3 55.626831 2021-04-01 23:00:00 pm2.5 11.014272 2021-04-01 23:00:00 pm10 18.551624 <p>72 rows \u00d7 2 columns</p> In\u00a0[39]: Copied! <pre># Backtesting with test data\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = forecaster.max_step,\n         initial_train_size = len(data.loc[:end_validation, :]),  # Training + Validation Data\n         refit              = False\n     )\n\nmetrics, predictions = backtesting_forecaster_multiseries(\n    forecaster        = forecaster,\n    series            = data,\n    cv                = cv,\n    levels            = forecaster.levels,\n    metric            = \"mean_absolute_error\",\n    suppress_warnings = True,\n    verbose           = False\n)\n</pre> # Backtesting with test data # ============================================================================== cv = TimeSeriesFold(          steps              = forecaster.max_step,          initial_train_size = len(data.loc[:end_validation, :]),  # Training + Validation Data          refit              = False      )  metrics, predictions = backtesting_forecaster_multiseries(     forecaster        = forecaster,     series            = data,     cv                = cv,     levels            = forecaster.levels,     metric            = \"mean_absolute_error\",     suppress_warnings = True,     verbose           = False ) <pre>Epoch 1/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 100ms/step - loss: 0.0071 - val_loss: 0.0059\nEpoch 2/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0070 - val_loss: 0.0057\nEpoch 3/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0069 - val_loss: 0.0056\nEpoch 4/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0068 - val_loss: 0.0056\nEpoch 5/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 95ms/step - loss: 0.0067 - val_loss: 0.0055\nEpoch 6/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0066 - val_loss: 0.0054\nEpoch 7/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0065 - val_loss: 0.0054\nEpoch 8/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 94ms/step - loss: 0.0064 - val_loss: 0.0053\nEpoch 9/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0063 - val_loss: 0.0052\nEpoch 10/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 95ms/step - loss: 0.0061 - val_loss: 0.0052\nEpoch 11/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 94ms/step - loss: 0.0061 - val_loss: 0.0052\nEpoch 12/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 94ms/step - loss: 0.0061 - val_loss: 0.0051\nEpoch 13/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 94ms/step - loss: 0.0060 - val_loss: 0.0051\nEpoch 14/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 95ms/step - loss: 0.0059 - val_loss: 0.0051\nEpoch 15/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 94ms/step - loss: 0.0057 - val_loss: 0.0051\nEpoch 16/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0058 - val_loss: 0.0049\nEpoch 17/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 96ms/step - loss: 0.0057 - val_loss: 0.0049\nEpoch 18/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 95ms/step - loss: 0.0057 - val_loss: 0.0049\nEpoch 19/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 95ms/step - loss: 0.0055 - val_loss: 0.0049\nEpoch 20/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 92ms/step - loss: 0.0056 - val_loss: 0.0049\nEpoch 21/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 93ms/step - loss: 0.0055 - val_loss: 0.0048\nEpoch 22/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 95ms/step - loss: 0.0054 - val_loss: 0.0047\nEpoch 23/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 94ms/step - loss: 0.0053 - val_loss: 0.0047\nEpoch 24/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 101ms/step - loss: 0.0052 - val_loss: 0.0047\nEpoch 25/25\n47/47 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 97ms/step - loss: 0.0052 - val_loss: 0.0047\n</pre> <pre>  0%|          | 0/92 [00:00&lt;?, ?it/s]</pre> In\u00a0[40]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nmetric_multivariate_multioutput = metrics.loc[metrics[\"levels\"] == \"o3\", \"mean_absolute_error\"].iat[0]\nmetrics\n</pre> # Backtesting metrics # ============================================================================== metric_multivariate_multioutput = metrics.loc[metrics[\"levels\"] == \"o3\", \"mean_absolute_error\"].iat[0] metrics Out[40]: levels mean_absolute_error 0 o3 11.682385 1 pm2.5 4.018549 2 pm10 12.209744 3 average 9.303559 4 weighted_average 9.303559 5 pooling 9.303559 In\u00a0[41]: Copied! <pre># Plot all the predicted variables as rows in the plot\n# ==============================================================================\nfig, ax = plt.subplots(len(levels), 1, figsize=(8, 2 * len(levels)), sharex=True)\nfor i, level in enumerate(levels):\n    data_test[level].plot(ax=ax[i], label=\"test\")\n    predictions.loc[predictions[\"level\"] == level, \"pred\"].plot(ax=ax[i], label=\"predictions\")\n    ax[i].set_title(level)\n    ax[i].legend()\n\nplt.tight_layout()\nplt.show()\n</pre> # Plot all the predicted variables as rows in the plot # ============================================================================== fig, ax = plt.subplots(len(levels), 1, figsize=(8, 2 * len(levels)), sharex=True) for i, level in enumerate(levels):     data_test[level].plot(ax=ax[i], label=\"test\")     predictions.loc[predictions[\"level\"] == level, \"pred\"].plot(ax=ax[i], label=\"predictions\")     ax[i].set_title(level)     ax[i].legend()  plt.tight_layout() plt.show() In\u00a0[42]: Copied! <pre># Metric comparison\n# ==============================================================================\nresults = {\n    \"Single-Series, Multi-Step\": metric_single_series,\n    \"Multi-Series, Single-Output\": metric_multivariate,\n    \"Multi-Series, Multi-Output\": metric_multivariate_multioutput\n}\n\ntable_results = pd.DataFrame.from_dict(results, orient='index', columns=['O3 MAE'])\ntable_results = table_results.style.highlight_min(axis=0, color='green').format(precision=4)\ntable_results\n</pre> # Metric comparison # ============================================================================== results = {     \"Single-Series, Multi-Step\": metric_single_series,     \"Multi-Series, Single-Output\": metric_multivariate,     \"Multi-Series, Multi-Output\": metric_multivariate_multioutput }  table_results = pd.DataFrame.from_dict(results, orient='index', columns=['O3 MAE']) table_results = table_results.style.highlight_min(axis=0, color='green').format(precision=4) table_results Out[42]: O3 MAE Single-Series, Multi-Step 11.2719 Multi-Series, Single-Output 10.9049 Multi-Series, Multi-Output 11.6824 <p>In this example, the single-series and simple multivariate approaches produce similar errors, while adding more targets as outputs (multi-output) increases the prediction error. However, there is no universal rule: the best strategy depends on your data, domain, and prediction goals.</p> <p>It's important to experiment with different architectures and compare their metrics to select the most appropriate model for your specific use case.</p> In\u00a0[43]: Copied! <pre># Data download\n# ==============================================================================\ndata_exog = fetch_dataset(name='bike_sharing', raw=False)\ndata_exog = data_exog[['users', 'temp', 'hum', 'windspeed', 'holiday']]\ndata_exog = data_exog.loc['2011-04-01 00:00:00':'2012-10-20 23:00:00', :].copy()\ndata_exog.head(3)\n</pre> # Data download # ============================================================================== data_exog = fetch_dataset(name='bike_sharing', raw=False) data_exog = data_exog[['users', 'temp', 'hum', 'windspeed', 'holiday']] data_exog = data_exog.loc['2011-04-01 00:00:00':'2012-10-20 23:00:00', :].copy() data_exog.head(3) <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 11)\n</pre> Out[43]: users temp hum windspeed holiday date_time 2011-04-01 00:00:00 6.0 10.66 100.0 11.0014 0.0 2011-04-01 01:00:00 4.0 10.66 100.0 11.0014 0.0 2011-04-01 02:00:00 7.0 10.66 93.0 12.9980 0.0 In\u00a0[44]: Copied! <pre># Calendar features\n# ==============================================================================\nfeatures_to_extract = [\n    'month',\n    'week',\n    'day_of_week',\n    'hour'\n]\ncalendar_transformer = DatetimeFeatures(\n    variables           = 'index',\n    features_to_extract = features_to_extract,\n    drop_original       = False,\n)\n\n# Cyclical encoding of calendar features\n# ==============================================================================\nfeatures_to_encode = [\n    \"month\",\n    \"week\",\n    \"day_of_week\",\n    \"hour\",\n]\nmax_values = {\n    \"month\": 12,\n    \"week\": 52,\n    \"day_of_week\": 7,\n    \"hour\": 24,\n}\ncyclical_encoder = CyclicalFeatures(\n                       variables     = features_to_encode,\n                       max_values    = max_values,\n                       drop_original = True\n                   )\n\nexog_transformer = make_pipeline(\n                       calendar_transformer,\n                       cyclical_encoder\n                   )\n\ndata_exog = exog_transformer.fit_transform(data_exog)\nexog_features = data_exog.columns.difference(['users']).tolist()\n\nprint(f\"Exogenous features: {exog_features}\")\ndata_exog.head(3)\n</pre> # Calendar features # ============================================================================== features_to_extract = [     'month',     'week',     'day_of_week',     'hour' ] calendar_transformer = DatetimeFeatures(     variables           = 'index',     features_to_extract = features_to_extract,     drop_original       = False, )  # Cyclical encoding of calendar features # ============================================================================== features_to_encode = [     \"month\",     \"week\",     \"day_of_week\",     \"hour\", ] max_values = {     \"month\": 12,     \"week\": 52,     \"day_of_week\": 7,     \"hour\": 24, } cyclical_encoder = CyclicalFeatures(                        variables     = features_to_encode,                        max_values    = max_values,                        drop_original = True                    )  exog_transformer = make_pipeline(                        calendar_transformer,                        cyclical_encoder                    )  data_exog = exog_transformer.fit_transform(data_exog) exog_features = data_exog.columns.difference(['users']).tolist()  print(f\"Exogenous features: {exog_features}\") data_exog.head(3) <pre>Exogenous features: ['day_of_week_cos', 'day_of_week_sin', 'holiday', 'hour_cos', 'hour_sin', 'hum', 'month_cos', 'month_sin', 'temp', 'week_cos', 'week_sin', 'windspeed']\n</pre> Out[44]: users temp hum windspeed holiday month_sin month_cos week_sin week_cos day_of_week_sin day_of_week_cos hour_sin hour_cos date_time 2011-04-01 00:00:00 6.0 10.66 100.0 11.0014 0.0 0.866025 -0.5 1.0 6.123234e-17 -0.433884 -0.900969 0.000000 1.000000 2011-04-01 01:00:00 4.0 10.66 100.0 11.0014 0.0 0.866025 -0.5 1.0 6.123234e-17 -0.433884 -0.900969 0.258819 0.965926 2011-04-01 02:00:00 7.0 10.66 93.0 12.9980 0.0 0.866025 -0.5 1.0 6.123234e-17 -0.433884 -0.900969 0.500000 0.866025 In\u00a0[45]: Copied! <pre># Split train-validation-test\n# ==============================================================================\nend_train = '2012-06-30 23:59:00'\nend_validation = '2012-10-01 23:59:00'\ndata_exog_train = data_exog.loc[: end_train, :]\ndata_exog_val   = data_exog.loc[end_train:end_validation, :]\ndata_exog_test  = data_exog.loc[end_validation:, :]\n\nprint(f\"Dates train      : {data_exog_train.index.min()} --- {data_exog_train.index.max()}  (n={len(data_exog_train)})\")\nprint(f\"Dates validation : {data_exog_val.index.min()} --- {data_exog_val.index.max()}  (n={len(data_exog_val)})\")\nprint(f\"Dates test       : {data_exog_test.index.min()} --- {data_exog_test.index.max()}  (n={len(data_exog_test)})\")\n</pre> # Split train-validation-test # ============================================================================== end_train = '2012-06-30 23:59:00' end_validation = '2012-10-01 23:59:00' data_exog_train = data_exog.loc[: end_train, :] data_exog_val   = data_exog.loc[end_train:end_validation, :] data_exog_test  = data_exog.loc[end_validation:, :]  print(f\"Dates train      : {data_exog_train.index.min()} --- {data_exog_train.index.max()}  (n={len(data_exog_train)})\") print(f\"Dates validation : {data_exog_val.index.min()} --- {data_exog_val.index.max()}  (n={len(data_exog_val)})\") print(f\"Dates test       : {data_exog_test.index.min()} --- {data_exog_test.index.max()}  (n={len(data_exog_test)})\") <pre>Dates train      : 2011-04-01 00:00:00 --- 2012-06-30 23:00:00  (n=10968)\nDates validation : 2012-07-01 00:00:00 --- 2012-10-01 23:00:00  (n=2232)\nDates test       : 2012-10-02 00:00:00 --- 2012-10-20 23:00:00  (n=456)\n</pre> <p>The architecture of your deep learning model must be able to accept extra inputs alongside the main time series data. The <code>create_and_compile_model</code> function makes this straightforward: simply pass the exogenous variables as a DataFrame to the <code>exog</code> argument.</p> In\u00a0[46]: Copied! <pre># `create_and_compile_model` with exogenous variables\n# ==============================================================================\nseries = ['users']\nlevels = ['users']\nlags = 72\n\nmodel = create_and_compile_model(\n    series                  = data_exog[series],         # Single-series\n    levels                  = levels,                    # One target series to predict\n    lags                    = lags, \n    steps                   = 36, \n    exog                    = data_exog[exog_features],  # Exogenous variables\n    recurrent_layer         = \"LSTM\",\n    recurrent_units         = [128, 64],\n    recurrent_layers_kwargs = {\"activation\": \"tanh\"},\n    dense_units             = [64, 32],\n    compile_kwargs          = {'optimizer': Adam(learning_rate=0.01), 'loss': 'mse'},\n    model_name              = \"Single-Series-Multi-Step-Exog\"\n)\n\nmodel.summary()\n</pre> # `create_and_compile_model` with exogenous variables # ============================================================================== series = ['users'] levels = ['users'] lags = 72  model = create_and_compile_model(     series                  = data_exog[series],         # Single-series     levels                  = levels,                    # One target series to predict     lags                    = lags,      steps                   = 36,      exog                    = data_exog[exog_features],  # Exogenous variables     recurrent_layer         = \"LSTM\",     recurrent_units         = [128, 64],     recurrent_layers_kwargs = {\"activation\": \"tanh\"},     dense_units             = [64, 32],     compile_kwargs          = {'optimizer': Adam(learning_rate=0.01), 'loss': 'mse'},     model_name              = \"Single-Series-Multi-Step-Exog\" )  model.summary() <pre>keras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"Single-Series-Multi-Step-Exog\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)        \u2503 Output Shape      \u2503    Param # \u2503 Connected to      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input        \u2502 (None, 72, 1)     \u2502          0 \u2502 -                 \u2502\n\u2502 (InputLayer)        \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_1 (LSTM)       \u2502 (None, 72, 128)   \u2502     66,560 \u2502 series_input[0][\u2026 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_2 (LSTM)       \u2502 (None, 64)        \u2502     49,408 \u2502 lstm_1[0][0]      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 repeat_vector       \u2502 (None, 36, 64)    \u2502          0 \u2502 lstm_2[0][0]      \u2502\n\u2502 (RepeatVector)      \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 exog_input          \u2502 (None, 36, 12)    \u2502          0 \u2502 -                 \u2502\n\u2502 (InputLayer)        \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 concat_exog         \u2502 (None, 36, 76)    \u2502          0 \u2502 repeat_vector[0]\u2026 \u2502\n\u2502 (Concatenate)       \u2502                   \u2502            \u2502 exog_input[0][0]  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_td_1          \u2502 (None, 36, 64)    \u2502      4,928 \u2502 concat_exog[0][0] \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_td_2          \u2502 (None, 36, 32)    \u2502      2,080 \u2502 dense_td_1[0][0]  \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_la\u2026 \u2502 (None, 36, 1)     \u2502         33 \u2502 dense_td_2[0][0]  \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 123,009 (480.50 KB)\n</pre> <pre> Trainable params: 123,009 (480.50 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[47]: Copied! <pre># Plotting the model architecture (require `pydot` and `graphviz`)\n# ==============================================================================\n# from keras.utils import plot_model\n# plot_model(model, show_shapes=True, show_layer_names=True, to_file='model-architecture-exog.png')\n</pre> # Plotting the model architecture (require `pydot` and `graphviz`) # ============================================================================== # from keras.utils import plot_model # plot_model(model, show_shapes=True, show_layer_names=True, to_file='model-architecture-exog.png') <p> </p> In\u00a0[48]: Copied! <pre># Forecaster Creation\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=levels,\n    lags=lags, \n    transformer_series=MinMaxScaler(),\n    transformer_exog=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 25, \n        \"batch_size\": 1024, \n        \"callbacks\": [\n            EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n            ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1)\n        ],  # Callback to stop training when it is no longer learning and to reduce learning rate.\n        \"series_val\": data_exog_val[series],      # Validation data for model training.\n        \"exog_val\": data_exog_val[exog_features]  # Validation data for exogenous variables\n    },\n)\n\n# Fit forecaster with exogenous variables\n# ==============================================================================\nforecaster.fit(\n    series = data_exog_train[series], \n    exog   = data_exog_train[exog_features]\n)\n</pre> # Forecaster Creation # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=levels,     lags=lags,      transformer_series=MinMaxScaler(),     transformer_exog=MinMaxScaler(),     fit_kwargs={         \"epochs\": 25,          \"batch_size\": 1024,          \"callbacks\": [             EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),             ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1)         ],  # Callback to stop training when it is no longer learning and to reduce learning rate.         \"series_val\": data_exog_val[series],      # Validation data for model training.         \"exog_val\": data_exog_val[exog_features]  # Validation data for exogenous variables     }, )  # Fit forecaster with exogenous variables # ============================================================================== forecaster.fit(     series = data_exog_train[series],      exog   = data_exog_train[exog_features] ) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n</pre> <pre>Epoch 1/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 650ms/step - loss: 0.1219 - val_loss: 0.0686 - learning_rate: 0.0100\nEpoch 2/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 654ms/step - loss: 0.0240 - val_loss: 0.0448 - learning_rate: 0.0100\nEpoch 3/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 643ms/step - loss: 0.0161 - val_loss: 0.0405 - learning_rate: 0.0100\nEpoch 4/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 648ms/step - loss: 0.0144 - val_loss: 0.0360 - learning_rate: 0.0100\nEpoch 5/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 654ms/step - loss: 0.0131 - val_loss: 0.0312 - learning_rate: 0.0100\nEpoch 6/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 652ms/step - loss: 0.0118 - val_loss: 0.0305 - learning_rate: 0.0100\nEpoch 7/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 691ms/step - loss: 0.0104 - val_loss: 0.0291 - learning_rate: 0.0100\nEpoch 8/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 685ms/step - loss: 0.0096 - val_loss: 0.0270 - learning_rate: 0.0100\nEpoch 9/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 666ms/step - loss: 0.0085 - val_loss: 0.0203 - learning_rate: 0.0100\nEpoch 10/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 640ms/step - loss: 0.0082 - val_loss: 0.0263 - learning_rate: 0.0100\nEpoch 11/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 668ms/step - loss: 0.0074 - val_loss: 0.0200 - learning_rate: 0.0100\nEpoch 12/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 578ms/step - loss: 0.0066 - val_loss: 0.0230 - learning_rate: 0.0100\nEpoch 13/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 573ms/step - loss: 0.0065 - val_loss: 0.0189 - learning_rate: 0.0100\nEpoch 14/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 558ms/step - loss: 0.0061 - val_loss: 0.0213 - learning_rate: 0.0100\nEpoch 15/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 516ms/step - loss: 0.0055\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 564ms/step - loss: 0.0055 - val_loss: 0.0201 - learning_rate: 0.0100\nEpoch 16/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 575ms/step - loss: 0.0053 - val_loss: 0.0198 - learning_rate: 0.0050\n</pre> In\u00a0[49]: Copied! <pre># Training and overfitting tracking\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\n_ = forecaster.plot_history(ax=ax)\n</pre> # Training and overfitting tracking # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) _ = forecaster.plot_history(ax=ax) <p>The training history shows that while the training loss decreases smoothly, the validation loss stays higher and fluctuates across epochs. This suggests that the model is likely overfitting: it learns the training data well but struggles to generalize to new, unseen data. To address this, you could try adding regularization such as dropout, simplifying the model by reducing its size, or revisiting the choice of exogenous features to help improve validation performance.</p> <p>When using exogenous variables, the predict requires additional information about the future values of these variables. This data must be provided through the <code>exog</code> argument in the <code>predict</code> method.</p> In\u00a0[50]: Copied! <pre># Prediction with exogenous variables\n# ==============================================================================\npredictions = forecaster.predict(exog=data_exog_val[exog_features])\npredictions.head(4)\n</pre> # Prediction with exogenous variables # ============================================================================== predictions = forecaster.predict(exog=data_exog_val[exog_features]) predictions.head(4) Out[50]: level pred 2012-07-01 00:00:00 users 112.488235 2012-07-01 01:00:00 users 76.882027 2012-07-01 02:00:00 users 56.777168 2012-07-01 03:00:00 users 33.115002 In\u00a0[51]: Copied! <pre># Backtesting with test data and exogenous variables\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = forecaster.max_step,\n         initial_train_size = len(data_exog.loc[:end_validation, :]),  # Training + Validation Data\n         refit              = False\n     )\n\nmetrics, predictions = backtesting_forecaster_multiseries(\n    forecaster        = forecaster,\n    series            = data_exog[series],\n    exog              = data_exog[exog_features],\n    cv                = cv,\n    levels            = forecaster.levels,\n    metric            = \"mean_absolute_error\",\n    suppress_warnings = True,\n    verbose           = False\n)\n</pre> # Backtesting with test data and exogenous variables # ============================================================================== cv = TimeSeriesFold(          steps              = forecaster.max_step,          initial_train_size = len(data_exog.loc[:end_validation, :]),  # Training + Validation Data          refit              = False      )  metrics, predictions = backtesting_forecaster_multiseries(     forecaster        = forecaster,     series            = data_exog[series],     exog              = data_exog[exog_features],     cv                = cv,     levels            = forecaster.levels,     metric            = \"mean_absolute_error\",     suppress_warnings = True,     verbose           = False ) <pre>Epoch 1/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 760ms/step - loss: 0.0074 - val_loss: 0.0121 - learning_rate: 0.0050\nEpoch 2/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 640ms/step - loss: 0.0066 - val_loss: 0.0111 - learning_rate: 0.0050\nEpoch 3/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 628ms/step - loss: 0.0061 - val_loss: 0.0102 - learning_rate: 0.0050\nEpoch 4/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 635ms/step - loss: 0.0057 - val_loss: 0.0102 - learning_rate: 0.0050\nEpoch 5/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 628ms/step - loss: 0.0055 - val_loss: 0.0093 - learning_rate: 0.0050\nEpoch 6/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 683ms/step - loss: 0.0053 - val_loss: 0.0082 - learning_rate: 0.0050\nEpoch 7/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 713ms/step - loss: 0.0049 - val_loss: 0.0079 - learning_rate: 0.0050\nEpoch 8/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 798ms/step - loss: 0.0046 - val_loss: 0.0073 - learning_rate: 0.0050\nEpoch 9/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 794ms/step - loss: 0.0044 - val_loss: 0.0068 - learning_rate: 0.0050\nEpoch 10/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 17s 1s/step - loss: 0.0042 - val_loss: 0.0067 - learning_rate: 0.0050\nEpoch 11/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 16s 1s/step - loss: 0.0041 - val_loss: 0.0063 - learning_rate: 0.0050\nEpoch 12/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 16s 1s/step - loss: 0.0039 - val_loss: 0.0059 - learning_rate: 0.0050\nEpoch 13/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14s 1s/step - loss: 0.0038 - val_loss: 0.0062 - learning_rate: 0.0050\nEpoch 14/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 956ms/step - loss: 0.0037 - val_loss: 0.0056 - learning_rate: 0.0050\nEpoch 15/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11s 825ms/step - loss: 0.0037 - val_loss: 0.0069 - learning_rate: 0.0050\nEpoch 16/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 752ms/step - loss: 0.0040\nEpoch 16: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 805ms/step - loss: 0.0040 - val_loss: 0.0065 - learning_rate: 0.0050\nEpoch 17/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 795ms/step - loss: 0.0035 - val_loss: 0.0052 - learning_rate: 0.0025\nEpoch 18/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11s 821ms/step - loss: 0.0033 - val_loss: 0.0050 - learning_rate: 0.0025\nEpoch 19/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 692ms/step - loss: 0.0033 - val_loss: 0.0049 - learning_rate: 0.0025\nEpoch 20/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 670ms/step - loss: 0.0032\nEpoch 20: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 728ms/step - loss: 0.0032 - val_loss: 0.0049 - learning_rate: 0.0025\nEpoch 21/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 696ms/step - loss: 0.0032 - val_loss: 0.0048 - learning_rate: 0.0012\nEpoch 22/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 713ms/step - loss: 0.0031 - val_loss: 0.0048 - learning_rate: 0.0012\nEpoch 23/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 712ms/step - loss: 0.0031 - val_loss: 0.0047 - learning_rate: 0.0012\nEpoch 24/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 710ms/step - loss: 0.0031 - val_loss: 0.0047 - learning_rate: 0.0012\nEpoch 25/25\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 651ms/step - loss: 0.0031\nEpoch 25: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n13/13 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 704ms/step - loss: 0.0031 - val_loss: 0.0046 - learning_rate: 0.0012\n</pre> <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> In\u00a0[52]: Copied! <pre># Backtesting metrics\n# ==============================================================================\nmetrics\n</pre> # Backtesting metrics # ============================================================================== metrics Out[52]: levels mean_absolute_error 0 users 53.456182 In\u00a0[53]: Copied! <pre># Plotting predictions vs real values in the test set\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\ndata_exog_test[\"users\"].plot(ax=ax, label=\"test\")\npredictions.loc[predictions[\"level\"] == \"users\", \"pred\"].plot(ax=ax, label=\"predictions\")\nax.set_title(\"users\")\nax.legend();\n</pre> # Plotting predictions vs real values in the test set # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) data_exog_test[\"users\"].plot(ax=ax, label=\"test\") predictions.loc[predictions[\"level\"] == \"users\", \"pred\"].plot(ax=ax, label=\"predictions\") ax.set_title(\"users\") ax.legend(); In\u00a0[54]: Copied! <pre># Store in-sample residuals\n# ==============================================================================\nforecaster.set_in_sample_residuals(\n    series=data_exog_train[series], exog=data_exog_train[exog_features]\n)\n</pre> # Store in-sample residuals # ============================================================================== forecaster.set_in_sample_residuals(     series=data_exog_train[series], exog=data_exog_train[exog_features] ) In\u00a0[55]: Copied! <pre># Prediction intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(\n    steps                   = None,\n    exog                    = data_exog_val.loc[:, exog_features],\n    interval                = [10, 90],  # 80% prediction interval\n    method                  = 'conformal',\n    use_in_sample_residuals = True\n)\n\npredictions.head(4)\n</pre> # Prediction intervals # ============================================================================== predictions = forecaster.predict_interval(     steps                   = None,     exog                    = data_exog_val.loc[:, exog_features],     interval                = [10, 90],  # 80% prediction interval     method                  = 'conformal',     use_in_sample_residuals = True )  predictions.head(4) Out[55]: level pred lower_bound upper_bound 2012-07-01 00:00:00 users 112.488240 21.198998 203.777481 2012-07-01 01:00:00 users 76.882029 -14.407213 168.171270 2012-07-01 02:00:00 users 56.777169 -34.512072 148.066411 2012-07-01 03:00:00 users 33.115003 -58.174238 124.404245 In\u00a0[56]: Copied! <pre># Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_exog_val,\n    target_variable     = \"users\",\n    title               = \"Predicted intervals\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.4, 'zorder': 1}\n)\n</pre> # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions         = predictions,     y_true              = data_exog_val,     target_variable     = \"users\",     title               = \"Predicted intervals\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.4, 'zorder': 1} ) In\u00a0[57]: Copied! <pre># Create custom loss function\n# ==============================================================================\n@keras.saving.register_keras_serializable(package=\"custom\", name=\"weighted_mae\")\ndef weighted_mae(y_true, y_pred):\n    error = tf.abs(y_true - y_pred)\n    weights = tf.abs(y_true)\n    \n    return tf.reduce_mean(error * weights)\n</pre> # Create custom loss function # ============================================================================== @keras.saving.register_keras_serializable(package=\"custom\", name=\"weighted_mae\") def weighted_mae(y_true, y_pred):     error = tf.abs(y_true - y_pred)     weights = tf.abs(y_true)          return tf.reduce_mean(error * weights) In\u00a0[58]: Copied! <pre># `create_and_compile_model` with custom loss function\n# ==============================================================================\nseries = ['users']\nlevels = ['users']\nlags = 72\n\nmodel = create_and_compile_model(\n    series                  = data_exog[series],\n    levels                  = levels, \n    lags                    = lags, \n    steps                   = 36, \n    exog                    = data_exog[exog_features], \n    recurrent_layer         = \"LSTM\",\n    recurrent_units         = [128, 64],\n    recurrent_layers_kwargs = {\"activation\": \"tanh\"},\n    dense_units             = [64, 32],\n    compile_kwargs          = {'optimizer': Adam(learning_rate=0.01), 'loss': weighted_mae},  # Custom loss function\n    model_name              = \"Single-Series-Multi-Step-Exog\"\n)\n\nmodel.summary()\n</pre> # `create_and_compile_model` with custom loss function # ============================================================================== series = ['users'] levels = ['users'] lags = 72  model = create_and_compile_model(     series                  = data_exog[series],     levels                  = levels,      lags                    = lags,      steps                   = 36,      exog                    = data_exog[exog_features],      recurrent_layer         = \"LSTM\",     recurrent_units         = [128, 64],     recurrent_layers_kwargs = {\"activation\": \"tanh\"},     dense_units             = [64, 32],     compile_kwargs          = {'optimizer': Adam(learning_rate=0.01), 'loss': weighted_mae},  # Custom loss function     model_name              = \"Single-Series-Multi-Step-Exog\" )  model.summary() <pre>keras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"Single-Series-Multi-Step-Exog\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)        \u2503 Output Shape      \u2503    Param # \u2503 Connected to      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input        \u2502 (None, 72, 1)     \u2502          0 \u2502 -                 \u2502\n\u2502 (InputLayer)        \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_1 (LSTM)       \u2502 (None, 72, 128)   \u2502     66,560 \u2502 series_input[0][\u2026 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_2 (LSTM)       \u2502 (None, 64)        \u2502     49,408 \u2502 lstm_1[0][0]      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 repeat_vector       \u2502 (None, 36, 64)    \u2502          0 \u2502 lstm_2[0][0]      \u2502\n\u2502 (RepeatVector)      \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 exog_input          \u2502 (None, 36, 12)    \u2502          0 \u2502 -                 \u2502\n\u2502 (InputLayer)        \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 concat_exog         \u2502 (None, 36, 76)    \u2502          0 \u2502 repeat_vector[0]\u2026 \u2502\n\u2502 (Concatenate)       \u2502                   \u2502            \u2502 exog_input[0][0]  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_td_1          \u2502 (None, 36, 64)    \u2502      4,928 \u2502 concat_exog[0][0] \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_td_2          \u2502 (None, 36, 32)    \u2502      2,080 \u2502 dense_td_1[0][0]  \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_la\u2026 \u2502 (None, 36, 1)     \u2502         33 \u2502 dense_td_2[0][0]  \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 123,009 (480.50 KB)\n</pre> <pre> Trainable params: 123,009 (480.50 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[59]: Copied! <pre># Forecaster Creation\n# ==============================================================================\nforecaster = ForecasterRnn(\n    regressor=model,\n    levels=levels,\n    lags=lags, \n    transformer_series=MinMaxScaler(),\n    transformer_exog=MinMaxScaler(),\n    fit_kwargs={\n        \"epochs\": 25, \n        \"batch_size\": 1024, \n        \"callbacks\": [\n            EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n            ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1)\n        ],  # Callback to stop training when it is no longer learning and to reduce learning rate.\n        \"series_val\": data_exog_val[series],      # Validation data for model training.\n        \"exog_val\": data_exog_val[exog_features]  # Validation data for exogenous variables\n    },\n)\n\n# Fit forecaster with exogenous variables\n# ==============================================================================\nforecaster.fit(\n    series = data_exog_train[series], \n    exog   = data_exog_train[exog_features]\n)\n</pre> # Forecaster Creation # ============================================================================== forecaster = ForecasterRnn(     regressor=model,     levels=levels,     lags=lags,      transformer_series=MinMaxScaler(),     transformer_exog=MinMaxScaler(),     fit_kwargs={         \"epochs\": 25,          \"batch_size\": 1024,          \"callbacks\": [             EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),             ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1)         ],  # Callback to stop training when it is no longer learning and to reduce learning rate.         \"series_val\": data_exog_val[series],      # Validation data for model training.         \"exog_val\": data_exog_val[exog_features]  # Validation data for exogenous variables     }, )  # Fit forecaster with exogenous variables # ============================================================================== forecaster.fit(     series = data_exog_train[series],      exog   = data_exog_train[exog_features] ) <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n</pre> <pre>Epoch 1/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 15s 1s/step - loss: 0.0483 - val_loss: 0.0742 - learning_rate: 0.0100\nEpoch 2/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 866ms/step - loss: 0.0257 - val_loss: 0.0601 - learning_rate: 0.0100\nEpoch 3/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 868ms/step - loss: 0.0217 - val_loss: 0.0586 - learning_rate: 0.0100\nEpoch 4/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 869ms/step - loss: 0.0193 - val_loss: 0.0615 - learning_rate: 0.0100\nEpoch 5/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 887ms/step - loss: 0.0182 - val_loss: 0.0526 - learning_rate: 0.0100\nEpoch 6/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 868ms/step - loss: 0.0165 - val_loss: 0.0532 - learning_rate: 0.0100\nEpoch 7/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 825ms/step - loss: 0.0170\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 913ms/step - loss: 0.0170 - val_loss: 0.0565 - learning_rate: 0.0100\nEpoch 8/25\n11/11 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 877ms/step - loss: 0.0158 - val_loss: 0.0535 - learning_rate: 0.0050\n</pre> In\u00a0[60]: Copied! <pre># Training and overfitting tracking\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\n_ = forecaster.plot_history(ax=ax)\n</pre> # Training and overfitting tracking # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) _ = forecaster.plot_history(ax=ax) In\u00a0[61]: Copied! <pre># Prediction with exogenous variables\n# ==============================================================================\npredictions = forecaster.predict(exog=data_exog_val[exog_features])\npredictions.head(4)\n</pre> # Prediction with exogenous variables # ============================================================================== predictions = forecaster.predict(exog=data_exog_val[exog_features]) predictions.head(4) Out[61]: level pred 2012-07-01 00:00:00 users 101.560883 2012-07-01 01:00:00 users 93.905083 2012-07-01 02:00:00 users 97.217865 2012-07-01 03:00:00 users 107.752930 In\u00a0[62]: Copied! <pre># Model summary `create_and_compile_model`\n# ==============================================================================\nmodel = create_and_compile_model(\n            series          = data, \n            levels          = [\"o3\"], \n            lags            = 32, \n            steps           = 24, \n            recurrent_layer = \"GRU\", \n            recurrent_units = 100,\n            dense_units     = 64 \n        )\n\nmodel.summary()\n</pre> # Model summary `create_and_compile_model` # ============================================================================== model = create_and_compile_model(             series          = data,              levels          = [\"o3\"],              lags            = 32,              steps           = 24,              recurrent_layer = \"GRU\",              recurrent_units = 100,             dense_units     = 64          )  model.summary() <pre>keras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"functional_2\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input (InputLayer)       \u2502 (None, 32, 10)         \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gru_1 (GRU)                     \u2502 (None, 100)            \u2502        33,600 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 64)             \u2502         6,464 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_layer (Dense)   \u2502 (None, 24)             \u2502         1,560 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 reshape (Reshape)               \u2502 (None, 24, 1)          \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 41,624 (162.59 KB)\n</pre> <pre> Trainable params: 41,624 (162.59 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> Layer Name Type Output Shape Param # Description series_input <code>InputLayer</code> <code>(None, 32, 10)</code> 0 Input layer of the model. It receives input sequences of length 32 (lags) with 10 features (predictors series) per step. gru_1 <code>GRU</code> <code>(None, 100)</code> 33,600 GRU (Gated Recurrent Unit) layer with 100 units and 'tanh' activation. Learns patterns and dependencies over time in the input data. dense_1 <code>Dense</code> <code>(None, 64)</code> 6,464 Fully connected (dense) layer with 64 units and ReLU activation. Processes the features extracted by the GRU layer. output_dense_td_layer <code>Dense</code> <code>(None, 24)</code> 1,560 Dense output layer with 24 units (one for each of the 24 future time steps to predict), linear activation. reshape <code>Reshape</code> <code>(None, 24, 1)</code> 0 Reshapes the output to match the format (steps, output variables). Here, <code>steps=24</code> and <code>levels=[\"o3\"]</code>, so the final output is <code>(None, 24, 1)</code>. <p>Total params: 41,624 \u00a0\u00a0 Trainable params: 41,624 \u00a0\u00a0 Non-trainable params: 0</p> In\u00a0[63]: Copied! <pre># Create calendar exogenous variables\n# ==============================================================================\ndata['hour'] = data.index.hour\ndata['day_of_week'] = data.index.dayofweek\ndata = pd.get_dummies(\n    data, columns=['hour', 'day_of_week'], drop_first=True, dtype=float\n)\ndata.head(3)\n</pre> # Create calendar exogenous variables # ============================================================================== data['hour'] = data.index.hour data['day_of_week'] = data.index.dayofweek data = pd.get_dummies(     data, columns=['hour', 'day_of_week'], drop_first=True, dtype=float ) data.head(3) Out[63]: so2 co no no2 pm10 nox o3 veloc. direc. pm2.5 ... hour_20 hour_21 hour_22 hour_23 day_of_week_1 day_of_week_2 day_of_week_3 day_of_week_4 day_of_week_5 day_of_week_6 datetime 2019-01-01 00:00:00 8.0 0.2 3.0 36.0 22.0 40.0 16.0 0.5 262.0 19.0 ... 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 2019-01-01 01:00:00 8.0 0.1 2.0 40.0 32.0 44.0 6.0 0.6 248.0 26.0 ... 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 2019-01-01 02:00:00 8.0 0.1 11.0 42.0 36.0 58.0 3.0 0.3 224.0 31.0 ... 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 <p>3 rows \u00d7 39 columns</p> In\u00a0[64]: Copied! <pre># Model summary `create_and_compile_model` with exogenous variables\n# ==============================================================================\nseries = ['so2', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.', 'pm2.5']\nexog_features = data.columns.difference(series).tolist()  # dayofweek_* and hour_*\nlevels = ['o3', 'pm2.5', 'pm10']  # Multiple target series to predict\n\nprint(\"Target series:\", levels)\nprint(\"Series as predictors:\", series)\nprint(\"Exogenous variables:\", exog_features)\nprint(\"\")\n\nmodel = create_and_compile_model(\n    series                    = data[series],\n    levels                    = levels, \n    lags                      = 32,\n    steps                     = 24,\n    exog                      = data[exog_features],  \n    recurrent_layer           = \"LSTM\",    \n    recurrent_units           = [128, 64],  \n    recurrent_layers_kwargs   = [{'activation': 'tanh'}, {'activation': 'relu'}],\n    dense_units               = [128, 64],\n    dense_layers_kwargs       = {'activation': 'relu'},\n    output_dense_layer_kwargs = {'activation': 'linear'},\n    compile_kwargs            = {'optimizer': Adam(), 'loss': MeanSquaredError()},\n    model_name                = None\n)\n\nmodel.summary()\n</pre> # Model summary `create_and_compile_model` with exogenous variables # ============================================================================== series = ['so2', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.', 'pm2.5'] exog_features = data.columns.difference(series).tolist()  # dayofweek_* and hour_* levels = ['o3', 'pm2.5', 'pm10']  # Multiple target series to predict  print(\"Target series:\", levels) print(\"Series as predictors:\", series) print(\"Exogenous variables:\", exog_features) print(\"\")  model = create_and_compile_model(     series                    = data[series],     levels                    = levels,      lags                      = 32,     steps                     = 24,     exog                      = data[exog_features],       recurrent_layer           = \"LSTM\",         recurrent_units           = [128, 64],       recurrent_layers_kwargs   = [{'activation': 'tanh'}, {'activation': 'relu'}],     dense_units               = [128, 64],     dense_layers_kwargs       = {'activation': 'relu'},     output_dense_layer_kwargs = {'activation': 'linear'},     compile_kwargs            = {'optimizer': Adam(), 'loss': MeanSquaredError()},     model_name                = None )  model.summary() <pre>Target series: ['o3', 'pm2.5', 'pm10']\nSeries as predictors: ['so2', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.', 'pm2.5']\nExogenous variables: ['day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'hour_1', 'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_2', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9']\n\nkeras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"functional_3\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)        \u2503 Output Shape      \u2503    Param # \u2503 Connected to      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input        \u2502 (None, 32, 10)    \u2502          0 \u2502 -                 \u2502\n\u2502 (InputLayer)        \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_1 (LSTM)       \u2502 (None, 32, 128)   \u2502     71,168 \u2502 series_input[0][\u2026 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 lstm_2 (LSTM)       \u2502 (None, 64)        \u2502     49,408 \u2502 lstm_1[0][0]      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 repeat_vector       \u2502 (None, 24, 64)    \u2502          0 \u2502 lstm_2[0][0]      \u2502\n\u2502 (RepeatVector)      \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 exog_input          \u2502 (None, 24, 29)    \u2502          0 \u2502 -                 \u2502\n\u2502 (InputLayer)        \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 concat_exog         \u2502 (None, 24, 93)    \u2502          0 \u2502 repeat_vector[0]\u2026 \u2502\n\u2502 (Concatenate)       \u2502                   \u2502            \u2502 exog_input[0][0]  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_td_1          \u2502 (None, 24, 128)   \u2502     12,032 \u2502 concat_exog[0][0] \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_td_2          \u2502 (None, 24, 64)    \u2502      8,256 \u2502 dense_td_1[0][0]  \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_la\u2026 \u2502 (None, 24, 3)     \u2502        195 \u2502 dense_td_2[0][0]  \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 141,059 (551.01 KB)\n</pre> <pre> Trainable params: 141,059 (551.01 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> Layer Name Type Output Shape Param # Description series_input <code>InputLayer</code> <code>(None, 32, 10)</code> 0 Input layer for the main time series. Receives sequences of 32 time steps (<code>lags</code>) for 10 features (predictor series). lstm_1 <code>LSTM</code> <code>(None, 32, 128)</code> 71,168 First LSTM layer with 128 units, <code>'tanh'</code> activation. Learns temporal patterns and dependencies from the input sequences. lstm_2 <code>LSTM</code> <code>(None, 64)</code> 49,408 Second LSTM layer with 64 units, <code>'relu'</code> activation. Further summarizes the temporal information. repeat_vector <code>RepeatVector</code> <code>(None, 24, 64)</code> 0 Repeats the output of the previous LSTM layer 24 times, one for each future time step to predict. exog_input <code>InputLayer</code> <code>(None, 24, 29)</code> 0 Input layer for the 29 exogenous variables (calendar and hour features) for each of the 24 future time steps. concat_exog <code>Concatenate</code> <code>(None, 24, 93)</code> 0 Concatenates the repeated LSTM output and the exogenous variables for each prediction time step, joining all features together. dense_td_1 <code>TimeDistributed (Dense)</code> <code>(None, 24, 128)</code> 12,032 Dense layer (128 units, ReLU) applied independently to each of the 24 time steps, learning complex relationships from all features. dense_td_2 <code>TimeDistributed (Dense)</code> <code>(None, 24, 64)</code> 8,256 Second dense layer (64 units, ReLU), also applied to each time step, further processes the combined features. output_dense_td_layer <code>TimeDistributed (Dense)</code> <code>(None, 24, 3)</code> 195 Final output layer, predicts 3 target variables (<code>levels</code>) for each of the 24 future steps (<code>'linear'</code> activation). <p>Total params: 141,059\u2003\u2003Trainable params: 141,059\u2003\u2003Non-trainable params: 0</p> <pre>pip install torch --index-url https://download.pytorch.org/whl/cu126\n</pre> <ol> <li>Check if your GPU is available in Python:</li> </ol> In\u00a0[65]: Copied! <pre># Check if GPU is available\n# ==============================================================================\nimport torch\n\nprint(\"Torch version  :\", torch.__version__)\nprint(\"Cuda available :\", torch.cuda.is_available())\nprint(\"Device name    :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n</pre> # Check if GPU is available # ============================================================================== import torch  print(\"Torch version  :\", torch.__version__) print(\"Cuda available :\", torch.cuda.is_available()) print(\"Device name    :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\") <pre>Torch version  : 2.7.1+cu128\nCuda available : True\nDevice name    : NVIDIA T1200 Laptop GPU\n</pre> <ol> <li>Run your code as usual. If a GPU is detected, skforecast will use it automatically.</li> </ol> <p> \u26a0 Warning </p> <p>If any data transformations is applied, it will affect the output matrices. Consequently, the predictions generated in this transformed scale may require additional steps to revert back to the original data scale.</p> In\u00a0[66]: Copied! <pre># Split train-test\n# ==============================================================================\nend_train = \"2021-09-30 23:59:00\"\ndata_train = data.loc[:end_train, :].copy()\ndata_test = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\"\n)\nprint(\n    f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\"\n)\n\ndata_train.head(3)\n</pre> # Split train-test # ============================================================================== end_train = \"2021-09-30 23:59:00\" data_train = data.loc[:end_train, :].copy() data_test = data.loc[end_train:, :].copy()  print(     f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\" ) print(     f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\" )  data_train.head(3) <pre>Dates train : 2019-01-01 00:00:00 --- 2021-09-30 23:00:00  (n=24096)\nDates test  : 2021-10-01 00:00:00 --- 2021-12-31 23:00:00  (n=2208)\n</pre> Out[66]: so2 co no no2 pm10 nox o3 veloc. direc. pm2.5 ... hour_20 hour_21 hour_22 hour_23 day_of_week_1 day_of_week_2 day_of_week_3 day_of_week_4 day_of_week_5 day_of_week_6 datetime 2019-01-01 00:00:00 8.0 0.2 3.0 36.0 22.0 40.0 16.0 0.5 262.0 19.0 ... 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 2019-01-01 01:00:00 8.0 0.1 2.0 40.0 32.0 44.0 6.0 0.6 248.0 26.0 ... 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 2019-01-01 02:00:00 8.0 0.1 11.0 42.0 36.0 58.0 3.0 0.3 224.0 31.0 ... 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 <p>3 rows \u00d7 39 columns</p> In\u00a0[67]: Copied! <pre># Model summary `create_and_compile_model` with exogenous variables\n# ==============================================================================\nlags = 5\nseries = ['so2', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.', 'pm2.5']\nexog_features = data_train.columns.difference(series).tolist()  # dayofweek_* and hour_*\nlevels = ['o3', 'pm2.5', 'pm10']  # Multiple target series to predict\n\nprint(\"Target series:\", levels)\nprint(\"Series as predictors:\", series)\nprint(\"Exogenous variables:\", exog_features)\nprint(\"\")\n\nmodel = create_and_compile_model(\n            series          = data_train[series],\n            levels          = levels, \n            lags            = lags,\n            steps           = 4,\n            exog            = data_train[exog_features],  \n            recurrent_layer = \"GRU\",    \n            recurrent_units = 64,\n            dense_units     = 32\n        )\n\nmodel.summary()\n</pre> # Model summary `create_and_compile_model` with exogenous variables # ============================================================================== lags = 5 series = ['so2', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.', 'pm2.5'] exog_features = data_train.columns.difference(series).tolist()  # dayofweek_* and hour_* levels = ['o3', 'pm2.5', 'pm10']  # Multiple target series to predict  print(\"Target series:\", levels) print(\"Series as predictors:\", series) print(\"Exogenous variables:\", exog_features) print(\"\")  model = create_and_compile_model(             series          = data_train[series],             levels          = levels,              lags            = lags,             steps           = 4,             exog            = data_train[exog_features],               recurrent_layer = \"GRU\",                 recurrent_units = 64,             dense_units     = 32         )  model.summary() <pre>Target series: ['o3', 'pm2.5', 'pm10']\nSeries as predictors: ['so2', 'co', 'no', 'no2', 'pm10', 'nox', 'o3', 'veloc.', 'direc.', 'pm2.5']\nExogenous variables: ['day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'hour_1', 'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_2', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9']\n\nkeras version: 3.10.0\nUsing backend: tensorflow\ntensorflow version: 2.19.0\n\n</pre> <pre>Model: \"functional_4\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)        \u2503 Output Shape      \u2503    Param # \u2503 Connected to      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 series_input        \u2502 (None, 5, 10)     \u2502          0 \u2502 -                 \u2502\n\u2502 (InputLayer)        \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gru_1 (GRU)         \u2502 (None, 64)        \u2502     14,592 \u2502 series_input[0][\u2026 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 repeat_vector       \u2502 (None, 4, 64)     \u2502          0 \u2502 gru_1[0][0]       \u2502\n\u2502 (RepeatVector)      \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 exog_input          \u2502 (None, 4, 29)     \u2502          0 \u2502 -                 \u2502\n\u2502 (InputLayer)        \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 concat_exog         \u2502 (None, 4, 93)     \u2502          0 \u2502 repeat_vector[0]\u2026 \u2502\n\u2502 (Concatenate)       \u2502                   \u2502            \u2502 exog_input[0][0]  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_td_1          \u2502 (None, 4, 32)     \u2502      3,008 \u2502 concat_exog[0][0] \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 output_dense_td_la\u2026 \u2502 (None, 4, 3)      \u2502         99 \u2502 dense_td_1[0][0]  \u2502\n\u2502 (TimeDistributed)   \u2502                   \u2502            \u2502                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 17,699 (69.14 KB)\n</pre> <pre> Trainable params: 17,699 (69.14 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[68]: Copied! <pre># Forecaster creation\n# ==============================================================================\nforecaster = ForecasterRnn(\n                 regressor          = model, \n                 levels             = levels, \n                 lags               = lags, \n                 transformer_series = MinMaxScaler(), \n                 transformer_exog   = MinMaxScaler(),\n             )\n\nforecaster.fit(series=data_train[series], exog=data_train[exog_features])\nforecaster\n</pre> # Forecaster creation # ============================================================================== forecaster = ForecasterRnn(                  regressor          = model,                   levels             = levels,                   lags               = lags,                   transformer_series = MinMaxScaler(),                   transformer_exog   = MinMaxScaler(),              )  forecaster.fit(series=data_train[series], exog=data_train[exog_features]) forecaster <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 16 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n</pre> <pre>753/753 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 3ms/step - loss: 0.0152\n</pre> Out[68]: ForecasterRnn General Information <ul> <li>Regressor: Functional</li> <li>Layers names: ['series_input', 'gru_1', 'repeat_vector', 'exog_input', 'concat_exog', 'dense_td_1', 'output_dense_td_layer']</li> <li>Lags: [1 2 3 4 5]</li> <li>Window size: 5</li> <li>Maximum steps to predict: [1 2 3 4]</li> <li>Exogenous included: True</li> <li>Creation date: 2025-08-20 11:44:32</li> <li>Last fit date: 2025-08-20 11:44:35</li> <li>Keras backend: tensorflow</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     day_of_week_1, day_of_week_2, day_of_week_3, day_of_week_4, day_of_week_5, day_of_week_6, hour_1, hour_10, hour_11, hour_12, hour_13, hour_14, hour_15, hour_16, hour_17, hour_18, hour_19, hour_2, hour_20, hour_21, hour_22, hour_23, hour_3, hour_4, hour_5, hour_6, hour_7, hour_8, hour_9                 </ul> Data Transformations <ul> <li>Transformer for series: MinMaxScaler()</li> <li>Transformer for exog: MinMaxScaler()</li> </ul> Training Information <ul> <li>Series names: so2, co, no, no2, pm10, nox, o3, veloc., direc., pm2.5</li> <li>Target series (levels): ['o3', 'pm2.5', 'pm10']</li> <li>Training range: [Timestamp('2019-01-01 00:00:00'), Timestamp('2021-09-30 23:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: h</li> </ul> Regressor Parameters <ul>                     {'name': 'functional_4', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 5, 10), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'series_input'}, 'registered_name': None, 'name': 'series_input', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'return_sequences': False, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'Orthogonal', 'config': {'seed': None, 'gain': 1.0}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 5, 10]}, 'name': 'gru_1', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 5, 10), 'dtype': 'float32', 'keras_history': ['series_input', 0, 0]}},), 'kwargs': {'training': False, 'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'RepeatVector', 'config': {'name': 'repeat_vector', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'n': 4}, 'registered_name': None, 'build_config': {'input_shape': [None, 64]}, 'name': 'repeat_vector', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 64), 'dtype': 'float32', 'keras_history': ['gru_1', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 4, 29), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'exog_input'}, 'registered_name': None, 'name': 'exog_input', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'Concatenate', 'config': {'name': 'concat_exog', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'axis': -1}, 'registered_name': None, 'build_config': {'input_shape': [[None, 4, 64], [None, 4, 29]]}, 'name': 'concat_exog', 'inbound_nodes': [{'args': ([{'class_name': '__keras_tensor__', 'config': {'shape': (None, 4, 64), 'dtype': 'float32', 'keras_history': ['repeat_vector', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': (None, 4, 29), 'dtype': 'float32', 'keras_history': ['exog_input', 0, 0]}}],), 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'dense_td_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 93]}}}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 93]}, 'name': 'dense_td_1', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 4, 93), 'dtype': 'float32', 'keras_history': ['concat_exog', 0, 0]}},), 'kwargs': {'mask': None}}]}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'output_dense_td_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 3, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 32]}}}, 'registered_name': None, 'build_config': {'input_shape': [None, 4, 32]}, 'name': 'output_dense_td_layer', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 4, 32), 'dtype': 'float32', 'keras_history': ['dense_td_1', 0, 0]}},), 'kwargs': {'mask': None}}]}], 'input_layers': [['series_input', 0, 0], ['exog_input', 0, 0]], 'output_layers': [['output_dense_td_layer', 0, 0]]}                 </ul> Compile Parameters <ul>                     {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.0010000000474974513, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'MeanSquaredError', 'config': {'name': 'mean_squared_error', 'reduction': 'sum_over_batch_size'}, 'registered_name': None}, 'loss_weights': None, 'metrics': None, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>The <code>ForecasterRNN</code> provides the method <code>create_train_X_y</code>, which allows you to directly extract the matrices used for training. This can be very helpful for debugging, feature engineering, or advanced analysis. This method returns four elements:</p> <ul> <li><p>X_train: Training values (predictors) for each step. The resulting array has 3 dimensions: (<code>n_observations</code>, <code>n_lags</code>, <code>n_series</code>)</p> </li> <li><p>exog_train: Value of exogenous variables (if any) aligned with <code>X_train</code>. (<code>n_observations</code>, <code>n_steps</code>, <code>n_exog</code>)</p> </li> <li><p>y_train: Values (target) of the time series related to each row of <code>X_train</code>. The resulting array has 3 dimensions: (<code>n_observations</code>, <code>n_steps</code>, <code>n_levels</code>)</p> </li> <li><p>dimension_names: A dictionary with the labels (names) for each axis in the arrays, helping you interpret which dimension corresponds to lags, features, steps, etc.</p> </li> </ul> In\u00a0[69]: Copied! <pre># Extract training info\n# ==============================================================================\nX_train, exog_train, y_train, dimension_names = forecaster.create_train_X_y(\n    series=data_train[series], exog=data_train[exog_features]\n)\n</pre> # Extract training info # ============================================================================== X_train, exog_train, y_train, dimension_names = forecaster.create_train_X_y(     series=data_train[series], exog=data_train[exog_features] ) In\u00a0[70]: Copied! <pre># Check the shape of the training data\n# ==============================================================================\nprint(f\"X_train shape        : {X_train.shape} --- (train index, lags, predictors)\")\nprint(f\"exog_train shape     : {exog_train.shape} --- (train index, steps, exogenous variables)\")\nprint(f\"y_train shape        : {y_train.shape}  --- (train index, steps, levels)\")\nprint(f\"dimension_names keys : {list(dimension_names.keys())}\")\n</pre> # Check the shape of the training data # ============================================================================== print(f\"X_train shape        : {X_train.shape} --- (train index, lags, predictors)\") print(f\"exog_train shape     : {exog_train.shape} --- (train index, steps, exogenous variables)\") print(f\"y_train shape        : {y_train.shape}  --- (train index, steps, levels)\") print(f\"dimension_names keys : {list(dimension_names.keys())}\") <pre>X_train shape        : (24088, 5, 10) --- (train index, lags, predictors)\nexog_train shape     : (24088, 4, 29) --- (train index, steps, exogenous variables)\ny_train shape        : (24088, 4, 3)  --- (train index, steps, levels)\ndimension_names keys : ['X_train', 'y_train', 'exog_train']\n</pre> In\u00a0[71]: Copied! <pre># X_train dimension names\n# ==============================================================================\ndimension_names['X_train']  # (train index, lags, predictors)\n</pre> # X_train dimension names # ============================================================================== dimension_names['X_train']  # (train index, lags, predictors) Out[71]: <pre>{0: DatetimeIndex(['2019-01-01 05:00:00', '2019-01-01 06:00:00',\n                '2019-01-01 07:00:00', '2019-01-01 08:00:00',\n                '2019-01-01 09:00:00', '2019-01-01 10:00:00',\n                '2019-01-01 11:00:00', '2019-01-01 12:00:00',\n                '2019-01-01 13:00:00', '2019-01-01 14:00:00',\n                ...\n                '2021-09-30 11:00:00', '2021-09-30 12:00:00',\n                '2021-09-30 13:00:00', '2021-09-30 14:00:00',\n                '2021-09-30 15:00:00', '2021-09-30 16:00:00',\n                '2021-09-30 17:00:00', '2021-09-30 18:00:00',\n                '2021-09-30 19:00:00', '2021-09-30 20:00:00'],\n               dtype='datetime64[ns]', name='datetime', length=24088, freq='h'),\n 1: ['lag_5', 'lag_4', 'lag_3', 'lag_2', 'lag_1'],\n 2: ['so2',\n  'co',\n  'no',\n  'no2',\n  'pm10',\n  'nox',\n  'o3',\n  'veloc.',\n  'direc.',\n  'pm2.5']}</pre> In\u00a0[72]: Copied! <pre># exog_train dimension names\n# ==============================================================================\ndimension_names['exog_train']  # (train index, steps, exogenous variables)\n</pre> # exog_train dimension names # ============================================================================== dimension_names['exog_train']  # (train index, steps, exogenous variables) Out[72]: <pre>{0: DatetimeIndex(['2019-01-01 05:00:00', '2019-01-01 06:00:00',\n                '2019-01-01 07:00:00', '2019-01-01 08:00:00',\n                '2019-01-01 09:00:00', '2019-01-01 10:00:00',\n                '2019-01-01 11:00:00', '2019-01-01 12:00:00',\n                '2019-01-01 13:00:00', '2019-01-01 14:00:00',\n                ...\n                '2021-09-30 11:00:00', '2021-09-30 12:00:00',\n                '2021-09-30 13:00:00', '2021-09-30 14:00:00',\n                '2021-09-30 15:00:00', '2021-09-30 16:00:00',\n                '2021-09-30 17:00:00', '2021-09-30 18:00:00',\n                '2021-09-30 19:00:00', '2021-09-30 20:00:00'],\n               dtype='datetime64[ns]', name='datetime', length=24088, freq='h'),\n 1: ['step_1', 'step_2', 'step_3', 'step_4'],\n 2: ['day_of_week_1',\n  'day_of_week_2',\n  'day_of_week_3',\n  'day_of_week_4',\n  'day_of_week_5',\n  'day_of_week_6',\n  'hour_1',\n  'hour_10',\n  'hour_11',\n  'hour_12',\n  'hour_13',\n  'hour_14',\n  'hour_15',\n  'hour_16',\n  'hour_17',\n  'hour_18',\n  'hour_19',\n  'hour_2',\n  'hour_20',\n  'hour_21',\n  'hour_22',\n  'hour_23',\n  'hour_3',\n  'hour_4',\n  'hour_5',\n  'hour_6',\n  'hour_7',\n  'hour_8',\n  'hour_9']}</pre> In\u00a0[73]: Copied! <pre># y_train dimension names\n# ==============================================================================\ndimension_names['y_train']  # (train index, steps, levels)\n</pre> # y_train dimension names # ============================================================================== dimension_names['y_train']  # (train index, steps, levels) Out[73]: <pre>{0: DatetimeIndex(['2019-01-01 05:00:00', '2019-01-01 06:00:00',\n                '2019-01-01 07:00:00', '2019-01-01 08:00:00',\n                '2019-01-01 09:00:00', '2019-01-01 10:00:00',\n                '2019-01-01 11:00:00', '2019-01-01 12:00:00',\n                '2019-01-01 13:00:00', '2019-01-01 14:00:00',\n                ...\n                '2021-09-30 11:00:00', '2021-09-30 12:00:00',\n                '2021-09-30 13:00:00', '2021-09-30 14:00:00',\n                '2021-09-30 15:00:00', '2021-09-30 16:00:00',\n                '2021-09-30 17:00:00', '2021-09-30 18:00:00',\n                '2021-09-30 19:00:00', '2021-09-30 20:00:00'],\n               dtype='datetime64[ns]', name='datetime', length=24088, freq='h'),\n 1: ['step_1', 'step_2', 'step_3', 'step_4'],\n 2: ['o3', 'pm2.5', 'pm10']}</pre> <p>We can obtain the training predictions using the <code>predict</code> method of the regressor stored inside the forecaster object. By examining the predictions on the training data, analysts can get a better understanding of how the model is performing and make adjustments as necessary.</p> In\u00a0[74]: Copied! <pre># Training predictions using the internal regressor\n# ==============================================================================\nforecaster.regressor.predict(\n    x=X_train if exog_train is None else [X_train, exog_train], verbose=0\n)[:3]\n</pre> # Training predictions using the internal regressor # ============================================================================== forecaster.regressor.predict(     x=X_train if exog_train is None else [X_train, exog_train], verbose=0 )[:3] Out[74]: <pre>array([[[-0.01943135,  0.22262725,  0.10962185],\n        [-0.06106462,  0.22753291,  0.11213958],\n        [-0.06663699,  0.2191191 ,  0.11425731],\n        [ 0.00685285,  0.23398478,  0.12302737]],\n\n       [[-0.05425242,  0.19892402,  0.10047271],\n        [-0.06374246,  0.19064924,  0.10031357],\n        [ 0.00967497,  0.20425256,  0.10949169],\n        [ 0.05100808,  0.18814905,  0.10226992]],\n\n       [[-0.03402879,  0.13570628,  0.07301349],\n        [ 0.03605088,  0.14536186,  0.08544247],\n        [ 0.07774818,  0.12938036,  0.07927521],\n        [ 0.1765889 ,  0.13087219,  0.05503563]]], dtype=float32)</pre> <p>Skforecast provides the <code>create_predict_X</code> method to generate the matrices that the forecaster is using to make predictions. This method can be used to gain insight into the specific data manipulations that occur during the prediction process.</p> In\u00a0[75]: Copied! <pre># Create input matrix for predict method\n# ==============================================================================\nX_predict, exog_predict = forecaster.create_predict_X(\n    steps=None, exog=data_test[exog_features]\n)\nX_predict  # (None, lags, predictors)\n</pre> # Create input matrix for predict method # ============================================================================== X_predict, exog_predict = forecaster.create_predict_X(     steps=None, exog=data_test[exog_features] ) X_predict  # (None, lags, predictors) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DataTransformationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The output matrix is in the transformed scale due to the inclusion of                \u2502\n\u2502 transformations in the Forecaster. As a result, any predictions generated using this \u2502\n\u2502 matrix will also be in the transformed scale. Please refer to the documentation for  \u2502\n\u2502 more details:                                                                        \u2502\n\u2502 https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : DataTransformationWarning                                                 \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\deep_le \u2502\n\u2502 arning\\_forecaster_rnn.py:1337                                                       \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=DataTransformationWarning)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[75]: so2 co no no2 pm10 nox o3 veloc. direc. pm2.5 lag_5 0.000000 0.000000 0.003086 0.054054 0.037383 0.015517 0.551471 0.062500 0.169444 0.074074 lag_4 0.055556 0.000000 0.003086 0.054054 0.040498 0.015517 0.536765 0.046875 0.130556 0.083333 lag_3 0.055556 0.006494 0.003086 0.060811 0.040498 0.018966 0.441176 0.046875 0.102778 0.074074 lag_2 0.111111 0.012987 0.003086 0.074324 0.031153 0.022414 0.419118 0.015625 0.105556 0.064815 lag_1 0.111111 0.019481 0.003086 0.074324 0.040498 0.022414 0.389706 0.015625 0.047222 0.083333 In\u00a0[76]: Copied! <pre># Input matrix for predict method exogenous variables\n# ==============================================================================\nexog_predict  # (None, steps, exogenous variables)\n</pre> # Input matrix for predict method exogenous variables # ============================================================================== exog_predict  # (None, steps, exogenous variables) Out[76]: day_of_week_1 day_of_week_2 day_of_week_3 day_of_week_4 day_of_week_5 day_of_week_6 hour_1 hour_10 hour_11 hour_12 ... hour_21 hour_22 hour_23 hour_3 hour_4 hour_5 hour_6 hour_7 hour_8 hour_9 step_1 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 step_2 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 step_3 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 step_4 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 <p>4 rows \u00d7 29 columns</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#forecasting-with-deep-learning","title":"Forecasting with Deep Learning\u00b6","text":"<p>Deep learning models have become increasingly popular for time series forecasting, especially when traditional statistical approaches struggle to capture non-linear relationships or complex temporal patterns. By leveraging neural network architectures, deep learning methods can automatically learn features and dependencies directly from raw data, offering significant advantages for large datasets, multivariate time series, and problems where classic models fall short.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#introduction-to-recurrent-neural-networks-rnn-lstm-and-gru","title":"Introduction to Recurrent Neural Networks (RNN), LSTM, and GRU\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#lstm-architecture-and-gates","title":"LSTM Architecture and Gates\u00b6","text":"<p>Long Short-Term Memory (LSTM) networks are a widely used type of recurrent neural network designed to effectively capture long-range dependencies in sequential data. Unlike simple RNNs, LSTMs use a more sophisticated architecture based on a system of memory cells and gates that control the flow of information over time.</p> <p>The core component of an LSTM is the memory cell, which maintains information across time steps. Three gates regulate how information is added, retained, or discarded at each step:</p> <ul> <li><p>Forget Gate: Decides which information from the previous cell state should be removed. It uses the current input and previous hidden state, applying a sigmoid activation to produce a value between 0 and 1 (where 0 means \u201ccompletely forget\u201d and 1 means \u201ccompletely keep\u201d).</p> </li> <li><p>Input Gate: Controls how much new information is added to the cell state, again using the current input and previous hidden state with a sigmoid activation.</p> </li> <li><p>Output Gate: Determines how much of the cell state is exposed as output and passed to the next hidden state.</p> </li> </ul> <p>This gating mechanism enables LSTMs to selectively remember or forget information, making them highly effective for modeling sequences with long-term patterns.</p> <p> Diagram of the inputs and outputs of an LSTM. Source: codificandobits https://databasecamp.de/wp-content/uploads/lstm-architecture-1024x709.png. </p> <p>Gated Recurrent Unit (GRU) cells are a simplified alternative to LSTMs, using only two gates (reset and update) but often achieving similar performance. GRUs require fewer parameters and can be computationally more efficient, which may be an advantage for some tasks or larger datasets.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#building-rnn-based-models-easily-with-create_and_compile_model","title":"Building RNN-based models easily with create_and_compile_model\u00b6","text":"<p>skforecast provides the utility function <code>create_and_compile_model</code> to simplify the creation of recurrent neural network architectures (RNN, LSTM, or GRU) for time series forecasting. This function is designed to make it easy for both beginners and advanced users to build and compile Keras models with just a few lines of code.</p> <p>Basic usage</p> <p>For most forecasting scenarios, you can simply specify the time series data, the number of lagged observations, the number of steps to predict, and the type of recurrent layer you wish to use (LSTM, GRU, or SimpleRNN). By default, the function sets reasonable parameters for each layer, but all architectural details can be adjusted to fit specific requirements.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#types-of-problems-in-time-series-forecasting","title":"Types of problems in time series forecasting\u00b6","text":"<p>Deep learning models for time series can handle a wide variety of forecasting scenarios, depending on how you structure your input data and define your prediction targets. These models are flexible enough to:</p> <ul> <li><p>Predict a single value or multiple future values (single-step vs multi-step forecasting).</p> </li> <li><p>Work with a single time series or multiple series (both as predictors and as targets).</p> </li> <li><p>Incorporate exogenous variables (external features or known future information) alongside your main time series data.</p> </li> </ul> <p>By adjusting your data inputs (number of series, steps ahead to predict, and exogenous variables) deep learning architectures can be adapted to solve almost any classical or advanced forecasting problem.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#1-single-series-single-step-forecasting-11","title":"1. Single-Series, Single-Step Forecasting (1:1)\u00b6","text":"<p>In this scenario, the goal is to predict the next value in a single time series, using only its own past observations as predictors. This is known as a univariate autoregressive forecasting problem.</p> <p>For example: Given a sequence of values ${y_{t-3}, y_{t-2}, y_{t-1}}$, predict $y_{t+1}$.</p> <p>This setup is common for classic time series tasks and serves as a good starting point for experimenting with deep learning models.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#2-single-series-multi-step-forecasting-11-multiple-steps","title":"2. Single-Series, Multi-Step Forecasting (1:1, Multiple Steps)\u00b6","text":"<p>In this scenario, the objective is to predict multiple future values of a single time series using only its own past observations as predictors. This is known as multi-step univariate forecasting.</p> <p>For example: Given a sequence of values ${y_{t-24}, ..., y_{t-1}}$, predict ${y_{t+1}, y_{t+2}, ..., y_{t+n}}$, where $n$ is the prediction horizon (number of steps ahead).</p> <p>This setup is common when you want to forecast several periods into the future (e.g., the next 24 hours of ozone concentration).</p> <p>Model Architecture</p> <p>You can use a similar network architecture as in the single-step case, but predicting multiple steps ahead usually benefits from increasing the capacity of the model (e.g., more units in LSTM/GRU layers or additional dense layers). This allows the model to better capture the complexity of forecasting several points at once.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#3-multi-series-single-output-forecasting-n1-multiple-steps","title":"3. Multi-Series, Single-Output Forecasting (N:1, Multiple Steps)\u00b6","text":"<p>In this scenario, the goal is to predict future values of a single target time series by leveraging the past values of multiple related series as predictors. This is known as multivariate forecasting, where the model uses the historical data from several variables to improve the prediction of one specific series.</p> <p>For example: Suppose you want to forecast ozone concentration (<code>o3</code>) for the next 24 hours. In addition to past <code>o3</code> values, you may include other series\u2014such as temperature, wind speed, or other pollutant concentrations\u2014as predictors. The model will then use the combined information from all available series to make a more accurate forecast.</p> <p>Model setup</p> <p>To handle this type of problem, the neural network architecture becomes a bit more complex. An additional recurrent layer is used to process the information from multiple input series, and another dense (fully connected) layer further processes the output from the recurrent layer. With skforecast, building such a model is straightforward: simply pass a list of integers to the <code>recurrent_units</code> and <code>dense_units</code> arguments to add multiple recurrent and dense layers as needed.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#4-multi-series-multi-output-forecasting-nm-multiple-steps","title":"4. Multi-Series, Multi-Output Forecasting (N:M, Multiple Steps)\u00b6","text":"<p>In this scenario, the goal is to predict multiple future values for several time series at once, using the historical data from all available series as input. This is known as multivariate-multioutput forecasting.</p> <p>With this approach, a single model learns to predict several target series simultaneously, capturing relationships and dependencies not only within each series, but also across different series.</p> <p>Real-world applications include:</p> <ul> <li><p>Forecasting the sales of multiple products in an online store, leveraging past sales, pricing history, promotions, and other product-related variables.</p> </li> <li><p>Study the flue gas emissions of a gas turbine, where you want to predict the concentration of multiple pollutants (e.g., NOX, CO) based on past emissions data and other related variables.</p> </li> <li><p>Modeling environmental variables (e.g., pollution, temperature, humidity) together, where the evolution of one variable may influence or be influenced by others.</p> </li> </ul>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#comparing-forecasting-strategies","title":"Comparing Forecasting Strategies\u00b6","text":"<p>As we have seen, various deep learning architectures and modeling strategies can be employed for time series forecasting. In summary, the forecasting approaches can be categorized into:</p> <ul> <li><p>Single series, multi-step forecasting: Predict future values of a single series using only its past values.</p> </li> <li><p>Multivariate, single-output, multi-step forecasting: Use several series as predictors to forecast a target series over multiple future time steps.</p> </li> <li><p>Multivariate, multi-output, multi-step forecasting: Use multiple predictor series to forecast several targets over multiple steps.</p> </li> </ul> <p>Below is a summary table comparing the Mean Absolute Error (MAE) for each approach, calculated on the same target series, <code>\"o3\"</code>:</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#exogenous-variables-in-deep-learning-models","title":"Exogenous variables in deep learning models\u00b6","text":"<p>Exogenous variables are external predictors (such as weather, holidays, or special events) that can influence the target series but are not part of its own historical values. When building deep learning models for time series forecasting, including these variables can help capture important patterns and improve accuracy, as long as their future values are available at prediction time.</p> <p>In this section, we\u2019ll demonstrate how to use exogenous variables in deep learning models with a new dataset: <code>bike_sharing</code>, which contains hourly bike usage in Washington D.C., together with weather and holiday information.</p> <p>To learn more about exogenous variables in skforecast visit the exogenous variables user guide.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#probabilistic-forecasting-with-deep-learning-models","title":"Probabilistic forecasting with deep learning models\u00b6","text":"<p>Conformal prediction is a framework for constructing prediction intervals that are guaranteed to contain the true value with a specified probability (coverage probability). It works by combining the predictions of a point-forecasting model with its past residuals, differences between previous predictions and actual values. These residuals help estimate the uncertainty in the forecast and determine the width of the prediction interval that is then added to the point forecast.</p> <p>To learn more about conformal predictions in skforecast, visit the Probabilistic Forecasting: Conformal Prediction user guide.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#using-custom-loss-functions-in-deep-learning-models","title":"Using Custom Loss Functions in Deep Learning Models\u00b6","text":"<p>By default, Keras models in skforecast can be trained using common loss functions such as <code>MeanSquaredError</code> or <code>MeanAbsoluteError</code>. However, in many forecasting tasks it is useful to design a custom loss function that reflects the specific goals of your problem. For example, you may want to penalize underestimation more than overestimation, handle imbalanced data, or directly optimize for a business-specific metric.</p> <p>Keras and skforecast make this process straightforward:</p> <ul> <li><p>You can define a loss as a Python function that takes <code>y_true</code> and <code>y_pred</code> tensors and returns a scalar loss value.</p> </li> <li><p>For reproducibility and model saving/loading, custom losses should be registered with <code>@keras.saving.register_keras_serializable()</code>.</p> </li> </ul> <p>This flexibility enables you to customize the training process according to your forecasting domain, ensuring that the model optimizes the metrics that are most relevant to your use case.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#understanding-create_and_compile_model-in-depth","title":"Understanding create_and_compile_model in depth\u00b6","text":"<p>The <code>create_and_compile_model</code> function is designed to streamline the process of building and compiling RNN-based Keras models for time series forecasting, with or without exogenous variables. This function allows both rapid prototyping (with sensible defaults) and fine-grained customization for advanced users.</p> <p>How the function works</p> <p>At its core, <code>create_and_compile_model</code> builds a neural network that consists of three main building blocks:</p> <ul> <li><p>Recurrent Layers (LSTM, GRU, or SimpleRNN): These layers capture temporal dependencies in the data. You can control the type, number, and configuration of recurrent layers using <code>recurrent_layer</code>, recurrent_units, and <code>recurrent_layers_kwargs</code>.</p> </li> <li><p>Dense (Fully Connected) Layers: After temporal features are extracted, dense layers help model nonlinear relationships between learned features and the forecasting target(s). The structure is controlled by <code>dense_units</code> and <code>dense_layers_kwargs</code>.</p> </li> <li><p>Output Layer: The final dense layer matches the number of forecasting targets (<code>levels</code>) and steps (<code>steps</code>). Its configuration can be adjusted with <code>output_dense_layer_kwargs</code>.</p> </li> </ul> <p>If you include exogenous variables (<code>exog</code>), the function automatically adjusts the input structure so that the model receives both the main time series and additional features.</p> <p>Parameters</p> <ul> <li><p><code>series</code>: Main time series data (as a DataFrame), each column is treated as an input feature.</p> </li> <li><p><code>lags</code>: Number of past time steps to use as predictors. Defines the input sequence length. The same value must be used later in the ForecasterRnn <code>lags</code> argument.</p> </li> <li><p><code>steps</code>: Number of future time steps to predict.</p> </li> <li><p><code>levels</code>: List of variables to predict (target variables). Can be one or many columns from <code>series</code>. If <code>None</code>, defaults to the names of input series.</p> </li> <li><p><code>exog</code>: Exogenous variables (optional), given as a DataFrame. Must be aligned with <code>series</code>.</p> </li> <li><p><code>recurrent_layer</code>: Type of recurrent layer, choose between <code>'LSTM'</code>, <code>'GRU'</code>, or <code>'RNN'</code>. Keras API: LSTM, GRU, SimpleRNN.</p> </li> <li><p><code>recurrent_units</code>: Number of units per recurrent layer. Accepts a single int (for one layer) or a list/tuple for multiple stacked layers.</p> </li> <li><p><code>recurrent_layers_kwargs</code>: Dictionary (same for all layers) or lists of dictionaries (one per layer) with keyword arguments for the respective recurrent layers (e.g., activation functions, dropout, etc.).</p> </li> <li><p><code>dense_units</code>: Number of units per dense layer. Accepts a single int (for one layer) or a list/tuple for multiple stacked layers.</p> </li> <li><p><code>dense_layers_kwargs</code>: Dictionary (same for all layers) or lists of dictionaries (one per layer) with keyword arguments for the respective dense layers (e.g., activation functions, dropout, etc.).</p> </li> <li><p><code>output_dense_layer_kwargs</code>: Dictionary with keyword arguments for the output dense layer (e.g., activation function, dropout, etc.). Defaults to <code>{'activation': 'linear'}</code>.</p> </li> <li><p><code>compile_kwargs</code>: Dictionary of parameters for Keras\u2019s <code>compile()</code> method, e.g. optimizer, loss function. Defaults to <code>{'optimizer': Adam(), 'loss': MeanSquaredError()}</code>.</p> </li> <li><p><code>model_name</code>: Name of the model.</p> </li> </ul> <p>Visit the full API documentation for <code>create_and_compile_model</code> for more details.</p>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#example-model-summary-and-layer-by-layer-explanation-no-exog","title":"Example: Model summary and layer-by-layer explanation (no exog)\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#example-model-summary-and-layer-by-layer-explanation-exog","title":"Example: Model summary and layer-by-layer explanation (exog)\u00b6","text":""},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#running-on-gpu","title":"Running on GPU\u00b6","text":"<p>skforecast is fully compatible with GPU acceleration. If your computer has a compatible GPU and the right software installed, skforecast will automatically use the GPU to speed up training.</p> <p>Tips for GPU Training</p> <ul> <li><p>Batch size matters: Large batch sizes (for example, 64, 128, 256, or even more) allow the GPU to process more data in one go, making training much faster compared to a CPU. Small batch sizes (for example, 8 or 16) don\u2019t use all the power of the GPU, so training may be only a little faster\u2014or sometimes not faster at all\u2014than using just the CPU.</p> </li> <li><p>Performance boost: On a suitable GPU, training can be many times faster than on CPU. For example, with a large batch size, an NVIDIA T4 GPU can reduce training time from over a minute (CPU) to just a few seconds (GPU).</p> </li> </ul> <p>How to use the GPU with skforecast</p> <ol> <li>Install the GPU version of PyTorch (with CUDA support). Visit the PyTorch installation page and follow the instructions for your system. Make sure to select the version that matches your GPU and CUDA version. For example, to install PyTorch with CUDA 12.6, you can run:</li> </ol>"},{"location":"user_guides/forecasting-with-deep-learning-rnn-lstm.html#how-to-extract-training-and-test-matrices","title":"How to Extract training and test matrices\u00b6","text":"<p>While forecasting models are mainly used to predict future values, it's just as important to understand how the model is learning from the training data. Analyzing input and output matrices used during training, predictions on the training data or exploring the prediction matrices is crucial for assessing model performance and understanding areas for optimization. This process can reveal whether the model is overfitting, underfitting, or struggling with specific patterns in the data.</p>"},{"location":"user_guides/forecasting-xgboost-lightgbm.html","title":"Forecasting with XGBoost and LightGBM","text":"<p> \u270e Note </p> <p> All of the gradient boosting libraries mentioned above - XGBoost, Lightgbm, HistGradientBoostingRegressor, and CatBoost - can handle categorical features natively, but they require specific encoding techniques that may not be entirely intuitive. Detailed information can be found in categorical features and in this two use cases:  <ul> <li>Forecasting time series with gradient boosting</li> <li>Forecasting with XGBoost</li> <li>Forecasting energy demand with machine learning</li> </ul> </p> <p> XGBoost, Lightgbm and CatBoost can benefit from GPU acceleration, which can significantly speed up the training process. To learn more about how to enable GPU acceleration, please refer to the following link: Skforecast GPU acceleration. </p> <p> \ud83d\udca1 Tip </p> <p>Tree-based models, including decision trees, random forests and gradient boosting machines (GBMs) have limitations when it comes to extrapolation, i.e. making predictions or estimates beyond the range of observed data. This limitation becomes particularly critical when forecasting time series data with a trend. Because these models cannot predict values beyond the observed range during training, their predicted values will deviate from the underlying trend.</p> <p>Several strategies have been proposed to address this challenge, with one of the most common approaches being the use of differentiation. The differentiation process involves computing the differences between consecutive observations in the time series. Instead of directly modeling the absolute values, the focus shifts to modeling the relative change ratios. Skforecast introduces the <code>differentiation</code> parameter within its forecaster classes to indicate that a differentiation process must be applied before training the model. It is worth noting that the entire differentiation process is automated and its effects are seamlessly reversed during the prediction phase. This ensures that the resulting forecast values are in the original scale of the time series data.</p> <p>For more details in this topic visit: Modelling time series trend with tree based models.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.recursive import ForecasterRecursive\n</pre> # Libraries # ============================================================================== from xgboost import XGBRegressor from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.preprocessing import RollingFeatures from skforecast.recursive import ForecasterRecursive In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\"h2o_exog\")\n</pre> # Download data # ============================================================================== data = fetch_dataset(\"h2o_exog\") <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[3]: Copied! <pre># Data preprocessing\n# ==============================================================================\ndata.index.name = 'date'\nsteps = 36\ndata_train = data.iloc[:-steps, :]\ndata_test  = data.iloc[-steps:, :]\n</pre> # Data preprocessing # ============================================================================== data.index.name = 'date' steps = 36 data_train = data.iloc[:-steps, :] data_test  = data.iloc[-steps:, :] In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 8,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[7]),\n             )\n\nforecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 8,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[7]),              )  forecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']]) forecaster Out[4]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3 4 5 6 7 8]</li> <li>Window features: ['roll_mean_7']</li> <li>Window size: 8</li> <li>Series name: y</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:55:18</li> <li>Last fit date: 2025-08-06 13:55:22</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     exog_1, exog_2                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']])\n</pre> # Predict # ============================================================================== forecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']]) Out[5]: <pre>2005-07-01    0.941524\n2005-08-01    0.927756\n2005-09-01    1.065236\n2005-10-01    1.088430\n2005-11-01    1.083675\n2005-12-01    1.168836\n2006-01-01    0.967025\n2006-02-01    0.751336\n2006-03-01    0.816116\n2006-04-01    0.801357\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[6]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[6]: feature importance 10 exog_2 121 1 lag_2 94 0 lag_1 59 9 exog_1 43 5 lag_6 42 3 lag_4 41 4 lag_5 37 7 lag_8 25 6 lag_7 22 2 lag_3 14 8 roll_mean_7 11 In\u00a0[7]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = XGBRegressor(random_state=123, enable_categorical=True),\n                 lags            = 8,\n                 window_features = RollingFeatures(stats=['mean'], window_sizes=[7]),\n             )\n\nforecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = XGBRegressor(random_state=123, enable_categorical=True),                  lags            = 8,                  window_features = RollingFeatures(stats=['mean'], window_sizes=[7]),              )  forecaster.fit(y=data_train['y'], exog=data_train[['exog_1', 'exog_2']]) forecaster Out[7]: ForecasterRecursive General Information <ul> <li>Regressor: XGBRegressor</li> <li>Lags: [1 2 3 4 5 6 7 8]</li> <li>Window features: ['roll_mean_7']</li> <li>Window size: 8</li> <li>Series name: y</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:55:23</li> <li>Last fit date: 2025-08-06 13:55:25</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     exog_1, exog_2                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2005-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': True, 'eval_metric': None, 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 123, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[8]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']])\n</pre> # Predict # ============================================================================== forecaster.predict(steps=10, exog=data_test[['exog_1', 'exog_2']]) Out[8]: <pre>2005-07-01    0.881743\n2005-08-01    0.985714\n2005-09-01    1.070262\n2005-10-01    1.099444\n2005-11-01    1.116030\n2005-12-01    1.206317\n2006-01-01    0.977022\n2006-02-01    0.679524\n2006-03-01    0.740902\n2006-04-01    0.742273\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[9]: Copied! <pre># Feature importances\n# ==============================================================================\nforecaster.get_feature_importances()\n</pre> # Feature importances # ============================================================================== forecaster.get_feature_importances() Out[9]: feature importance 0 lag_1 0.299734 10 exog_2 0.255852 1 lag_2 0.105777 6 lag_7 0.100981 4 lag_5 0.083807 7 lag_8 0.063430 9 exog_1 0.055471 3 lag_4 0.017550 5 lag_6 0.010896 2 lag_3 0.004139 8 roll_mean_7 0.002362"},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecasting-with-xgboost-lightgbm-and-other-gradient-boosting-models","title":"Forecasting with XGBoost, LightGBM and other Gradient Boosting models\u00b6","text":"<p>Gradient boosting models have gained popularity in the machine learning community due to their ability to achieve excellent results in a wide range of use cases, including both regression and classification. Although these models have traditionally been less common in forecasting, recent research has shown that they can be highly effective in this domain. Some of the key advantages of using gradient boosting models for forecasting include:</p> <ul> <li><p>The ease with which exogenous variables, in addition to autoregressive variables, can be incorporated into the model.</p> </li> <li><p>The ability to capture non-linear relationships between variables.</p> </li> <li><p>High scalability, which enables the models to handle large volumes of data.</p> </li> </ul> <p>There are several popular implementations of gradient boosting in Python, with four of the most popular being XGBoost, LightGBM, scikit-learn HistGradientBoostingRegressor and CatBoost. All of these libraries follow the scikit-learn API, making them compatible with skforecast.</p>"},{"location":"user_guides/forecasting-xgboost-lightgbm.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecaster-lightgbm","title":"Forecaster LightGBM\u00b6","text":""},{"location":"user_guides/forecasting-xgboost-lightgbm.html#forecaster-xgboost","title":"Forecaster XGBoost\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html","title":"Hyperparameter tuning and lags selection","text":"<p> \u270e Note </p> <p>All backtesting and hyperparameter search functions in the <code>model_selection</code> module include the <code>n_jobs</code> argument, enabling multi-process parallelization to improve computational performance.</p> <p>Its effectiveness depends on factors like the regressor type, the number of model fits to perform, and the volume of data. When <code>n_jobs</code> is set to <code>'auto'</code>, the level of parallelization is automatically determined using heuristic rules designed to select the most efficient configuration for each scenario.</p> <p>For more information, see the guide Parallelization in skforecast.</p> <p> \u270e Note </p> <p>For a more detailed comparison of the results (execution time and metric) obtained with each strategy, visit Hyperparameters and lags search: backtesting vs one-step-ahead.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.model_selection import (\n    TimeSeriesFold,\n    OneStepAheadFold,\n    grid_search_forecaster,\n    random_search_forecaster,\n    bayesian_search_forecaster\n)\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from lightgbm import LGBMRegressor from sklearn.metrics import mean_squared_error from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.plot import set_dark_theme from skforecast.model_selection import (     TimeSeriesFold,     OneStepAheadFold,     grid_search_forecaster,     random_search_forecaster,     bayesian_search_forecaster ) import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data[['y']]\ndata = data.sort_index()\ndata.head(3)\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data[['y']] data = data.sort_index() data.head(3) <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> Out[2]: y datetime 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 In\u00a0[3]: Copied! <pre># Train-val-test dates\n# ==============================================================================\nend_train = '2001-01-01 23:59:00'\nend_val = '2006-01-01 23:59:00'\n\nprint(\n    f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"\n    f\"  (n={len(data.loc[:end_train])})\"\n)\nprint(\n    f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}\"\n    f\"  (n={len(data.loc[end_train:end_val])})\"\n)\nprint(\n    f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}\"\n    f\" (n={len(data.loc[end_val:])})\"\n)\nprint(\"\")\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[:end_train, 'y'].plot(ax=ax, label='train')\ndata.loc[end_train:end_val, 'y'].plot(ax=ax, label='validation')\ndata.loc[end_val:, 'y'].plot(ax=ax, label='test')\nax.legend()\nplt.show();\n</pre> # Train-val-test dates # ============================================================================== end_train = '2001-01-01 23:59:00' end_val = '2006-01-01 23:59:00'  print(     f\"Train dates      : {data.index.min()} --- {data.loc[:end_train].index.max()}\"     f\"  (n={len(data.loc[:end_train])})\" ) print(     f\"Validation dates : {data.loc[end_train:].index.min()} --- {data.loc[:end_val].index.max()}\"     f\"  (n={len(data.loc[end_train:end_val])})\" ) print(     f\"Test dates       : {data.loc[end_val:].index.min()} --- {data.index.max()}\"     f\" (n={len(data.loc[end_val:])})\" ) print(\"\")  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) data.loc[:end_train, 'y'].plot(ax=ax, label='train') data.loc[end_train:end_val, 'y'].plot(ax=ax, label='validation') data.loc[end_val:, 'y'].plot(ax=ax, label='test') ax.legend() plt.show(); <pre>Train dates      : 1991-07-01 00:00:00 --- 2001-01-01 00:00:00  (n=115)\nValidation dates : 2001-02-01 00:00:00 --- 2006-01-01 00:00:00  (n=60)\nTest dates       : 2006-02-01 00:00:00 --- 2008-06-01 00:00:00 (n=29)\n\n</pre> <p> \ud83d\udca1 Tip </p> <p>When using backtesting as the validation strategy, the computational cost of the tuning largely depends on the strategy used to evaluate each hyperparameter combination. In general, the more re-trainings required, the longer the tuning process will take.</p> <p>To speed up the prototyping phase, a two-step approach is recommended. First, run the search with <code>refit=False</code> to explore a broad range of values quickly. Then, refine the search within the most promising region using a tailored backtesting strategy aligned with the specific needs of the use case.</p> <p>For more guidance, refer to the following resource: Which backtesting strategy should I use?.</p> In\u00a0[4]: Copied! <pre># Grid search hyperparameters and lags\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10  # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = {\n    'lags_1': 3,\n    'lags_2': 10,\n    'lags_3': [1, 2, 3, 20]\n}\n\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\n# Folds\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = '2001-01-01 23:59:00',  # Same as len(data.loc[:end_train])\n         refit              = False\n     )\n\nresults = grid_search_forecaster(\n              forecaster    = forecaster,\n              y             = data.loc[:end_val, 'y'],\n              param_grid    = param_grid,\n              lags_grid     = lags_grid,\n              cv            = cv,\n              metric        = 'mean_squared_error',\n              return_best   = True,\n              n_jobs        = 'auto',\n              verbose       = False,\n              show_progress = True\n          )\nresults\n</pre> # Grid search hyperparameters and lags # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10  # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = {     'lags_1': 3,     'lags_2': 10,     'lags_3': [1, 2, 3, 20] }  # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  # Folds cv = TimeSeriesFold(          steps              = 12,          initial_train_size = '2001-01-01 23:59:00',  # Same as len(data.loc[:end_train])          refit              = False      )  results = grid_search_forecaster(               forecaster    = forecaster,               y             = data.loc[:end_val, 'y'],               param_grid    = param_grid,               lags_grid     = lags_grid,               cv            = cv,               metric        = 'mean_squared_error',               return_best   = True,               n_jobs        = 'auto',               verbose       = False,               show_progress = True           ) results <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 5, 'n_estimators': 100}\n  Backtesting metric: 0.04387531272712768\n</pre> Out[4]: lags lags_label params mean_squared_error max_depth n_estimators 0 [1, 2, 3] lags_1 {'max_depth': 5, 'n_estimators': 100} 0.043875 5 100 1 [1, 2, 3] lags_1 {'max_depth': 10, 'n_estimators': 100} 0.043875 10 100 2 [1, 2, 3] lags_1 {'max_depth': 15, 'n_estimators': 100} 0.043875 15 100 3 [1, 2, 3, 20] lags_3 {'max_depth': 10, 'n_estimators': 100} 0.044074 10 100 4 [1, 2, 3, 20] lags_3 {'max_depth': 5, 'n_estimators': 100} 0.044074 5 100 5 [1, 2, 3, 20] lags_3 {'max_depth': 15, 'n_estimators': 100} 0.044074 15 100 6 [1, 2, 3] lags_1 {'max_depth': 10, 'n_estimators': 50} 0.045423 10 50 7 [1, 2, 3] lags_1 {'max_depth': 15, 'n_estimators': 50} 0.045423 15 50 8 [1, 2, 3] lags_1 {'max_depth': 5, 'n_estimators': 50} 0.045423 5 50 9 [1, 2, 3, 20] lags_3 {'max_depth': 10, 'n_estimators': 50} 0.046221 10 50 10 [1, 2, 3, 20] lags_3 {'max_depth': 15, 'n_estimators': 50} 0.046221 15 50 11 [1, 2, 3, 20] lags_3 {'max_depth': 5, 'n_estimators': 50} 0.046221 5 50 12 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 10, 'n_estimators': 100} 0.047896 10 100 13 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 5, 'n_estimators': 100} 0.047896 5 100 14 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 15, 'n_estimators': 100} 0.047896 15 100 15 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 5, 'n_estimators': 50} 0.051399 5 50 16 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 10, 'n_estimators': 50} 0.051399 10 50 17 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] lags_2 {'max_depth': 15, 'n_estimators': 50} 0.051399 15 50 <p>Since <code>return_best = True</code>, the forecaster object is updated with the best configuration found and trained with the whole data set. This means that the final model obtained from grid search will have the best combination of lags and hyperparameters that resulted in the highest performance metric. This final model can then be used for future predictions on new data.</p> In\u00a0[5]: Copied! <pre>forecaster\n</pre> forecaster Out[5]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3]</li> <li>Window features: None</li> <li>Window size: 3</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-11 15:55:27</li> <li>Last fit date: 2025-08-11 15:55:31</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2006-01-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 5, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[6]: Copied! <pre># Random search hyperparameters and lags\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10  # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\n# Regressor hyperparameters\nparam_distributions = {\n    'n_estimators': np.arange(start=10, stop=100, step=1, dtype=int),\n    'max_depth': np.arange(start=5, stop=30, step=1, dtype=int)\n}\n\n# Folds\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False,\n     )\n\nresults = random_search_forecaster(\n              forecaster          = forecaster,\n              y                   = data.loc[:end_val, 'y'],\n              lags_grid           = lags_grid,\n              param_distributions = param_distributions,\n              cv                  = cv,\n              n_iter              = 5,\n              metric              = 'mean_squared_error',\n              return_best         = True,\n              random_state        = 123,\n              n_jobs              = 'auto',\n              verbose             = False,\n              show_progress       = True\n          )\nresults.head(4)\n</pre> # Random search hyperparameters and lags # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10  # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 5]  # Regressor hyperparameters param_distributions = {     'n_estimators': np.arange(start=10, stop=100, step=1, dtype=int),     'max_depth': np.arange(start=5, stop=30, step=1, dtype=int) }  # Folds cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data.loc[:end_train]),          refit              = False,      )  results = random_search_forecaster(               forecaster          = forecaster,               y                   = data.loc[:end_val, 'y'],               lags_grid           = lags_grid,               param_distributions = param_distributions,               cv                  = cv,               n_iter              = 5,               metric              = 'mean_squared_error',               return_best         = True,               random_state        = 123,               n_jobs              = 'auto',               verbose             = False,               show_progress       = True           ) results.head(4) <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5] \n  Parameters: {'n_estimators': np.int64(96), 'max_depth': np.int64(19)}\n  Backtesting metric: 0.04313147793349785\n</pre> Out[6]: lags lags_label params mean_squared_error n_estimators max_depth 0 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'n_estimators': 96, 'max_depth': 19} 0.043131 96 19 1 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'n_estimators': 94, 'max_depth': 28} 0.043171 94 28 2 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'n_estimators': 77, 'max_depth': 17} 0.043663 77 17 3 [1, 2, 3] [1, 2, 3] {'n_estimators': 96, 'max_depth': 19} 0.043868 96 19 <p> \u26a0 Warning </p> <p>The <code>lags_grid</code> argument is not required when using <code>bayesian_search_forecaster</code>. Instead, the <code>lags</code> can be included directly in the <code>search_space</code>, allowing them to be optimized jointly with the other regressor hyperparameters during the search.</p> <p>In skforecast, Bayesian optimization is implemented using Optuna and its <code>Study object</code>. The goal of the optimization is to minimize the metric returned by the validation strategy (either backtesting or one-step-ahead).</p> <p>You can customize the optimization process by passing additional arguments through the <code>kwargs_create_study</code> and <code>kwargs_study_optimize</code> parameters. These are forwarded to Optuna\u2019s <code>create_study</code> and <code>optimize method</code>, respectively.</p> <p>To define the hyperparameter search space, the <code>search_space</code> argument must be a function that takes an Optuna Trial object and returns a dictionary of parameters to evaluate.</p> In\u00a0[7]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10  # Placeholder, the value will be overwritten\n             )\n\n\n# Search space\ndef search_space(trial):\n    search_space  = {\n        'lags'            : trial.suggest_categorical('lags', [3, 5]),\n        'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n    }\n    \n    return search_space\n\n\n# Folds\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data.loc[:end_train]),  # Can use a date: '2001-01-01 23:59:00'\n         refit              = False,\n     )\n\nresults, best_trial = bayesian_search_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data.loc[:end_val, 'y'],\n                          search_space          = search_space,\n                          cv                    = cv,\n                          metric                = 'mean_absolute_error',\n                          n_trials              = 10,\n                          random_state          = 123,\n                          return_best           = False,\n                          n_jobs                = 'auto',\n                          verbose               = False,\n                          show_progress         = True,\n                          kwargs_create_study   = {},\n                          kwargs_study_optimize = {}\n                      )\nresults.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10  # Placeholder, the value will be overwritten              )   # Search space def search_space(trial):     search_space  = {         'lags'            : trial.suggest_categorical('lags', [3, 5]),         'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),         'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])     }          return search_space   # Folds cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data.loc[:end_train]),  # Can use a date: '2001-01-01 23:59:00'          refit              = False,      )  results, best_trial = bayesian_search_forecaster(                           forecaster            = forecaster,                           y                     = data.loc[:end_val, 'y'],                           search_space          = search_space,                           cv                    = cv,                           metric                = 'mean_absolute_error',                           n_trials              = 10,                           random_state          = 123,                           return_best           = False,                           n_jobs                = 'auto',                           verbose               = False,                           show_progress         = True,                           kwargs_create_study   = {},                           kwargs_study_optimize = {}                       ) results.head(4) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> Out[7]: lags params mean_absolute_error n_estimators min_samples_leaf max_features 0 [1, 2, 3, 4, 5] {'n_estimators': 19, 'min_samples_leaf': 3, 'm... 0.126995 19 3 sqrt 1 [1, 2, 3] {'n_estimators': 15, 'min_samples_leaf': 4, 'm... 0.153278 15 4 sqrt 2 [1, 2, 3] {'n_estimators': 13, 'min_samples_leaf': 3, 'm... 0.160396 13 3 sqrt 3 [1, 2, 3, 4, 5] {'n_estimators': 14, 'min_samples_leaf': 5, 'm... 0.172366 14 5 log2 <p>The <code>best_trial</code> return contains the details of the trial that achieved the best result during optimization. For more information, refer to the Study class.</p> In\u00a0[8]: Copied! <pre># Optuna best trial in the study\n# ==============================================================================\nbest_trial\n</pre> # Optuna best trial in the study # ============================================================================== best_trial Out[8]: <pre>FrozenTrial(number=7, state=1, values=[0.1269945910624239], datetime_start=datetime.datetime(2025, 8, 11, 15, 55, 32, 363650), datetime_complete=datetime.datetime(2025, 8, 11, 15, 55, 32, 430415), params={'lags': 5, 'n_estimators': 19, 'min_samples_leaf': 3, 'max_features': 'sqrt'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lags': CategoricalDistribution(choices=(3, 5)), 'n_estimators': IntDistribution(high=20, log=False, low=10, step=1), 'min_samples_leaf': IntDistribution(high=10, log=False, low=1, step=1), 'max_features': CategoricalDistribution(choices=('log2', 'sqrt'))}, trial_id=7, value=None)</pre> <p> \ud83d\udca1 Tip </p> <p>For a more detailed comparison of the results (execution time and metric) obtained with each strategy, visit Hyperparameters and lags search: backtesting vs one-step-ahead.</p> In\u00a0[9]: Copied! <pre># Bayesian search with OneStepAheadFold\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10  # Placeholder, the value will be overwritten\n             )\n\n\n# Search space\ndef search_space(trial):\n    search_space  = {\n        'lags'            : trial.suggest_categorical('lags', [3, 5]),\n        'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n    }\n    \n    return search_space\n\n\n# Folds\ncv = OneStepAheadFold(\n    initial_train_size = len(data.loc[:end_train])  # Can use a date: '2001-01-01 23:59:00'\n)\n\nresults, best_trial = bayesian_search_forecaster(\n                          forecaster            = forecaster,\n                          y                     = data.loc[:end_val, 'y'],\n                          search_space          = search_space,\n                          cv                    = cv,\n                          metric                = 'mean_absolute_error',\n                          n_trials              = 10,\n                          random_state          = 123,\n                          return_best           = False,\n                          n_jobs                = 'auto',\n                          verbose               = False,\n                          show_progress         = True,\n                          kwargs_create_study   = {},\n                          kwargs_study_optimize = {}\n                      )\nresults.head(4)\n</pre> # Bayesian search with OneStepAheadFold # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10  # Placeholder, the value will be overwritten              )   # Search space def search_space(trial):     search_space  = {         'lags'            : trial.suggest_categorical('lags', [3, 5]),         'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),         'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])     }          return search_space   # Folds cv = OneStepAheadFold(     initial_train_size = len(data.loc[:end_train])  # Can use a date: '2001-01-01 23:59:00' )  results, best_trial = bayesian_search_forecaster(                           forecaster            = forecaster,                           y                     = data.loc[:end_val, 'y'],                           search_space          = search_space,                           cv                    = cv,                           metric                = 'mean_absolute_error',                           n_trials              = 10,                           random_state          = 123,                           return_best           = False,                           n_jobs                = 'auto',                           verbose               = False,                           show_progress         = True,                           kwargs_create_study   = {},                           kwargs_study_optimize = {}                       ) results.head(4) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> Out[9]: lags params mean_absolute_error n_estimators min_samples_leaf max_features 0 [1, 2, 3, 4, 5] {'n_estimators': 20, 'min_samples_leaf': 6, 'm... 0.180137 20 6 log2 1 [1, 2, 3, 4, 5] {'n_estimators': 14, 'min_samples_leaf': 5, 'm... 0.180815 14 5 log2 2 [1, 2, 3, 4, 5] {'n_estimators': 16, 'min_samples_leaf': 9, 'm... 0.187584 16 9 log2 3 [1, 2, 3] {'n_estimators': 14, 'min_samples_leaf': 7, 'm... 0.188359 14 7 log2 <p> \ud83d\udca1 Tip </p> <p>More information about time series forecasting metrics can be found in the Metrics guide.</p> In\u00a0[10]: Copied! <pre># Custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred, y_train=None):\n    \"\"\"\n    Calculate the mean squared error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_squared_error(y_true[mask], y_pred[mask])\n    \n    return metric\n</pre> # Custom metric # ============================================================================== def custom_metric(y_true, y_pred, y_train=None):     \"\"\"     Calculate the mean squared error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_squared_error(y_true[mask], y_pred[mask])          return metric In\u00a0[11]: Copied! <pre># Grid search hyperparameter and lags with custom metric\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10  # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\n# Folds\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False,\n     )\n\nresults = grid_search_forecaster(\n              forecaster    = forecaster,\n              y             = data.loc[:end_val, 'y'],\n              cv            = cv,\n              param_grid    = param_grid,\n              lags_grid     = lags_grid,\n              metric        = custom_metric,\n              return_best   = True,\n              n_jobs        = 'auto',\n              verbose       = False,\n              show_progress = True\n          )\n\nresults.head(4)\n</pre> # Grid search hyperparameter and lags with custom metric # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10  # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  # Folds cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data.loc[:end_train]),          refit              = False,      )  results = grid_search_forecaster(               forecaster    = forecaster,               y             = data.loc[:end_val, 'y'],               cv            = cv,               param_grid    = param_grid,               lags_grid     = lags_grid,               metric        = custom_metric,               return_best   = True,               n_jobs        = 'auto',               verbose       = False,               show_progress = True           )  results.head(4) <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3 20] \n  Parameters: {'max_depth': 5, 'n_estimators': 100}\n  Backtesting metric: 0.0681822427249296\n</pre> Out[11]: lags lags_label params custom_metric max_depth n_estimators 0 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 5, 'n_estimators': 100} 0.068182 5 100 1 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.068182 10 100 2 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 15, 'n_estimators': 100} 0.068182 15 100 3 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.070472 5 100 <p> \ud83d\udca1 Tip </p> <p>More information about time series forecasting metrics can be found in the Metrics guide.</p> In\u00a0[12]: Copied! <pre># Grid search hyperparameter and lags with multiple metrics\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10  # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\n# Folds\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False,\n     )\n\nresults = grid_search_forecaster(\n              forecaster    = forecaster,\n              y             = data.loc[:end_val, 'y'],\n              param_grid    = param_grid,\n              lags_grid     = lags_grid,\n              cv            = cv,\n              metric        = ['mean_absolute_error', mean_squared_error, custom_metric],\n              return_best   = True,\n              n_jobs        = 'auto',\n              verbose       = False,\n              show_progress = True\n          )\n\nresults.head(4)\n</pre> # Grid search hyperparameter and lags with multiple metrics # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10  # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  # Folds cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data.loc[:end_train]),          refit              = False,      )  results = grid_search_forecaster(               forecaster    = forecaster,               y             = data.loc[:end_val, 'y'],               param_grid    = param_grid,               lags_grid     = lags_grid,               cv            = cv,               metric        = ['mean_absolute_error', mean_squared_error, custom_metric],               return_best   = True,               n_jobs        = 'auto',               verbose       = False,               show_progress = True           )  results.head(4) <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 5, 'n_estimators': 100}\n  Backtesting metric: 0.18359367014650177\n</pre> Out[12]: lags lags_label params mean_absolute_error mean_squared_error custom_metric max_depth n_estimators 0 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 100} 0.183594 0.043875 0.070472 5 100 1 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 100} 0.183594 0.043875 0.070472 10 100 2 [1, 2, 3] [1, 2, 3] {'max_depth': 15, 'n_estimators': 100} 0.183594 0.043875 0.070472 15 100 3 [1, 2, 3, 20] [1, 2, 3, 20] {'max_depth': 10, 'n_estimators': 100} 0.184901 0.044074 0.068182 10 100 In\u00a0[13]: Copied! <pre># Models to compare\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge\n\nmodels = [\n    RandomForestRegressor(random_state=123), \n    LGBMRegressor(random_state=123, verbose=-1),\n    Ridge(random_state=123)\n]\n\n# Hyperparameter to search for each model\nparam_grids = {\n    'RandomForestRegressor': {'n_estimators': [50, 100], 'max_depth': [5, 15]},\n    'LGBMRegressor': {'n_estimators': [20, 50], 'max_depth': [5, 10]},\n    'Ridge': {'alpha': [0.01, 0.1, 1]}\n}\n\n# Lags used as predictors\nlags_grid = [3, 5]\n\n# Folds\ncv = TimeSeriesFold(\n         steps              = 3,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False,\n     )\n\ndf_results = pd.DataFrame()\nfor i, model in enumerate(models):\n\n    print(f\"Grid search for regressor: {model}\")\n    print(\"-------------------------\")\n\n    forecaster = ForecasterRecursive(\n                     regressor = model,\n                     lags      = 3\n                 )\n\n    # Regressor hyperparameters\n    param_grid = param_grids[list(param_grids)[i]]\n\n    results = grid_search_forecaster(\n                  forecaster    = forecaster,\n                  y             = data.loc[:end_val, 'y'],\n                  param_grid    = param_grid,\n                  lags_grid     = lags_grid,\n                  cv            = cv,\n                  metric        = 'mean_squared_error',\n                  return_best   = False,\n                  n_jobs        = 'auto',\n                  verbose       = False,\n                  show_progress = True\n              )\n    \n    # Create a column with model name\n    results['model'] = list(param_grids)[i]\n    \n    df_results = pd.concat([df_results, results])\n\ndf_results = df_results.sort_values(by='mean_squared_error')\ndf_results.head(10)\n</pre> # Models to compare from sklearn.ensemble import RandomForestRegressor from lightgbm import LGBMRegressor from sklearn.linear_model import Ridge  models = [     RandomForestRegressor(random_state=123),      LGBMRegressor(random_state=123, verbose=-1),     Ridge(random_state=123) ]  # Hyperparameter to search for each model param_grids = {     'RandomForestRegressor': {'n_estimators': [50, 100], 'max_depth': [5, 15]},     'LGBMRegressor': {'n_estimators': [20, 50], 'max_depth': [5, 10]},     'Ridge': {'alpha': [0.01, 0.1, 1]} }  # Lags used as predictors lags_grid = [3, 5]  # Folds cv = TimeSeriesFold(          steps              = 3,          initial_train_size = len(data.loc[:end_train]),          refit              = False,      )  df_results = pd.DataFrame() for i, model in enumerate(models):      print(f\"Grid search for regressor: {model}\")     print(\"-------------------------\")      forecaster = ForecasterRecursive(                      regressor = model,                      lags      = 3                  )      # Regressor hyperparameters     param_grid = param_grids[list(param_grids)[i]]      results = grid_search_forecaster(                   forecaster    = forecaster,                   y             = data.loc[:end_val, 'y'],                   param_grid    = param_grid,                   lags_grid     = lags_grid,                   cv            = cv,                   metric        = 'mean_squared_error',                   return_best   = False,                   n_jobs        = 'auto',                   verbose       = False,                   show_progress = True               )          # Create a column with model name     results['model'] = list(param_grids)[i]          df_results = pd.concat([df_results, results])  df_results = df_results.sort_values(by='mean_squared_error') df_results.head(10) <pre>Grid search for regressor: RandomForestRegressor(random_state=123)\n-------------------------\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Grid search for regressor: LGBMRegressor(random_state=123, verbose=-1)\n-------------------------\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>Grid search for regressor: Ridge(random_state=123)\n-------------------------\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[13]: lags lags_label params mean_squared_error max_depth n_estimators model alpha 1 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'max_depth': 5, 'n_estimators': 50} 0.050180 5.0 50.0 LGBMRegressor NaN 0 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'max_depth': 10, 'n_estimators': 50} 0.050180 10.0 50.0 LGBMRegressor NaN 3 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 50} 0.050907 10.0 50.0 LGBMRegressor NaN 2 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 50} 0.050907 5.0 50.0 LGBMRegressor NaN 5 [1, 2, 3] [1, 2, 3] {'max_depth': 5, 'n_estimators': 20} 0.056990 5.0 20.0 LGBMRegressor NaN 4 [1, 2, 3] [1, 2, 3] {'max_depth': 10, 'n_estimators': 20} 0.056990 10.0 20.0 LGBMRegressor NaN 7 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'max_depth': 10, 'n_estimators': 20} 0.057542 10.0 20.0 LGBMRegressor NaN 6 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'max_depth': 5, 'n_estimators': 20} 0.057542 5.0 20.0 LGBMRegressor NaN 0 [1, 2, 3] [1, 2, 3] {'alpha': 0.01} 0.059814 NaN NaN Ridge 0.01 1 [1, 2, 3] [1, 2, 3] {'alpha': 0.1} 0.060078 NaN NaN Ridge 0.10 In\u00a0[14]: Copied! <pre># Save results to file\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 10  # Placeholder, the value will be overwritten\n             )\n\n# Lags used as predictors\nlags_grid = [3, 10, [1, 2, 3, 20]]\n\n# Regressor hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10, 15]\n}\n\n# Folds\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False\n     )\n\nresults = grid_search_forecaster(\n              forecaster    = forecaster,\n              y             = data.loc[:end_val, 'y'],\n              param_grid    = param_grid,\n              lags_grid     = lags_grid,\n              cv            = cv,\n              metric        = 'mean_squared_error',\n              return_best   = True,\n              n_jobs        = 'auto',\n              verbose       = False,\n              show_progress = True,\n              output_file   = \"results_grid_search.txt\"\n          )\n</pre> # Save results to file # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 10  # Placeholder, the value will be overwritten              )  # Lags used as predictors lags_grid = [3, 10, [1, 2, 3, 20]]  # Regressor hyperparameters param_grid = {     'n_estimators': [50, 100],     'max_depth': [5, 10, 15] }  # Folds cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data.loc[:end_train]),          refit              = False      )  results = grid_search_forecaster(               forecaster    = forecaster,               y             = data.loc[:end_val, 'y'],               param_grid    = param_grid,               lags_grid     = lags_grid,               cv            = cv,               metric        = 'mean_squared_error',               return_best   = True,               n_jobs        = 'auto',               verbose       = False,               show_progress = True,               output_file   = \"results_grid_search.txt\"           ) <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3] \n  Parameters: {'max_depth': 5, 'n_estimators': 100}\n  Backtesting metric: 0.04387531272712768\n</pre> In\u00a0[15]: Copied! <pre># Read results file\n# ==============================================================================\npd.read_csv(\"results_grid_search.txt\", sep=\"\\t\")\n</pre> # Read results file # ============================================================================== pd.read_csv(\"results_grid_search.txt\", sep=\"\\t\") Out[15]: lags lags_label params mean_squared_error max_depth n_estimators 0 [1 2 3] [1 2 3] {'max_depth': 5, 'n_estimators': 50} 0.045423 5 50 1 [1 2 3] [1 2 3] {'max_depth': 5, 'n_estimators': 100} 0.043875 5 100 2 [1 2 3] [1 2 3] {'max_depth': 10, 'n_estimators': 50} 0.045423 10 50 3 [1 2 3] [1 2 3] {'max_depth': 10, 'n_estimators': 100} 0.043875 10 100 4 [1 2 3] [1 2 3] {'max_depth': 15, 'n_estimators': 50} 0.045423 15 50 5 [1 2 3] [1 2 3] {'max_depth': 15, 'n_estimators': 100} 0.043875 15 100 6 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 5, 'n_estimators': 50} 0.051399 5 50 7 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 5, 'n_estimators': 100} 0.047896 5 100 8 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 10, 'n_estimators': 50} 0.051399 10 50 9 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 10, 'n_estimators': 100} 0.047896 10 100 10 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 15, 'n_estimators': 50} 0.051399 15 50 11 [ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10] {'max_depth': 15, 'n_estimators': 100} 0.047896 15 100 12 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 5, 'n_estimators': 50} 0.046221 5 50 13 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 5, 'n_estimators': 100} 0.044074 5 100 14 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 10, 'n_estimators': 50} 0.046221 10 50 15 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 10, 'n_estimators': 100} 0.044074 10 100 16 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 15, 'n_estimators': 50} 0.046221 15 50 17 [ 1  2  3 20] [ 1  2  3 20] {'max_depth': 15, 'n_estimators': 100} 0.044074 15 100"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#hyperparameter-tuning-and-lags-selection","title":"Hyperparameter tuning and lags selection\u00b6","text":"<p>Hyperparameter tuning is a key step in building accurate and robust machine learning models. Hyperparameters are configuration values that cannot be learned directly from data and must be defined by the user before training. These values can significantly affect model performance, and carefully tuning them helps improve both accuracy and generalization.</p> <p>In forecasting models, the selection of lags (past time steps used as predictors) is considered an additional hyperparameter, as it directly influences the model's input structure and learning capacity.</p> <p>Hyperparameter tuning consists of systematically evaluating combinations of hyperparameters (including lags) to find the configuration that yields the best predictive performance. The skforecast library supports several tuning strategies: grid search, random search, and Bayesian search. These strategies can be used with either backtesting or one-step-ahead validation to determine the optimal parameter set for a given forecasting task.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#validation-strategies","title":"Validation strategies\u00b6","text":"<p>Hyperparameter and lag tuning involves systematically testing different values or combinations of hyperparameters (and/or lags) to find the optimal configuration that gives the best performance. The skforecast library provides two different methods to evaluate each candidate configuration:</p> <ul> <li><p>Backtesting: Simulates a real deployment scenario by generating multi-step forecasts in repeated iterations, using the defined forecast horizon and retraining frequency. This approach provides a realistic estimate of performance over time. Use the <code>TimeSeriesFold</code> class for this validation strategy. More information.</p> </li> <li><p>One-Step-Ahead: Evaluates model performance using only one-step-ahead forecasts ($t+1$). This method is faster, as it requires fewer iterations, but it only tests the model's performance in the immediate next time step. Use the <code>OneStepAheadFold</code> class for the one-step-ahead strategy. More information.</p> </li> </ul> <p>Although the two methods may produce different results, they tend to converge on similar hyperparameter selections over time. The one-step-ahead method is faster than backtesting because it requires fewer iterations; however, it only tests the model's performance in the next immediate time step. For a more accurate multi-step performance estimate, it is recommended to backtest the final model.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#grid-search","title":"Grid search\u00b6","text":"<p>Grid search is a popular hyperparameter tuning technique that evaluate an exaustive list of combinations of hyperparameters and lags to find the optimal configuration for a forecasting model. To perform a grid search with the skforecast library, two grids are needed: one with different lags (<code>lags_grid</code>) and another with the hyperparameters (<code>param_grid</code>).</p> <p>The grid search process involves the following steps:</p> <ol> <li><p><code>grid_search_forecaster</code> replaces the <code>lags</code> argument with the first option appearing in <code>lags_grid</code>.</p> </li> <li><p>The function validates all combinations of hyperparameters presented in <code>param_grid</code> using backtesting or one-step-ahead validation validation.</p> </li> <li><p>The function repeats these two steps until it has evaluated all possible combinations of lags and hyperparameters.</p> </li> <li><p>If <code>return_best = True</code>, the original forecaster is trained with the best lags and hyperparameters configuration found during the grid search process.</p> </li> </ol>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#random-search","title":"Random search\u00b6","text":"<p>Random search is another hyperparameter tuning strategy available in the skforecast library. In contrast to grid search, which tries out all possible combinations of hyperparameters and lags, randomized search samples a fixed number of values from the specified possibilities. The number of combinations that are evaluated is given by <code>n_iter</code>.</p> <p>It is important to note that random sampling is only applied to the model hyperparameters, but not to the lags. All lags specified by the user are evaluated.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#bayesian-search","title":"Bayesian search\u00b6","text":"<p>Grid and random search can yield good results, especially when the search space is well-defined. However, these methods do not consider past results, which limits their ability to focus on the most promising regions and avoid uninformative ones.</p> <p>A more efficient alternative is Bayesian optimization, which builds a probabilistic model of the objective function, typically the validation metric (e.g. RMSE, AUC, accuracy). Based on the results observed so far, the algorithm iteratively refines the search, concentrating on regions with the highest potential. This approach reduces the number of evaluations needed by prioritizing the most relevant hyperparameter combinations. It is especially useful when the search space is large or model training is computationally expensive.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#one-step-ahead-validation","title":"One-step-ahead validation\u00b6","text":"<p>One-Step-Ahead evaluates model performance using only one-step-ahead forecasts ($t+1$). This method is faster, as it requires fewer iterations, but it only tests the model's performance in the immediate next time step. Use the <code>OneStepAheadFold</code> class for the one-step-ahead strategy.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#hyperparameter-tuning-with-custom-metric","title":"Hyperparameter tuning with custom metric\u00b6","text":"<p>In addition to standard metrics such as <code>mean_squared_error</code> or <code>mean_absolute_error</code>, users can define custom metric functions, provided they accept the arguments <code>y_true</code> (true values), <code>y_pred</code> (predicted values) and optionally <code>y_train</code> (train values), and return a numeric value (<code>float</code> or <code>int</code>).</p> <p>This flexibility allows evaluating model performance under specific conditions, for example, focusing only on certain months, days, non-holiday periods, or the last step of the forecast horizon.</p> <p>To illustrate this, consider a scenario where a 12-month forecast is generated, but only the last three months of each year are relevant for evaluation. This can be handled by defining a custom metric function that filters the desired months before computing the error, and then passing that function to the backtesting or hyperparameter tuning process.</p> <p>The example below shows how to optimize model parameters using a custom metric focused on the last three months of each forecasted year.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>The functions <code>grid_search_forecaster</code>, <code>random_search_forecaster</code>, and <code>bayesian_search_forecaster</code> support the evaluation of multiple metrics for each forecaster configuration by passing a <code>list</code> of metric functions. This list can include both built-in metrics (e.g. <code>mean_squared_error</code>, <code>mean_absolute_error</code>) and custom-defined ones.</p> <p>When multiple metrics are provided, the first metric in the list is used to select the best model.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#compare-multiple-regressors","title":"Compare multiple regressors\u00b6","text":"<p>The search process can be easily extended to compare several machine learning models. This can be achieved by using a simple for loop that iterates over each regressor and applying the desired function (for example, <code>grid_search_forecaster</code>). This approach allows for a more thorough exploration and can help you select the best model.</p>"},{"location":"user_guides/hyperparameter-tuning-and-lags-selection.html#saving-results-to-file","title":"Saving results to file\u00b6","text":"<p>The results of the hyperparameter search process can be saved to a file by setting the <code>output_file</code> argument to the desired path. The results will be saved in a tab-separated values (TSV) format containing the hyperparameters, lags, and metrics of each configuration evaluated during the search.</p> <p>The saving process occurs after each hyperparameter evaluation, which means that if the optimization is stopped in the middle of the process, the logs of the first part of the evaluation have already been stored in the file. This can be useful for further analysis or to keep a record of the tuning process.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html","title":"Independent multi-time series forecasting","text":"<p> \u270e Note </p> <p>Skforecast offers additional approaches to building global forecasting models:</p> <ul> <li> Global Forecasting Models: Time series with different lengths and different exogenous variables </li> <li> Global Forecasting Models: Dependent multi-series forecasting (Multivariate forecasting) </li> <li> Global Forecasting Models: Forecasting with Deep Learning </li> </ul> <p>To learn more about global forecasting models visit our examples:</p> <ul> <li> Global Forecasting Models: Multi-series forecasting with Python and skforecast </li> <li> Scalable Forecasting: Modeling thousand time series with a single global model </li> <li> Global Forecasting Models: Comparative Analysis of Single and Multi-Series Forecasting Modeling </li> <li> Forecasting with Deep Learning </li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.preprocessing import (\n    reshape_series_wide_to_long,\n    reshape_series_long_to_dict, \n    RollingFeatures\n)\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\nfrom skforecast.model_selection import (\n    OneStepAheadFold,\n    TimeSeriesFold,\n    backtesting_forecaster_multiseries,\n    grid_search_forecaster_multiseries,\n    random_search_forecaster_multiseries,\n    bayesian_search_forecaster_multiseries\n)\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler, MinMaxScaler from sklearn.metrics import mean_absolute_error from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.preprocessing import (     reshape_series_wide_to_long,     reshape_series_long_to_dict,      RollingFeatures ) from skforecast.recursive import ForecasterRecursiveMultiSeries from skforecast.model_selection import (     OneStepAheadFold,     TimeSeriesFold,     backtesting_forecaster_multiseries,     grid_search_forecaster_multiseries,     random_search_forecaster_multiseries,     bayesian_search_forecaster_multiseries ) from skforecast.plot import set_dark_theme <p>The class <code>ForecasterRecursiveMultiSeries</code> allows the simultaneous modeling of time series, which may have equal or varying lengths. Various input types are accepted:</p> <ul> <li><p>If <code>series</code> is a wide-format pandas DataFrame, each column represents a different time series, and the index must be either a <code>DatetimeIndex</code> or a <code>RangeIndex</code> with frequency or step size, as appropriate</p> </li> <li><p>If <code>series</code> is a long-format pandas DataFrame with a MultiIndex, the first level of the index must contain the series IDs, and the second level must be a <code>DatetimeIndex</code> with the same frequency across all series.</p> </li> <li><p>If <code>series</code> is a dictionary, each key must be a series ID, and each value must be a named <code>pandas Series</code>. All series must have the same index, which must be either a <code>DatetimeIndex</code> or a <code>RangeIndex</code>, and they must share the same frequency or step size, as appropriate.</p> </li> </ul> Series type Index requirements <code>Wide DataFrame</code> <code>pandas DatetimeIndex</code> or <code>RangeIndex</code> (all series same step/frequency) <code>MultiIndex DataFrame</code> First level <code>series_id</code>, second <code>datetime</code> (pandas <code>DatetimeIndex</code>) <code>dict</code> <code>pandas DatetimeIndex</code> or <code>RangeIndex</code> (all series same step/frequency) <p> \ud83d\udca1 Tip </p> <p> In terms of performance, using a <code>dict</code> is more efficient than a <code>pandas DataFrame</code>, either wide or long format, especially for larger datasets. This is because dictionaries enable faster access and manipulation of individual time series, without the structural overhead associated with DataFrames. </p> <p>If your original data is not in the desired format, skforecast provides several utility functions to perform the necessary transformations:</p> <ul> <li><p><code>reshape_series_wide_to_long</code>: Converts a wide-format DataFrame into a long-format DataFrame with a MultiIndex, where the first level contains the series IDs and the second level contains a <code>DatetimeIndex</code>.</p> </li> <li><p><code>reshape_series_long_to_dict</code>: Converts a long-format DataFrame with a MultiIndex into a dictionary of pandas Series, where the keys are the series IDs and the values are the Series with the same index as the original DataFrame.</p> </li> </ul> In\u00a0[2]: Copied! <pre># DataFrame wide to long\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\ndisplay(data.head(3))\n\nprint(\"DataFrame wide to long:\")\nseries_long = reshape_series_wide_to_long(data=data)\ndisplay(series_long.head(3))\n</pre> # DataFrame wide to long # ============================================================================== data = fetch_dataset(name=\"items_sales\") display(data.head(3))  print(\"DataFrame wide to long:\") series_long = reshape_series_wide_to_long(data=data) display(series_long.head(3)) <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 <pre>DataFrame wide to long:\n</pre> value series_id datetime item_1 2012-01-01 8.253175 2012-01-02 22.777826 2012-01-03 27.549099 <p>The long format DataFrame have a <code>MultiIndex</code> with the first level containing the series ID, <code>'series_id'</code>, and the second level, <code>'datetime'</code>, containing a <code>DatetimeIndex</code> with the same frequency for each series.</p> In\u00a0[3]: Copied! <pre># MultiIndex\n# ==============================================================================\nseries_long.index[:5]\n</pre> # MultiIndex # ============================================================================== series_long.index[:5] Out[3]: <pre>MultiIndex([('item_1', '2012-01-01'),\n            ('item_1', '2012-01-02'),\n            ('item_1', '2012-01-03'),\n            ('item_1', '2012-01-04'),\n            ('item_1', '2012-01-05')],\n           names=['series_id', 'datetime'])</pre> <p>If you want to leverage the performance of a dictionary, you can use the function <code>reshape_series_long_to_dict</code> to transform a long format DataFrame into a dictionary. The resulting dictionary will have the series ID as keys and the corresponding <code>pandas Series</code> as values.</p> In\u00a0[4]: Copied! <pre># Load time series\n# ==============================================================================\ndisplay(series_long.reset_index().head(3))\n\nprint(\"DataFrame long to dict:\")\nprint(\"\")\nseries_dict = reshape_series_long_to_dict(\n    data      = series_long.reset_index(),\n    series_id = 'series_id',\n    index     = 'datetime',\n    values    = 'value',\n    freq      = 'D'\n)\n\nprint(series_dict.keys())\nprint(\"\")\nprint(series_dict['item_1'].head(3))\n</pre> # Load time series # ============================================================================== display(series_long.reset_index().head(3))  print(\"DataFrame long to dict:\") print(\"\") series_dict = reshape_series_long_to_dict(     data      = series_long.reset_index(),     series_id = 'series_id',     index     = 'datetime',     values    = 'value',     freq      = 'D' )  print(series_dict.keys()) print(\"\") print(series_dict['item_1'].head(3)) series_id datetime value 0 item_1 2012-01-01 8.253175 1 item_1 2012-01-02 22.777826 2 item_1 2012-01-03 27.549099 <pre>DataFrame long to dict:\n\ndict_keys(['item_1', 'item_2', 'item_3'])\n\n2012-01-01     8.253175\n2012-01-02    22.777826\n2012-01-03    27.549099\nFreq: D, Name: item_1, dtype: float64\n</pre> <p>These input formats can be used to train the <code>ForecasterRecursiveMultiSeries</code> class. In this guide, we will use a long format DataFrame with a MultiIndex as input data.</p> <p> \u270e Note </p> <p> When working with time series of different lengths and distinct exogenous variables, it is recommended to use both <code>series</code> and <code>exog</code> as dictionaries. This approach simplifies data management and reduces the likelihood of errors.  <p>For a detailed example, see the user guide Global Forecasting Models: Time series with different lengths and different exogenous variables.</p> </p> In\u00a0[5]: Copied! <pre># Split data into train-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\nseries_long_train = series_long.loc[series_long.index.get_level_values('datetime') &lt;= end_train, :].copy()\nseries_long_test  = series_long.loc[series_long.index.get_level_values('datetime') &gt; end_train, :].copy()\n</pre> # Split data into train-test # ============================================================================== end_train = '2014-07-15 23:59:00' series_long_train = series_long.loc[series_long.index.get_level_values('datetime') &lt;= end_train, :].copy() series_long_test  = series_long.loc[series_long.index.get_level_values('datetime') &gt; end_train, :].copy() In\u00a0[6]: Copied! <pre># Description of each partition\n# ==============================================================================\nfor sid in series_long.index.levels[0]:\n    print(f\"{sid}:\")\n    try:\n        train_sub = series_long_train.loc[sid]\n        print(\n            f\"\\tTrain: len={len(train_sub)}, {train_sub.index[0]} --- {train_sub.index[-1]}\"\n        )\n    except IndexError:\n        print(\"\\tTrain: len=0\")\n    try:\n        test_sub = series_long_test.loc[sid]\n        print(\n            f\"\\tTest : len={len(test_sub)}, {test_sub.index[0]} --- {test_sub.index[-1]}\"\n        )\n    except IndexError:\n        print(\"\\tTest : len=0\")\n</pre> # Description of each partition # ============================================================================== for sid in series_long.index.levels[0]:     print(f\"{sid}:\")     try:         train_sub = series_long_train.loc[sid]         print(             f\"\\tTrain: len={len(train_sub)}, {train_sub.index[0]} --- {train_sub.index[-1]}\"         )     except IndexError:         print(\"\\tTrain: len=0\")     try:         test_sub = series_long_test.loc[sid]         print(             f\"\\tTest : len={len(test_sub)}, {test_sub.index[0]} --- {test_sub.index[-1]}\"         )     except IndexError:         print(\"\\tTest : len=0\") <pre>item_1:\n\tTrain: len=927, 2012-01-01 00:00:00 --- 2014-07-15 00:00:00\n\tTest : len=170, 2014-07-16 00:00:00 --- 2015-01-01 00:00:00\nitem_2:\n\tTrain: len=927, 2012-01-01 00:00:00 --- 2014-07-15 00:00:00\n\tTest : len=170, 2014-07-16 00:00:00 --- 2015-01-01 00:00:00\nitem_3:\n\tTrain: len=927, 2012-01-01 00:00:00 --- 2014-07-15 00:00:00\n\tTest : len=170, 2014-07-16 00:00:00 --- 2015-01-01 00:00:00\n</pre> In\u00a0[7]: Copied! <pre># Plot time series\n# ==============================================================================\nset_dark_theme()\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)\n\nfor i, sid in enumerate(series_long.index.levels[0]):\n    series_long_train.loc[sid, 'value'].plot(ax=axes[i], label='train')\n    series_long_test.loc[sid, 'value'].plot(ax=axes[i], label='test')\n    axes[i].set_title(sid)\n    axes[i].set_ylabel('sales')\n    axes[i].set_xlabel('')\n    axes[i].legend(loc='upper right')\n\nfig.tight_layout()\nplt.show();\n</pre> # Plot time series # ============================================================================== set_dark_theme() fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 5), sharex=True)  for i, sid in enumerate(series_long.index.levels[0]):     series_long_train.loc[sid, 'value'].plot(ax=axes[i], label='train')     series_long_test.loc[sid, 'value'].plot(ax=axes[i], label='test')     axes[i].set_title(sid)     axes[i].set_ylabel('sales')     axes[i].set_xlabel('')     axes[i].legend(loc='upper right')  fig.tight_layout() plt.show(); In\u00a0[8]: Copied! <pre># Create and train ForecasterRecursiveMultiSeries\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),\n                 encoding           = 'ordinal',\n                 transformer_series = None,\n                 transformer_exog   = None,\n                 weight_func        = None,\n                 series_weights     = None,\n                 differentiation    = None,\n                 dropna_from_series = False,\n                 fit_kwargs         = None,\n                 forecaster_id      = None\n             )\n\nforecaster.fit(series=series_long_train, store_in_sample_residuals=True)\nforecaster\n</pre> # Create and train ForecasterRecursiveMultiSeries # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),                  encoding           = 'ordinal',                  transformer_series = None,                  transformer_exog   = None,                  weight_func        = None,                  series_weights     = None,                  differentiation    = None,                  dropna_from_series = False,                  fit_kwargs         = None,                  forecaster_id      = None              )  forecaster.fit(series=series_long_train, store_in_sample_residuals=True) forecaster <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[8]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]</li> <li>Window features: ['roll_mean_24', 'roll_mean_48']</li> <li>Window size: 48</li> <li>Series encoding: ordinal</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Series weights: None</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:57:49</li> <li>Last fit date: 2025-08-06 13:57:51</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): item_1, item_2, item_3</li> <li>Training range: 'item_1': ['2012-01-01', '2014-07-15'], 'item_2': ['2012-01-01', '2014-07-15'], 'item_3': ['2012-01-01', '2014-07-15']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>Two methods can be use to predict the next n steps: <code>predict()</code> or <code>predict_interval()</code>. The argument <code>levels</code> is used to indicate for which series estimate predictions. If <code>None</code> all series will be predicted.</p> In\u00a0[9]: Copied! <pre># Predictions and prediction intervals\n# ==============================================================================\nsteps = 24\n\n# Predictions for item_1\npredictions_item_1 = forecaster.predict(steps=steps, levels='item_1')\ndisplay(predictions_item_1.head(3))\nprint(\"\")\n\n# Interval predictions for item_1 and item_2\npredictions_intervals = forecaster.predict_interval(\n    steps    = steps,\n    levels   = ['item_1', 'item_2'],\n    method   = \"conformal\",\n    interval = 0.9\n)\ndisplay(predictions_intervals.head(3))\n</pre> # Predictions and prediction intervals # ============================================================================== steps = 24  # Predictions for item_1 predictions_item_1 = forecaster.predict(steps=steps, levels='item_1') display(predictions_item_1.head(3)) print(\"\")  # Interval predictions for item_1 and item_2 predictions_intervals = forecaster.predict_interval(     steps    = steps,     levels   = ['item_1', 'item_2'],     method   = \"conformal\",     interval = 0.9 ) display(predictions_intervals.head(3)) level pred 2014-07-16 item_1 25.698703 2014-07-17 item_1 25.676440 2014-07-18 item_1 25.269030 <pre>\n</pre> level pred lower_bound upper_bound 2014-07-16 item_1 25.698703 23.630908 27.766498 2014-07-16 item_2 10.469603 8.967505 11.971702 2014-07-17 item_1 25.676440 23.608645 27.744235 In\u00a0[10]: Copied! <pre># Backtesting multiple time series\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24,\n         initial_train_size = '2014-07-15 23:59:00',  # end_train\n         refit              = True\n     )\n\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = series_long,\n    exog                  = None,\n    cv                    = cv,\n    levels                = None,\n    metric                = 'mean_absolute_error',\n    add_aggregated_metric = True\n)\n\nprint(\"Backtest metrics\")\ndisplay(metrics_levels)\nprint(\"\")\nprint(\"Backtest predictions\")\nbacktest_predictions.head(4)\n</pre> # Backtesting multiple time series # ============================================================================== cv = TimeSeriesFold(          steps              = 24,          initial_train_size = '2014-07-15 23:59:00',  # end_train          refit              = True      )  metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(     forecaster            = forecaster,     series                = series_long,     exog                  = None,     cv                    = cv,     levels                = None,     metric                = 'mean_absolute_error',     add_aggregated_metric = True )  print(\"Backtest metrics\") display(metrics_levels) print(\"\") print(\"Backtest predictions\") backtest_predictions.head(4) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> levels mean_absolute_error 0 item_1 1.194108 1 item_2 2.595094 2 item_3 3.316836 3 average 2.368679 4 weighted_average 2.368679 5 pooling 2.368679 <pre>\nBacktest predictions\n</pre> Out[10]: level pred 2014-07-16 item_1 25.698703 2014-07-16 item_2 10.469603 2014-07-16 item_3 11.329472 2014-07-17 item_1 25.676440 <p> \u270e Note </p> <p>     The <code>ForecasterRecursiveMultiSeries</code> supports the use of distinct exogenous variables for each individual series. For a comprehensive guide on handling time series with varying lengths and exogenous variables, refer to the      Global Forecasting Models: Time Series with Different Lengths and Different Exogenous Variables.      Additionally, for a more general overview of using exogenous variables in forecasting, please consult the      Exogenous Variables User Guide. </p> In\u00a0[11]: Copied! <pre># Generate the exogenous variable month\n# ==============================================================================\nexog_wide = pd.DataFrame(\n    index = series_long.index.get_level_values('datetime').unique()\n)\nexog_wide['month'] = exog_wide.index.month\n\n# Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\nexog_wide_train = exog_wide.loc[:end_train, :].copy()\nexog_wide_test  = exog_wide.loc[end_train:, :].copy()\n\nexog_wide_train.head(3)\n</pre> # Generate the exogenous variable month # ============================================================================== exog_wide = pd.DataFrame(     index = series_long.index.get_level_values('datetime').unique() ) exog_wide['month'] = exog_wide.index.month  # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' exog_wide_train = exog_wide.loc[:end_train, :].copy() exog_wide_test  = exog_wide.loc[end_train:, :].copy()  exog_wide_train.head(3) Out[11]: month datetime 2012-01-01 1 2012-01-02 1 2012-01-03 1 In\u00a0[12]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 12,\n                 window_features = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),\n                 encoding        = 'ordinal'\n             )\n\nforecaster.fit(\n    series = series_long_train, \n    exog   = exog_wide_train\n)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 12,                  window_features = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),                  encoding        = 'ordinal'              )  forecaster.fit(     series = series_long_train,      exog   = exog_wide_train ) forecaster <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[12]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12]</li> <li>Window features: ['roll_mean_24', 'roll_mean_48']</li> <li>Window size: 48</li> <li>Series encoding: ordinal</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Series weights: None</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:57:55</li> <li>Last fit date: 2025-08-06 13:57:56</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     month                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): item_1, item_2, item_3</li> <li>Training range: 'item_1': ['2012-01-01', '2014-07-15'], 'item_2': ['2012-01-01', '2014-07-15'], 'item_3': ['2012-01-01', '2014-07-15']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>If the <code>Forecaster</code> has been trained using exogenous variables, they should be provided during the prediction phase.</p> In\u00a0[13]: Copied! <pre># Predict with exogenous variables\n# ==============================================================================\npredictions = forecaster.predict(steps=24, exog=exog_wide_test)\npredictions.head(3)\n</pre> # Predict with exogenous variables # ============================================================================== predictions = forecaster.predict(steps=24, exog=exog_wide_test) predictions.head(3) Out[13]: level pred 2014-07-16 item_1 25.609387 2014-07-16 item_2 10.674722 2014-07-16 item_3 11.716675 <p>As mentioned earlier, the <code>month</code> exogenous variable is replicated for each of the series. This can be easily demonstrated using the <code>create_train_X_y</code> method, which returns the matrix used in the <code>fit</code> method.</p> In\u00a0[14]: Copied! <pre># X_train matrix\n# ==============================================================================\nX_train = forecaster.create_train_X_y(\n              series            = series_long_train, \n              exog              = exog_wide_train,\n              suppress_warnings = True\n          )[0]\n</pre> # X_train matrix # ============================================================================== X_train = forecaster.create_train_X_y(               series            = series_long_train,                exog              = exog_wide_train,               suppress_warnings = True           )[0] In\u00a0[15]: Copied! <pre># X_train slice for item_1\n# ==============================================================================\nX_train.loc[X_train['_level_skforecast'] == 0].head(3)\n</pre> # X_train slice for item_1 # ============================================================================== X_train.loc[X_train['_level_skforecast'] == 0].head(3) Out[15]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 roll_mean_24 roll_mean_48 _level_skforecast month 2012-02-18 25.609772 27.646380 25.061150 23.61924 20.78839 19.558775 22.208947 23.424717 22.807790 22.861086 23.008517 21.763739 23.796391 23.031106 0 2 2012-02-19 22.504042 25.609772 27.646380 25.06115 23.61924 20.788390 19.558775 22.208947 23.424717 22.807790 22.861086 23.008517 23.536248 23.327999 0 2 2012-02-20 20.838095 22.504042 25.609772 27.64638 25.06115 23.619240 20.788390 19.558775 22.208947 23.424717 22.807790 22.861086 23.408320 23.287588 0 2 In\u00a0[16]: Copied! <pre># X_train slice for item_2\n# ==============================================================================\nX_train.loc[X_train['_level_skforecast'] == 1].head(3)\n</pre> # X_train slice for item_2 # ============================================================================== X_train.loc[X_train['_level_skforecast'] == 1].head(3) Out[16]: lag_1 lag_2 lag_3 lag_4 lag_5 lag_6 lag_7 lag_8 lag_9 lag_10 lag_11 lag_12 roll_mean_24 roll_mean_48 _level_skforecast month 2012-02-18 20.221875 28.195833 22.970833 19.903125 19.239583 18.446875 19.858333 20.844792 17.282292 17.295833 18.459375 18.979167 21.431597 21.400473 1 2 2012-02-19 19.176042 20.221875 28.195833 22.970833 19.903125 19.239583 18.446875 19.858333 20.844792 17.282292 17.295833 18.459375 21.121788 21.361480 1 2 2012-02-20 21.991667 19.176042 20.221875 28.195833 22.970833 19.903125 19.239583 18.446875 19.858333 20.844792 17.282292 17.295833 21.214800 21.265929 1 2 <p>To use exogenous variables in backtesting or hyperparameter tuning, they must be specified with the <code>exog</code> argument.</p> In\u00a0[17]: Copied! <pre># Backtesting Multi-Series with exog\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24,\n         initial_train_size = '2014-07-15 23:59:00',\n         refit              = True,\n     )\n\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = series_long,\n    exog                  = exog_wide,\n    cv                    = cv,\n    levels                = None,\n    metric                = 'mean_absolute_error',\n    add_aggregated_metric = True,\n    suppress_warnings     = True\n)\n\ndisplay(metrics_levels)\nbacktest_predictions.head(4)\n</pre> # Backtesting Multi-Series with exog # ============================================================================== cv = TimeSeriesFold(          steps              = 24,          initial_train_size = '2014-07-15 23:59:00',          refit              = True,      )  metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(     forecaster            = forecaster,     series                = series_long,     exog                  = exog_wide,     cv                    = cv,     levels                = None,     metric                = 'mean_absolute_error',     add_aggregated_metric = True,     suppress_warnings     = True )  display(metrics_levels) backtest_predictions.head(4) <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> levels mean_absolute_error 0 item_1 1.277778 1 item_2 2.497446 2 item_3 3.137108 3 average 2.304111 4 weighted_average 2.304111 5 pooling 2.304111 Out[17]: level pred 2014-07-16 item_1 25.609387 2014-07-16 item_2 10.674722 2014-07-16 item_3 11.716675 2014-07-17 item_1 25.267054 <p><code>ForecasterRecursiveMultiSeries</code> allows to transform series before training the model using the argument <code>transformer_series</code>, three diferent options are available:</p> <ul> <li><p><code>transformer_series</code> is a single transformer: When a single transformer is provided, it is automatically cloned for each individual series. Each cloned transformer is then trained separately on one of the series.</p> </li> <li><p><code>transformer_series</code> is a dictionary: A different transformer can be specified for each series by passing a dictionary where the keys correspond to the series names and the values are the transformers. Each series is transformed according to its designated transformer. When this option is used, it is mandatory to include a transformer for unknown series, which is indicated by the key <code>'_unknown_level'</code>.</p> </li> <li><p><code>transformer_series</code> is <code>None</code>: no transformations are applied to any of the series.</p> </li> </ul> <p>Regardless of the configuration, each series is transformed independently. Even when using a single transformer, it is cloned internally and applied separately to each series.</p> In\u00a0[18]: Copied! <pre># Series transformation: same transformation for all series\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\n\nforecaster.fit(series=series_long_train, suppress_warnings=True)\nforecaster\n</pre> # Series transformation: same transformation for all series # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),                  encoding           = 'ordinal',                  transformer_series = StandardScaler(),                  transformer_exog   = None              )  forecaster.fit(series=series_long_train, suppress_warnings=True) forecaster Out[18]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]</li> <li>Window features: ['roll_mean_24', 'roll_mean_48']</li> <li>Window size: 48</li> <li>Series encoding: ordinal</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Series weights: None</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 13:57:59</li> <li>Last fit date: 2025-08-06 13:57:59</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: StandardScaler()</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): item_1, item_2, item_3</li> <li>Training range: 'item_1': ['2012-01-01', '2014-07-15'], 'item_2': ['2012-01-01', '2014-07-15'], 'item_3': ['2012-01-01', '2014-07-15']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>It is possible to access the fitted transformers for each series through the <code>transformers_series_</code> attribute. This allows verification that each transformer has been trained independently.</p> In\u00a0[19]: Copied! <pre># Mean and scale of the transformer for each series\n# ==============================================================================\nfor k, v in forecaster.transformer_series_.items():\n    print(f\"Series {k}: {v} mean={v.mean_}, scale={v.scale_}\")\n</pre> # Mean and scale of the transformer for each series # ============================================================================== for k, v in forecaster.transformer_series_.items():     print(f\"Series {k}: {v} mean={v.mean_}, scale={v.scale_}\") <pre>Series item_1: StandardScaler() mean=[22.47606719], scale=[2.56240321]\nSeries item_2: StandardScaler() mean=[16.41739687], scale=[5.00145466]\nSeries item_3: StandardScaler() mean=[17.30064109], scale=[5.53439225]\nSeries _unknown_level: StandardScaler() mean=[18.73136838], scale=[5.2799675]\n</pre> In\u00a0[20]: Copied! <pre># Series transformation: different transformation for each series\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),\n                 encoding           = 'ordinal',\n                 transformer_series = {'item_1': StandardScaler(), 'item_2': MinMaxScaler(), '_unknown_level': StandardScaler()},\n                 transformer_exog   = None\n             )\n\nforecaster.fit(series=series_long_train)\n</pre> # Series transformation: different transformation for each series # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),                  encoding           = 'ordinal',                  transformer_series = {'item_1': StandardScaler(), 'item_2': MinMaxScaler(), '_unknown_level': StandardScaler()},                  transformer_exog   = None              )  forecaster.fit(series=series_long_train) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 IgnoredArgumentWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 {'item_3'} not present in `transformer_series`. No transformation is applied to      \u2502\n\u2502 these series.                                                                        \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : IgnoredArgumentWarning                                                    \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:372                                                                          \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=IgnoredArgumentWarning)          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[21]: Copied! <pre># Transformer trained for each series\n# ==============================================================================\nfor k, v in forecaster.transformer_series_.items():\n    if v is not None:\n        print(f\"Series {k}: {v.get_params()}\")\n    else:\n        print(f\"Series {k}: {v}\")\n</pre> # Transformer trained for each series # ============================================================================== for k, v in forecaster.transformer_series_.items():     if v is not None:         print(f\"Series {k}: {v.get_params()}\")     else:         print(f\"Series {k}: {v}\") <pre>Series item_1: {'copy': True, 'with_mean': True, 'with_std': True}\nSeries item_2: {'clip': False, 'copy': True, 'feature_range': (0, 1)}\nSeries item_3: None\nSeries _unknown_level: {'copy': True, 'with_mean': True, 'with_std': True}\n</pre> <p> \ud83d\udca1 Tip </p> <p>More information about time series forecasting metrics can be found in the Metrics guide.</p> In\u00a0[22]: Copied! <pre># Create Forecaster Multi-Series\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal'\n             )\n</pre> # Create Forecaster Multi-Series # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal'              ) In\u00a0[23]: Copied! <pre># Grid search Multi-Series\n# ==============================================================================\nlags_grid = {\n    '24 lags': 24,\n    '48 lags': 48\n}\n\nparam_grid = {\n    'n_estimators': [10, 20],\n    'max_depth': [3, 7]\n}\n\nlevels = ['item_1', 'item_2', 'item_3']\n\ncv = TimeSeriesFold(\n         steps              = 24,\n         initial_train_size = '2014-07-15 23:59:00',  # end_train\n         refit              = False\n     )\n\nresults = grid_search_forecaster_multiseries(\n              forecaster        = forecaster,\n              series            = series_long,\n              exog              = exog_wide,\n              lags_grid         = lags_grid,\n              param_grid        = param_grid,\n              cv                = cv,\n              levels            = levels,\n              metric            = 'mean_absolute_error',\n              aggregate_metric  = 'weighted_average',\n              suppress_warnings = True\n          )\n\nresults.head(4)\n</pre> # Grid search Multi-Series # ============================================================================== lags_grid = {     '24 lags': 24,     '48 lags': 48 }  param_grid = {     'n_estimators': [10, 20],     'max_depth': [3, 7] }  levels = ['item_1', 'item_2', 'item_3']  cv = TimeSeriesFold(          steps              = 24,          initial_train_size = '2014-07-15 23:59:00',  # end_train          refit              = False      )  results = grid_search_forecaster_multiseries(               forecaster        = forecaster,               series            = series_long,               exog              = exog_wide,               lags_grid         = lags_grid,               param_grid        = param_grid,               cv                = cv,               levels            = levels,               metric            = 'mean_absolute_error',               aggregate_metric  = 'weighted_average',               suppress_warnings = True           )  results.head(4) <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] \n  Parameters: {'max_depth': 7, 'n_estimators': 20}\n  Backtesting metric: 2.302108637662591\n  Levels: ['item_1', 'item_2', 'item_3']\n\n</pre> Out[23]: levels lags lags_label params mean_absolute_error__weighted_average max_depth n_estimators 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 48 lags {'max_depth': 7, 'n_estimators': 20} 2.302109 7 20 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 24 lags {'max_depth': 7, 'n_estimators': 20} 2.363481 7 20 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 48 lags {'max_depth': 3, 'n_estimators': 20} 2.439888 3 20 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... 24 lags {'max_depth': 3, 'n_estimators': 20} 2.486789 3 20 <p>It is also possible to perform a bayesian optimization with Optuna using the <code>bayesian_search_forecaster_multiseries</code> function. For more information about this type of optimization, visit the user guide.</p> In\u00a0[24]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal'\n             )\n\nlevels = ['item_1', 'item_2', 'item_3']\n\n# Search space\ndef search_space(trial):\n    search_space  = {\n        'lags'            : trial.suggest_categorical('lags', [24, 48]),\n        'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n    }\n\n    return search_space\n\ncv = OneStepAheadFold(initial_train_size = '2014-07-15 23:59:00')\n\nresults, best_trial = bayesian_search_forecaster_multiseries(\n    forecaster        = forecaster,\n    series            = series_long,\n    exog              = exog_wide,\n    search_space      = search_space,\n    cv                = cv,\n    levels            = levels,\n    metric            = 'mean_absolute_error',\n    aggregate_metric  = ['weighted_average', 'average', 'pooling'],\n    n_trials          = 5,\n    suppress_warnings = True\n)\n\nresults.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal'              )  levels = ['item_1', 'item_2', 'item_3']  # Search space def search_space(trial):     search_space  = {         'lags'            : trial.suggest_categorical('lags', [24, 48]),         'n_estimators'    : trial.suggest_int('n_estimators', 10, 20),         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),         'max_features'    : trial.suggest_categorical('max_features', ['log2', 'sqrt'])     }      return search_space  cv = OneStepAheadFold(initial_train_size = '2014-07-15 23:59:00')  results, best_trial = bayesian_search_forecaster_multiseries(     forecaster        = forecaster,     series            = series_long,     exog              = exog_wide,     search_space      = search_space,     cv                = cv,     levels            = levels,     metric            = 'mean_absolute_error',     aggregate_metric  = ['weighted_average', 'average', 'pooling'],     n_trials          = 5,     suppress_warnings = True )  results.head(4) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] \n  Parameters: {'n_estimators': 16, 'min_samples_leaf': 9, 'max_features': 'log2'}\n  Backtesting metric: 2.1762208064165525\n  Levels: ['item_1', 'item_2', 'item_3']\n\n</pre> <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\jupyter_client\\session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\nc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\jupyter_client\\session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n</pre> Out[24]: levels lags params mean_absolute_error__weighted_average mean_absolute_error__average mean_absolute_error__pooling n_estimators min_samples_leaf max_features 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'n_estimators': 16, 'min_samples_leaf': 9, 'm... 2.176221 2.176221 2.176221 16 9 log2 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'n_estimators': 15, 'min_samples_leaf': 4, 'm... 2.212821 2.212821 2.212821 15 4 sqrt 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'n_estimators': 14, 'min_samples_leaf': 8, 'm... 2.238313 2.238313 2.238313 14 8 log2 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'n_estimators': 13, 'min_samples_leaf': 3, 'm... 2.263358 2.263358 2.263358 13 3 sqrt <p>The <code>best_trial</code> return contains the details of the trial that achieved the best result during optimization. For more information, refer to the Study class.</p> In\u00a0[25]: Copied! <pre># Optuna best trial in the study\n# ==============================================================================\nbest_trial\n</pre> # Optuna best trial in the study # ============================================================================== best_trial Out[25]: <pre>FrozenTrial(number=3, state=1, values=[2.1762208064165525], datetime_start=datetime.datetime(2025, 8, 6, 13, 58, 4, 399533), datetime_complete=datetime.datetime(2025, 8, 6, 13, 58, 4, 663409), params={'lags': 48, 'n_estimators': 16, 'min_samples_leaf': 9, 'max_features': 'log2'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lags': CategoricalDistribution(choices=(24, 48)), 'n_estimators': IntDistribution(high=20, log=False, low=10, step=1), 'min_samples_leaf': IntDistribution(high=10, log=False, low=1, step=1), 'max_features': CategoricalDistribution(choices=('log2', 'sqrt'))}, trial_id=3, value=None)</pre> <p>The <code>ForecasterRecursiveMultiSeries</code> class supports simultaneous modeling of multiple time series, even when they differ in length and use distinct exogenous variables. Various input formats are supported, provided the following conditions are met:</p> <ul> <li><p>When all series have the same length and share the same exogenous variables:</p> <ul> <li>Use a wide-format DataFrame, where each column represents a different time series or exogenous variable. This is the simplest format.</li> </ul> </li> <li><p>When series have different lengths, two formats are supported:</p> <ul> <li><p>A long-format DataFrame with a MultiIndex: The first index level must contain the series IDs, and the second must be a <code>pandas.DatetimeIndex</code> with the same frequency across all series.</p> </li> <li><p>A dictionary: Each key is a series ID and each value is a named <code>pandas.Series</code>. All series must have either a <code>pandas.DatetimeIndex</code> or <code>RangeIndex</code> with consistent frequency or step size.</p> </li> </ul> </li> <li><p>When exogenous variables differ between series, two formats are supported:</p> <ul> <li><p>A long-format DataFrame with a MultiIndex: The first index level must contain the series IDs, and the second must be a <code>pandas.DatetimeIndex</code>. Each column represents a different exogenous variable. No frequency or step size is required.</p> </li> <li><p>A dictionary: Each key is a series ID and each value is a named <code>pandas.Series</code> or <code>DataFrame</code> containing the exogenous variables.  No frequency or step size is required.</p> </li> </ul> </li> </ul> <p>Note: Frequency or step size is required only for the series index. Exogenous variables do not require a specific frequency or step size, as they may not span the entire time period of the series.</p> Series Type Exog Type Index Requirements <code>Wide DataFrame</code> <code>Wide DataFrame</code>, <code>MultiIndex DataFrame</code>, <code>dict</code> <code>pandas.DatetimeIndex</code> or <code>RangeIndex</code> <code>MultiIndex DataFrame</code> <code>Wide DataFrame</code>, <code>MultiIndex DataFrame</code>, <code>dict</code> <code>pandas.DatetimeIndex</code> <code>dict</code> <code>Wide DataFrame</code>, <code>MultiIndex DataFrame</code>, <code>dict</code> <code>pandas.DatetimeIndex</code> or <code>RangeIndex</code> In\u00a0[47]: Copied! <pre># Series and exog as DataFrames \n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 4,\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler()\n             )\n\nX_train, y_train = forecaster.create_train_X_y(\n                       series            = series_long_train,\n                       exog              = exog_wide_train,\n                       suppress_warnings = True\n                   )\nX_train.head(3)\n</pre> # Series and exog as DataFrames  # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 4,                  encoding           = 'ordinal',                  transformer_series = StandardScaler()              )  X_train, y_train = forecaster.create_train_X_y(                        series            = series_long_train,                        exog              = exog_wide_train,                        suppress_warnings = True                    ) X_train.head(3) Out[47]: lag_1 lag_2 lag_3 lag_4 _level_skforecast month 2012-01-05 1.334476 1.979794 0.117764 -5.550607 0 1 2012-01-06 -0.428047 1.334476 1.979794 0.117764 0 1 2012-01-07 -0.534430 -0.428047 1.334476 1.979794 0 1 <p>When <code>exog</code> is a dictionary of <code>pandas Series</code> or <code>DataFrames</code>, different exogenous variables can be used for each series or the same exogenous variable can have different values for each series.</p> In\u00a0[48]: Copied! <pre># Ilustrative example of different values for the same exogenous variable\n# ==============================================================================\nn_rows_train = len(exog_wide_train)\nn_rows_test = len(exog_wide_test)\n\nexog_1_item_1_train = pd.Series([1] * n_rows_train, name='exog_1', index=exog_wide_train.index)\nexog_1_item_2_train = pd.Series([10] * n_rows_train, name='exog_1', index=exog_wide_train.index)\nexog_1_item_3_train = pd.Series([100] * n_rows_train, name='exog_1', index=exog_wide_train.index)\n\nexog_1_item_1_test = pd.Series([1] * n_rows_test, name='exog_1', index=exog_wide_test.index)\nexog_1_item_2_test = pd.Series([10] * n_rows_test, name='exog_1', index=exog_wide_test.index)\nexog_1_item_3_test = pd.Series([100] * n_rows_test, name='exog_1', index=exog_wide_test.index)\n</pre> # Ilustrative example of different values for the same exogenous variable # ============================================================================== n_rows_train = len(exog_wide_train) n_rows_test = len(exog_wide_test)  exog_1_item_1_train = pd.Series([1] * n_rows_train, name='exog_1', index=exog_wide_train.index) exog_1_item_2_train = pd.Series([10] * n_rows_train, name='exog_1', index=exog_wide_train.index) exog_1_item_3_train = pd.Series([100] * n_rows_train, name='exog_1', index=exog_wide_train.index)  exog_1_item_1_test = pd.Series([1] * n_rows_test, name='exog_1', index=exog_wide_test.index) exog_1_item_2_test = pd.Series([10] * n_rows_test, name='exog_1', index=exog_wide_test.index) exog_1_item_3_test = pd.Series([100] * n_rows_test, name='exog_1', index=exog_wide_test.index) In\u00a0[49]: Copied! <pre># Series as DataFrame and exog as dict\n# ==============================================================================\nexog_dict_train = {\n    'item_1': exog_1_item_1_train,\n    'item_2': exog_1_item_2_train,\n    'item_3': exog_1_item_3_train\n}\n\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 4,\n                 encoding  = 'ordinal'\n             )\n\nX_train, y_train = forecaster.create_train_X_y(\n                       series            = series_long_train,\n                       exog              = exog_dict_train,\n                       suppress_warnings = True\n                   )\n\ndisplay(X_train.head(3))\nprint(\"\")\nprint(\"Column `exog_1` as different values for each item ('_level_skforecast' id):\")\nX_train['exog_1'].value_counts()\n</pre> # Series as DataFrame and exog as dict # ============================================================================== exog_dict_train = {     'item_1': exog_1_item_1_train,     'item_2': exog_1_item_2_train,     'item_3': exog_1_item_3_train }  forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 4,                  encoding  = 'ordinal'              )  X_train, y_train = forecaster.create_train_X_y(                        series            = series_long_train,                        exog              = exog_dict_train,                        suppress_warnings = True                    )  display(X_train.head(3)) print(\"\") print(\"Column `exog_1` as different values for each item ('_level_skforecast' id):\") X_train['exog_1'].value_counts() lag_1 lag_2 lag_3 lag_4 _level_skforecast exog_1 2012-01-05 25.895533 27.549099 22.777826 8.253175 0 1 2012-01-06 21.379238 25.895533 27.549099 22.777826 0 1 2012-01-07 21.106643 21.379238 25.895533 27.549099 0 1 <pre>\nColumn `exog_1` as different values for each item ('_level_skforecast' id):\n</pre> Out[49]: <pre>exog_1\n1      923\n10     923\n100    923\nName: count, dtype: int64</pre> In\u00a0[50]: Copied! <pre># Predict with series as DataFrame and exog as dict\n# ==============================================================================\nforecaster.fit(\n    series            = series_long_train,\n    exog              = exog_dict_train,\n    suppress_warnings = True\n)\n\nexog_dict_pred = {\n    'item_1': exog_1_item_1_test,\n    'item_2': exog_1_item_2_test,\n    'item_3': exog_1_item_3_test\n}\n\npredictions = forecaster.predict(steps=24, exog=exog_dict_pred)\npredictions.head(3)\n</pre> # Predict with series as DataFrame and exog as dict # ============================================================================== forecaster.fit(     series            = series_long_train,     exog              = exog_dict_train,     suppress_warnings = True )  exog_dict_pred = {     'item_1': exog_1_item_1_test,     'item_2': exog_1_item_2_test,     'item_3': exog_1_item_3_test }  predictions = forecaster.predict(steps=24, exog=exog_dict_pred) predictions.head(3) Out[50]: level pred 2014-07-16 item_1 25.648910 2014-07-16 item_2 10.677388 2014-07-16 item_3 12.402632 <p> \ud83d\udca1 Tip </p> <p> When working with time series of different lengths and distinct exogenous variables, it is recommended to use both <code>series</code> and <code>exog</code> as dictionaries. This approach simplifies data management and reduces the likelihood of errors.  <p>For a detailed example, see the user guide Global Forecasting Models: Time series with different lengths and different exogenous variables.</p> <p>For a broader overview of how to incorporate exogenous variables in forecasting models, refer to the Exogenous Variables User Guide.</p> </p> <p> \u26a0 Warning </p> <p><code>ForecasterRecursiveMultiSeries</code> class can use <code>encoding='ordinal_category'</code> for encoding time series identifiers. This approach creates a new column (_level_skforecast) of type pandas <code>category</code>. Consequently, the regressors must be able to handle categorical variables. If the regressors do not support categorical variables, the user should set the encoding to <code>'ordinal'</code> or <code>'onehot'</code> for compatibility.</p> <p>Some examples of regressors that support categorical variables and how to enable them are:</p> <p>HistGradientBoostingRegressor</p> <pre>HistGradientBoostingRegressor(categorical_features=\"from_dtype\")\n</pre> <p>LightGBM</p> <p><code>LGBMRegressor</code> does not allow configuration of categorical features during initialization, but rather in its <code>fit</code> method. Therefore, use Forecaster' argument <code>fit_kwargs = {'categorical_feature':'auto'}</code>. This is the default behavior of <code>LGBMRegressor</code> if no indication is given.</p> <p>XGBoost</p> <pre>XGBRegressor(enable_categorical=True)\n</pre> In\u00a0[51]: Copied! <pre># Ordinal_category encoding\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'ordinal_category'\n             )\n\nX, y = forecaster.create_train_X_y(series=series_long_train, suppress_warnings=True)\n\ndisplay(X.head(3))\nprint(\"\")\nprint(X.dtypes)\nprint(\"\")\nprint(X['_level_skforecast'].value_counts())\n</pre> # Ordinal_category encoding # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'ordinal_category'              )  X, y = forecaster.create_train_X_y(series=series_long_train, suppress_warnings=True)  display(X.head(3)) print(\"\") print(X.dtypes) print(\"\") print(X['_level_skforecast'].value_counts()) lag_1 lag_2 lag_3 _level_skforecast 2012-01-04 27.549099 22.777826 8.253175 0 2012-01-05 25.895533 27.549099 22.777826 0 2012-01-06 21.379238 25.895533 27.549099 0 <pre>\nlag_1                 float64\nlag_2                 float64\nlag_3                 float64\n_level_skforecast    category\ndtype: object\n\n_level_skforecast\n0    924\n1    924\n2    924\nName: count, dtype: int64\n</pre> In\u00a0[52]: Copied! <pre># Ordinal encoding\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'ordinal'\n             )\n\nX, y = forecaster.create_train_X_y(series=series_long_train)\n\ndisplay(X.head(3))\nprint(\"\")\nprint(X.dtypes)\nprint(\"\")\nprint(X['_level_skforecast'].value_counts())\n</pre> # Ordinal encoding # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'ordinal'              )  X, y = forecaster.create_train_X_y(series=series_long_train)  display(X.head(3)) print(\"\") print(X.dtypes) print(\"\") print(X['_level_skforecast'].value_counts()) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> lag_1 lag_2 lag_3 _level_skforecast 2012-01-04 27.549099 22.777826 8.253175 0 2012-01-05 25.895533 27.549099 22.777826 0 2012-01-06 21.379238 25.895533 27.549099 0 <pre>\nlag_1                float64\nlag_2                float64\nlag_3                float64\n_level_skforecast      int64\ndtype: object\n\n_level_skforecast\n0    924\n1    924\n2    924\nName: count, dtype: int64\n</pre> In\u00a0[53]: Copied! <pre># Onehot encoding (one column per series)\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'onehot'\n             )\n\nX, y = forecaster.create_train_X_y(series=series_long_train, suppress_warnings=True)\n\ndisplay(X.head(3))\nprint(\"\")\nprint(X.dtypes)\nprint(\"\")\nprint(X['item_1'].value_counts())\n</pre> # Onehot encoding (one column per series) # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'onehot'              )  X, y = forecaster.create_train_X_y(series=series_long_train, suppress_warnings=True)  display(X.head(3)) print(\"\") print(X.dtypes) print(\"\") print(X['item_1'].value_counts()) lag_1 lag_2 lag_3 item_1 item_2 item_3 2012-01-04 27.549099 22.777826 8.253175 1 0 0 2012-01-05 25.895533 27.549099 22.777826 1 0 0 2012-01-06 21.379238 25.895533 27.549099 1 0 0 <pre>\nlag_1     float64\nlag_2     float64\nlag_3     float64\nitem_1      int64\nitem_2      int64\nitem_3      int64\ndtype: object\n\nitem_1\n0    1848\n1     924\nName: count, dtype: int64\n</pre> In\u00a0[54]: Copied! <pre># Onehot encoding (one column per series)\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = None\n             )\n\nX, y = forecaster.create_train_X_y(series=series_long_train, suppress_warnings=True)\n\ndisplay(X.head(3))\nprint(\"\")\nprint(X.dtypes)\n</pre> # Onehot encoding (one column per series) # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = None              )  X, y = forecaster.create_train_X_y(series=series_long_train, suppress_warnings=True)  display(X.head(3)) print(\"\") print(X.dtypes) lag_1 lag_2 lag_3 2012-01-04 27.549099 22.777826 8.253175 2012-01-05 25.895533 27.549099 22.777826 2012-01-06 21.379238 25.895533 27.549099 <pre>\nlag_1    float64\nlag_2    float64\nlag_3    float64\ndtype: object\n</pre> <p> \u26a0 Warning </p> <p>Since the unknown series are encoded as NaN when the forecaster uses the <code>'ordinal_category'</code> or <code>'ordinal'</code> encoding, only regressors that can handle missing values can be used, otherwise an error will be raised.</p> In\u00a0[55]: Copied! <pre># Forecaster trainied with series item_1, item_2 and item_3\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'ordinal'\n             )\n\nforecaster.fit(series=series_long_train)\nprint(f\"Series seen by during training: {forecaster.series_names_in_}\")\n</pre> # Forecaster trainied with series item_1, item_2 and item_3 # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'ordinal'              )  forecaster.fit(series=series_long_train) print(f\"Series seen by during training: {forecaster.series_names_in_}\") <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>Series seen by during training: ['item_1', 'item_2', 'item_3']\n</pre> <p>The <code>last_window</code> argument is used to provide a pandas <code>DataFrame</code> containing the most recent observations of the target series required to generate the predictors. If no past observations are available, and the underlying regressor is capable of handling missing values, <code>last_window</code> can consist of a <code>DataFrame</code> filled with <code>NaNs</code>.</p> In\u00a0[56]: Copied! <pre># Forecasting a new series not seen in the training\n# ==============================================================================\nlast_window_item_4 = pd.DataFrame(\n    data    = [23.46, 22.3587, 29.348],\n    columns = ['item_4'],\n    index   = pd.date_range(start=\"2014-07-13\", periods=3, freq=\"D\"),\n)\nlast_window_item_4\n</pre> # Forecasting a new series not seen in the training # ============================================================================== last_window_item_4 = pd.DataFrame(     data    = [23.46, 22.3587, 29.348],     columns = ['item_4'],     index   = pd.date_range(start=\"2014-07-13\", periods=3, freq=\"D\"), ) last_window_item_4 Out[56]: item_4 2014-07-13 23.4600 2014-07-14 22.3587 2014-07-15 29.3480 In\u00a0[57]: Copied! <pre># Forecasting a new series not seen in the training\n# ==============================================================================\nforecaster.predict(\n    levels            = 'item_4', \n    steps             = 3, \n    last_window       = last_window_item_4,\n    suppress_warnings = False\n)\n</pre> # Forecasting a new series not seen in the training # ============================================================================== forecaster.predict(     levels            = 'item_4',      steps             = 3,      last_window       = last_window_item_4,     suppress_warnings = False ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 UnknownLevelWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `levels` {'item_4'} were not included in training. Unknown levels are encoded as     \u2502\n\u2502 NaN, which may cause the prediction to fail if the regressor does not accept NaN     \u2502\n\u2502 values.                                                                              \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : UnknownLevelWarning                                                       \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:910                                                                          \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=UnknownLevelWarning)             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[57]: level pred 2014-07-16 item_4 24.351416 2014-07-17 item_4 25.779253 2014-07-18 item_4 25.637366 <p>If the forecaster's <code>encoding</code> parameter is set to <code>None</code>, the model does not take the series ID into account. As a result, it can generate forecasts for previously unseen series, as long as their corresponding <code>last_window</code> is provided.</p> In\u00a0[58]: Copied! <pre># Forecaster trainied with series item_1, item_2 and item_3 without encoding\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = None\n             )\n\nforecaster.fit(series=series_long_train, suppress_warnings=True)\n</pre> # Forecaster trainied with series item_1, item_2 and item_3 without encoding # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = None              )  forecaster.fit(series=series_long_train, suppress_warnings=True) In\u00a0[59]: Copied! <pre># Forecasting a new series not seen in the training\n# ==============================================================================\nforecaster.predict(\n    levels            = 'item_4', \n    steps             = 3, \n    last_window       = last_window_item_4,\n    suppress_warnings = False\n)\n</pre> # Forecasting a new series not seen in the training # ============================================================================== forecaster.predict(     levels            = 'item_4',      steps             = 3,      last_window       = last_window_item_4,     suppress_warnings = False ) Out[59]: level pred 2014-07-16 item_4 21.897883 2014-07-17 item_4 19.992323 2014-07-18 item_4 19.847593 <p>Forecast intervals for previously unseen series are also supported. In this case, a random sample of residuals\u2014stored under the <code>_unknown_level</code> key\u2014is drawn from the residuals of the known series. The reliability of the resulting intervals depends on the degree of similarity between the unknown series and those used during training.</p> In\u00a0[60]: Copied! <pre># Forecasting intervals for an unknown series\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 3,\n                 encoding  = 'ordinal'\n             )\n\nforecaster.fit(\n    series=series_long_train, store_in_sample_residuals=True, suppress_warnings=True\n)\n\n# Number of in-sample residuals by bin\n# ==============================================================================\nfor k, v in forecaster.in_sample_residuals_.items():\n    print(f\"Residuals for {k}: n={len(v)}\")\n</pre> # Forecasting intervals for an unknown series # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 3,                  encoding  = 'ordinal'              )  forecaster.fit(     series=series_long_train, store_in_sample_residuals=True, suppress_warnings=True )  # Number of in-sample residuals by bin # ============================================================================== for k, v in forecaster.in_sample_residuals_.items():     print(f\"Residuals for {k}: n={len(v)}\") <pre>Residuals for item_1: n=924\nResiduals for item_2: n=924\nResiduals for item_3: n=924\nResiduals for _unknown_level: n=2772\n</pre> In\u00a0[61]: Copied! <pre># Forecasting intervals for an unknown series\n# ==============================================================================\nforecaster.predict_interval(\n    levels                  = 'item_4',\n    steps                   = 3,\n    last_window             = last_window_item_4,\n    use_in_sample_residuals = True,\n    suppress_warnings       = False\n)\n</pre> # Forecasting intervals for an unknown series # ============================================================================== forecaster.predict_interval(     levels                  = 'item_4',     steps                   = 3,     last_window             = last_window_item_4,     use_in_sample_residuals = True,     suppress_warnings       = False ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 UnknownLevelWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `levels` {'item_4'} were not included in training. Unknown levels are encoded as     \u2502\n\u2502 NaN, which may cause the prediction to fail if the regressor does not accept NaN     \u2502\n\u2502 values.                                                                              \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : UnknownLevelWarning                                                       \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:910                                                                          \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=UnknownLevelWarning)             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 UnknownLevelWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `levels` {'item_4'} are not present in `forecaster.in_sample_residuals_by_bin_`,     \u2502\n\u2502 most likely because they were not present in the training data. A random sample of   \u2502\n\u2502 the residuals from other levels will be used. This can lead to inaccurate intervals  \u2502\n\u2502 for the unknown levels.                                                              \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : UnknownLevelWarning                                                       \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:1274                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=UnknownLevelWarning)             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[61]: level pred lower_bound upper_bound 2014-07-16 item_4 24.351416 20.331618 28.371213 2014-07-17 item_4 25.779253 21.759456 29.799050 2014-07-18 item_4 25.637366 21.617569 29.657163 <p>For the use of out-of-sample residuals (<code>use_in_sample_residuals = False</code>), the user can provide the residuals using the <code>set_out_sample_residuals</code> method and a random sample of residuals will be drawn to predict the unknown series.</p> In\u00a0[62]: Copied! <pre># Weights in Multi-Series\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n    \n    return weights\n\n\n# Series Weights, it is equivalent to {'item_2': 2.}\nseries_weights = {\n    'item_1': 1., \n    'item_2': 2., \n    'item_3': 1.\n}\n\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler(),\n                 weight_func        = custom_weights,\n                 series_weights     = series_weights\n             )\n\nforecaster.fit(series=series_long_train, suppress_warnings=True)\nforecaster.predict(steps=24).head(3)\n</pre> # Weights in Multi-Series # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.     \"\"\"     weights = np.where(                   (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),                    0,                    1               )          return weights   # Series Weights, it is equivalent to {'item_2': 2.} series_weights = {     'item_1': 1.,      'item_2': 2.,      'item_3': 1. }  forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),                  encoding           = 'ordinal',                  transformer_series = StandardScaler(),                  weight_func        = custom_weights,                  series_weights     = series_weights              )  forecaster.fit(series=series_long_train, suppress_warnings=True) forecaster.predict(steps=24).head(3) Out[62]: level pred 2014-07-16 item_1 25.928016 2014-07-16 item_2 11.429994 2014-07-16 item_3 11.717830 <p> \u26a0 Warning </p> <p>The <code>weight_func</code> and <code>series_weights</code> arguments will be ignored if the regressor does not accept <code>sample_weight</code> in its <code>fit</code> method.</p> <p>The source code of the <code>weight_func</code> added to the forecaster is stored in the argument <code>source_code_weight_func</code>. If <code>weight_func</code> is a <code>dict</code>, it will be a <code>dict</code> of the form <code>{'series_name': source_code_weight_func}</code> .</p> In\u00a0[63]: Copied! <pre># Source code weight function\n# ==============================================================================\nprint(forecaster.source_code_weight_func)\n</pre> # Source code weight function # ============================================================================== print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between '2013-01-01' and '2013-01-31', 1 otherwise.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2013-01-01') &amp; (index &lt;= '2013-01-31'),\n                   0,\n                   1\n              )\n\n    return weights\n\n</pre> <p> \ud83d\udca1 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[64]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 24,\n                 window_features = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),\n                 differentiation = 1  # Same as {'item_1': 1, 'item_2': 1, 'item_3': 1, '_unknown_level': 1}\n             )\n\nforecaster.fit(series=series_long_train, suppress_warnings=True)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 24,                  window_features = RollingFeatures(stats=['mean', 'mean'], window_sizes=[24, 48]),                  differentiation = 1  # Same as {'item_1': 1, 'item_2': 1, 'item_3': 1, '_unknown_level': 1}              )  forecaster.fit(series=series_long_train, suppress_warnings=True) forecaster Out[64]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]</li> <li>Window features: ['roll_mean_24', 'roll_mean_48']</li> <li>Window size: 49</li> <li>Series encoding: ordinal</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Series weights: None</li> <li>Differentiation order: 1</li> <li>Creation date: 2025-08-06 13:58:35</li> <li>Last fit date: 2025-08-06 13:58:36</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): item_1, item_2, item_3</li> <li>Training range: 'item_1': ['2012-01-01', '2014-07-15'], 'item_2': ['2012-01-01', '2014-07-15'], 'item_3': ['2012-01-01', '2014-07-15']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[65]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=24)\npredictions.head(3)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=24) predictions.head(3) Out[65]: level pred 2014-07-16 item_1 26.332222 2014-07-16 item_2 10.707126 2014-07-16 item_3 10.825073 <p> \ud83d\udca1 Tip </p> <p>More information about time series forecasting metrics can be found in the Metrics guide.</p> In\u00a0[66]: Copied! <pre># Grid search Multi-Series with multiple metrics\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal'\n             )\n\n\ndef custom_metric(y_true, y_pred):\n    \"\"\"\n    Calculate the mean absolute error using only the predicted values of the last\n    3 months of the year.\n    \"\"\"\n    mask = y_true.index.month.isin([10, 11, 12])\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\n\nlags_grid = [24, 48]\nparam_grid = {\n    'n_estimators': [10, 20],\n    'max_depth': [3, 7]\n}\n\ncv = TimeSeriesFold(\n         steps              = 24,\n         initial_train_size = '2014-07-15 23:59:00',\n         refit              = True,\n     )\n\nresults = grid_search_forecaster_multiseries(\n              forecaster        = forecaster,\n              series            = series_long,\n              exog              = None,\n              lags_grid         = lags_grid,\n              param_grid        = param_grid,\n              cv                = cv,\n              levels            = None,\n              metric            = [mean_absolute_error, custom_metric, 'mean_squared_error'],\n              aggregate_metric  = ['weighted_average', 'average', 'pooling'],\n              suppress_warnings = True\n          )\n\nresults.head(4)\n</pre> # Grid search Multi-Series with multiple metrics # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal'              )   def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error using only the predicted values of the last     3 months of the year.     \"\"\"     mask = y_true.index.month.isin([10, 11, 12])     metric = mean_absolute_error(y_true[mask], y_pred[mask])          return metric   lags_grid = [24, 48] param_grid = {     'n_estimators': [10, 20],     'max_depth': [3, 7] }  cv = TimeSeriesFold(          steps              = 24,          initial_train_size = '2014-07-15 23:59:00',          refit              = True,      )  results = grid_search_forecaster_multiseries(               forecaster        = forecaster,               series            = series_long,               exog              = None,               lags_grid         = lags_grid,               param_grid        = param_grid,               cv                = cv,               levels            = None,               metric            = [mean_absolute_error, custom_metric, 'mean_squared_error'],               aggregate_metric  = ['weighted_average', 'average', 'pooling'],               suppress_warnings = True           )  results.head(4) <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] \n  Parameters: {'max_depth': 7, 'n_estimators': 20}\n  Backtesting metric: 2.288448178505924\n  Levels: ['item_1', 'item_2', 'item_3']\n\n</pre> Out[66]: levels lags lags_label params mean_absolute_error__weighted_average mean_absolute_error__average mean_absolute_error__pooling custom_metric__weighted_average custom_metric__average custom_metric__pooling mean_squared_error__weighted_average mean_squared_error__average mean_squared_error__pooling max_depth n_estimators 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 20} 2.288448 2.288448 2.288448 2.314922 2.314922 2.314922 9.462712 9.462712 9.462712 7 20 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 20} 2.357734 2.357734 2.357734 2.306112 2.306112 2.306112 10.121310 10.121310 10.121310 7 20 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 20} 2.377537 2.377537 2.377537 2.302790 2.302790 2.302790 10.179578 10.179578 10.179578 3 20 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 20} 2.474797 2.474797 2.474797 2.365732 2.365732 2.365732 10.708651 10.708651 10.708651 3 20"},{"location":"user_guides/independent-multi-time-series-forecasting.html#global-forecasting-models-independent-multi-series-forecasting","title":"Global Forecasting Models: Independent multi-series forecasting\u00b6","text":"<p>Univariate time series forecasting focuses on modeling a single time series as a linear or nonlinear function of its own past values (lags), using historical observations to predict future ones.</p> <p>Global forecasting builds a single predictive model that considers all time series simultaneously. This approach seeks to learn the shared patterns that underlie the different series, helping to reduce the influence of noise present in individual time series. It is computationally efficient, easier to maintain, and often yields more robust generalization across series.</p> <p>In independent multi-series forecasting, a single model is trained using all time series, but each series is treated independently\u2014past values of one series are not used to predict another. Modeling them together is still beneficial when the series share similar temporal dynamics. For example, sales of products A and B in the same store may not be directly related, but both are influenced by the same underlying store-level patterns.</p> <p> Internal Forecaster transformation of two time series and an exogenous variable into the matrices needed to train a machine learning model in a multi-series context. </p> <p>To predict the next n steps, the strategy of recursive multi-step forecasting is applied, with the only difference being that the series name for which to estimate the predictions needs to be indicated.</p> <p> Diagram of recursive forecasting with multiple independent time series. </p> <p>Using the <code>ForecasterRecursiveMultiSeries</code> it is possible to easily build machine learning models for independent multi-series forecasting.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#input-data","title":"Input data\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#forecasterrecursivemultiseries","title":"ForecasterRecursiveMultiSeries\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#backtesting-multiple-series","title":"Backtesting multiple series\u00b6","text":"<p>As in the <code>predict</code> method, the <code>levels</code> at which backtesting is performed must be indicated. The argument can also be set to <code>None</code> to perform backtesting at all levels. In addition to the individual metric(s) for each series, the aggregated value is calculated using the following methods:</p> <ul> <li><p>average: the average (arithmetic mean) of all levels.</p> </li> <li><p>weighted_average: the average of the metrics weighted by the number of predicted values of each level.</p> </li> <li><p>pooling: the values of all levels are pooled and then the metric is calculated.</p> </li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#exogenous-variables-in-multi-series-forecasting","title":"Exogenous variables in multi-series forecasting\u00b6","text":"<p>Exogenous variables are predictors that are independent of the model being used for forecasting, and their future values must be known in order to include them in the prediction process. The class <code>ForecasterRecursiveMultiSeries</code> supports multiple strategies for including exogenous variables:</p> <ul> <li><p>If <code>exog</code> is a wide-format pandas DataFrame, it must share the same index type as series. Each column represents a different exogenous variable, and the same values are applied to all time series.</p> </li> <li><p>If <code>exog</code> is a long-format pandas Series or DataFrame with a MultiIndex, the first level contains the series IDs to which it belongs, and the second level must be a pandas <code>DatetimeIndex</code>. Each exogenous variable must be represented as a separate column.</p> </li> <li><p>If <code>exog</code> is a dictionary, each key must correspond to a series ID, and each value must be either a named pandas <code>Series</code> or <code>DataFrame</code> with the same index type as <code>series</code>, or <code>None</code>. It is not required for all series to contain all exogenous variables, but data types must be consistent across series for each variable.</p> </li> </ul> Exog type Index requirements <code>Wide DataFrame</code> <code>pandas DatetimeIndex</code> or <code>RangeIndex</code> (all series same step/frequency) <code>MultiIndex DataFrame</code> First level <code>series_id</code>, second <code>datetime</code> (pandas <code>DatetimeIndex</code>) <code>dict</code> <code>pandas DatetimeIndex</code> or <code>RangeIndex</code> (all series same step/frequency)"},{"location":"user_guides/independent-multi-time-series-forecasting.html#series-transformations","title":"Series transformations\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#hyperparameter-search-and-lags-selection","title":"Hyperparameter search and lags selection\u00b6","text":"<p>Hyperparameter tuning consists of systematically evaluating combinations of hyperparameters (including lags) to find the configuration that yields the best predictive performance. The skforecast library supports several tuning strategies: grid search, random search, and Bayesian search. These strategies can be used with either backtesting or one-step-ahead validation to determine the optimal parameter set for a given forecasting task.</p> <p>The functions <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>bayesian_search_forecaster_multiseries</code> from the <code>model_selection</code> module allow for lags and hyperparameter optimization.</p> <p>The <code>levels</code> argument is crucial in these functions, as it specifies which time series are considered during the optimization:</p> <ul> <li><p>If <code>levels</code> is a <code>list</code>, the function searches for the configuration that minimizes the aggregated error across the selected series.</p> </li> <li><p>If <code>levels = None</code>, all available series are used in the optimization.</p> </li> <li><p>If <code>levels = 'item_1'</code> (equivalent to <code>levels = ['item_1']</code>), the optimization is based solely on the error of that specific series, and the returned metric corresponds to it.</p> </li> </ul> <p>When optimizing across multiple series, the resulting metrics must be aggregated. The available methods are:</p> <ul> <li><p><code>'average'</code>, the average (arithmetic mean) of all series (<code>levels</code>).</p> </li> <li><p><code>'weighted_average'</code>, the average of the metrics weighted by the number of predicted values of each series (<code>levels</code>).</p> </li> <li><p><code>'pooling'</code>, the values of all series (<code>levels</code>) are pooled and then the metric is calculated.</p> </li> </ul> <p>If a <code>list</code> of aggregation methods is provided to the <code>aggregate_metric</code> argument, all are computed, but only the first method is used to select the best configuration.</p> <p>The following example demonstrates how to use <code>grid_search_forecaster_multiseries</code> to find the best lags and hyperparameters for all series (all <code>levels</code>) using <code>'weighted_average'</code> as the aggregation strategy.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#series-with-different-lengths-and-different-exogenous-variables","title":"Series with different lengths and different exogenous variables\u00b6","text":""},{"location":"user_guides/independent-multi-time-series-forecasting.html#series-encoding-in-multi-series","title":"Series Encoding in multi-series\u00b6","text":"<p>When creating the training matrices, the <code>ForecasterRecursiveMultiSeries</code> class encodes the series names to identify the series to which the observations belong. Different encoding methods can be used:</p> <ul> <li><p><code>'ordinal'</code> (default), a single column (<code>_level_skforecast</code>) is created with integer values from 0 to n_series - 1.</p> </li> <li><p><code>'ordinal_category'</code>, a single column (<code>_level_skforecast</code>) is created with integer values from 0 to n_series - 1. Then, the column is transformed into <code>pandas.category</code> dtype so that it can be used as a categorical variable.</p> </li> <li><p><code>'onehot'</code>, a binary column is created for each series.</p> </li> <li><p><code>None</code>, no encoding is performed (no column is created).</p> </li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#forecasting-unknown-series","title":"Forecasting unknown series\u00b6","text":"<p><code>ForecasterRecursiveMultiSeries</code> allows the prediction of unknown series (levels). If a series not seen during training is found during the prediction phase, the forecaster will encode the series according to the following rules:</p> <ul> <li><p>If <code>encoding</code> is <code>'onehot'</code>, all dummy columns are set to 0.</p> </li> <li><p>If <code>encoding</code> is <code>'ordinal_category'</code> or <code>'ordinal'</code>, the value of the column <code>_level_skforecast</code> is set to <code>NaN</code>.</p> </li> </ul> <p>Since the series was not present during training, the last window of the series must be provided when calling the <code>predict</code> method.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#weights-in-multi-series","title":"Weights in multi-series\u00b6","text":"<p>The weights are used to control the influence that each observation has on the training of the model. <code>ForecasterRecursiveMultiSeries</code> accepts two types of weights:</p> <ul> <li><p><code>series_weights</code> controls the relative importance of each series. If a series has twice as much weight as the others, the observations of that series influence the training twice as much. The higher the weight of a series relative to the others, the more the model will focus on trying to learn that series.</p> </li> <li><p><code>weight_func</code> controls the relative importance of each observation according to its index value. For example, a function that assigns a lower weight to certain dates.</p> </li> </ul> <p>If the two types of weights are indicated, they are multiplied to create the final weights. The resulting <code>sample_weight</code> cannot have negative values.</p> <p> Weights in multi-series. </p> <ul> <li><p><code>series_weights</code> is a dict of the form <code>{'series_name': float}</code>. If a series is used during <code>fit</code> and is not present in <code>series_weights</code>, it will have a weight of 1.</p> </li> <li><p><code>weight_func</code> is a function that defines the individual weights of each sample based on the index.</p> <ul> <li><p>If it is a <code>callable</code>, the same function will apply to all series.</p> </li> <li><p>If it is a <code>dict</code> of the form <code>{'series_name': callable}</code>, a different function can be used for each series. A weight of 1 is given to all series not present in <code>weight_func</code>.</p> </li> </ul> </li> </ul>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#differentiation","title":"Differentiation\u00b6","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p> <p>In the <code>ForecasterRecursiveMultiSeries</code> class, the <code>differentiation</code> argument can be:</p> <ul> <li><p><code>int</code>: all series are differentiated <code>int</code> times.</p> </li> <li><p><code>dict</code>: a different order of differentiation can be specified for each series. For example, <code>differentiation = {'item_1': 1, 'item_2': 2, 'item_3': None, '_unknown_level': 1}</code>. The <code>_unknown_level</code> key is used to differentiate the unknown series when predicting.</p> </li> </ul> <p>When using a <code>dict</code>, the value needed to include in the <code>differentiation</code> argument of a <code>cv</code> (e.g. <code>TimeSeriesFold</code> for backtesting) object is the maximum differentiation order of all series. This value is available in the <code>differentiation_max</code> attribute.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#probabilistic-forecasting","title":"Probabilistic forecasting\u00b6","text":"<p>Skforecast allows to apply all its implemented probabilistic forecasting methods (bootstrapping, conformal prediction and quantile regression) to global models. This means that the model is trained with all the available time series and the forecast is made for all the time series.</p> <p>Visit Probabilistic forecasting: Global Models for more information.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#feature-selection-in-multi-series","title":"Feature selection in multi-series\u00b6","text":"<p>Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons: to simplify models to make them easier to interpret, to reduce training time, to avoid the curse of dimensionality, to improve generalization by reducing overfitting (formally, variance reduction), and others.</p> <p>Skforecast is compatible with the feature selection methods implemented in the scikit-learn library. Visit Global Forecasting Models: Feature Selection for more information.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#compare-multiple-metrics","title":"Compare multiple metrics\u00b6","text":"<p>The functions <code>backtesting_forecaster_multiseries</code>, <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code>, and <code>bayesian_search_forecaster_multiseries</code> support the evaluation of multiple metrics by passing a <code>list</code> of metric functions. This list can include both built-in metrics (e.g. <code>mean_squared_error</code>, <code>mean_absolute_error</code>) and custom-defined ones.</p> <p>When multiple metrics are provided, the first metric in the list and the first aggregation method are used to select the best model.</p>"},{"location":"user_guides/independent-multi-time-series-forecasting.html#training-and-prediction-matrices","title":"Training and prediction matrices\u00b6","text":"<p>While the primary goal of building forecasting models is to predict future values, it is equally important to evaluate if the model is effectively learning from the training data. Analyzing predictions on the training data or exploring the prediction matrices is crucial for assessing model performance and understanding areas for optimization. This process can help identify issues like overfitting or underfitting, as well as provide deeper insights into the model\u2019s decision-making process. Check the How to Extract Training and Prediction Matrices user guide for more information.</p>"},{"location":"user_guides/input-data.html","title":"Input data","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\n</pre> # Libraries # ============================================================================== import pandas as pd from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0}\n)\ndata[\"date\"] = pd.to_datetime(data[\"date\"], format=\"%Y-%m-%d\")\ndata = data.set_index(\"date\")\ndata = data.asfreq(\"MS\")\ndata\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0} ) data[\"date\"] = pd.to_datetime(data[\"date\"], format=\"%Y-%m-%d\") data = data.set_index(\"date\") data = data.asfreq(\"MS\") data <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> Out[2]: y date 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 1991-10-01 0.492543 1991-11-01 0.502369 ... ... 2008-02-01 0.761822 2008-03-01 0.649435 2008-04-01 0.827887 2008-05-01 0.816255 2008-06-01 0.762137 <p>204 rows \u00d7 1 columns</p> In\u00a0[3]: Copied! <pre># Index type and frequency\n# ==============================================================================\nprint(f\"Index type      : {type(data.index)}\")\nprint(f\"Index frequency : {data.index.freq}\")\n</pre> # Index type and frequency # ============================================================================== print(f\"Index type      : {type(data.index)}\") print(f\"Index frequency : {data.index.freq}\") <pre>Index type      : &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\nIndex frequency : &lt;MonthBegin&gt;\n</pre> In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 5\n             )\n\nforecaster.fit(y=data['y'])\n\n# Predictions\n# ==============================================================================\nforecaster.predict(steps=5)\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 5              )  forecaster.fit(y=data['y'])  # Predictions # ============================================================================== forecaster.predict(steps=5) Out[4]: <pre>2008-07-01    0.861239\n2008-08-01    0.871102\n2008-09-01    0.835840\n2008-10-01    0.938713\n2008-11-01    1.004192\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Data without datetime index\n# ==============================================================================\ndata = data.reset_index(drop=True)\ndata\n</pre> # Data without datetime index # ============================================================================== data = data.reset_index(drop=True) data Out[5]: y 0 0.429795 1 0.400906 2 0.432159 3 0.492543 4 0.502369 ... ... 199 0.761822 200 0.649435 201 0.827887 202 0.816255 203 0.762137 <p>204 rows \u00d7 1 columns</p> In\u00a0[6]: Copied! <pre># Index type and step\n# ==============================================================================\nprint(f\"Index type : {type(data.index)}\")\nprint(f\"Index step : {data.index.step}\")\n</pre> # Index type and step # ============================================================================== print(f\"Index type : {type(data.index)}\") print(f\"Index step : {data.index.step}\") <pre>Index type : &lt;class 'pandas.core.indexes.range.RangeIndex'&gt;\nIndex step : 1\n</pre> In\u00a0[7]: Copied! <pre># Fit - Predict\n# ==============================================================================\nforecaster.fit(y=data['y'])\nforecaster.predict(steps=5)\n</pre> # Fit - Predict # ============================================================================== forecaster.fit(y=data['y']) forecaster.predict(steps=5) Out[7]: <pre>204    0.861239\n205    0.871102\n206    0.835840\n207    0.938713\n208    1.004192\nName: pred, dtype: float64</pre>"},{"location":"user_guides/input-data.html#input-data","title":"Input data\u00b6","text":"<p>Working with sequential or time series data requires a consistent and regular spacing between observations. Uneven or irregularly spaced data can lead to ambiguous results and unreliable forecasts. For this reason, skforecast strictly enforces the use of regular indices.</p> <p>To ensure reproducibility and clarity in forecasting tasks, skforecast only allows two types of index:</p> <ul> <li><p>DatetimeIndex with frequency: A time-based index with a defined and regular frequency (e.g., daily, monthly).</p> </li> <li><p>RangeIndex with step: A default integer index, regularly spaced.</p> </li> </ul> <p>Other index types (such as <code>DatetimeIndex</code> without frequency, or custom indices) are not supported, and their use will raise an error.</p>"},{"location":"user_guides/input-data.html#number-of-time-series","title":"Number of time series\u00b6","text":"<p>The skforecast library offers a variety of forecaster types, each tailored to specific requirements such as single or multiple time series, direct or recursive strategies, or custom predictors. Regardless of the specific forecaster type, all instances share the same API.</p> Forecaster Single series Multiple series Recursive strategy Direct strategy Probabilistic prediction Time series differentiation Exogenous features Window features ForecasterRecursive \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterDirect \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterRecursiveMultiSeries \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterDirectMultiVariate \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterRNN \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f ForecasterSarimax \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f"},{"location":"user_guides/input-data.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/input-data.html#train-and-predict-using-input-with-datetimeindex-and-frequency","title":"Train and predict using input with DatetimeIndex and frequency\u00b6","text":""},{"location":"user_guides/input-data.html#train-and-predict-using-input-with-rangeindex","title":"Train and predict using input with RangeIndex\u00b6","text":""},{"location":"user_guides/metrics.html","title":"Metrics","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\nfrom sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\nfrom skforecast.metrics import mean_absolute_scaled_error\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error from skforecast.metrics import mean_absolute_scaled_error from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Dowlnoad dataset\n# ==============================================================================\ndata = fetch_dataset('website_visits', raw=True)\ndata['date'] = pd.to_datetime(data['date'], format='%d/%m/%y')\ndata = data.set_index('date')\ndata = data.asfreq('1D')\ndata = data.sort_index()\ndata.head(3)\n</pre> # Dowlnoad dataset # ============================================================================== data = fetch_dataset('website_visits', raw=True) data['date'] = pd.to_datetime(data['date'], format='%d/%m/%y') data = data.set_index('date') data = data.asfreq('1D') data = data.sort_index() data.head(3) <pre>website_visits\n--------------\nDaily visits to the cienciadedatos.net website registered with the google\nanalytics service.\nAmat Rodrigo, J. (2021). cienciadedatos.net (1.0.0). Zenodo.\nhttps://doi.org/10.5281/zenodo.10006330\nShape of the dataset: (421, 2)\n</pre> Out[2]: users date 2020-07-01 2324 2020-07-02 2201 2020-07-03 2146 In\u00a0[3]: Copied! <pre># Backtesting to generate predictions over the test set and calculate metrics\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 7\n             )\n\nmetrics = [\n    'mean_absolute_error',\n    'mean_squared_error',\n    'mean_absolute_percentage_error',\n    'mean_absolute_scaled_error'\n]\n\ncv = TimeSeriesFold(steps=7, initial_train_size = len(data) // 2)\n\nbacktest_metrics, predictions = backtesting_forecaster(\n                                    forecaster = forecaster,\n                                    y          = data['users'],\n                                    cv         = cv,\n                                    metric     = metrics,\n                                )\nbacktest_metrics\n</pre> # Backtesting to generate predictions over the test set and calculate metrics # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 7              )  metrics = [     'mean_absolute_error',     'mean_squared_error',     'mean_absolute_percentage_error',     'mean_absolute_scaled_error' ]  cv = TimeSeriesFold(steps=7, initial_train_size = len(data) // 2)  backtest_metrics, predictions = backtesting_forecaster(                                     forecaster = forecaster,                                     y          = data['users'],                                     cv         = cv,                                     metric     = metrics,                                 ) backtest_metrics <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> Out[3]: mean_absolute_error mean_squared_error mean_absolute_percentage_error mean_absolute_scaled_error 0 298.298903 165698.930884 0.176136 0.695103 <p>It is possible to pass a list of functions instead of names.</p> In\u00a0[4]: Copied! <pre># Backtesting with callable metrics\n# ==============================================================================\nmetrics = [\n    mean_absolute_error,\n    mean_squared_error,\n    mean_absolute_percentage_error,\n    mean_absolute_scaled_error\n]\n\nbacktest_metrics, predictions = backtesting_forecaster(\n                                    forecaster = forecaster,\n                                    y          = data['users'],\n                                    cv         = cv,\n                                    metric     = metrics,\n                                )\nbacktest_metrics\n</pre> # Backtesting with callable metrics # ============================================================================== metrics = [     mean_absolute_error,     mean_squared_error,     mean_absolute_percentage_error,     mean_absolute_scaled_error ]  backtest_metrics, predictions = backtesting_forecaster(                                     forecaster = forecaster,                                     y          = data['users'],                                     cv         = cv,                                     metric     = metrics,                                 ) backtest_metrics <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> Out[4]: mean_absolute_error mean_squared_error mean_absolute_percentage_error mean_absolute_scaled_error 0 298.298903 165698.930884 0.176136 0.695103 <p>Since functions can be passed as arguments, it is possible to define custom metrics. The custom metric must be a function that takes two arguments: the true values (<code>y_true</code>) and the predicted values (<code>y_pred</code>), and returns a single value representing the metric. Optionally, it can take additional arguments <code>y_train</code> with the training data.</p> In\u00a0[\u00a0]: Copied! <pre># Backtesting with custom metric\n# ==============================================================================\ndef custom_metric(y_true, y_pred):\n    \"\"\"\n    Calculate the mean absolute error excluding weekends.\n    \"\"\"\n    mask = y_true.index.weekday &lt; 5\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n\n    return metric\n\n\nbacktest_metrics, predictions = backtesting_forecaster(\n                                    forecaster = forecaster,\n                                    y          = data['users'],\n                                    cv         = cv,\n                                    metric     = custom_metric,\n                                )\nbacktest_metrics\n</pre> # Backtesting with custom metric # ============================================================================== def custom_metric(y_true, y_pred):     \"\"\"     Calculate the mean absolute error excluding weekends.     \"\"\"     mask = y_true.index.weekday &lt; 5     metric = mean_absolute_error(y_true[mask], y_pred[mask])      return metric   backtest_metrics, predictions = backtesting_forecaster(                                     forecaster = forecaster,                                     y          = data['users'],                                     cv         = cv,                                     metric     = custom_metric,                                 ) backtest_metrics <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> Out[\u00a0]: custom_metric 0 298.375447 <p>When working with multiple time series, it is common to calculate metrics across all series to get an overall measure of performance, in addition to individual values. Skforecast provides 3 different aggregation methods:</p> <ul> <li><p>average: the average (arithmetic mean) of all levels.</p> </li> <li><p>weighted_average: the average of the metrics weighted by the number of predicted values of each level.</p> </li> <li><p>pooling: the values of all levels are pooled and then the metric is calculated.</p> </li> </ul> <p>To include the aggregated metrics in the output of <code>backtesting_forecaster_multiseries</code>, the <code>add_aggregated_metric</code> argument must be set to <code>True</code>.</p> In\u00a0[6]: Copied! <pre># Libraries\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\nfrom skforecast.model_selection import backtesting_forecaster_multiseries\nfrom skforecast.model_selection import grid_search_forecaster_multiseries\n</pre> # Libraries # ============================================================================== import matplotlib.pyplot as plt from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursiveMultiSeries from skforecast.model_selection import backtesting_forecaster_multiseries from skforecast.model_selection import grid_search_forecaster_multiseries In\u00a0[7]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\ndata.head()\n</pre> # Data download # ============================================================================== data = fetch_dataset(name=\"items_sales\") data.head() <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> Out[7]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[8]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\nend_train = '2014-07-15 23:59:00'\ndata_train = data.loc[:end_train, :].copy()\ndata_test  = data.loc[end_train:, :].copy()\n\nprint(\n    f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== end_train = '2014-07-15 23:59:00' data_train = data.loc[:end_train, :].copy() data_test  = data.loc[end_train:, :].copy()  print(     f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}   \"     f\"(n={len(data_train)})\" ) print(     f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}   \"     f\"(n={len(data_test)})\" ) <pre>Train dates : 2012-01-01 00:00:00 --- 2014-07-15 00:00:00   (n=927)\nTest dates  : 2014-07-16 00:00:00 --- 2015-01-01 00:00:00   (n=170)\n</pre> In\u00a0[9]: Copied! <pre># Plot time series\n# ==============================================================================\nset_dark_theme()\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7, 5), sharex=True)\nfor i, item in enumerate(['item_1', 'item_2', 'item_3'], start=0):\n    data_train[item].plot(label='train', ax=axes[i])\n    data_test[item].plot(label='test', ax=axes[i])\n    axes[i].set_ylabel('sales')\n    axes[i].set_title(f'Item {i + 1}')\n    axes[i].set_xlabel('')\n    axes[i].legend(loc='upper left')\nfig.tight_layout()\nplt.show()\n</pre> # Plot time series # ============================================================================== set_dark_theme() fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(7, 5), sharex=True) for i, item in enumerate(['item_1', 'item_2', 'item_3'], start=0):     data_train[item].plot(label='train', ax=axes[i])     data_test[item].plot(label='test', ax=axes[i])     axes[i].set_ylabel('sales')     axes[i].set_title(f'Item {i + 1}')     axes[i].set_xlabel('')     axes[i].legend(loc='upper left') fig.tight_layout() plt.show() In\u00a0[10]: Copied! <pre># Create and fit a Forecaster Multi-Series\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal'\n             )\nforecaster.fit(series=data_train)\n</pre> # Create and fit a Forecaster Multi-Series # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal'              ) forecaster.fit(series=data_train) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[11]: Copied! <pre># Backtesting forecaster on multiple time series\n# ==============================================================================\nmetrics = [\n    'mean_absolute_error',\n    'mean_squared_error',\n    'mean_absolute_percentage_error',\n    'mean_absolute_scaled_error'\n]\n\ncv = TimeSeriesFold(steps=24, initial_train_size=len(data_train))\n\nbacktest_metrics, backtest_predictions = backtesting_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = data,\n    cv                    = cv,\n    levels                = None,\n    metric                = metrics,\n    add_aggregated_metric = True\n)\n\nprint(\"Backtest metrics\")\nbacktest_metrics\n</pre> # Backtesting forecaster on multiple time series # ============================================================================== metrics = [     'mean_absolute_error',     'mean_squared_error',     'mean_absolute_percentage_error',     'mean_absolute_scaled_error' ]  cv = TimeSeriesFold(steps=24, initial_train_size=len(data_train))  backtest_metrics, backtest_predictions = backtesting_forecaster_multiseries(     forecaster            = forecaster,     series                = data,     cv                    = cv,     levels                = None,     metric                = metrics,     add_aggregated_metric = True )  print(\"Backtest metrics\") backtest_metrics <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>  0%|          | 0/8 [00:00&lt;?, ?it/s]</pre> <pre>Backtest metrics\n</pre> Out[11]: levels mean_absolute_error mean_squared_error mean_absolute_percentage_error mean_absolute_scaled_error 0 item_1 1.163837 2.748612 0.057589 0.762485 1 item_2 2.634182 12.358668 0.171335 1.118262 2 item_3 3.111351 16.096772 0.200664 0.840188 3 average 2.303123 10.401350 0.143196 0.906978 4 weighted_average 2.303123 10.401350 0.143196 0.906978 5 pooling 2.303123 10.401350 0.143196 0.910909 In\u00a0[12]: Copied! <pre># Backtest predictions\n# ==============================================================================\nprint(\"Backtest predictions\")\nbacktest_predictions.head(4)\n</pre> # Backtest predictions # ============================================================================== print(\"Backtest predictions\") backtest_predictions.head(4) <pre>Backtest predictions\n</pre> Out[12]: level pred 2014-07-16 item_1 25.906323 2014-07-16 item_2 10.522491 2014-07-16 item_3 12.034587 2014-07-17 item_1 25.807194 <p>When using the hyperparameter optimization functions, the metrics are computed for each level and then aggregated. The <code>aggregate_metric</code> argument determines the aggregation method(s) to be used. Some considerations when choosing the aggregation method are:</p> <ul> <li><p>'average' and 'weighted_average' will be different if the number of predicted values is different for each series (level).</p> </li> <li><p>When all series have the same number of predicted values, 'average', 'weighted_average' and 'pooling' are equivalent except for the case of scaled metrics (<code>mean_absolute_scaled_error</code>, <code>root_mean_squared_scaled_error</code>) as the naive forecast is calculated individually for each series.</p> </li> </ul> <p>In this example, <code>metric = ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']</code> and <code>aggregate_metric = ['weighted_average', 'average', 'pooling']</code> so that the following metrics are calculated:</p> <ul> <li><p>mean_absolute_scaled_error__weighted_average, mean_absolute_scaled_error__average, mean_absolute_scaled_error__pooling</p> </li> <li><p>root_mean_squared_scaled_error__weighted_average, root_mean_squared_scaled_error__average, root_mean_squared_scaled_error__pooling</p> </li> </ul> <p>Since <code>return_best</code> is set to <code>True</code>, the best model is returned. The best model is the one that minimizes the metric obtained by the first specified metric and aggregation method, <code>mean_absolute_scaled_error__weighted_average</code>.</p> In\u00a0[13]: Copied! <pre># Grid search Multi-Series\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=123, verbose=-1),\n                 lags      = 24,\n                 encoding  = 'ordinal'\n             )\n\nlags_grid = [24, 48]\nparam_grid = {\n    'n_estimators': [10, 20],\n    'max_depth': [3, 7]\n}\n\ncv = TimeSeriesFold(steps=24, initial_train_size=len(data_train), refit=False)\n\nlevels = ['item_1', 'item_2', 'item_3']\n\nresults = grid_search_forecaster_multiseries(\n              forecaster       = forecaster,\n              series           = data,\n              lags_grid        = lags_grid,\n              param_grid       = param_grid,\n              cv               = cv,\n              levels           = levels,  # Same as levels = None\n              metric           = ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error'],\n              aggregate_metric = ['weighted_average', 'average', 'pooling'],\n          )\n\nresults\n</pre> # Grid search Multi-Series # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=123, verbose=-1),                  lags      = 24,                  encoding  = 'ordinal'              )  lags_grid = [24, 48] param_grid = {     'n_estimators': [10, 20],     'max_depth': [3, 7] }  cv = TimeSeriesFold(steps=24, initial_train_size=len(data_train), refit=False)  levels = ['item_1', 'item_2', 'item_3']  results = grid_search_forecaster_multiseries(               forecaster       = forecaster,               series           = data,               lags_grid        = lags_grid,               param_grid       = param_grid,               cv               = cv,               levels           = levels,  # Same as levels = None               metric           = ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error'],               aggregate_metric = ['weighted_average', 'average', 'pooling'],           )  results <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\utils\\u \u2502\n\u2502 tils.py:2350                                                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>lags grid:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] \n  Parameters: {'max_depth': 7, 'n_estimators': 20}\n  Backtesting metric: 0.8965371058488802\n  Levels: ['item_1', 'item_2', 'item_3']\n\n</pre> Out[13]: levels lags lags_label params mean_absolute_scaled_error__weighted_average mean_absolute_scaled_error__average mean_absolute_scaled_error__pooling root_mean_squared_scaled_error__weighted_average root_mean_squared_scaled_error__average root_mean_squared_scaled_error__pooling max_depth n_estimators 0 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 20} 0.896537 0.896537 0.891585 0.873490 0.873490 0.862816 7 20 1 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 20} 0.968697 0.968697 0.946763 0.927639 0.927639 0.896254 3 20 2 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 20} 0.970085 0.970085 0.962275 0.930273 0.930273 0.921523 7 20 3 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 20} 0.992591 0.992591 0.971936 0.937760 0.937760 0.910280 3 20 4 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 10} 1.295626 1.295626 1.213049 1.172435 1.172435 1.077139 7 10 5 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 10} 1.369425 1.369425 1.292356 1.237487 1.237487 1.143031 3 10 6 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 3, 'n_estimators': 10} 1.378819 1.378819 1.291499 1.244084 1.244084 1.140353 3 10 7 [item_1, item_2, item_3] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'max_depth': 7, 'n_estimators': 10} 1.410147 1.410147 1.317691 1.277628 1.277628 1.172467 7 10"},{"location":"user_guides/metrics.html#time-series-forecasting-metrics","title":"Time Series Forecasting Metrics\u00b6","text":"<p>In time series forecasting, evaluating the performance of predictive models is crucial to ensure accurate and reliable forecasts. Forecasting metrics are quantitative measures used to assess the accuracy and effectiveness of these models. They help in comparing different models, diagnosing errors, and making informed decisions based on forecast results.</p> <p>Skforecast is compatible with most of the regression metrics from scikit-learn and includes additional metrics specifically designed for time series forecasting.</p> <ul> <li><p>Mean Squared Error (<code>mean_squared_error</code>)</p> </li> <li><p>Mean Absolute Error (<code>mean_absolute_error</code>)</p> </li> <li><p>Mean Absolute Percentage Error (<code>mean_absolute_percentage_error</code>)</p> </li> <li><p>Symmetric Mean Absolute Percentage Error (<code>symmetric_mean_absolute_percentage_error</code>)</p> </li> <li><p>Mean Squared Log Error (<code>mean_squared_log_error</code>)</p> </li> <li><p>Median Absolute Error (<code>median_absolute_error</code>)</p> </li> <li><p>Mean Absolute Scaled Error (<code>mean_absolute_scaled_error</code>)</p> </li> <li><p>Root Mean Squared Scaled Error (<code>root_mean_squared_scaled_error</code>)</p> </li> </ul> <p>In addition, Skforecast allows the user to define their own custom metrics. This can be done by creating a function that takes two arguments: the true values (<code>y_true</code>) and the predicted values (<code>y_pred</code>), and returns a single value representing the metric. The custom metric, optionally takes an additional argument <code>y_train</code> with the training data used to fit the model.</p> <p>In most cases, the metrics are calculated using the predictions generated in a backtesting process. For this, the <code>backtesting_forecaster</code> and <code>backtesting_forecaster_multiseries</code> functions take the <code>metrics</code> argument to specify the metric(s) to be calculated in addition to the predictions.</p>"},{"location":"user_guides/metrics.html#metrics-for-single-series-forecasting","title":"Metrics for single series forecasting\u00b6","text":"<p>The following code shows how to calculate metrics when a single series is forecasted. The example uses the <code>backtesting_forecaster</code> function to generate predictions and calculate the metrics.</p>"},{"location":"user_guides/metrics.html#metrics-for-multiple-time-series-forecasting","title":"Metrics for multiple time series forecasting\u00b6","text":""},{"location":"user_guides/migration-guide.html","title":"Skforecast 0.14 Migration guide","text":"<p>This document contains a set of instructions on how to update your code to work with Skforecast 0.14 or later versions.</p> <p>Skforecast modules have been reorganized and renamed to make it more intuitive. The new import of the library is as follows:</p> skforecast &lt; 0.14 skforecast \u2265 0.14 <pre>from skforecast.ForecasterAutoreg import ForecasterAutoreg\nfrom skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n</pre> <pre>from skforecast.recursive import ForecasterRecursive\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\n</pre> <pre>from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nfrom skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom\n</pre> <p>Not exist any more, use <code>ForecasterRecursive</code> or <code>ForecasterRecursiveMultiSeries</code> instead and the <code>window_features</code> to include any window or custom feature.</p> <pre>from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\nfrom skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\n</pre> <pre>from skforecast.direct import ForecasterDirect\nfrom skforecast.direct import ForecasterDirectMultiVariate\n</pre> <pre>from model_selection import backtesting_forecaster\n</pre> <pre>from model_selection import TimeSeriesFold\nfrom model_selection import backtesting_forecaster\n</pre> <pre>from model_selection_multiseries import backtesting_forecaster_multiseries\nfrom model_selection_multiseries import grid_search_forecaster_multiseries\nfrom model_selection_multiseries import random_search_forecaster_multiseries\nfrom model_selection_multiseries import bayesian_search_forecaster_multiseries\n</pre> <pre>from model_selection import backtesting_forecaster_multiseries\nfrom model_selection import grid_search_forecaster_multiseries\nfrom model_selection import random_search_forecaster_multiseries\nfrom model_selection import bayesian_search_forecaster_multiseries\n</pre> <pre>from model_selection import select_features\nfrom model_selection_multiseries import select_features_multiseries\n</pre> <pre>from feature_selection import select_features\nfrom feature_selection import select_features_multiseries\n</pre> <pre>from skforecast.Sarimax import Sarimax\nfrom model_selection_sarimax import backtesting_sarimax\nfrom model_selection_sarimax import grid_search_sarimax\nfrom model_selection_sarimax import random_search_sarimax\n</pre> <pre>from skforecast.sarimax import Sarimax\nfrom model_selection import backtesting_sarimax\nfrom model_selection import grid_search_sarimax\nfrom model_selection import random_search_sarimax\n</pre> <p>Forecasters <code>ForecasterAutoregCustom</code> and <code>ForecasterAutoregMultiSeriesCustom</code> have been removed. Now, you can use <code>ForecasterRecursive</code> and <code>ForecasterDirect</code> and include custom features using the <code>window_features</code> parameter. To learn more about how to include custom features, check the documentation.</p> skforecast &lt; 0.14 skforecast \u2265 0.14 <pre>from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\nimport numpy as np\n\ndef create_predictors(y):\n    \"\"\"\n    Create first 10 lags of a time series.\n    Calculate moving average with window 20.\n    Calculate the moving standard deviation with window 20.\n    Calculate moving minimum and maximum with window 20.\n    \"\"\"\n    lags = y[-1:-11:-1]\n    mean = np.mean(y[-20:])\n    std = np.std(y[-20:])\n    min_val = np.min(y[-20:])\n    max_val = np.max(y[-20:])\n\n    predictors = np.hstack([lags, mean, std, min_val, max_val])\n\n    return predictors\n\nfeature_names = [f\"lag {i}\" for i in range(1, 11)] + [\n    \"moving_avg_20\",\n    \"moving_std_20\",\n    \"moving_min_20\",\n    \"moving_max_20\",\n]\n\nforecaster = ForecasterAutoregCustom(\n    regressor       = LGBMRegressor(random_state=123, verbose=-1),\n    fun_predictors  = create_predictors,\n    name_predictors = feature_names,\n    window_size     = 20\n)\n</pre> <pre>from skforecast.recursive import ForecasterRecursive\nfrom skforecast.preprocessing import RollingFeatures\n\nrolling = RollingFeatures(\n    ststs        = ['mean', 'std', 'min', 'max'],\n    window_sizes = [20, 20, 20, 20]\n)\n\nforecaster = ForecasterRecursive(\n    regressor       = LGBMRegressor(random_state=123, verbose=-1),\n    lags            = 10,\n    window_features = rolling\n)\n</pre> <p>The arguments <code>initial_train_size</code>, <code>window_size</code>, <code>differentiation</code>, <code>refit</code>, <code>fixed_train_size</code>, <code>gap</code>, <code>skip_folds</code>, <code>allow_incomplete_fold</code>, <code>return_all_indexes</code>, and <code>verbose</code> are no longer defined in the <code>backtesting_forecaster</code> function. Instead, an instance of <code>TimeSeriesFolds</code> should be created, and these arguments should be specified in the class constructor. This change not only allows the same folds to be reused across different methods, but also provides the ability to extract fold indexes.</p> skforecast &lt; 0.14 skforecast \u2265 0.14 <pre>from skforecast.model_selection import (\n    backtesting_forecaster\n)\n\nbacktesting_forecaster(\n    forecaster            = forecaster,\n    y                     = y,\n    steps                 = 10,\n    initial_train_size    = 100,\n    metric                = 'mean_absolute_error',\n    fixed_train_size      = True,\n    gap                   = 0,\n    skip_folds            = None,\n    allow_incomplete_fold = True,\n    refit                 = False,\n    n_jobs                = 'auto',\n    verbose               = False,\n    show_progress         = True\n)\n</pre> <pre>from skforecast.model_selection import (\n    TimeSeriesFold,\n    backtesting_forecaster\n)\n\ncv = TimeSeriesFold(\n    steps                 = 10,\n    initial_train_size    = 100,\n    fixed_train_size      = True,\n    gap                   = 0,\n    skip_folds            = None,\n    allow_incomplete_fold = True,\n    refit                 = False\n)\n\nbacktesting_forecaster(\n    forecaster            = forecaster,\n    y                     = y,\n    cv                    = cv,\n    metric                = 'mean_absolute_error',\n    n_jobs                = 'auto',\n    verbose               = False,\n    show_progress         = True\n)\n</pre> <p>The arguments <code>initial_train_size</code>, <code>window_size</code>, <code>differentiation</code>, <code>refit</code>, <code>fixed_train_size</code>, <code>gap</code>, <code>skip_folds</code>, <code>allow_incomplete_fold</code>, <code>return_all_indexes</code>, and <code>verbose</code> are no longer defined in the <code>grid_search_forecaster</code>, <code>random_search_forecaster</code> and <code>bayesian_search_forecaster</code> functions. Instead, an instance of <code>TimeSeriesFolds</code> or <code>OneStepAheadFold</code> should be created, and these arguments should be specified in the class constructor.</p> skforecast &lt; 0.14 skforecast \u2265 0.14 <pre>from skforecast.model_selection import (\n    grid_search_forecaster\n)\n\ngrid_search_forecaster(\n    forecaster            = forecaster,\n    y                     = data,\n    param_grid            = param_grid,\n    lags_grid             = lags_grid,\n    steps                 = 10,\n    refit                 = False,\n    metric                = 'mean_squared_error',\n    initial_train_size    = 100,\n    fixed_train_size      = False,\n    allow_incomplete_fold = True,\n    return_best           = True,\n    n_jobs                = 'auto',\n    verbose               = False,\n    show_progress         = True\n)\n</pre> <pre>from skforecast.model_selection import (\n    grid_search_forecaster,\n    TimeSeriesFold\n)\n\ncv = TimeSeriesFold(\n    steps                 = 10,\n    initial_train_size    = 100,\n    fixed_train_size      = False,\n    gap                   = 0,\n    skip_folds            = None,\n    allow_incomplete_fold = True,\n    refit                 = False\n)\n\ngrid_search_forecaster(\n    forecaster         = forecaster,\n    y                  = data,\n    param_grid         = param_grid,\n    lags_grid          = lags_grid,\n    cv                 = cv,\n    metric             = 'mean_squared_error',\n    return_best        = True,\n    n_jobs             = 'auto',\n    verbose            = False,\n    show_progress      = True\n)\n</pre> skforecast &lt; 0.14 skforecast \u2265 0.14 <pre>|-- skforecast\n    |-- ForecasterAutoreg\n        |-- ForecasterAutoreg.py              -&gt; ForecasterAutoreg \n    |-- ForecasterAutoregCustom.py\n        |-- ForecasterAutoregCustom.py        -&gt; ForecasterAutoregCustom\n    |-- ForecasterAutoregDirect\n        |-- ForecasterAutoregDirect.py        -&gt; ForecasterAutoregDirect\n    |-- ForecasterAutoregMultiSeries\n        |-- ForecasterAutoregMultiSeries.py   -&gt; ForecasterAutoregMultiSeries\n    |-- ForecasterAutoregMultiVariate\n        |-- ForecasterAutoregMultiVariate.py  -&gt; ForecasterAutoregMultiVariate\n    |-- ForecasterRnn\n        |-- ForecasterRnn.py                  -&gt; ForecasterRnn\n    |-- ForecsaterBase\n        |-- ForecasterBase.py                 -&gt; ForecasterBase\n    |-- ForecasterSarimax\n        |-- ForecasterSarimax.py              -&gt; ForecasterSarimax\n    |-- Sarimax\n        |-- Sarimax.py                        -&gt; Sarimax\n</pre> <pre>|-- skforecast\n    |-- recursive\n        |-- _forecaster_recursive.py              -&gt; ForecasterRecursive\n        |-- _forecaster_recursive_multiseries.py  -&gt; ForecasterRecursiveMultiSeries\n        |-- _forecaster_sarimax.py                -&gt; ForecasterSarimax\n        |-- _forecaster_equivalent_date.py        -&gt; ForecasterEquivalentDate\n    |-- direct\n        |-- _forecaster_direct.py                 -&gt; ForecasterDirect\n        |-- _forecaster_direct_multivariate.py    -&gt; ForecasterDirectMultiVariate\n    |-- deep_learning\n        |-- _forecaster_rnn.py                    -&gt; ForecasterRnn\n    |-- base\n        |-- _forecaster_base.py                   -&gt; ForecasterBase\n    |-- sarimax\n        |-- _sarimax.py                           -&gt; Sarimax\n</pre>"},{"location":"user_guides/migration-guide.html#skforecast-014-migration-guide","title":"Skforecast 0.14 migration guide\u00b6","text":""},{"location":"user_guides/migration-guide.html#changes-in-the-import-of-the-library","title":"Changes in the import of the library\u00b6","text":""},{"location":"user_guides/migration-guide.html#custom-features-window-features","title":"Custom features (window features)\u00b6","text":""},{"location":"user_guides/migration-guide.html#changes-the-backtesting","title":"Changes the backtesting\u00b6","text":""},{"location":"user_guides/migration-guide.html#changes-in-hyperparameters-search","title":"Changes in hyperparameters search\u00b6","text":""},{"location":"user_guides/migration-guide.html#overall-structure-of-the-repository","title":"Overall structure of the repository\u00b6","text":""},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html","title":"Series with different lengths and different exogenous variables","text":"<p>When faced with a multi-series forecasting problem, it is common for the series to have varying lengths due to differences in the starting times of data recording. To address this scenario, the ForecasterRecursiveMultiSeries class supports simultaneous modeling of multiple time series, even when they differ in length and use distinct exogenous variables. The various input formats can be combined, provided that the following requirements are met:</p> Series type Exog type Index requirements <code>Wide DataFrame</code> <code>Wide DataFrame</code>, <code>MultiIndex DataFrame</code>, <code>dict</code> <code>pandas DatetimeIndex</code> or <code>RangeIndex</code> <code>MultiIndex DataFrame</code> <code>Wide DataFrame</code>, <code>MultiIndex DataFrame</code>, <code>dict</code> <code>pandas DatetimeIndex</code> <code>dict</code> <code>Wide DataFrame</code>, <code>MultiIndex DataFrame</code>, <code>dict</code> <code>pandas DatetimeIndex</code> or <code>RangeIndex</code> <p>Regarding the presence of missing values in the series, the <code>ForecasterRecursiveMultiSeries</code> class can handle all scenarios:</p> Series values Allowed <code>[NaN, NaN, NaN, NaN, 4, 5, 6, 7, 8, 9]</code> \u2714\ufe0f <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, NaN]</code> \u2714\ufe0f <code>[0, 1, 2, 3, 4, NaN, 6, 7, 8, 9]</code> \u2714\ufe0f <code>[NaN, NaN, 2, 3, 4, NaN, 6, 7, 8, 9]</code> \u2714\ufe0f <p> \ud83d\udca1 Tip </p> <p>API Reference ForecasterRecursiveMultiSeries.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nfrom lightgbm import LGBMRegressor\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.preprocessing import (\n    reshape_series_long_to_dict, \n    reshape_exog_long_to_dict, \n    RollingFeatures\n)\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\nfrom skforecast.model_selection import (\n    TimeSeriesFold,\n    backtesting_forecaster_multiseries,\n    bayesian_search_forecaster_multiseries\n)\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import matplotlib.pyplot as plt from pprint import pprint from lightgbm import LGBMRegressor from skforecast.plot import set_dark_theme from skforecast.preprocessing import (     reshape_series_long_to_dict,      reshape_exog_long_to_dict,      RollingFeatures ) from skforecast.recursive import ForecasterRecursiveMultiSeries from skforecast.model_selection import (     TimeSeriesFold,     backtesting_forecaster_multiseries,     bayesian_search_forecaster_multiseries ) <p>In this example, the data is stored in long-format DataFrame. The <code>series_id</code> column identifies the time series each observation belongs to, the <code>timestamp</code> column contains the corresponding dates, and the <code>value</code> column holds the observed values for each series. Each time series may have a different length.</p> <p>The exogenous variables are stored in a separate long-format DataFrame. As with the main series, the <code>series_id</code> column indicates the associated time series, the <code>timestamp</code> column records the dates, and the remaining columns contain the values of the exogenous variables at each timestamp.</p> In\u00a0[2]: Copied! <pre># Load time series of multiple lengths and exogenous variables\n# ==============================================================================\nseries = pd.read_csv(\n    'https://raw.githubusercontent.com/skforecast/skforecast-datasets/main/data/demo_multi_series.csv'\n)\nexog = pd.read_csv(\n    'https://raw.githubusercontent.com/skforecast/skforecast-datasets/main/data/demo_multi_series_exog.csv'\n)\n\nseries['timestamp'] = pd.to_datetime(series['timestamp'])\nexog['timestamp'] = pd.to_datetime(exog['timestamp'])\n\ndisplay(series.head(3))\nprint(\"\")\ndisplay(exog.head(3))\n</pre> # Load time series of multiple lengths and exogenous variables # ============================================================================== series = pd.read_csv(     'https://raw.githubusercontent.com/skforecast/skforecast-datasets/main/data/demo_multi_series.csv' ) exog = pd.read_csv(     'https://raw.githubusercontent.com/skforecast/skforecast-datasets/main/data/demo_multi_series_exog.csv' )  series['timestamp'] = pd.to_datetime(series['timestamp']) exog['timestamp'] = pd.to_datetime(exog['timestamp'])  display(series.head(3)) print(\"\") display(exog.head(3)) series_id timestamp value 0 id_1000 2016-01-01 1012.500694 1 id_1000 2016-01-02 1158.500099 2 id_1000 2016-01-03 983.000099 <pre>\n</pre> series_id timestamp sin_day_of_week cos_day_of_week air_temperature wind_speed 0 id_1000 2016-01-01 -0.433884 -0.900969 6.416639 4.040115 1 id_1000 2016-01-02 -0.974928 -0.222521 6.366474 4.530395 2 id_1000 2016-01-03 -0.781831 0.623490 6.555272 3.273064 <p>In terms of performance, using a <code>dict</code> is more efficient than a <code>pandas DataFrame</code>, either wide or long format, especially for larger datasets. This is because dictionaries enable faster access and manipulation of individual time series, without the structural overhead associated with DataFrames.</p> <p>To convert the input data, the <code>reshape_series_long_to_dict</code> function can be used. It takes a long-format pandas DataFrame and returns a dictionary where each key corresponds to a series ID and each value is a <code>pandas Series</code> representing that time series.</p> <p>Similarly, exogenous variables can be transformed into a dictionary using the <code>reshape_exog_long_to_dict</code> function. This function also takes a long-format pandas DataFrame as input and returns a dictionary in which the keys are the series IDs and the values are the corresponding exogenous variables, represented as <code>pandas DataFrames</code>.</p> In\u00a0[3]: Copied! <pre># Transform series and exog to dictionaries\n# ==============================================================================\nseries_dict = reshape_series_long_to_dict(\n    data      = series,\n    series_id = 'series_id',\n    index     = 'timestamp',\n    values    = 'value',\n    freq      = 'D'\n)\n\nexog_dict = reshape_exog_long_to_dict(\n    data      = exog,\n    series_id = 'series_id',\n    index     = 'timestamp',\n    freq      = 'D'\n)\n</pre> # Transform series and exog to dictionaries # ============================================================================== series_dict = reshape_series_long_to_dict(     data      = series,     series_id = 'series_id',     index     = 'timestamp',     values    = 'value',     freq      = 'D' )  exog_dict = reshape_exog_long_to_dict(     data      = exog,     series_id = 'series_id',     index     = 'timestamp',     freq      = 'D' ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MissingValuesWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Series 'id_1003' is incomplete. NaNs have been introduced after setting the          \u2502\n\u2502 frequency.                                                                           \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : MissingValuesWarning                                                      \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\preproc \u2502\n\u2502 essing\\preprocessing.py:509                                                          \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=MissingValuesWarning)            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <p>Some exogenous variables are intentionally omitted for series 1 and 3 to illustrate that each series can use a different set of exogenous variables.</p> In\u00a0[4]: Copied! <pre># Drop some exogenous variables for series 'id_1000' and 'id_1003'\n# ==============================================================================\nexog_dict['id_1000'] = exog_dict['id_1000'].drop(columns=['air_temperature', 'wind_speed'])\nexog_dict['id_1003'] = exog_dict['id_1003'].drop(columns=['cos_day_of_week'])\n</pre> # Drop some exogenous variables for series 'id_1000' and 'id_1003' # ============================================================================== exog_dict['id_1000'] = exog_dict['id_1000'].drop(columns=['air_temperature', 'wind_speed']) exog_dict['id_1003'] = exog_dict['id_1003'].drop(columns=['cos_day_of_week']) In\u00a0[5]: Copied! <pre># Partition data in train and test\n# ==============================================================================\nend_train = '2016-07-31 23:59:00'\nseries_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()}\nexog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()}\nseries_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()}\nexog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()}\n</pre> # Partition data in train and test # ============================================================================== end_train = '2016-07-31 23:59:00' series_dict_train = {k: v.loc[: end_train,] for k, v in series_dict.items()} exog_dict_train   = {k: v.loc[: end_train,] for k, v in exog_dict.items()} series_dict_test  = {k: v.loc[end_train:,] for k, v in series_dict.items()} exog_dict_test    = {k: v.loc[end_train:,] for k, v in exog_dict.items()} In\u00a0[6]: Copied! <pre># Plot series\n# ==============================================================================\nset_dark_theme()\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\nfig, axs = plt.subplots(5, 1, figsize=(8, 4), sharex=True)\n\nfor i, s in enumerate(series_dict.values()):\n    axs[i].plot(s, label=s.name, color=colors[i])\n    axs[i].legend(loc='upper right', fontsize=8)\n    axs[i].tick_params(axis='both', labelsize=8)\n    axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train\n\nfig.suptitle('Series in `series_dict`', fontsize=15)\nplt.tight_layout()\n</pre> # Plot series # ============================================================================== set_dark_theme() colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] fig, axs = plt.subplots(5, 1, figsize=(8, 4), sharex=True)  for i, s in enumerate(series_dict.values()):     axs[i].plot(s, label=s.name, color=colors[i])     axs[i].legend(loc='upper right', fontsize=8)     axs[i].tick_params(axis='both', labelsize=8)     axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train  fig.suptitle('Series in `series_dict`', fontsize=15) plt.tight_layout() In\u00a0[7]: Copied! <pre># Description of each partition\n# ==============================================================================\nfor k in series_dict.keys():\n    print(f\"{k}:\")\n    try:\n        print(\n            f\"\\tTrain: len={len(series_dict_train[k])}, {series_dict_train[k].index[0]}\"\n            f\" --- {series_dict_train[k].index[-1]}\"\n        )\n    except IndexError:\n        print(\"\\tTrain: len=0\")\n    try:\n        print(\n            f\"\\tTest : len={len(series_dict_test[k])}, {series_dict_test[k].index[0]}\"\n            f\" --- {series_dict_test[k].index[-1]}\"\n        )\n    except IndexError:\n        print(\"\\tTest : len=0\")\n</pre> # Description of each partition # ============================================================================== for k in series_dict.keys():     print(f\"{k}:\")     try:         print(             f\"\\tTrain: len={len(series_dict_train[k])}, {series_dict_train[k].index[0]}\"             f\" --- {series_dict_train[k].index[-1]}\"         )     except IndexError:         print(\"\\tTrain: len=0\")     try:         print(             f\"\\tTest : len={len(series_dict_test[k])}, {series_dict_test[k].index[0]}\"             f\" --- {series_dict_test[k].index[-1]}\"         )     except IndexError:         print(\"\\tTest : len=0\") <pre>id_1000:\n\tTrain: len=213, 2016-01-01 00:00:00 --- 2016-07-31 00:00:00\n\tTest : len=153, 2016-08-01 00:00:00 --- 2016-12-31 00:00:00\nid_1001:\n\tTrain: len=30, 2016-07-02 00:00:00 --- 2016-07-31 00:00:00\n\tTest : len=153, 2016-08-01 00:00:00 --- 2016-12-31 00:00:00\nid_1002:\n\tTrain: len=183, 2016-01-01 00:00:00 --- 2016-07-01 00:00:00\n\tTest : len=0\nid_1003:\n\tTrain: len=213, 2016-01-01 00:00:00 --- 2016-07-31 00:00:00\n\tTest : len=153, 2016-08-01 00:00:00 --- 2016-12-31 00:00:00\nid_1004:\n\tTrain: len=91, 2016-05-02 00:00:00 --- 2016-07-31 00:00:00\n\tTest : len=31, 2016-08-01 00:00:00 --- 2016-08-31 00:00:00\n</pre> In\u00a0[8]: Copied! <pre># Exogenous variables for each series\n# ==============================================================================\nfor k in series_dict.keys():\n    print(f\"{k}:\")\n    try:\n        print(f\"\\t{exog_dict[k].columns.to_list()}\")\n    except IndexError:\n        print(\"\\tNo exogenous variables\")\n</pre> # Exogenous variables for each series # ============================================================================== for k in series_dict.keys():     print(f\"{k}:\")     try:         print(f\"\\t{exog_dict[k].columns.to_list()}\")     except IndexError:         print(\"\\tNo exogenous variables\") <pre>id_1000:\n\t['sin_day_of_week', 'cos_day_of_week']\nid_1001:\n\t['sin_day_of_week', 'cos_day_of_week', 'air_temperature', 'wind_speed']\nid_1002:\n\t['sin_day_of_week', 'cos_day_of_week', 'air_temperature', 'wind_speed']\nid_1003:\n\t['sin_day_of_week', 'air_temperature', 'wind_speed']\nid_1004:\n\t['sin_day_of_week', 'cos_day_of_week', 'air_temperature', 'wind_speed']\n</pre> In\u00a0[9]: Copied! <pre># Fit forecaster\n# ==============================================================================\nwindow_features = RollingFeatures(stats=['mean', 'mean'], window_sizes=[7, 14])\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1, max_depth=5), \n                 lags               = 14, \n                 window_features    = window_features,\n                 encoding           = \"ordinal\", \n                 dropna_from_series = False\n             )\n\nforecaster.fit(series=series_dict_train, exog=exog_dict_train, suppress_warnings=True)\nforecaster\n</pre> # Fit forecaster # ============================================================================== window_features = RollingFeatures(stats=['mean', 'mean'], window_sizes=[7, 14]) forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1, max_depth=5),                   lags               = 14,                   window_features    = window_features,                  encoding           = \"ordinal\",                   dropna_from_series = False              )  forecaster.fit(series=series_dict_train, exog=exog_dict_train, suppress_warnings=True) forecaster Out[9]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]</li> <li>Window features: ['roll_mean_7', 'roll_mean_14']</li> <li>Window size: 14</li> <li>Series encoding: ordinal</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Series weights: None</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 14:02:26</li> <li>Last fit date: 2025-08-06 14:02:28</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     sin_day_of_week, cos_day_of_week, air_temperature, wind_speed                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): id_1000, id_1001, id_1002, id_1003, id_1004</li> <li>Training range: 'id_1000': ['2016-01-01', '2016-07-31'], 'id_1001': ['2016-07-02', '2016-07-31'], 'id_1002': ['2016-01-01', '2016-07-01'], 'id_1003': ['2016-01-01', '2016-07-31'], 'id_1004': ['2016-05-02', '2016-07-31']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 5, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>Only series whose last window ends at the same datetime index can be predicted simultaneously. If <code>levels=None</code>, any series that does not extend to the maximum datetime index is excluded from the prediction.</p> <p>In this example, series <code>'id_1002'</code> is excluded because its last available observation occurs before the latest index shared by the other series.</p> In\u00a0[10]: Copied! <pre># Internal last window maximum index\n# ==============================================================================\npprint(\n    {k: v.index[-1] for k, v in forecaster.last_window_.items() if v is not None}\n)\n</pre> # Internal last window maximum index # ============================================================================== pprint(     {k: v.index[-1] for k, v in forecaster.last_window_.items() if v is not None} ) <pre>{'id_1000': Timestamp('2016-07-31 00:00:00'),\n 'id_1001': Timestamp('2016-07-31 00:00:00'),\n 'id_1002': Timestamp('2016-07-01 00:00:00'),\n 'id_1003': Timestamp('2016-07-31 00:00:00'),\n 'id_1004': Timestamp('2016-07-31 00:00:00')}\n</pre> In\u00a0[11]: Copied! <pre># Predict\n# ==============================================================================\npredictions = forecaster.predict(steps=5, exog=exog_dict_test, suppress_warnings=True)\npredictions.head(9)\n</pre> # Predict # ============================================================================== predictions = forecaster.predict(steps=5, exog=exog_dict_test, suppress_warnings=True) predictions.head(9) Out[11]: level pred 2016-08-01 id_1000 1453.312971 2016-08-01 id_1001 2849.347882 2016-08-01 id_1003 2706.851726 2016-08-01 id_1004 7496.555367 2016-08-02 id_1000 1440.763196 2016-08-02 id_1001 2947.579536 2016-08-02 id_1003 2310.075968 2016-08-02 id_1004 8685.425990 2016-08-03 id_1000 1410.151437 In\u00a0[12]: Copied! <pre># Sample data with interspersed NaNs\n# ==============================================================================\nseries_dict_nan = {\n    'id_1000': series_dict['id_1000'].copy(),\n    'id_1003': series_dict['id_1003'].copy()\n}\n\n# Create NaNs\nseries_dict_nan['id_1000'].loc['2016-03-01':'2016-04-01',] = np.nan\nseries_dict_nan['id_1000'].loc['2016-05-01':'2016-05-07',] = np.nan\nseries_dict_nan['id_1003'].loc['2016-07-01',] = np.nan\n\n# Plot series\n# ==============================================================================\nfig, axs = plt.subplots(2, 1, figsize=(8, 2.5), sharex=True)\n\nfor i, s in enumerate(series_dict_nan.values()):\n    axs[i].plot(s, label=s.name, color=colors[i])\n    axs[i].legend(loc='upper right', fontsize=8)\n    axs[i].tick_params(axis='both', labelsize=8)\n    axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train\n\nfig.suptitle('Series in `series_dict_nan`', fontsize=15)\nplt.tight_layout()\n</pre> # Sample data with interspersed NaNs # ============================================================================== series_dict_nan = {     'id_1000': series_dict['id_1000'].copy(),     'id_1003': series_dict['id_1003'].copy() }  # Create NaNs series_dict_nan['id_1000'].loc['2016-03-01':'2016-04-01',] = np.nan series_dict_nan['id_1000'].loc['2016-05-01':'2016-05-07',] = np.nan series_dict_nan['id_1003'].loc['2016-07-01',] = np.nan  # Plot series # ============================================================================== fig, axs = plt.subplots(2, 1, figsize=(8, 2.5), sharex=True)  for i, s in enumerate(series_dict_nan.values()):     axs[i].plot(s, label=s.name, color=colors[i])     axs[i].legend(loc='upper right', fontsize=8)     axs[i].tick_params(axis='both', labelsize=8)     axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train  fig.suptitle('Series in `series_dict_nan`', fontsize=15) plt.tight_layout() <p>When <code>dropna_from_series=False,</code> any <code>NaN</code> values in <code>X_train</code> are retained, and a warning is issued. This setting is useful when the user intends to preserve missing values in the input series and use a regressor that can handle them natively.</p> In\u00a0[13]: Copied! <pre># Create Matrices, dropna_from_series = False\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1, max_depth=5), \n                 lags               = 3, \n                 encoding           = \"ordinal\", \n                 dropna_from_series = False\n             )\n\nX, y = forecaster.create_train_X_y(series=series_dict_nan)\n\ndisplay(X.head(3))\nprint(\"Observations per series:\")\nprint(X['_level_skforecast'].value_counts())\nprint(\"\")\nprint(\"NaNs per series:\")\nprint(X.isnull().sum())\n</pre> # Create Matrices, dropna_from_series = False # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1, max_depth=5),                   lags               = 3,                   encoding           = \"ordinal\",                   dropna_from_series = False              )  X, y = forecaster.create_train_X_y(series=series_dict_nan)  display(X.head(3)) print(\"Observations per series:\") print(X['_level_skforecast'].value_counts()) print(\"\") print(\"NaNs per series:\") print(X.isnull().sum()) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MissingValuesWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 NaNs detected in `y_train`. They have been dropped because the target variable       \u2502\n\u2502 cannot have NaN values. Same rows have been dropped from `X_train` to maintain       \u2502\n\u2502 alignment. This is caused by series with interspersed NaNs.                          \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : MissingValuesWarning                                                      \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\recursi \u2502\n\u2502 ve\\_forecaster_recursive_multiseries.py:1226                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=MissingValuesWarning)            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MissingValuesWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 NaNs detected in `X_train`. Some regressors do not allow NaN values during training. \u2502\n\u2502 If you want to drop them, set `forecaster.dropna_from_series = True`.                \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : MissingValuesWarning                                                      \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\recursi \u2502\n\u2502 ve\\_forecaster_recursive_multiseries.py:1248                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=MissingValuesWarning)            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> lag_1 lag_2 lag_3 _level_skforecast 2016-01-04 983.000099 1158.500099 1012.500694 0 2016-01-05 1675.750496 983.000099 1158.500099 0 2016-01-06 1586.250694 1675.750496 983.000099 0 <pre>Observations per series:\n_level_skforecast\n0    324\n1    216\nName: count, dtype: int64\n\nNaNs per series:\nlag_1                 5\nlag_2                 9\nlag_3                13\n_level_skforecast     0\ndtype: int64\n</pre> <p>When <code>dropna_from_series=True</code>, any <code>NaN</code> values in <code>X_train</code> are removed, and a warning is issued. This option is useful when the chosen regressor cannot handle missing values and requires a fully populated training matrix.</p> In\u00a0[14]: Copied! <pre># Create Matrices, dropna_from_series = False\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1, max_depth=5), \n                 lags               = 3, \n                 encoding           = \"ordinal\", \n                 dropna_from_series = True\n             )\n\nX, y = forecaster.create_train_X_y(series=series_dict_nan)\n\ndisplay(X.head(3))\nprint(\"Observations per series:\")\nprint(X['_level_skforecast'].value_counts())\nprint(\"\")\nprint(\"NaNs per series:\")\nprint(X.isnull().sum())\n</pre> # Create Matrices, dropna_from_series = False # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1, max_depth=5),                   lags               = 3,                   encoding           = \"ordinal\",                   dropna_from_series = True              )  X, y = forecaster.create_train_X_y(series=series_dict_nan)  display(X.head(3)) print(\"Observations per series:\") print(X['_level_skforecast'].value_counts()) print(\"\") print(\"NaNs per series:\") print(X.isnull().sum()) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MissingValuesWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 NaNs detected in `y_train`. They have been dropped because the target variable       \u2502\n\u2502 cannot have NaN values. Same rows have been dropped from `X_train` to maintain       \u2502\n\u2502 alignment. This is caused by series with interspersed NaNs.                          \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : MissingValuesWarning                                                      \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\recursi \u2502\n\u2502 ve\\_forecaster_recursive_multiseries.py:1226                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=MissingValuesWarning)            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MissingValuesWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 NaNs detected in `X_train`. They have been dropped. If you want to keep them, set    \u2502\n\u2502 `forecaster.dropna_from_series = False`. Same rows have been removed from `y_train`  \u2502\n\u2502 to maintain alignment. This caused by series with interspersed NaNs.                 \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : MissingValuesWarning                                                      \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\skforecast\\recursi \u2502\n\u2502 ve\\_forecaster_recursive_multiseries.py:1239                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=MissingValuesWarning)            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> lag_1 lag_2 lag_3 _level_skforecast 2016-01-04 983.000099 1158.500099 1012.500694 0 2016-01-05 1675.750496 983.000099 1158.500099 0 2016-01-06 1586.250694 1675.750496 983.000099 0 <pre>Observations per series:\n_level_skforecast\n0    318\n1    207\nName: count, dtype: int64\n\nNaNs per series:\nlag_1                0\nlag_2                0\nlag_3                0\n_level_skforecast    0\ndtype: int64\n</pre> <p>During the training process, the warnings can be suppressed by setting <code>suppress_warnings = True</code>.</p> In\u00a0[15]: Copied! <pre># Suppress warnings during fit method\n# ==============================================================================\nforecaster.fit(series=series_dict_nan, suppress_warnings=True)\nforecaster\n</pre> # Suppress warnings during fit method # ============================================================================== forecaster.fit(series=series_dict_nan, suppress_warnings=True) forecaster Out[15]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3]</li> <li>Window features: None</li> <li>Window size: 3</li> <li>Series encoding: ordinal</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Series weights: None</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-06 14:02:29</li> <li>Last fit date: 2025-08-06 14:02:29</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): id_1000, id_1003</li> <li>Training range: 'id_1000': ['2016-01-01', '2016-12-31'], 'id_1003': ['2016-01-01', '2016-12-31']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 5, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[16]: Copied! <pre># Backtesting\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1, max_depth=5), \n                 lags               = 14, \n                 window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[7, 14]),\n                 encoding           = \"ordinal\", \n                 dropna_from_series = False\n             )\n\ncv = TimeSeriesFold(\n         steps              = 24,\n         initial_train_size = \"2016-07-31 23:59:00\",\n     )\n\nmetrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n    forecaster            = forecaster,\n    series                = series_dict,\n    exog                  = exog_dict,\n    cv                    = cv,\n    levels                = None,\n    metric                = \"mean_absolute_error\",\n    add_aggregated_metric = True,\n    suppress_warnings     = True\n)\n\ndisplay(metrics_levels)\nbacktest_predictions\n</pre> # Backtesting # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1, max_depth=5),                   lags               = 14,                   window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[7, 14]),                  encoding           = \"ordinal\",                   dropna_from_series = False              )  cv = TimeSeriesFold(          steps              = 24,          initial_train_size = \"2016-07-31 23:59:00\",      )  metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(     forecaster            = forecaster,     series                = series_dict,     exog                  = exog_dict,     cv                    = cv,     levels                = None,     metric                = \"mean_absolute_error\",     add_aggregated_metric = True,     suppress_warnings     = True )  display(metrics_levels) backtest_predictions <pre>  0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> levels mean_absolute_error 0 id_1000 167.502214 1 id_1001 1103.313887 2 id_1002 NaN 3 id_1003 280.492603 4 id_1004 711.078359 5 average 565.596766 6 weighted_average 572.944127 7 pooling 572.944127 Out[16]: level pred 2016-08-01 id_1000 1453.312971 2016-08-01 id_1001 2849.347882 2016-08-01 id_1003 2706.851726 2016-08-01 id_1004 7496.555367 2016-08-02 id_1000 1440.763196 ... ... ... 2016-12-30 id_1001 1132.535774 2016-12-30 id_1003 2089.261345 2016-12-31 id_1000 1393.128313 2016-12-31 id_1001 1106.034061 2016-12-31 id_1003 2064.475030 <p>507 rows \u00d7 2 columns</p> <p>Note that if a series has no observations in the test set, the backtesting process will not generate any predictions for that series, and the corresponding metric will be <code>NaN</code>. This is the case for series <code>'id_1002'</code> in the current example.</p> In\u00a0[17]: Copied! <pre># Plot backtesting predictions\n# ==============================================================================\nfig, axs = plt.subplots(5, 1, figsize=(8, 4), sharex=True)\n\nfor i, s in enumerate(series_dict.keys()):\n    axs[i].plot(series_dict[s], label=series_dict[s].name, color=colors[i])\n    axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train\n    try:\n        axs[i].plot(backtest_predictions[s], label='prediction', color=\"white\")\n    except KeyError:\n        pass\n    axs[i].legend(loc='upper right', fontsize=8)\n    axs[i].tick_params(axis='both', labelsize=8)\n\nfig.suptitle('Backtest Predictions', fontsize=15)\nplt.tight_layout()\n</pre> # Plot backtesting predictions # ============================================================================== fig, axs = plt.subplots(5, 1, figsize=(8, 4), sharex=True)  for i, s in enumerate(series_dict.keys()):     axs[i].plot(series_dict[s], label=series_dict[s].name, color=colors[i])     axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train     try:         axs[i].plot(backtest_predictions[s], label='prediction', color=\"white\")     except KeyError:         pass     axs[i].legend(loc='upper right', fontsize=8)     axs[i].tick_params(axis='both', labelsize=8)  fig.suptitle('Backtest Predictions', fontsize=15) plt.tight_layout() In\u00a0[18]: Copied! <pre># Bayesian search hyperparameters and lags with Optuna\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1), \n                 lags               = 14, \n                 window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[7, 14]),\n                 encoding           = \"ordinal\", \n                 dropna_from_series = False\n             )\n\n# Search space\ndef search_space(trial):\n    search_space  = {\n        'lags'            : trial.suggest_categorical('lags', [7, 14]),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 30),\n        'max_depth'       : trial.suggest_int('max_depth', 3, 7)\n    }\n\n    return search_space\n\ncv = TimeSeriesFold(\n         steps              = 24,\n         initial_train_size = \"2016-07-31 23:59:00\",\n     )\n\nresults, best_trial = bayesian_search_forecaster_multiseries(\n    forecaster        = forecaster,\n    series            = series_dict,\n    exog              = exog_dict,\n    search_space      = search_space,\n    cv                = cv,\n    levels            = None,\n    metric            = 'mean_absolute_error',\n    n_trials          = 10,\n    suppress_warnings = True\n)\n\nresults.head(4)\n</pre> # Bayesian search hyperparameters and lags with Optuna # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                   lags               = 14,                   window_features    = RollingFeatures(stats=['mean', 'mean'], window_sizes=[7, 14]),                  encoding           = \"ordinal\",                   dropna_from_series = False              )  # Search space def search_space(trial):     search_space  = {         'lags'            : trial.suggest_categorical('lags', [7, 14]),         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 30),         'max_depth'       : trial.suggest_int('max_depth', 3, 7)     }      return search_space  cv = TimeSeriesFold(          steps              = 24,          initial_train_size = \"2016-07-31 23:59:00\",      )  results, best_trial = bayesian_search_forecaster_multiseries(     forecaster        = forecaster,     series            = series_dict,     exog              = exog_dict,     search_space      = search_space,     cv                = cv,     levels            = None,     metric            = 'mean_absolute_error',     n_trials          = 10,     suppress_warnings = True )  results.head(4) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14] \n  Parameters: {'min_samples_leaf': 29, 'max_depth': 5}\n  Backtesting metric: 531.5731935466865\n  Levels: ['id_1000', 'id_1001', 'id_1002', 'id_1003', 'id_1004']\n\n</pre> <pre>c:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\jupyter_client\\session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\nc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py12\\Lib\\site-packages\\jupyter_client\\session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n</pre> Out[18]: levels lags params mean_absolute_error__weighted_average mean_absolute_error__average mean_absolute_error__pooling min_samples_leaf max_depth 0 [id_1000, id_1001, id_1002, id_1003, id_1004] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'min_samples_leaf': 29, 'max_depth': 5} 531.573194 510.885119 510.885119 29 5 1 [id_1000, id_1001, id_1002, id_1003, id_1004] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'min_samples_leaf': 11, 'max_depth': 5} 542.700963 557.169748 557.169748 11 5 2 [id_1000, id_1001, id_1002, id_1003, id_1004] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] {'min_samples_leaf': 18, 'max_depth': 4} 572.011630 559.198862 559.198862 18 4 3 [id_1000, id_1001, id_1002, id_1003, id_1004] [1, 2, 3, 4, 5, 6, 7] {'min_samples_leaf': 30, 'max_depth': 6} 591.817179 595.234722 595.234722 30 6 <p> \u26a0 Warning </p> <p>Starting with version <code>0.13.0</code>, the <code>ForecasterRecursiveMultiSeries</code> class can use <code>encoding='ordinal_category'</code> for encoding time series identifiers. This approach creates a new column (_level_skforecast) of type pandas <code>category</code>. Consequently, the regressors must be able to handle categorical variables. If the regressors do not support categorical variables, the user should set the encoding to <code>'ordinal'</code> or <code>'onehot'</code> for compatibility.</p> <p>Some examples of regressors that support categorical variables and how to enable them are:</p> <p>HistGradientBoostingRegressor</p> <pre>HistGradientBoostingRegressor(categorical_features=\"from_dtype\")\n</pre> <p>LightGBM</p> <p><code>LGBMRegressor</code> does not allow configuration of categorical features during initialization, but rather in its <code>fit</code> method. Therefore, use Forecaster' argument <code>fit_kwargs = {'categorical_feature':'auto'}</code>. This is the default behavior of <code>LGBMRegressor</code> if no indication is given.</p> <p>XGBoost</p> <pre>XGBRegressor(enable_categorical=True)\n</pre>"},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#global-forecasting-models-time-series-with-different-lengths-and-different-exogenous-variables","title":"Global Forecasting Models: Time series with different lengths and different exogenous variables\u00b6","text":""},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#train-and-predict","title":"Train and predict\u00b6","text":"<p>The <code>fit</code> method is used to train the model, it is passed the dictionary of series and the dictionary of exogenous variables where the keys of each dictionary are the names of the series.</p>"},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#missing-values-in-the-series","title":"Missing values in the series\u00b6","text":"<p>When working with time series of different lengths, it is common for some series to contain missing values. Since not all regressors can handle missing data, the <code>dropna_from_series</code> argument controls how missing values in the predictors are treated when building the training matrices.</p> <ul> <li><p>If set to <code>False</code> (default), <code>NaN</code> values in <code>X_train</code> are retained, and a warning is issued.</p> </li> <li><p>If set to <code>True</code>, rows with <code>NaN</code> values in <code>X_train</code> are dropped, and the corresponding rows in y_train are removed as well. A warning is also issued.</p> </li> </ul> <p>Regardless of this setting, any <code>NaN</code> values in <code>y_train</code> are always dropped, along with the corresponding rows in <code>X_train</code>, as the target variable cannot contain missing values.</p>"},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#backtesting","title":"Backtesting\u00b6","text":"<p>As with the <code>predict</code> method, the <code>levels</code> parameter must be specified when performing backtesting. When backtesting time series of varying lengths, two important behaviors should be considered:</p> <ul> <li><p>Handling missing values in the series: Backtesting only produces predictions for the date-times that are present in each individual series. If there are gaps in the validation or test sets, no predictions are generated for those missing periods. This ensures that evaluation metrics are not distorted by the absence of true values.</p> </li> <li><p>Size of the training set: If the <code>initial_train_size</code> parameter is specified as an <code>integer</code>, it is applied relative to the earliest timestamp across all series. Consequently, series that start later may have a shorter effective training window, or even no available training data during the initial folds. Alternatively, <code>initial_train_size</code> can be specified as a datetime, which can make the training window easier to interpret and align with specific points in time.</p> </li> </ul>"},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#hyperparameter-search-and-lags-selection","title":"Hyperparameter search and lags selection\u00b6","text":"<p>Hyperparameter tuning consists of systematically evaluating combinations of hyperparameters (including lags) to find the configuration that yields the best predictive performance. The skforecast library supports several tuning strategies: grid search, random search, and Bayesian search. These strategies can be used with either backtesting or one-step-ahead validation to determine the optimal parameter set for a given forecasting task.</p> <p>The functions <code>grid_search_forecaster_multiseries</code>, <code>random_search_forecaster_multiseries</code> and <code>bayesian_search_forecaster_multiseries</code> from the <code>model_selection</code> module allow for lags and hyperparameter optimization.</p> <p>The <code>levels</code> argument is crucial in these functions, as it specifies which time series are considered during the optimization:</p> <ul> <li><p>If <code>levels</code> is a <code>list</code>, the function searches for the configuration that minimizes the aggregated error across the selected series.</p> </li> <li><p>If <code>levels = None</code>, all available series are used in the optimization.</p> </li> <li><p>If <code>levels = 'item_1'</code> (equivalent to <code>levels = ['item_1']</code>), the optimization is based solely on the error of that specific series, and the returned metric corresponds to it.</p> </li> </ul> <p>When optimizing across multiple series, the resulting metrics must be aggregated. The available methods are:</p> <ul> <li><p><code>'average'</code>, the average (arithmetic mean) of all series (<code>levels</code>).</p> </li> <li><p><code>'weighted_average'</code>, the average of the metrics weighted by the number of predicted values of each series (<code>levels</code>).</p> </li> <li><p><code>'pooling'</code>, the values of all series (<code>levels</code>) are pooled and then the metric is calculated.</p> </li> </ul> <p>If a <code>list</code> of aggregation methods is provided to the <code>aggregate_metric</code> argument, all are computed, but only the first method is used to select the best configuration.</p> <p>The following example demonstrates how to use <code>grid_search_forecaster_multiseries</code> to find the best lags and hyperparameters for all series (all <code>levels</code>) using <code>'weighted_average'</code> as the aggregation strategy.</p>"},{"location":"user_guides/multi-series-with-different-length-and-different_exog.html#categorical-variables","title":"Categorical variables\u00b6","text":""},{"location":"user_guides/plotting.html","title":"Plotting","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.plot import plot_residuals\nfrom skforecast.plot import calculate_lag_autocorrelation\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster from skforecast.plot import plot_residuals from skforecast.plot import calculate_lag_autocorrelation from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0}\n)\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n\n# Plot data\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.plot(ax=ax);\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0} )  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS')  # Plot data # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) data.plot(ax=ax); <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> In\u00a0[3]: Copied! <pre># Train and backtest forecaster\n# ==============================================================================\nn_backtest = 36 * 3\ndata_train = data[:-n_backtest]\ndata_test  = data[-n_backtest:]\n\nforecaster = ForecasterRecursive(\n                 regressor = Ridge(),\n                 lags      = 5 \n             )\n\ncv = TimeSeriesFold(steps=36, initial_train_size=len(data_train))\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data['y'],\n                          cv         = cv,\n                          metric     = 'mean_squared_error',\n                          verbose    = True\n                      )\n \npredictions.head()\n</pre> # Train and backtest forecaster # ============================================================================== n_backtest = 36 * 3 data_train = data[:-n_backtest] data_test  = data[-n_backtest:]  forecaster = ForecasterRecursive(                  regressor = Ridge(),                  lags      = 5               )  cv = TimeSeriesFold(steps=36, initial_train_size=len(data_train))  metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data['y'],                           cv         = cv,                           metric     = 'mean_squared_error',                           verbose    = True                       )   predictions.head() <pre>Information of folds\n--------------------\nNumber of observations used for initial training: 96\nNumber of observations used for backtesting: 108\n    Number of folds: 3\n    Number skipped folds: 0 \n    Number of steps per fold: 36\n    Number of steps to exclude between last observed data (last window) and predictions (gap): 0\n\nFold: 0\n    Training:   1991-07-01 00:00:00 -- 1999-06-01 00:00:00  (n=96)\n    Validation: 1999-07-01 00:00:00 -- 2002-06-01 00:00:00  (n=36)\nFold: 1\n    Training:   No training in this fold\n    Validation: 2002-07-01 00:00:00 -- 2005-06-01 00:00:00  (n=36)\nFold: 2\n    Training:   No training in this fold\n    Validation: 2005-07-01 00:00:00 -- 2008-06-01 00:00:00  (n=36)\n\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> Out[3]: pred 1999-07-01 0.667651 1999-08-01 0.655759 1999-09-01 0.652177 1999-10-01 0.641377 1999-11-01 0.635245 <p>The <code>plot_residuals</code> function can be used in two ways: with pre-calculated residuals or by passing the predicted and actual values of the series.</p> In\u00a0[4]: Copied! <pre># Plot residuals\n# ======================================================================================\nresiduals = predictions['pred'] - data_test['y']\n_ = plot_residuals(residuals=residuals, figsize=(7, 3.5))\n</pre> # Plot residuals # ====================================================================================== residuals = predictions['pred'] - data_test['y'] _ = plot_residuals(residuals=residuals, figsize=(7, 3.5)) In\u00a0[5]: Copied! <pre>_ = plot_residuals(y_true=data_test['y'], y_pred=predictions['pred'], figsize=(7, 3.5))\n</pre> _ = plot_residuals(y_true=data_test['y'], y_pred=predictions['pred'], figsize=(7, 3.5)) <p>It is possible to customize the plot by by either passing a pre-existing matplotlib figure object or using additional keyword arguments that are passed to <code>matplotlib.pyplot.figure()</code>.</p> In\u00a0[6]: Copied! <pre># Libraries\n# ==============================================================================\nfrom skforecast.plot import plot_prediction_intervals\n</pre> # Libraries # ============================================================================== from skforecast.plot import plot_prediction_intervals In\u00a0[7]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name='h2o_exog', raw=True, verbose=False)\n\n# Data preparation\n# ==============================================================================\ndata = data.rename(columns={'fecha': 'date'})\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Split data into train-test\n# ==============================================================================\nsteps = 36\ndata_train = data[:-steps]\ndata_test  = data[-steps:]\nprint(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Data download # ============================================================================== data = fetch_dataset(name='h2o_exog', raw=True, verbose=False)  # Data preparation # ============================================================================== data = data.rename(columns={'fecha': 'date'}) data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') data = data.sort_index()  # Split data into train-test # ============================================================================== steps = 36 data_train = data[:-steps] data_test  = data[-steps:] print(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Train dates : 1992-04-01 00:00:00 --- 2005-06-01 00:00:00  (n=159)\nTest dates  : 2005-07-01 00:00:00 --- 2008-06-01 00:00:00  (n=36)\n</pre> In\u00a0[8]: Copied! <pre># Create and train forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = Ridge(alpha=0.1, random_state=765),\n                 lags      = 15\n             )\nforecaster.fit(y=data_train['y'], store_in_sample_residuals=True)\n\n# Prediction intervals\n# ==============================================================================\npredictions = forecaster.predict_interval(\n                  steps    = steps,\n                  interval = [1, 99],\n                  n_boot   = 500\n              )\n</pre> # Create and train forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor = Ridge(alpha=0.1, random_state=765),                  lags      = 15              ) forecaster.fit(y=data_train['y'], store_in_sample_residuals=True)  # Prediction intervals # ============================================================================== predictions = forecaster.predict_interval(                   steps    = steps,                   interval = [1, 99],                   n_boot   = 500               ) In\u00a0[9]: Copied! <pre># Plot forecasts with prediction intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_test,\n    target_variable     = \"y\",\n    title               = \"Real value vs predicted in test data\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}\n)\n</pre> # Plot forecasts with prediction intervals # ============================================================================== plot_prediction_intervals(     predictions         = predictions,     y_true              = data_test,     target_variable     = \"y\",     title               = \"Real value vs predicted in test data\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1} ) In\u00a0[10]: Copied! <pre># Libraries\n# ==============================================================================\nfrom skforecast.plot import plot_multivariate_time_series_corr\nfrom skforecast.utils import multivariate_time_series_corr\n</pre> # Libraries # ============================================================================== from skforecast.plot import plot_multivariate_time_series_corr from skforecast.utils import multivariate_time_series_corr In\u00a0[11]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name='air_quality_valencia', raw=False, verbose=True)\ndata\n</pre> # Data download # ============================================================================== data = fetch_dataset(name='air_quality_valencia', raw=False, verbose=True) data <pre>air_quality_valencia\n--------------------\nHourly measures of several air chemical pollutant at Valencia city (Avd.\nFrancia) from 2019-01-01 to 20213-12-31. Including the following variables:\npm2.5 (\u00b5g/m\u00b3), CO (mg/m\u00b3), NO (\u00b5g/m\u00b3), NO2 (\u00b5g/m\u00b3), PM10 (\u00b5g/m\u00b3), NOx (\u00b5g/m\u00b3),\nO3 (\u00b5g/m\u00b3), Veloc. (m/s), Direc. (degrees), SO2 (\u00b5g/m\u00b3).\nRed de Vigilancia y Control de la Contaminaci\u00f3n Atmosf\u00e9rica, 46250047-Val\u00e8ncia -\nAv. Fran\u00e7a, https://mediambient.gva.es/es/web/calidad-ambiental/datos-\nhistoricos.\nShape of the dataset: (43824, 10)\n</pre> Out[11]: so2 co no no2 pm10 nox o3 veloc. direc. pm2.5 datetime 2019-01-01 00:00:00 8.0 0.2 3.0 36.0 22.0 40.0 16.0 0.5 262.0 19.0 2019-01-01 01:00:00 8.0 0.1 2.0 40.0 32.0 44.0 6.0 0.6 248.0 26.0 2019-01-01 02:00:00 8.0 0.1 11.0 42.0 36.0 58.0 3.0 0.3 224.0 31.0 2019-01-01 03:00:00 10.0 0.1 15.0 41.0 35.0 63.0 3.0 0.2 220.0 30.0 2019-01-01 04:00:00 11.0 0.1 16.0 39.0 36.0 63.0 3.0 0.4 221.0 30.0 ... ... ... ... ... ... ... ... ... ... ... 2023-12-31 19:00:00 3.0 0.1 6.0 18.0 8.0 26.0 47.0 1.7 246.0 7.0 2023-12-31 20:00:00 3.0 0.1 6.0 19.0 7.0 27.0 49.0 1.3 239.0 6.0 2023-12-31 21:00:00 3.0 0.1 4.0 15.0 5.0 22.0 55.0 1.5 247.0 4.0 2023-12-31 22:00:00 3.0 0.1 5.0 13.0 5.0 20.0 57.0 1.1 246.0 5.0 2023-12-31 23:00:00 3.0 0.1 5.0 12.0 5.0 20.0 55.0 0.5 247.0 4.0 <p>43824 rows \u00d7 10 columns</p> In\u00a0[12]: Copied! <pre># Correlation between target series and the lags of the other series\n# ======================================================================================\ncorr = multivariate_time_series_corr(\n           time_series = data['pm2.5'],\n           other       = data,\n           lags        = 24\n       )\ncorr.head(3)\n</pre> # Correlation between target series and the lags of the other series # ====================================================================================== corr = multivariate_time_series_corr(            time_series = data['pm2.5'],            other       = data,            lags        = 24        ) corr.head(3) Out[12]: so2 co no no2 pm10 nox o3 veloc. direc. pm2.5 lag 0 0.110520 0.167426 0.330658 0.463759 0.687683 0.444082 -0.344922 -0.184636 0.007725 1.000000 1 0.098191 0.168089 0.313363 0.454940 0.633034 0.428927 -0.328899 -0.197767 -0.006486 0.939515 2 0.082010 0.157377 0.272874 0.424717 0.574097 0.388446 -0.294490 -0.200191 -0.021277 0.870223 In\u00a0[13]: Copied! <pre>_ = plot_multivariate_time_series_corr(corr, figsize=(7, 7))\n</pre> _ = plot_multivariate_time_series_corr(corr, figsize=(7, 7)) In\u00a0[14]: Copied! <pre>data = fetch_dataset(name=\"h2o\", verbose=False)['x']\ncalculate_lag_autocorrelation(data=data, n_lags=24)\n</pre> data = fetch_dataset(name=\"h2o\", verbose=False)['x'] calculate_lag_autocorrelation(data=data, n_lags=24) Out[14]: lag partial_autocorrelation_abs partial_autocorrelation autocorrelation_abs autocorrelation 0 1 0.758537 0.758537 0.754819 0.754819 1 12 0.669411 0.669411 0.838637 0.838637 2 13 0.635662 -0.635662 0.630189 0.630189 3 11 0.398290 0.398290 0.650629 0.650629 4 14 0.378889 -0.378889 0.437928 0.437928 5 10 0.249200 0.249200 0.479473 0.479473 6 9 0.229079 0.229079 0.334505 0.334505 7 16 0.196553 0.196553 0.139812 0.139812 8 8 0.178435 0.178435 0.202611 0.202611 9 7 0.133492 0.133492 0.115865 0.115865 10 24 0.111123 0.111123 0.714203 0.714203 11 18 0.104232 0.104232 0.023526 0.023526 12 15 0.089035 -0.089035 0.276531 0.276531 13 20 0.086720 0.086720 0.140834 0.140834 14 4 0.086598 -0.086598 0.227902 0.227902 15 6 0.068177 0.068177 0.087365 0.087365 16 23 0.065784 0.065784 0.554988 0.554988 17 3 0.062278 -0.062278 0.385916 0.385916 18 19 0.059611 -0.059611 0.050680 0.050680 19 2 0.028086 -0.028086 0.557929 0.557929 20 22 0.025117 0.025117 0.390331 0.390331 21 5 0.009320 0.009320 0.126263 0.126263 22 21 0.007966 0.007966 0.259156 0.259156 23 17 0.003154 0.003154 0.043142 0.043142"},{"location":"user_guides/plotting.html#plotting-utilities","title":"Plotting utilities\u00b6","text":""},{"location":"user_guides/plotting.html#plot-forecasting-residuals","title":"Plot forecasting residuals\u00b6","text":"<p>Analyzing the residuals (errors) of predictions is useful to understand the behavior of a forecaster. The function <code>skforecast.plot.plot_residuals</code> creates 3 plots:</p> <ul> <li><p>A time-ordered plot of residual values</p> </li> <li><p>A distribution plot that showcases the distribution of residuals</p> </li> <li><p>A plot showcasing the autocorrelation of residuals</p> </li> </ul> <p>By examining the residual values over time, you can determine whether there is a pattern in the errors made by the forecast model. The distribution plot helps you understand whether the residuals are normally distributed, and the autocorrelation plot helps you identify whether there are any dependencies or relationships between the residuals.</p>"},{"location":"user_guides/plotting.html#plot-prediction-intervals","title":"Plot prediction intervals\u00b6","text":""},{"location":"user_guides/plotting.html#plot-correlation-between-lags-of-multiple-time-series","title":"Plot correlation between lags of multiple time series\u00b6","text":"<p>When training a Global Forecasting Model of type Dependent multi-series, <code>ForecasterDirectMultiVariate</code>, it is useful to analyze the correlation between the lags of the different time series and the target series. The function <code>skforecast.plot.plot_correlation_lags</code> creates a heatmap that shows the correlation between the lags of the different time series.</p>"},{"location":"user_guides/plotting.html#autocorrelation-and-partial-autocorrelation","title":"Autocorrelation and partial autocorrelation\u00b6","text":"<p>The function <code>skforecast.plot.calculate_autocorrelation</code> calculates the autocorrelation and partial autocorrelation of a time series. The function returns a pandas DataFrame with the autocorrelation and partial autocorrelation values for different lags. Additionally, it also returns the absolute values of the autocorrelation and partial autocorrelation values, which can be useful to identify the most important lags.</p>"},{"location":"user_guides/probabilistic-forecasting-bootstrapped-residuals.html","title":"Bootstrapped residuals","text":"<p> \ud83d\udca1 Tip </p> <p>For more examples on how to use probabilistic forecasting, check out the following articles:</p> <ul> <li>              Probabilistic forecasting with machine learning          </li> <li>              Probabilistic forecasting: prediction intervals for multi-step time series forecasting          </li> <li>              Continuous Ranked Probability Score (CRPS) in probabilistic forecasting          </li> </ul> In\u00a0[\u00a0]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import (\n    set_dark_theme,\n    plot_residuals,\n    plot_prediction_distribution,\n    plot_prediction_intervals\n)\nfrom pprint import pprint\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom feature_engine.datetime import DatetimeFeatures\nfrom feature_engine.creation import CyclicalFeatures\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\nfrom skforecast.metrics import calculate_coverage\nfrom scipy.stats import norm\n\n# Configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('once')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd from skforecast.datasets import fetch_dataset  # Plots # ============================================================================== import matplotlib.pyplot as plt from skforecast.plot import (     set_dark_theme,     plot_residuals,     plot_prediction_distribution,     plot_prediction_intervals ) from pprint import pprint  # Modelling and Forecasting # ============================================================================== from lightgbm import LGBMRegressor from sklearn.pipeline import make_pipeline from feature_engine.datetime import DatetimeFeatures from feature_engine.creation import CyclicalFeatures from skforecast.recursive import ForecasterRecursive from skforecast.preprocessing import RollingFeatures from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster from skforecast.metrics import calculate_coverage from scipy.stats import norm  # Configuration # ============================================================================== import warnings warnings.filterwarnings('once') In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name='bike_sharing', raw=False)\ndata = data[['users', 'temp', 'hum', 'windspeed', 'holiday']]\ndata = data.loc['2011-04-01 00:00:00':'2012-10-20 23:00:00', :].copy()\ndata.head(3)\n</pre> # Data download # ============================================================================== data = fetch_dataset(name='bike_sharing', raw=False) data = data[['users', 'temp', 'hum', 'windspeed', 'holiday']] data = data.loc['2011-04-01 00:00:00':'2012-10-20 23:00:00', :].copy() data.head(3) <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 11)\n</pre> Out[2]: users temp hum windspeed holiday date_time 2011-04-01 00:00:00 6.0 10.66 100.0 11.0014 0.0 2011-04-01 01:00:00 4.0 10.66 100.0 11.0014 0.0 2011-04-01 02:00:00 7.0 10.66 93.0 12.9980 0.0 In\u00a0[3]: Copied! <pre># Calendar features\n# ==============================================================================\nfeatures_to_extract = [\n    'month',\n    'week',\n    'day_of_week',\n    'hour'\n]\ncalendar_transformer = DatetimeFeatures(\n    variables           = 'index',\n    features_to_extract = features_to_extract,\n    drop_original       = False,\n)\n\n# Cyclical encoding of calendar features\n# ==============================================================================\nfeatures_to_encode = [\n    \"month\",\n    \"week\",\n    \"day_of_week\",\n    \"hour\",\n]\nmax_values = {\n    \"month\": 12,\n    \"week\": 52,\n    \"day_of_week\": 7,\n    \"hour\": 24,\n}\ncyclical_encoder = CyclicalFeatures(\n                       variables     = features_to_encode,\n                       max_values    = max_values,\n                       drop_original = True\n                   )\n\nexog_transformer = make_pipeline(\n                       calendar_transformer,\n                       cyclical_encoder\n                   )\n\ndata = exog_transformer.fit_transform(data)\nexog_features = data.columns.difference(['users']).tolist()\ndata.head(3)\n</pre> # Calendar features # ============================================================================== features_to_extract = [     'month',     'week',     'day_of_week',     'hour' ] calendar_transformer = DatetimeFeatures(     variables           = 'index',     features_to_extract = features_to_extract,     drop_original       = False, )  # Cyclical encoding of calendar features # ============================================================================== features_to_encode = [     \"month\",     \"week\",     \"day_of_week\",     \"hour\", ] max_values = {     \"month\": 12,     \"week\": 52,     \"day_of_week\": 7,     \"hour\": 24, } cyclical_encoder = CyclicalFeatures(                        variables     = features_to_encode,                        max_values    = max_values,                        drop_original = True                    )  exog_transformer = make_pipeline(                        calendar_transformer,                        cyclical_encoder                    )  data = exog_transformer.fit_transform(data) exog_features = data.columns.difference(['users']).tolist() data.head(3) Out[3]: users temp hum windspeed holiday month_sin month_cos week_sin week_cos day_of_week_sin day_of_week_cos hour_sin hour_cos date_time 2011-04-01 00:00:00 6.0 10.66 100.0 11.0014 0.0 0.866025 -0.5 1.0 6.123234e-17 -0.433884 -0.900969 0.000000 1.000000 2011-04-01 01:00:00 4.0 10.66 100.0 11.0014 0.0 0.866025 -0.5 1.0 6.123234e-17 -0.433884 -0.900969 0.258819 0.965926 2011-04-01 02:00:00 7.0 10.66 93.0 12.9980 0.0 0.866025 -0.5 1.0 6.123234e-17 -0.433884 -0.900969 0.500000 0.866025 In\u00a0[4]: Copied! <pre># Split train-validation-test\n# ==============================================================================\nend_train = '2012-06-30 23:59:00'\nend_validation = '2012-10-01 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_val   = data.loc[end_train:end_validation, :]\ndata_test  = data.loc[end_validation:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-validation-test # ============================================================================== end_train = '2012-06-30 23:59:00' end_validation = '2012-10-01 23:59:00' data_train = data.loc[: end_train, :] data_val   = data.loc[end_train:end_validation, :] data_test  = data.loc[end_validation:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 2011-04-01 00:00:00 --- 2012-06-30 23:00:00  (n=10968)\nDates validation : 2012-07-01 00:00:00 --- 2012-10-01 23:00:00  (n=2232)\nDates test       : 2012-10-02 00:00:00 --- 2012-10-20 23:00:00  (n=456)\n</pre> In\u00a0[5]: Copied! <pre># Plot partitions\n# ==============================================================================\nset_dark_theme()\nplt.rcParams['lines.linewidth'] = 0.5\nfig, ax = plt.subplots(figsize=(8, 3))\nax.plot(data_train['users'], label='Train')\nax.plot(data_val['users'], label='Validation')\nax.plot(data_test['users'], label='Test')\nax.set_title('users')\nax.legend();\n</pre> # Plot partitions # ============================================================================== set_dark_theme() plt.rcParams['lines.linewidth'] = 0.5 fig, ax = plt.subplots(figsize=(8, 3)) ax.plot(data_train['users'], label='Train') ax.plot(data_val['users'], label='Validation') ax.plot(data_test['users'], label='Test') ax.set_title('users') ax.legend(); <p> \u270e Note </p> <p>Hiperparametrs used in this example have been previusly optimized using a bayesian search process. For more information about this process, please refer to: Hyperparameter tuning and lags selection.</p> In\u00a0[6]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nparams = {\n    \"max_depth\": 7,\n    \"n_estimators\": 300,\n    \"learning_rate\": 0.06,\n    \"verbose\": -1,\n    \"random_state\": 15926\n}\nlags = [1, 2, 3, 23, 24, 25, 167, 168, 169]\nwindow_features = RollingFeatures(stats=[\"mean\"], window_sizes=24 * 3)\n\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(**params),\n                 lags            = lags,\n                 window_features = window_features,\n             )\n\nforecaster.fit(\n    y    = data.loc[:end_validation, 'users'],\n    exog = data.loc[:end_validation, exog_features],\n    store_in_sample_residuals = False\n)\n</pre> # Create and fit forecaster # ============================================================================== params = {     \"max_depth\": 7,     \"n_estimators\": 300,     \"learning_rate\": 0.06,     \"verbose\": -1,     \"random_state\": 15926 } lags = [1, 2, 3, 23, 24, 25, 167, 168, 169] window_features = RollingFeatures(stats=[\"mean\"], window_sizes=24 * 3)  forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(**params),                  lags            = lags,                  window_features = window_features,              )  forecaster.fit(     y    = data.loc[:end_validation, 'users'],     exog = data.loc[:end_validation, exog_features],     store_in_sample_residuals = False ) <p>By default, <code>store_in_sample_residuals</code> is set to <code>False</code> to speed up the training process. To store in-sample residuals without retraining the model, you can call the <code>set_in_sample_residuals()</code> method and pass the training data or set <code>store_in_sample_residuals</code> to <code>True</code> when fitting the model.</p> In\u00a0[7]: Copied! <pre># Store in-sample residuals in case store_in_sample_residuals = False\n# ==============================================================================\nforecaster.set_in_sample_residuals(\n    y    = data.loc[:end_validation, 'users'],\n    exog = data.loc[:end_validation, exog_features]\n)\n</pre> # Store in-sample residuals in case store_in_sample_residuals = False # ============================================================================== forecaster.set_in_sample_residuals(     y    = data.loc[:end_validation, 'users'],     exog = data.loc[:end_validation, exog_features] ) In\u00a0[8]: Copied! <pre># In-sample residuals stored during fit\n# ==============================================================================\nprint(\"Amount of residuals stored:\", len(forecaster.in_sample_residuals_))\nforecaster.in_sample_residuals_\n</pre> # In-sample residuals stored during fit # ============================================================================== print(\"Amount of residuals stored:\", len(forecaster.in_sample_residuals_)) forecaster.in_sample_residuals_ <pre>Amount of residuals stored: 10000\n</pre> Out[8]: <pre>array([ 42.02327339,  -4.75342728, -39.26777553, ...,  -3.54886809,\n       -41.20842177, -13.42207696])</pre> <p>The <code>backtesting_forecaster()</code> function is used to estimate the prediction intervals for the entire test set. Few arguments are required to use this function:</p> <ul> <li><p><code>use_in_sample_residuals</code>: If <code>True</code>, the in-sample residuals are used to compute the prediction intervals. Since these residuals are obtained from the training set, they are always available, but usually lead to overoptimistic intervals. If <code>False</code>, the out-sample residuals are used to calculate the prediction intervals. These residuals are obtained from the validation set and are only available if the <code>set_out_sample_residuals()</code> method has been called. It is recommended to use out-sample residuals to achieve the desired coverage.</p> </li> <li><p>The <code>interval</code> argument indicates the quantiles used to calculate the prediction intervals. For example, if the 10th and 90th percentiles are used, the resulting prediction intervals will have a nominal coverage of 80%.</p> </li> <li><p><code>interval_method</code> is used to specify the method used to calculate the prediction intervals. In this case, the <code>bootstrapped</code> method is used but other methods are available.</p> </li> <li><p>The <code>n_boot</code> argument is used to specify the number of bootstrap samples to be used in estimating the prediction intervals. The larger the number of samples, the more accurate the prediction intervals will be, but the longer the calculation will take.</p> </li> </ul> In\u00a0[9]: Copied! <pre># Backtesting with prediction intervals in test data using in-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24, \n         initial_train_size = len(data.loc[:end_validation]),\n         refit              = False\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster              = forecaster,\n                          y                       = data['users'],\n                          exog                    = data[exog_features],\n                          cv                      = cv,\n                          metric                  = 'mean_absolute_error',\n                          interval                = [10, 90],  # 80% prediction interval\n                          interval_method         = 'bootstrapping',\n                          n_boot                  = 150,\n                          use_in_sample_residuals = True,  # Use in-sample residuals\n                          use_binned_residuals    = False\n                      )\npredictions.head(5)\n</pre> # Backtesting with prediction intervals in test data using in-sample residuals # ============================================================================== cv = TimeSeriesFold(          steps              = 24,           initial_train_size = len(data.loc[:end_validation]),          refit              = False      )  metric, predictions = backtesting_forecaster(                           forecaster              = forecaster,                           y                       = data['users'],                           exog                    = data[exog_features],                           cv                      = cv,                           metric                  = 'mean_absolute_error',                           interval                = [10, 90],  # 80% prediction interval                           interval_method         = 'bootstrapping',                           n_boot                  = 150,                           use_in_sample_residuals = True,  # Use in-sample residuals                           use_binned_residuals    = False                       ) predictions.head(5) <pre>  0%|          | 0/19 [00:00&lt;?, ?it/s]</pre> Out[9]: pred lower_bound upper_bound 2012-10-02 00:00:00 58.387527 32.799491 78.248425 2012-10-02 01:00:00 17.870302 -7.253084 50.190172 2012-10-02 02:00:00 7.901576 -17.057776 37.596425 2012-10-02 03:00:00 5.414332 -16.809510 40.194483 2012-10-02 04:00:00 10.379630 -12.696466 56.904041 In\u00a0[10]: Copied! <pre># Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_test,\n    target_variable     = \"users\",\n    title               = \"Prediction intervals in test data\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}\n)\n\n# Predicted interval coverage (on test data)\n# ==============================================================================\ncoverage = calculate_coverage(\n               y_true      = data.loc[end_validation:, 'users'],\n               lower_bound = predictions[\"lower_bound\"], \n               upper_bound = predictions[\"upper_bound\"]\n           )\nprint(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")\n\n# Area of the interval\n# ==============================================================================\narea = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum()\nprint(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions         = predictions,     y_true              = data_test,     target_variable     = \"users\",     title               = \"Prediction intervals in test data\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1} )  # Predicted interval coverage (on test data) # ============================================================================== coverage = calculate_coverage(                y_true      = data.loc[end_validation:, 'users'],                lower_bound = predictions[\"lower_bound\"],                 upper_bound = predictions[\"upper_bound\"]            ) print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")  # Area of the interval # ============================================================================== area = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum() print(f\"Area of the interval: {round(area, 2)}\") <pre>Predicted interval coverage: 61.84 %\nArea of the interval: 43177.89\n</pre> <p>The prediction intervals show overconfidence as they tend to be too narrow, resulting in a true coverage well below the nominal coverage of 80%. This phenomenon is due to the tendency of the in-sample residuals to often overestimate the predictive ability of the model.</p> In\u00a0[11]: Copied! <pre># Store results for later comparison\n# ==============================================================================\npredictions_in_sample_residuals = predictions.copy()\n</pre> # Store results for later comparison # ============================================================================== predictions_in_sample_residuals = predictions.copy() In\u00a0[12]: Copied! <pre># Backtesting on validation data to obtain out-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24, \n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False\n     )\n\n_, predictions_val = backtesting_forecaster(\n                         forecaster    = forecaster,\n                         y             = data.loc[:end_validation, 'users'],\n                         exog          = data.loc[:end_validation, exog_features],\n                         cv            = cv,\n                         metric        = 'mean_absolute_error'\n                     )\n</pre> # Backtesting on validation data to obtain out-sample residuals # ============================================================================== cv = TimeSeriesFold(          steps              = 24,           initial_train_size = len(data.loc[:end_train]),          refit              = False      )  _, predictions_val = backtesting_forecaster(                          forecaster    = forecaster,                          y             = data.loc[:end_validation, 'users'],                          exog          = data.loc[:end_validation, exog_features],                          cv            = cv,                          metric        = 'mean_absolute_error'                      ) <pre>  0%|          | 0/93 [00:00&lt;?, ?it/s]</pre> In\u00a0[13]: Copied! <pre># Out-sample residuals distribution\n# ==============================================================================\nresiduals = data.loc[predictions_val.index, 'users'] - predictions_val['pred']\nprint(pd.Series(np.where(residuals &lt; 0, 'negative', 'positive')).value_counts())\nplt.rcParams.update({'font.size': 8})\n_ = plot_residuals(residuals=residuals, figsize=(7, 4))\n</pre> # Out-sample residuals distribution # ============================================================================== residuals = data.loc[predictions_val.index, 'users'] - predictions_val['pred'] print(pd.Series(np.where(residuals &lt; 0, 'negative', 'positive')).value_counts()) plt.rcParams.update({'font.size': 8}) _ = plot_residuals(residuals=residuals, figsize=(7, 4)) <pre>positive    1297\nnegative     935\nName: count, dtype: int64\n</pre> <p>With the <code>set_out_sample_residuals()</code> method, the out-sample residuals are stored in the forecaster object so that they can be used to calibrate the prediction intervals.</p> In\u00a0[14]: Copied! <pre># Store out-sample residuals in the forecaster\n# ==============================================================================\nforecaster.set_out_sample_residuals(\n    y_true = data.loc[predictions_val.index, 'users'], \n    y_pred = predictions_val['pred']\n)\n</pre> # Store out-sample residuals in the forecaster # ============================================================================== forecaster.set_out_sample_residuals(     y_true = data.loc[predictions_val.index, 'users'],      y_pred = predictions_val['pred'] ) <p>Now that the new residuals have been added to the forecaster, the prediction intervals can be calculated using <code>use_in_sample_residuals = False</code>.</p> In\u00a0[15]: Copied! <pre># Backtesting with prediction intervals in test data using out-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24, \n         initial_train_size = len(data.loc[:end_validation]),\n         refit              = False\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster              = forecaster,\n                          y                       = data['users'],\n                          exog                    = data[exog_features],\n                          cv                      = cv,\n                          metric                  = 'mean_absolute_error',\n                          interval                = [10, 90],  # 80% prediction interval\n                          interval_method         = 'bootstrapping',\n                          n_boot                  = 150,\n                          use_in_sample_residuals = False,  # Use out-sample residuals\n                          use_binned_residuals    = False\n                      )\npredictions.head(5)\n</pre> # Backtesting with prediction intervals in test data using out-sample residuals # ============================================================================== cv = TimeSeriesFold(          steps              = 24,           initial_train_size = len(data.loc[:end_validation]),          refit              = False      )  metric, predictions = backtesting_forecaster(                           forecaster              = forecaster,                           y                       = data['users'],                           exog                    = data[exog_features],                           cv                      = cv,                           metric                  = 'mean_absolute_error',                           interval                = [10, 90],  # 80% prediction interval                           interval_method         = 'bootstrapping',                           n_boot                  = 150,                           use_in_sample_residuals = False,  # Use out-sample residuals                           use_binned_residuals    = False                       ) predictions.head(5) <pre>  0%|          | 0/19 [00:00&lt;?, ?it/s]</pre> Out[15]: pred lower_bound upper_bound 2012-10-02 00:00:00 58.387527 30.961801 133.296285 2012-10-02 01:00:00 17.870302 -14.031592 132.635745 2012-10-02 02:00:00 7.901576 -32.265023 142.360525 2012-10-02 03:00:00 5.414332 -32.495340 167.312462 2012-10-02 04:00:00 10.379630 -13.725526 252.174056 In\u00a0[16]: Copied! <pre># Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_test,\n    target_variable     = \"users\",\n    title               = \"Prediction intervals in test data\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}\n)\n\n# Predicted interval coverage (on test data)\n# ==============================================================================\ncoverage = calculate_coverage(\n               y_true      = data.loc[end_validation:, 'users'],\n               lower_bound = predictions[\"lower_bound\"], \n               upper_bound = predictions[\"upper_bound\"]\n           )\nprint(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")\n\n# Area of the interval\n# ==============================================================================\narea = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum()\nprint(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions         = predictions,     y_true              = data_test,     target_variable     = \"users\",     title               = \"Prediction intervals in test data\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1} )  # Predicted interval coverage (on test data) # ============================================================================== coverage = calculate_coverage(                y_true      = data.loc[end_validation:, 'users'],                lower_bound = predictions[\"lower_bound\"],                 upper_bound = predictions[\"upper_bound\"]            ) print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")  # Area of the interval # ============================================================================== area = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum() print(f\"Area of the interval: {round(area, 2)}\") <pre>Predicted interval coverage: 83.33 %\nArea of the interval: 101257.22\n</pre> <p>The resulting prediction intervals derived from the out-sample residuals are wider than those generated using the in-sample residuals. This results in an empirical coverage that is closer to the nominal coverage of 80%.</p> <p>By zooming in on the plot, the reader will notice that the intervals are particularly wide when the predicted values are low, indicating that the model is not able to properly localize the uncertainty of its predictions.</p> In\u00a0[17]: Copied! <pre># Plot intervals zoomed in [\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"]\n# ==============================================================================\nplot_prediction_intervals(\n    predictions     = predictions,\n    y_true          = data_test,\n    initial_x_zoom  = [\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"],\n    target_variable = \"users\",\n    title           = \"Prediction intervals in test data\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}\n)\n</pre> # Plot intervals zoomed in [\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"] # ============================================================================== plot_prediction_intervals(     predictions     = predictions,     y_true          = data_test,     initial_x_zoom  = [\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"],     target_variable = \"users\",     title           = \"Prediction intervals in test data\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1} ) In\u00a0[18]: Copied! <pre># Store results for later comparison\n# ==============================================================================\npredictions_out_sample_residuals = predictions.copy()\n</pre> # Store results for later comparison # ============================================================================== predictions_out_sample_residuals = predictions.copy() In\u00a0[19]: Copied! <pre># Create and train forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor     = LGBMRegressor(**params),\n                 lags          = lags,\n                 binner_kwargs = {'n_bins': 10}   \n             )\n\nforecaster.fit(\n    y    = data.loc[:end_validation, 'users'],\n    exog = data.loc[:end_validation, exog_features],\n    store_in_sample_residuals = True\n)\n</pre> # Create and train forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor     = LGBMRegressor(**params),                  lags          = lags,                  binner_kwargs = {'n_bins': 10}                 )  forecaster.fit(     y    = data.loc[:end_validation, 'users'],     exog = data.loc[:end_validation, exog_features],     store_in_sample_residuals = True ) <p>During the training process, the forecaster uses the in-sample predictions to define the intervals at which the residuals are stored depending on the predicted value to which they are related (<code>binner_intervals_</code> attribute).</p> <p>Although not used in this example, the in-sample residuals are divided into bins and stored in the <code>in_sample_residuals_by_bin_</code> attribute.</p> In\u00a0[20]: Copied! <pre># Intervals of the residual bins\n# ==============================================================================\npprint(forecaster.binner_intervals_)\n</pre> # Intervals of the residual bins # ============================================================================== pprint(forecaster.binner_intervals_) <pre>{0: (-0.8943794883271247, 10.608406827878845),\n 1: (10.608406827878845, 30.676160623625577),\n 2: (30.676160623625577, 74.1882217425986),\n 3: (74.1882217425986, 120.49652689276296),\n 4: (120.49652689276296, 164.97690978552197),\n 5: (164.97690978552197, 209.69596300954962),\n 6: (209.69596300954962, 268.3808557231679),\n 7: (268.3808557231679, 338.3331511270013),\n 8: (338.3331511270013, 460.65535040899925),\n 9: (460.65535040899925, 955.802117392104)}\n</pre> In\u00a0[21]: Copied! <pre># Number of in-sample residuals by bin\n# ==============================================================================\nfor k, v in forecaster.in_sample_residuals_by_bin_.items():\n    print(f\"Bin {k}: n={len(v)}\")\n</pre> # Number of in-sample residuals by bin # ============================================================================== for k, v in forecaster.in_sample_residuals_by_bin_.items():     print(f\"Bin {k}: n={len(v)}\") <pre>Bin 0: n=1000\nBin 1: n=1000\nBin 2: n=1000\nBin 3: n=1000\nBin 4: n=1000\nBin 5: n=1000\nBin 6: n=1000\nBin 7: n=1000\nBin 8: n=1000\nBin 9: n=1000\n</pre> <p>The <code>set_out_sample_residuals()</code> method will bin the residuals according to the intervals learned during fitting. To avoid using too much memory, the number of residuals stored per bin is limited to <code>10_000 // self.binner.n_bins_</code>. The predictions obtained in the backtesting are used.</p> In\u00a0[22]: Copied! <pre># Store out-sample residuals in the forecaster\n# ==============================================================================\nforecaster.set_out_sample_residuals(\n    y_true = data.loc[predictions_val.index, 'users'], \n    y_pred = predictions_val['pred']\n)\n</pre> # Store out-sample residuals in the forecaster # ============================================================================== forecaster.set_out_sample_residuals(     y_true = data.loc[predictions_val.index, 'users'],      y_pred = predictions_val['pred'] ) In\u00a0[23]: Copied! <pre># Number of out-sample residuals by bin\n# ==============================================================================\nfor k, v in forecaster.out_sample_residuals_by_bin_.items():\n    print(f\"Bin {k}: n={len(v)}\")\n</pre> # Number of out-sample residuals by bin # ============================================================================== for k, v in forecaster.out_sample_residuals_by_bin_.items():     print(f\"Bin {k}: n={len(v)}\") <pre>Bin 0: n=153\nBin 1: n=168\nBin 2: n=197\nBin 3: n=118\nBin 4: n=153\nBin 5: n=157\nBin 6: n=232\nBin 7: n=302\nBin 8: n=318\nBin 9: n=434\n</pre> In\u00a0[24]: Copied! <pre># Distribution of the residual by bin\n# ==============================================================================\nout_sample_residuals_by_bin_df = pd.DataFrame(\n    dict(\n        [(k, pd.Series(v))\n         for k, v in forecaster.out_sample_residuals_by_bin_.items()]\n    )\n)\nfig, ax = plt.subplots(figsize=(6, 3))\nout_sample_residuals_by_bin_df.boxplot(\n    flierprops=dict(marker='o', markerfacecolor='gray', markersize=6),\n    ax=ax\n)\nax.set_title(\"Distribution of residuals by bin\", fontsize=12)\nax.set_xlabel(\"Bin\", fontsize=10)\nax.set_ylabel(\"Residuals\", fontsize=10)\nplt.show();\n</pre> # Distribution of the residual by bin # ============================================================================== out_sample_residuals_by_bin_df = pd.DataFrame(     dict(         [(k, pd.Series(v))          for k, v in forecaster.out_sample_residuals_by_bin_.items()]     ) ) fig, ax = plt.subplots(figsize=(6, 3)) out_sample_residuals_by_bin_df.boxplot(     flierprops=dict(marker='o', markerfacecolor='gray', markersize=6),     ax=ax ) ax.set_title(\"Distribution of residuals by bin\", fontsize=12) ax.set_xlabel(\"Bin\", fontsize=10) ax.set_ylabel(\"Residuals\", fontsize=10) plt.show(); <p>The box plots show how the spread and magnitude of the residuals differ depending on the predicted value. The residuals are more dispersed when the predicted value is higher (higher bin), which is consistent with the intuition that errors tend to be larger when the predicted value is larger.</p> <p>Finally, the prediction intervals estimated again, this time using out-sample residuals conditioned on the predicted values.</p> In\u00a0[25]: Copied! <pre># Backtesting with prediction intervals in test data using out-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24, \n         initial_train_size = len(data.loc[:end_validation]),\n         refit              = False\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster              = forecaster,\n                          y                       = data['users'],\n                          exog                    = data[exog_features],\n                          cv                      = cv,\n                          metric                  = 'mean_absolute_error',\n                          interval                = [10, 90],  # 80% prediction interval\n                          interval_method         = 'bootstrapping',\n                          n_boot                  = 150,\n                          use_in_sample_residuals = False,  # Use out-sample residuals\n                          use_binned_residuals    = True,   # Residuals conditioned on predicted values\n                      )\npredictions.head(3)\n</pre> # Backtesting with prediction intervals in test data using out-sample residuals # ============================================================================== cv = TimeSeriesFold(          steps              = 24,           initial_train_size = len(data.loc[:end_validation]),          refit              = False      )  metric, predictions = backtesting_forecaster(                           forecaster              = forecaster,                           y                       = data['users'],                           exog                    = data[exog_features],                           cv                      = cv,                           metric                  = 'mean_absolute_error',                           interval                = [10, 90],  # 80% prediction interval                           interval_method         = 'bootstrapping',                           n_boot                  = 150,                           use_in_sample_residuals = False,  # Use out-sample residuals                           use_binned_residuals    = True,   # Residuals conditioned on predicted values                       ) predictions.head(3) <pre>  0%|          | 0/19 [00:00&lt;?, ?it/s]</pre> Out[25]: pred lower_bound upper_bound 2012-10-02 00:00:00 60.399647 40.609922 75.876519 2012-10-02 01:00:00 17.617464 5.186247 38.272262 2012-10-02 02:00:00 9.002365 4.862337 29.131771 In\u00a0[26]: Copied! <pre># Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_test,\n    target_variable     = \"users\",\n    title               = \"Prediction intervals in test data\",\n    xaxis_title         = \"Date time\",\n    yaxis_title         = \"users\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}\n)\n\n# Predicted interval coverage (on test data)\n# ==============================================================================\ncoverage = calculate_coverage(\n               y_true      = data.loc[end_validation:, 'users'],\n               lower_bound = predictions[\"lower_bound\"], \n               upper_bound = predictions[\"upper_bound\"]\n           )\nprint(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")\n\n# Area of the interval\n# ==============================================================================\narea = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum()\nprint(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions         = predictions,     y_true              = data_test,     target_variable     = \"users\",     title               = \"Prediction intervals in test data\",     xaxis_title         = \"Date time\",     yaxis_title         = \"users\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1} )  # Predicted interval coverage (on test data) # ============================================================================== coverage = calculate_coverage(                y_true      = data.loc[end_validation:, 'users'],                lower_bound = predictions[\"lower_bound\"],                 upper_bound = predictions[\"upper_bound\"]            ) print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")  # Area of the interval # ============================================================================== area = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum() print(f\"Area of the interval: {round(area, 2)}\") <pre>Predicted interval coverage: 85.96 %\nArea of the interval: 96214.57\n</pre> In\u00a0[27]: Copied! <pre># Plot intervals zoomed in [\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"]\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_test,\n    initial_x_zoom      = [\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"],\n    target_variable     = \"users\",\n    title               = \"Prediction intervals in test data\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}\n)\n</pre> # Plot intervals zoomed in [\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"] # ============================================================================== plot_prediction_intervals(     predictions         = predictions,     y_true              = data_test,     initial_x_zoom      = [\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"],     target_variable     = \"users\",     title               = \"Prediction intervals in test data\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1} ) <p>When using out-sample residuals conditioned on the predicted value, the area of the interval is significantly reduced and the uncertainty is mainly allocated to the predictions with high values. However, the empirical coverage is still above the expected coverage, which means that the estimated intervals are conservative.</p> <p>The follwing plot compares the prediction intervals obtained using in-sample residuals, out-sample residuals, and out-sample residuals conditioned on the predicted values.</p> In\u00a0[28]: Copied! <pre># Plot intervals using: in-sample residuals, out-sample residuals and binned residuals\n# ==============================================================================\npredictions_out_sample_residuals_binned = predictions.copy()\nfig, ax = plt.subplots(figsize=(8, 4))\nax.fill_between(\n    data_test.index,\n    predictions_out_sample_residuals[\"lower_bound\"],\n    predictions_out_sample_residuals[\"upper_bound\"],\n    color='white',\n    alpha=0.9,\n    label='Out-sample residuals',\n    zorder=1\n)\nax.fill_between(\n    data_test.index,\n    predictions_out_sample_residuals_binned[\"lower_bound\"],\n    predictions_out_sample_residuals_binned[\"upper_bound\"],\n    color='#fc4f30',\n    alpha=0.7,\n    label='Out-sample binned residuals',\n    zorder=2\n)\n\nax.fill_between(\n    data_test.index,\n    predictions_in_sample_residuals[\"lower_bound\"],\n    predictions_in_sample_residuals[\"upper_bound\"],\n    color='#30a2da',\n    alpha=0.9,\n    label='In-sample residuals',\n    zorder=3\n)\nax.set_xlim(pd.to_datetime([\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"]))\nax.set_title(\"Prediction intervals with different residuals\", fontsize=12)\nax.legend();\n</pre> # Plot intervals using: in-sample residuals, out-sample residuals and binned residuals # ============================================================================== predictions_out_sample_residuals_binned = predictions.copy() fig, ax = plt.subplots(figsize=(8, 4)) ax.fill_between(     data_test.index,     predictions_out_sample_residuals[\"lower_bound\"],     predictions_out_sample_residuals[\"upper_bound\"],     color='white',     alpha=0.9,     label='Out-sample residuals',     zorder=1 ) ax.fill_between(     data_test.index,     predictions_out_sample_residuals_binned[\"lower_bound\"],     predictions_out_sample_residuals_binned[\"upper_bound\"],     color='#fc4f30',     alpha=0.7,     label='Out-sample binned residuals',     zorder=2 )  ax.fill_between(     data_test.index,     predictions_in_sample_residuals[\"lower_bound\"],     predictions_in_sample_residuals[\"upper_bound\"],     color='#30a2da',     alpha=0.9,     label='In-sample residuals',     zorder=3 ) ax.set_xlim(pd.to_datetime([\"2012-10-08 00:00:00\", \"2012-10-15 00:00:00\"])) ax.set_title(\"Prediction intervals with different residuals\", fontsize=12) ax.legend(); <p> \u26a0 Warning </p> <p> Probabilistic forecasting in production <p>The correct estimation of prediction intervals with bootstrapped residuals depends on the residuals being representative of future errors. For this reason, out-of-sample residuals should be used. However, the dynamics of the series and models can change over time, so it is important to monitor and regularly update the residuals. It can be done easily using the <code>set_out_sample_residuals()</code> method.</p> </p> <p>The <code>backtesting_forecaster</code> function not only allows to estimate a single prediction interval, but also to estimate multiple quantiles (percentiles) from which multiple prediction intervals can be constructed. This is useful to evaluate the quality of the prediction intervals for a range of probabilities. Furthermore, it has almost no additional computational cost compared to estimating a single interval.</p> <p>Next, several percentiles are predicted and from these, prediction intervals are created for different nominal coverage levels - 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90% and 95% - and their actual coverage is assessed.</p> In\u00a0[29]: Copied! <pre># Prediction intervals for different nominal coverages\n# ==============================================================================\nquantiles = [2.5, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 97.5]\nintervals = [[2.5, 97.5], [5, 95], [10, 90], [15, 85], [20, 80], [30, 70], [35, 65], [40, 60], [45, 55]]\nobserved_coverages = []\nobserved_areas = []\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster              = forecaster,\n                          y                       = data['users'],\n                          exog                    = data[exog_features],\n                          cv                      = cv,\n                          metric                  = 'mean_absolute_error',\n                          interval                = quantiles,\n                          interval_method         = 'bootstrapping',\n                          n_boot                  = 150,\n                          use_in_sample_residuals = False,  # Use out-sample residuals\n                          use_binned_residuals    = True\n                      )\npredictions.head()\n</pre> # Prediction intervals for different nominal coverages # ============================================================================== quantiles = [2.5, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 97.5] intervals = [[2.5, 97.5], [5, 95], [10, 90], [15, 85], [20, 80], [30, 70], [35, 65], [40, 60], [45, 55]] observed_coverages = [] observed_areas = []  metric, predictions = backtesting_forecaster(                           forecaster              = forecaster,                           y                       = data['users'],                           exog                    = data[exog_features],                           cv                      = cv,                           metric                  = 'mean_absolute_error',                           interval                = quantiles,                           interval_method         = 'bootstrapping',                           n_boot                  = 150,                           use_in_sample_residuals = False,  # Use out-sample residuals                           use_binned_residuals    = True                       ) predictions.head() <pre>  0%|          | 0/19 [00:00&lt;?, ?it/s]</pre> Out[29]: pred p_2.5 p_5 p_10 p_15 p_20 p_25 p_30 p_35 p_40 ... p_55 p_60 p_65 p_70 p_75 p_80 p_85 p_90 p_95 p_97.5 2012-10-02 00:00:00 60.399647 26.892402 33.951113 40.609922 42.760696 44.244359 46.570077 48.923822 49.785492 50.587951 ... 55.370331 57.151158 58.458595 59.347441 61.621755 65.284405 69.355807 75.876519 86.239833 120.265754 2012-10-02 01:00:00 17.617464 2.689591 3.512207 5.186247 6.486508 8.169985 9.041067 9.761858 10.572733 11.359216 ... 17.526959 19.567121 20.340357 23.363116 25.280020 29.001507 33.544834 38.272262 51.626343 71.371264 2012-10-02 02:00:00 9.002365 1.437432 2.099155 4.862337 5.415269 6.460628 6.985618 7.847532 8.239015 8.931894 ... 11.559812 12.799507 13.812258 14.086847 15.113970 16.543545 20.604941 29.131771 35.799777 49.683165 2012-10-02 03:00:00 5.306644 -0.465970 1.157744 2.417394 3.083630 3.739066 4.018527 4.467039 5.003059 5.384454 ... 6.667717 7.496738 8.055827 8.567819 9.560928 10.495075 12.231564 14.223768 19.437481 23.503909 2012-10-02 04:00:00 9.439573 -0.401817 2.997039 4.808793 5.552889 6.464636 6.875786 8.056258 8.496770 9.126208 ... 10.863024 11.754925 12.132281 12.877582 13.524530 15.188979 18.552166 25.501337 30.655910 47.941695 <p>5 rows \u00d7 22 columns</p> In\u00a0[30]: Copied! <pre># Calculate coverage and area for each interval\n# ==============================================================================\nfor interval in intervals:\n    observed_coverage = calculate_coverage(\n                            y_true      = data.loc[end_validation:, 'users'],\n                            lower_bound = predictions[f\"p_{interval[0]}\"], \n                            upper_bound = predictions[f\"p_{interval[1]}\"]\n                        )\n    observed_area = (predictions[f\"p_{interval[1]}\"] - predictions[f\"p_{interval[0]}\"]).sum()\n    observed_coverages.append(100 * observed_coverage)\n    observed_areas.append(observed_area)\n\nresults = pd.DataFrame({\n              'Interval': intervals,\n              'Nominal coverage': [interval[1] - interval[0] for interval in intervals],\n              'Observed coverage': observed_coverages,\n              'Area': observed_areas\n          })\nresults.round(1)\n</pre> # Calculate coverage and area for each interval # ============================================================================== for interval in intervals:     observed_coverage = calculate_coverage(                             y_true      = data.loc[end_validation:, 'users'],                             lower_bound = predictions[f\"p_{interval[0]}\"],                              upper_bound = predictions[f\"p_{interval[1]}\"]                         )     observed_area = (predictions[f\"p_{interval[1]}\"] - predictions[f\"p_{interval[0]}\"]).sum()     observed_coverages.append(100 * observed_coverage)     observed_areas.append(observed_area)  results = pd.DataFrame({               'Interval': intervals,               'Nominal coverage': [interval[1] - interval[0] for interval in intervals],               'Observed coverage': observed_coverages,               'Area': observed_areas           }) results.round(1) Out[30]: Interval Nominal coverage Observed coverage Area 0 [2.5, 97.5] 95.0 96.5 164395.6 1 [5, 95] 90.0 93.6 131727.4 2 [10, 90] 80.0 86.0 96214.6 3 [15, 85] 70.0 80.3 74675.2 4 [20, 80] 60.0 72.4 58774.0 5 [30, 70] 40.0 50.0 34625.7 6 [35, 65] 30.0 37.7 25213.9 7 [40, 60] 20.0 26.8 16243.0 8 [45, 55] 10.0 14.3 8069.0 In\u00a0[31]: Copied! <pre># Plot coverage and area for different intervals\n# ==============================================================================\nfig, ax1 = plt.subplots(figsize=(8, 5))\n\n# Coverage plot\nax1.plot(results['Nominal coverage'], results['Observed coverage'], 'o-', label='Observed coverage')\nax1.plot([0, 100], [0, 100], '--', label='Ideal observed coverage', zorder=0, c=\"white\")\nax1.set_xlabel('Nominal coverage (%)')\nax1.set_ylabel('Observed coverage (%)')\nax1.set_xlim(0, 100)\nax1.set_ylim(0, 110)\n\n# Area plot\nax2 = ax1.twinx()\nax2.plot(results['Nominal coverage'], results['Area'], 's-', label='Interval area', c='C1')\nax2.set_ylabel('Interval area')\nax2.set_ylim(0, max(results['Area']) * 1.1)\n\n# Legend\nlines, labels = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines + lines2, labels + labels2, loc='upper left')\n\nplt.title('Coverage and Area of Prediction Intervals')\nplt.tight_layout()\nplt.show()\n</pre> # Plot coverage and area for different intervals # ============================================================================== fig, ax1 = plt.subplots(figsize=(8, 5))  # Coverage plot ax1.plot(results['Nominal coverage'], results['Observed coverage'], 'o-', label='Observed coverage') ax1.plot([0, 100], [0, 100], '--', label='Ideal observed coverage', zorder=0, c=\"white\") ax1.set_xlabel('Nominal coverage (%)') ax1.set_ylabel('Observed coverage (%)') ax1.set_xlim(0, 100) ax1.set_ylim(0, 110)  # Area plot ax2 = ax1.twinx() ax2.plot(results['Nominal coverage'], results['Area'], 's-', label='Interval area', c='C1') ax2.set_ylabel('Interval area') ax2.set_ylim(0, max(results['Area']) * 1.1)  # Legend lines, labels = ax1.get_legend_handles_labels() lines2, labels2 = ax2.get_legend_handles_labels() ax1.legend(lines + lines2, labels + labels2, loc='upper left')  plt.title('Coverage and Area of Prediction Intervals') plt.tight_layout() plt.show() <p>As the width of the prediction intervals increases, the area covered by the intervals grows at an accelerating rate. This plot helps to visualize this relationship, and is useful for selecting the most appropriate intervals for the specific problem, balancing the desired coverage with the acceptable width of the intervals.</p> <p>The previous sections have demonstrated the use of the backtesting process to estimate the prediction interval over a given period of time. The goal is to mimic the behavior of the model in production by running predictions at regular intervals, incrementally updating the input data.</p> <p>Alternatively, it is possible to run a single prediction that forecasts N steps ahead without going through the entire backtesting process. In such cases, skforecast provides four different methods: <code>predict_bootstrapping</code>, <code>predict_interval</code>, <code>predict_quantile</code> and <code>predict_distribution</code>.</p> <p>If the user needs to run a backtesting process using one of these prediction methods, the <code>interval</code> argument of <code>backtesting_forecaster()</code> can be specified as:</p> <ul> <li><p>If <code>list</code> or <code>tuple</code>: Sequence of percentiles to compute, each value must be between 0 and 100 inclusive. For example, a 95% confidence interval can be specified as <code>interval = [2.5, 97.5]</code> or multiple percentiles (e.g. 10, 50 and 90) as <code>interval = [10, 50, 90]</code>.</p> </li> <li><p>If <code>'bootstrapping'</code> (str): <code>n_boot</code> bootstrapping predictions will be generated.</p> </li> <li><p>If <code>scipy.stats distribution object</code>, the distribution parameters will be estimated for each prediction.</p> </li> </ul> <p> \ud83d\udca1 Tip </p> <p>All of these methods can be used either with in-sample or out-sample residuals using the <code>use_in_sample_residuals</code> argument, and with binned intervals conditioned on predicted values using the <code>use_binned_residuals</code> argument.</p> <p>Predict Bootstraping</p> <p>The <code>predict_bootstrapping</code> method performs the <code>n_boot</code> bootstrapping iterations that generate the alternative prediction paths. These are the underlying values used to compute the intervals, quantiles, and distributions.</p> In\u00a0[32]: Copied! <pre># Fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(**params),\n                 lags      = lags\n             )\nforecaster.fit(\n    y    = data.loc[:end_validation, 'users'],\n    exog = data.loc[:end_validation, exog_features],\n    store_in_sample_residuals = True\n)\n</pre> # Fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(**params),                  lags      = lags              ) forecaster.fit(     y    = data.loc[:end_validation, 'users'],     exog = data.loc[:end_validation, exog_features],     store_in_sample_residuals = True ) In\u00a0[33]: Copied! <pre># Predict 10 different forecasting sequences of 7 steps each using bootstrapping\n# ==============================================================================\nboot_predictions = forecaster.predict_bootstrapping(\n                       exog   = data_test[exog_features],\n                       steps  = 7,\n                       n_boot = 25\n                   )\nboot_predictions\n</pre> # Predict 10 different forecasting sequences of 7 steps each using bootstrapping # ============================================================================== boot_predictions = forecaster.predict_bootstrapping(                        exog   = data_test[exog_features],                        steps  = 7,                        n_boot = 25                    ) boot_predictions Out[33]: pred_boot_0 pred_boot_1 pred_boot_2 pred_boot_3 pred_boot_4 pred_boot_5 pred_boot_6 pred_boot_7 pred_boot_8 pred_boot_9 ... pred_boot_15 pred_boot_16 pred_boot_17 pred_boot_18 pred_boot_19 pred_boot_20 pred_boot_21 pred_boot_22 pred_boot_23 pred_boot_24 2012-10-02 00:00:00 88.879247 78.246636 49.984869 69.577690 51.915378 68.122753 102.705706 81.746083 53.560382 61.004076 ... 68.726941 52.789489 111.814563 60.801714 37.641149 52.365132 67.545112 56.850166 59.111774 72.659410 2012-10-02 01:00:00 57.657081 25.033177 31.180144 36.780890 19.035948 26.866682 45.984298 20.355401 5.630827 10.945272 ... 22.270334 21.766025 56.963715 48.210861 18.708850 15.789584 30.696750 19.456578 29.765112 4.228910 2012-10-02 02:00:00 12.887268 5.790686 29.702349 22.591337 11.045785 16.674854 18.753101 12.973743 3.777229 12.169120 ... 4.296488 4.401613 33.874989 15.179147 13.363857 5.768846 13.762247 7.355377 13.646284 5.185202 2012-10-02 03:00:00 1.325881 13.426438 5.424700 10.264177 2.517472 10.174256 3.838419 5.269279 3.745842 8.711563 ... 3.619119 7.233575 29.524688 4.340874 1.395143 3.450817 11.356415 1.752098 6.295690 9.212573 2012-10-02 04:00:00 5.937973 18.340258 7.026383 6.458254 9.293808 6.658706 5.311346 8.926221 19.061047 11.069167 ... 4.862457 9.439351 12.752172 8.151417 13.346762 9.430721 4.569697 10.416891 4.205799 12.035805 2012-10-02 05:00:00 45.659352 63.113540 22.249161 49.934912 44.016236 45.043004 32.716709 36.549307 45.088503 36.769994 ... 36.914090 44.082305 49.725989 15.250112 29.838267 69.812061 43.924128 66.655251 35.331056 60.768929 2012-10-02 06:00:00 176.606958 167.161502 120.994701 160.371600 197.391862 172.208403 140.714691 178.605939 154.379688 186.232511 ... 210.301702 156.894027 186.531477 104.184214 120.640401 172.695645 230.795897 170.612954 137.477798 188.347698 <p>7 rows \u00d7 25 columns</p> <p>A ridge plot is a useful way to visualize the uncertainty of a forecasting model. This plot estimates a kernel density for each step by using the bootstrapped predictions.</p> In\u00a0[34]: Copied! <pre># Ridge plot of bootstrapping predictions\n# ==============================================================================\n_ = plot_prediction_distribution(boot_predictions, figsize=(7, 4))\n</pre> # Ridge plot of bootstrapping predictions # ============================================================================== _ = plot_prediction_distribution(boot_predictions, figsize=(7, 4)) <p>Predict Interval</p> <p>In most cases, the user is interested in a specific interval rather than the entire bootstrapping simulation matrix. To address this need, skforecast provides the <code>predict_interval</code> method. This method internally uses <code>predict_bootstrapping</code> to obtain the bootstrapping matrix and estimates the upper and lower quantiles for each step, thus providing the user with the desired prediction intervals.</p> In\u00a0[35]: Copied! <pre># Predict intervals for next 7 steps, quantiles 10th and 90th\n# ==============================================================================\npredictions = forecaster.predict_interval(\n                  steps    = 7,\n                  exog     = data_test[exog_features],\n                  method   = 'bootstrapping',\n                  interval = [10, 90],\n                  n_boot   = 150\n              )\npredictions\n</pre> # Predict intervals for next 7 steps, quantiles 10th and 90th # ============================================================================== predictions = forecaster.predict_interval(                   steps    = 7,                   exog     = data_test[exog_features],                   method   = 'bootstrapping',                   interval = [10, 90],                   n_boot   = 150               ) predictions Out[35]: pred lower_bound upper_bound 2012-10-02 00:00:00 60.399647 42.568278 77.238528 2012-10-02 01:00:00 17.617464 6.978055 31.694925 2012-10-02 02:00:00 9.002365 3.421961 17.410508 2012-10-02 03:00:00 5.306644 1.326573 9.876715 2012-10-02 04:00:00 9.439573 4.638031 17.298454 2012-10-02 05:00:00 43.867127 29.845118 62.169581 2012-10-02 06:00:00 177.677903 131.161491 206.197206 <p> \ud83d\udca1 Tip </p> <p>The <code>predict_interval</code> method can also be used to estimate the conformal prediction intervals using the argument <code>method = 'conformal'</code>. For more information, check out the following user guide:</p> <p> Probabilistic Forecasting: Conformal Prediction </p> <p>Predict Quantile</p> <p>This method operates identically to <code>predict_interval</code>, with the added feature of enabling users to define a specific list of quantiles for estimation at each step. It's important to remember that these quantiles should be specified within the range of 0 to 1.</p> In\u00a0[36]: Copied! <pre># Predict quantiles for next 7 steps, quantiles 5th, 25th, 75th and 95th\n# ==============================================================================\npredictions = forecaster.predict_quantiles(\n                  steps     = 7,\n                  exog      = data_test[exog_features],\n                  quantiles = [0.05, 0.25, 0.75, 0.95],\n                  n_boot    = 150,\n              )\npredictions\n</pre> # Predict quantiles for next 7 steps, quantiles 5th, 25th, 75th and 95th # ============================================================================== predictions = forecaster.predict_quantiles(                   steps     = 7,                   exog      = data_test[exog_features],                   quantiles = [0.05, 0.25, 0.75, 0.95],                   n_boot    = 150,               ) predictions Out[36]: q_0.05 q_0.25 q_0.75 q_0.95 2012-10-02 00:00:00 36.652268 49.990237 67.536451 84.482214 2012-10-02 01:00:00 4.379827 12.421828 24.280527 38.289425 2012-10-02 02:00:00 2.445470 5.770482 11.869983 23.834125 2012-10-02 03:00:00 0.630186 3.124967 7.398633 13.196140 2012-10-02 04:00:00 3.665995 6.898361 11.656924 21.140225 2012-10-02 05:00:00 20.394163 37.323977 52.703749 69.742087 2012-10-02 06:00:00 108.153371 151.683616 195.516798 233.237341 <p>Predict Distribution</p> <p>The intervals estimated so far are distribution-free, which means that no assumptions are made about a particular distribution. The <code>predict_dist</code> method in skforecast allows fitting a parametric distribution to the bootstrapped prediction samples obtained with <code>predict_bootstrapping</code>. This is useful when there is reason to believe that the forecast errors follow a particular distribution, such as the normal distribution or the student's t-distribution. The <code>predict_dist</code> method allows the user to specify any continuous distribution from the scipy.stats module.</p> In\u00a0[37]: Copied! <pre># Predict the parameters of a normal distribution for the next 7 steps\n# ==============================================================================\npredictions = forecaster.predict_dist(\n                  steps        = 7,\n                  exog         = data_test[exog_features],\n                  distribution = norm,\n                  n_boot       = 150\n              )\npredictions\n</pre> # Predict the parameters of a normal distribution for the next 7 steps # ============================================================================== predictions = forecaster.predict_dist(                   steps        = 7,                   exog         = data_test[exog_features],                   distribution = norm,                   n_boot       = 150               ) predictions Out[37]: loc scale 2012-10-02 00:00:00 59.154606 14.563926 2012-10-02 01:00:00 19.109272 11.558214 2012-10-02 02:00:00 9.876689 7.157247 2012-10-02 03:00:00 5.630732 3.950459 2012-10-02 04:00:00 10.323647 5.963482 2012-10-02 05:00:00 45.587341 14.244366 2012-10-02 06:00:00 172.949056 37.702513 <p>All these types of predictions can be generated with a backtesting process. The <code>interval</code> argument in the <code>backtesting_forecaster</code> is the one that determines this output:</p> <ul> <li><p><code>interval = \"bootstrapping\"</code>: to generate the entire bootstrapping matrix.</p> </li> <li><p>List of two percentiles: to generate the prediction intervals. For example, <code>interval = [2.5, 97.5]</code>.</p> </li> <li><p>List of more than two percentiles: to generate the prediction for each quantile (percentile). For example, <code>interval = [10, 50, 90]</code>.</p> </li> <li><p>A scipy.stats distribution: to generate the distribution parameters. For example, <code>interval = scipy.stats.norm</code>.</p> </li> </ul>"},{"location":"user_guides/probabilistic-forecasting-bootstrapped-residuals.html#probabilistic-forecasting-bootstrapped-residuals","title":"Probabilistic Forecasting: Bootstrapped Residuals\u00b6","text":"<p>Forecasting intervals with bootstrapped residuals is a method used to estimate the uncertainty in predictions by resampling past prediction errors (residuals). The goal is to generate prediction intervals that capture the variability in the forecast, giving a range of possible future values instead of just a single point estimate.</p> <p>The error of a one-step-ahead forecast is defined as the difference between the actual value and the predicted value ($e_t = y_t - \\hat{y}_{t|t-1}$). By assuming that future errors will be similar to past errors, it is possible to simulate different predictions by taking samples from the collection of errors previously seen in the past (i.e., the residuals) and adding them to the predictions.</p> <p> Diagram bootstrapping prediction process. </p> <p>Repeatedly performing this process creates a collection of slightly different predictions, which represent the distribution of possible outcomes due to the expected variance in the forecasting process.</p> <p> Bootstrapping predictions. </p> <p>Using the outcome of the bootstrapping process, prediction intervals can be computed by calculating the $\u03b1/2$ and $1 \u2212 \u03b1/2$ percentiles at each forecasting horizon.</p> <p> </p> <p>Alternatively, it is also possible to fit a parametric distribution for each forecast horizon.</p> <p>One of the main advantages of this strategy is that it requires only a single model to estimate any interval. However, performing hundreds or thousands of bootstrapping iterations can be computationally expensive and may not always be feasible.</p>"},{"location":"user_guides/probabilistic-forecasting-bootstrapped-residuals.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-bootstrapped-residuals.html#intervals-with-in-sample-residuals","title":"Intervals with In-sample residuals\u00b6","text":"<p>By default, intervals can be computed using in-sample residuals (residuals from the training set), either by calling the <code>predict_interval()</code> method, or by performing a full backtesting procedure. However, this can result in intervals that are too narrow (overly optimistic).</p>"},{"location":"user_guides/probabilistic-forecasting-bootstrapped-residuals.html#out-sample-residuals-non-conditioned-on-predicted-values","title":"Out-sample residuals (non-conditioned on predicted values)\u00b6","text":"<p>To address the issue of overoptimistic intervals, it is possible to use out-sample residuals (residuals from a validation set not seen during training) to estimate the prediction intervals. These residuals can be obtained through backtesting.</p>"},{"location":"user_guides/probabilistic-forecasting-bootstrapped-residuals.html#intervals-conditioned-on-predicted-values-binned-residuals","title":"Intervals conditioned on predicted values (binned residuals)\u00b6","text":"<p>The bootstrapping process assumes that the residuals are independently distributed so that they can be used independently of the predicted value. In reality, this is rarely true; in most cases, the magnitude of the residuals is correlated with the magnitude of the predicted value. In this case, for example, one would hardly expect the error to be the same when the predicted number of users is close to zero as when it is in the hundreds.</p> <p>To account for the dependence between the residuals and the predicted values, skforecast allows to partition the residuals into K bins, where each bin is associated with a range of predicted values. Using this strategy, the bootstrapping process samples the residuals from different bins depending on the predicted value, which can improve the coverage of the interval while adjusting the width if necessary, allowing the model to better distribute the uncertainty of its predictions.</p> <p>Internally, skforecast uses a <code>QuantileBinner</code> class to bin data into quantile-based bins using <code>numpy.percentile</code>. This class is similar to KBinsDiscretizer but faster for binning data into quantile-based bins. Bin intervals are defined following the convention: bins[i-1] &lt;= x &lt; bins[i]. The binning process can be adjusted using the argument <code>binner_kwargs</code> of the Forecaster object.</p>"},{"location":"user_guides/probabilistic-forecasting-bootstrapped-residuals.html#prediction-of-multiple-intervals","title":"Prediction of multiple intervals\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-bootstrapped-residuals.html#predict-boostrap-interval-quantile-and-distribution","title":"Predict boostrap, interval, quantile and distribution\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-conformal-calibration.html","title":"Conformal calibration","text":"<p>Conformal prediction is a framework for constructing prediction intervals that are guaranteed to contain the true value with a specified probability (coverage probability). In addition to generating prediction intervals from point forecasts, conformal methods can also calibrate intervals produced by other techniques, such as quantile regression or bootstrapped residuals. In such cases, the conformal method adjusts the intervals\u2014either expanding or shrinking them\u2014to ensure they achieve the desired coverage.</p> <p>          Conformal calibration of prediction intervals. Source:                                Introduction To Conformal Prediction With Python: A Short Guide For Quantifying Uncertainty Of Machine Learning Models                        by Christoph Molnar          </p> <p>Skforecast provides this functionality through the <code>ConformalIntervalCalibrator</code> transformer that can be used for single series forecasting models as well as global forecasting models.</p> <p> \ud83d\udca1 Tip </p> <p>For more examples on how to use probabilistic forecasting, check out the following articles:</p> <ul> <li>              Probabilistic forecasting with machine learning          </li> <li>              Probabilistic forecasting: prediction intervals for multi-step time series forecasting          </li> <li>              Continuous Ranked Probability Score (CRPS) in probabilistic forecasting          </li> </ul> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme, plot_prediction_intervals\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom skforecast.recursive import ForecasterRecursive, ForecasterRecursiveMultiSeries\nfrom skforecast.preprocessing import ConformalIntervalCalibrator\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster, backtesting_forecaster_multiseries\nfrom skforecast.metrics import calculate_coverage\n\n# Configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('once')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd from skforecast.datasets import fetch_dataset  # Plots # ============================================================================== import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme, plot_prediction_intervals  # Modelling and Forecasting # ============================================================================== from lightgbm import LGBMRegressor from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from skforecast.recursive import ForecasterRecursive, ForecasterRecursiveMultiSeries from skforecast.preprocessing import ConformalIntervalCalibrator from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster, backtesting_forecaster_multiseries from skforecast.metrics import calculate_coverage  # Configuration # ============================================================================== import warnings warnings.filterwarnings('once') <p>To understand how calibration works, a interval with a coverage of 67% is simulated and then calibrated to a coverage of 80%.</p> In\u00a0[2]: Copied! <pre># Simulation of interval with coverage of 67%\n# ==============================================================================\nrng = np.random.default_rng(42)\ninterval = pd.DataFrame({\n        'lower_bound': np.sin(np.linspace(0, 4 * np.pi, 100)),\n        'upper_bound': np.sin(np.linspace(0, 4 * np.pi, 100)) + 5\n    },\n    index=pd.date_range(start='2024-01-01', periods=100, freq='D')\n)\ny_true = (interval['lower_bound'] + interval['upper_bound']) / 2 + rng.normal(0, 0.5, 100)\ny_true.name = \"series_1\"\ny_true.iloc[1::5] = interval.iloc[1::5, 0] - rng.normal(1, 1, 20)\ny_true.iloc[3::5] = interval.iloc[1::5, 1] + rng.normal(1, 1, 20)\n\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ninterval.plot(ax=ax, linestyle=\"--\")\ny_true.plot(ax=ax, label='True values')\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.legend(loc=\"upper right\", fontsize=8, ncol=3)\nplt.show()\n\ncoverage = calculate_coverage(\n               y_true      = y_true,\n               lower_bound = interval[\"lower_bound\"],\n               upper_bound = interval[\"upper_bound\"]\n           )\nprint(f'Coverage: {coverage:.2f}')\n</pre> # Simulation of interval with coverage of 67% # ============================================================================== rng = np.random.default_rng(42) interval = pd.DataFrame({         'lower_bound': np.sin(np.linspace(0, 4 * np.pi, 100)),         'upper_bound': np.sin(np.linspace(0, 4 * np.pi, 100)) + 5     },     index=pd.date_range(start='2024-01-01', periods=100, freq='D') ) y_true = (interval['lower_bound'] + interval['upper_bound']) / 2 + rng.normal(0, 0.5, 100) y_true.name = \"series_1\" y_true.iloc[1::5] = interval.iloc[1::5, 0] - rng.normal(1, 1, 20) y_true.iloc[3::5] = interval.iloc[1::5, 1] + rng.normal(1, 1, 20)  set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) interval.plot(ax=ax, linestyle=\"--\") y_true.plot(ax=ax, label='True values') ax.set_yticklabels([]) ax.set_xticklabels([]) ax.legend(loc=\"upper right\", fontsize=8, ncol=3) plt.show()  coverage = calculate_coverage(                y_true      = y_true,                lower_bound = interval[\"lower_bound\"],                upper_bound = interval[\"upper_bound\"]            ) print(f'Coverage: {coverage:.2f}') <pre>Coverage: 0.67\n</pre> <p>The interval has a coverage of 67%, which means that the true values are within the interval 67% of the time. Next, the <code>ConformalIntervalCalibrator</code> transformer is used to calibrate the prediction interval to ensure that it has a coverage of 80%.</p> In\u00a0[3]: Copied! <pre># Create and fit ConformalIntervalCalibrator\n# ==============================================================================\ncalibrator = ConformalIntervalCalibrator(nominal_coverage=0.8)\ncalibrator.fit(y_true=y_true, y_pred_interval=interval)\ncalibrator\n</pre> # Create and fit ConformalIntervalCalibrator # ============================================================================== calibrator = ConformalIntervalCalibrator(nominal_coverage=0.8) calibrator.fit(y_true=y_true, y_pred_interval=interval) calibrator Out[3]: ConformalIntervalCalibrator General Information <ul> <li>Nominal coverage: 0.8</li> <li>Coverage in fit data: {'series_1': 0.67}</li> <li>Symmetric interval: True</li> <li>Symmetric correction factor: {'series_1': 0.9488126715432241}</li> <li>Asymmetric correction factor lower: {'series_1': 0.7640361794875151}</li> <li>Asymmetric correction factor upper: {'series_1': 1.0384612609432264}</li> <li>Fitted series: ['series_1']</li> </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>The correction factor of 0.948 means that the interval needs to be expanded by 0.948 units in each direction to achieve the desired coverage of 80%.</p> In\u00a0[4]: Copied! <pre># Calibrate interval\n# ==============================================================================\ninterval_calibrated = calibrator.transform(interval)\n\nfig, ax = plt.subplots(figsize=(7, 3.5))\ninterval.plot(ax=ax, linestyle=\"--\")\ninterval_calibrated[\"lower_bound\"].plot(ax=ax, color=\"#30a2da\", label=\"Calibrated lower bound\")\ninterval_calibrated[\"upper_bound\"].plot(ax=ax, color=\"#fc4f30\", label=\"Calibrated upper bound\")\ny_true.plot(ax=ax, label=\"True values\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.legend(loc=\"upper right\", fontsize=8, ncol=4)\n\ncoverage = calculate_coverage(\n    y_true=y_true,\n    lower_bound=interval_calibrated[\"lower_bound\"],\n    upper_bound=interval_calibrated[\"upper_bound\"],\n)\nprint(f\"Coverage: {coverage:.2f}\")\n</pre> # Calibrate interval # ============================================================================== interval_calibrated = calibrator.transform(interval)  fig, ax = plt.subplots(figsize=(7, 3.5)) interval.plot(ax=ax, linestyle=\"--\") interval_calibrated[\"lower_bound\"].plot(ax=ax, color=\"#30a2da\", label=\"Calibrated lower bound\") interval_calibrated[\"upper_bound\"].plot(ax=ax, color=\"#fc4f30\", label=\"Calibrated upper bound\") y_true.plot(ax=ax, label=\"True values\") ax.set_yticklabels([]) ax.set_xticklabels([]) ax.legend(loc=\"upper right\", fontsize=8, ncol=4)  coverage = calculate_coverage(     y_true=y_true,     lower_bound=interval_calibrated[\"lower_bound\"],     upper_bound=interval_calibrated[\"upper_bound\"], ) print(f\"Coverage: {coverage:.2f}\") <pre>Coverage: 0.80\n</pre> <p>After calibration, the interval is widened to ensure that the true values are within the interval 80% of the time.</p> <p> \ud83d\udca1 Tip </p> <p>     For more details on conformal calibration method, see the faq section Calibration of probabilistic forecasting intervals. </p> In\u00a0[5]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name='bike_sharing', raw=False)\ndata = data[['users', 'temp', 'hum', 'windspeed', 'holiday']]\ndata = data.loc['2011-04-01 00:00:00':'2012-10-20 23:00:00', :].copy()\ndata.head(3)\n</pre> # Data download # ============================================================================== data = fetch_dataset(name='bike_sharing', raw=False) data = data[['users', 'temp', 'hum', 'windspeed', 'holiday']] data = data.loc['2011-04-01 00:00:00':'2012-10-20 23:00:00', :].copy() data.head(3) <pre>bike_sharing\n------------\nHourly usage of the bike share system in the city of Washington D.C. during the\nyears 2011 and 2012. In addition to the number of users per hour, information\nabout weather conditions and holidays is available.\nFanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\nhttps://doi.org/10.24432/C5W894.\nShape of the dataset: (17544, 11)\n</pre> Out[5]: users temp hum windspeed holiday date_time 2011-04-01 00:00:00 6.0 10.66 100.0 11.0014 0.0 2011-04-01 01:00:00 4.0 10.66 100.0 11.0014 0.0 2011-04-01 02:00:00 7.0 10.66 93.0 12.9980 0.0 In\u00a0[6]: Copied! <pre># Split data into: train-calibration-test\n# ==============================================================================\nend_train = '2012-07-30 23:59:00'\nend_calibration = '2012-09-20 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_cal   = data.loc[end_train:end_calibration, :]\ndata_test  = data.loc[end_calibration:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates calibration: {data_cal.index.min()} --- {data_cal.index.max()}  (n={len(data_cal)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split data into: train-calibration-test # ============================================================================== end_train = '2012-07-30 23:59:00' end_calibration = '2012-09-20 23:59:00' data_train = data.loc[: end_train, :] data_cal   = data.loc[end_train:end_calibration, :] data_test  = data.loc[end_calibration:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates calibration: {data_cal.index.min()} --- {data_cal.index.max()}  (n={len(data_cal)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 2011-04-01 00:00:00 --- 2012-07-30 23:00:00  (n=11688)\nDates calibration: 2012-07-31 00:00:00 --- 2012-09-20 23:00:00  (n=1248)\nDates test       : 2012-09-21 00:00:00 --- 2012-10-20 23:00:00  (n=720)\n</pre> In\u00a0[7]: Copied! <pre># Plot partitions\n# ==============================================================================\nset_dark_theme()\nplt.rcParams['lines.linewidth'] = 0.5\nfig, ax = plt.subplots(figsize=(8, 3))\nax.plot(data_train['users'], label='Train')\nax.plot(data_cal['users'], label='Calibration')\nax.plot(data_test['users'], label='Test')\nax.legend();\n</pre> # Plot partitions # ============================================================================== set_dark_theme() plt.rcParams['lines.linewidth'] = 0.5 fig, ax = plt.subplots(figsize=(8, 3)) ax.plot(data_train['users'], label='Train') ax.plot(data_cal['users'], label='Calibration') ax.plot(data_test['users'], label='Test') ax.legend(); <p> \u270e Note </p> <p>In this example, the data is divided into three partitions:</p> <ul> <li>Training: Used to train the model.</li> <li>Calibration: Used to determine the correction factor needed to calibrate the prediction intervals.</li> <li>Test: Used to evaluate the model\u2019s predictions.</li> </ul> <p>If hyperparameter optimization or feature selection is required, a fourth partition should be used for validation.</p> <p>Prediction intervals are calculated for the test set using the bootstrapped residuals method with in-sample residuals. A backtesting is performed to simulate a realistic scenario where the model is trained with historical data and used to forecast future data in different time periods (each 24 <code>steps</code>).</p> In\u00a0[8]: Copied! <pre># Create forecaster\n# ==============================================================================\nparams = {\n    \"max_depth\": 4,\n    \"n_estimators\": 50,\n    \"verbose\": -1,\n    \"random_state\": 15926\n}\nlags = [1, 2, 3, 23, 24, 25, 167, 168, 169]\n\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(**params),\n                 lags      = lags,\n             )\n</pre> # Create forecaster # ============================================================================== params = {     \"max_depth\": 4,     \"n_estimators\": 50,     \"verbose\": -1,     \"random_state\": 15926 } lags = [1, 2, 3, 23, 24, 25, 167, 168, 169]  forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(**params),                  lags      = lags,              ) In\u00a0[9]: Copied! <pre># Backtesting with prediction intervals in test data using in-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24, \n         initial_train_size = len(data.loc[:end_calibration]),\n         refit              = False\n     )\n\nmetric, predictions_test = backtesting_forecaster(\n                               forecaster              = forecaster,\n                               y                       = data['users'],\n                               cv                      = cv,\n                               metric                  = 'mean_absolute_error',\n                               interval                = [10, 90],  # 80% prediction interval\n                               interval_method         = 'bootstrapping',\n                               n_boot                  = 150,\n                               use_in_sample_residuals = True,  # Use in-sample residuals\n                               use_binned_residuals    = False\n                           )\npredictions_test.head(5)\n</pre> # Backtesting with prediction intervals in test data using in-sample residuals # ============================================================================== cv = TimeSeriesFold(          steps              = 24,           initial_train_size = len(data.loc[:end_calibration]),          refit              = False      )  metric, predictions_test = backtesting_forecaster(                                forecaster              = forecaster,                                y                       = data['users'],                                cv                      = cv,                                metric                  = 'mean_absolute_error',                                interval                = [10, 90],  # 80% prediction interval                                interval_method         = 'bootstrapping',                                n_boot                  = 150,                                use_in_sample_residuals = True,  # Use in-sample residuals                                use_binned_residuals    = False                            ) predictions_test.head(5) <pre>  0%|          | 0/30 [00:00&lt;?, ?it/s]</pre> Out[9]: pred lower_bound upper_bound 2012-09-21 00:00:00 101.022472 52.198336 141.019309 2012-09-21 01:00:00 62.635418 11.930605 118.710325 2012-09-21 02:00:00 31.062836 -5.705437 117.264033 2012-09-21 03:00:00 15.166505 -24.902332 106.931933 2012-09-21 04:00:00 14.567935 -23.636627 109.814730 In\u00a0[10]: Copied! <pre># Predicted interval coverage and area (on test data)\n# ==============================================================================\ncoverage = calculate_coverage(\n               y_true      = data.loc[end_calibration:, 'users'],\n               lower_bound = predictions_test[\"lower_bound\"], \n               upper_bound = predictions_test[\"upper_bound\"]\n           )\narea = (predictions_test[\"upper_bound\"] - predictions_test[\"lower_bound\"]).sum()\nprint(f\"Coverage: {round(100 * coverage, 2)} %\")\nprint(f\"Area: {round(area, 2)}\")\n\n# Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions_test,\n    y_true              = data_test,\n    target_variable     = \"users\",\n    title               = \"Prediction intervals in test data\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}\n)\n</pre> # Predicted interval coverage and area (on test data) # ============================================================================== coverage = calculate_coverage(                y_true      = data.loc[end_calibration:, 'users'],                lower_bound = predictions_test[\"lower_bound\"],                 upper_bound = predictions_test[\"upper_bound\"]            ) area = (predictions_test[\"upper_bound\"] - predictions_test[\"lower_bound\"]).sum() print(f\"Coverage: {round(100 * coverage, 2)} %\") print(f\"Area: {round(area, 2)}\")  # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions         = predictions_test,     y_true              = data_test,     target_variable     = \"users\",     title               = \"Prediction intervals in test data\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1} ) <pre>Coverage: 63.33 %\nArea: 90704.43\n</pre> <p>As expected, since in-sample residuals are used, the prediction intervals are too narrow and do not achieve the desired coverage probability of 80%.</p> <p>Conformal methods allow to calibrate prediction intervals generated by other techniques, such as quantile regression or bootstrapped residuals. The <code>ConformalIntervalCalibrator</code> transformer uses the Split Conformal Prediction (SCP) method to learn the correction factor needed to expand or shrink the prediction intervals to ensure they are valid with respect to a given coverage probability. This method consists of the following steps:</p> <ol> <li><p>Prediction intervals are estimated for the calibration set (bootstrapping).</p> </li> <li><p>Using the predicted intervals and the actual values of the calibration set, the <code>ConformalIntervalCalibrator</code> transformer learns the correction factor needed to calibrate this intervals.</p> </li> <li><p>Prediction intervals for the test set are adjusted using the correction factor learned from the calibration set.</p> </li> </ol> In\u00a0[11]: Copied! <pre># Predict intervals for the calibration set\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24, \n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False\n     )\n\n_, predictions_cal = backtesting_forecaster(\n                         forecaster              = forecaster,\n                         y                       = data.loc[:end_calibration, 'users'],\n                         cv                      = cv,\n                         metric                  = 'mean_absolute_error',\n                         interval                = [10, 90],  # 80% prediction interval\n                         interval_method         = 'bootstrapping',\n                         n_boot                  = 150,\n                         use_in_sample_residuals = True,  # Use in-sample residuals\n                         use_binned_residuals    = False\n                     )\n\npredictions_cal.head(5)\n</pre> # Predict intervals for the calibration set # ============================================================================== cv = TimeSeriesFold(          steps              = 24,           initial_train_size = len(data.loc[:end_train]),          refit              = False      )  _, predictions_cal = backtesting_forecaster(                          forecaster              = forecaster,                          y                       = data.loc[:end_calibration, 'users'],                          cv                      = cv,                          metric                  = 'mean_absolute_error',                          interval                = [10, 90],  # 80% prediction interval                          interval_method         = 'bootstrapping',                          n_boot                  = 150,                          use_in_sample_residuals = True,  # Use in-sample residuals                          use_binned_residuals    = False                      )  predictions_cal.head(5) <pre>  0%|          | 0/52 [00:00&lt;?, ?it/s]</pre> Out[11]: pred lower_bound upper_bound 2012-07-31 00:00:00 96.572601 52.524245 129.474640 2012-07-31 01:00:00 53.122252 -2.799689 130.694945 2012-07-31 02:00:00 23.674118 -20.949651 100.012332 2012-07-31 03:00:00 12.621803 -20.729283 72.077897 2012-07-31 04:00:00 15.537228 -15.251442 98.006128 In\u00a0[12]: Copied! <pre># Fit a ConformalIntervalCalibrator transformer using the calibration set\n# ==============================================================================\ncalibrator = ConformalIntervalCalibrator(nominal_coverage=0.8)\ncalibrator.fit(\n    y_true          = data.loc[predictions_cal.index, 'users'],\n    y_pred_interval = predictions_cal[['lower_bound', 'upper_bound']]\n)\ncalibrator\n</pre> # Fit a ConformalIntervalCalibrator transformer using the calibration set # ============================================================================== calibrator = ConformalIntervalCalibrator(nominal_coverage=0.8) calibrator.fit(     y_true          = data.loc[predictions_cal.index, 'users'],     y_pred_interval = predictions_cal[['lower_bound', 'upper_bound']] ) calibrator Out[12]: ConformalIntervalCalibrator General Information <ul> <li>Nominal coverage: 0.8</li> <li>Coverage in fit data: {'users': 0.7019230769230769}</li> <li>Symmetric interval: True</li> <li>Symmetric correction factor: {'users': 30.836441105280407}</li> <li>Asymmetric correction factor lower: {'users': 23.543396297791194}</li> <li>Asymmetric correction factor upper: {'users': 34.13029706829574}</li> <li>Fitted series: ['users']</li> </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p> \u26a0 Warning </p> <p> <p>It is highly recommended to review the coverage observed in the calibration set, stored in the <code>fit_coverage_</code> attribute of <code>ConformalIntervalCalibrator</code> object. The assumption is that similar coverage will be observed in the test set, allowing the correction factor to be applied effectively.</p> <p>However, if the coverage differs between the calibration and test sets, the correction factor may not be appropriate. For instance, suppose the nominal coverage is 80%, but the calibration set achieves only 70%. In this case, the calibrator will learn a correction factor that expands the intervals to reach the desired 80% coverage. However, if the test set already has 90% coverage, applying the same correction factor would further widen the intervals instead of shrinking them, resulting in incorrect calibration.</p> <p>By reviewing the calibration coverage beforehand, you can verify whether the correction factor is valid for the test set, preventing potential miscalibrations.</p> </p> <p>The correction factor is positive, indicating that the <code>ConformalIntervalCalibrator</code> is learning that the prediction intervals are too narrow and need to be widened to achieve the desired probability of coverage.</p> <p>The already calculated prediction intervals of the test set are now calibrated.</p> In\u00a0[13]: Copied! <pre># Calibrate prediction intervals of the test set\n# ==============================================================================\npredictions_test_calibrated = calibrator.transform(\n    predictions_test[[\"lower_bound\", \"upper_bound\"]]\n)\npredictions_test_calibrated.head(3)\n</pre> # Calibrate prediction intervals of the test set # ============================================================================== predictions_test_calibrated = calibrator.transform(     predictions_test[[\"lower_bound\", \"upper_bound\"]] ) predictions_test_calibrated.head(3) Out[13]: level lower_bound upper_bound 2012-09-21 00:00:00 users 21.361895 171.855751 2012-09-21 01:00:00 users -18.905836 149.546766 2012-09-21 02:00:00 users -36.541878 148.100474 In\u00a0[14]: Copied! <pre># Plot intervals\n# ==============================================================================\npredictions_test_calibrated['pred'] = predictions_test['pred']\nplot_prediction_intervals(\n    predictions         = predictions_test_calibrated,\n    y_true              = data_test,\n    target_variable     = \"users\",\n    title               = \"Calibrated prediction intervals in test data\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}\n)\n\n# Predicted interval coverage and area (on test data)\n# ==============================================================================\ncoverage = calculate_coverage(\n               y_true      = data.loc[end_calibration:, 'users'],\n               lower_bound = predictions_test_calibrated[\"lower_bound\"], \n               upper_bound = predictions_test_calibrated[\"upper_bound\"]\n           )\narea = (predictions_test_calibrated[\"upper_bound\"] - predictions_test_calibrated[\"lower_bound\"]).sum()\nprint(f\"Coverage: {round(100 * coverage, 2)} %\")\nprint(f\"Area: {round(area, 2)}\")\n</pre> # Plot intervals # ============================================================================== predictions_test_calibrated['pred'] = predictions_test['pred'] plot_prediction_intervals(     predictions         = predictions_test_calibrated,     y_true              = data_test,     target_variable     = \"users\",     title               = \"Calibrated prediction intervals in test data\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1} )  # Predicted interval coverage and area (on test data) # ============================================================================== coverage = calculate_coverage(                y_true      = data.loc[end_calibration:, 'users'],                lower_bound = predictions_test_calibrated[\"lower_bound\"],                 upper_bound = predictions_test_calibrated[\"upper_bound\"]            ) area = (predictions_test_calibrated[\"upper_bound\"] - predictions_test_calibrated[\"lower_bound\"]).sum() print(f\"Coverage: {round(100 * coverage, 2)} %\") print(f\"Area: {round(area, 2)}\") <pre>Coverage: 77.22 %\nArea: 135108.91\n</pre> <p>After calibration, the prediction intervals achieve an empirical coverage very close to the nominal coverage of 80%.</p> In\u00a0[15]: Copied! <pre># Data\n# ==============================================================================\ndata = fetch_dataset(name=\"ett_m2_extended\")\ndata = data.resample(rule=\"1h\", closed=\"left\", label=\"right\").mean()\ndata = data.loc[:'2016-10-31 23:59:00', :].copy()\ndata.head(2)\n</pre> # Data # ============================================================================== data = fetch_dataset(name=\"ett_m2_extended\") data = data.resample(rule=\"1h\", closed=\"left\", label=\"right\").mean() data = data.loc[:'2016-10-31 23:59:00', :].copy() data.head(2) <pre>ett_m2_extended\n---------------\nData from an electricity transformer station was collected between July 2016 and\nJuly 2018 (2 years x 365 days x 24 hours x 4 intervals per hour = 70,080 data\npoints). Each data point consists of 8 features, including the date of the\npoint, the predictive value \"Oil Temperature (OT)\", and 6 different types of\nexternal power load features: High UseFul Load (HUFL), High UseLess Load (HULL),\nMiddle UseFul Load (MUFL), Middle UseLess Load (MULL), Low UseFul Load (LUFL),\nLow UseLess Load (LULL). Additional variables are created based on calendar\ninformation (year, month, week, day of the week, and hour). These variables have\nbeen encoded using the cyclical encoding technique (sin and cos transformations)\nto preserve the cyclical nature of the data.\nZhou, Haoyi &amp; Zhang, Shanghang &amp; Peng, Jieqi &amp; Zhang, Shuai &amp; Li, Jianxin &amp;\nXiong, Hui &amp; Zhang, Wancai. (2020). Informer: Beyond Efficient Transformer for\nLong Sequence Time-Series Forecasting.\n[10.48550/arXiv.2012.07436](https://arxiv.org/abs/2012.07436).\nhttps://github.com/zhouhaoyi/ETDataset\nShape of the dataset: (69680, 16)\n</pre> Out[15]: HUFL HULL MUFL MULL LUFL LULL OT year month_sin month_cos week_sin week_cos day_of_week_sin day_of_week_cos hour_sin hour_cos date 2016-07-01 01:00:00 38.784501 10.88975 34.753500 8.551 4.12575 1.2605 37.83825 2016.0 -0.5 -0.866025 1.224647e-16 -1.0 -0.433884 -0.900969 0.000000 1.000000 2016-07-01 02:00:00 36.041249 9.44475 32.696001 7.137 3.59025 0.6290 36.84925 2016.0 -0.5 -0.866025 1.224647e-16 -1.0 -0.433884 -0.900969 0.258819 0.965926 In\u00a0[16]: Copied! <pre># Split data into: train-calibration-test\n# ==============================================================================\nend_train = '2016-08-31 23:59:00'\nend_calibration = '2016-09-30 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_cal   = data.loc[end_train:end_calibration, :]\ndata_test  = data.loc[end_calibration:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates calibration: {data_cal.index.min()} --- {data_cal.index.max()}  (n={len(data_cal)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split data into: train-calibration-test # ============================================================================== end_train = '2016-08-31 23:59:00' end_calibration = '2016-09-30 23:59:00' data_train = data.loc[: end_train, :] data_cal   = data.loc[end_train:end_calibration, :] data_test  = data.loc[end_calibration:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates calibration: {data_cal.index.min()} --- {data_cal.index.max()}  (n={len(data_cal)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 2016-07-01 01:00:00 --- 2016-08-31 23:00:00  (n=1487)\nDates calibration: 2016-09-01 00:00:00 --- 2016-09-30 23:00:00  (n=720)\nDates test       : 2016-10-01 00:00:00 --- 2016-10-31 23:00:00  (n=744)\n</pre> In\u00a0[17]: Copied! <pre># Plot partitions\n# ==============================================================================\nseries = ['HUFL', 'HULL', 'MULL']\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\nfig, axs = plt.subplots(3, 1, figsize=(7, 5), sharex=True)\nfor i, col in enumerate(series):\n    axs[i].plot(data[col], label=col, color=colors[i])\n    axs[i].legend(loc='lower left', fontsize=8)\n    axs[i].tick_params(axis='both', labelsize=8)\n    axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train\n    axs[i].axvline(pd.to_datetime(end_calibration), color='white', linestyle='--', linewidth=1)  # End validation\n\nplt.tight_layout()\nplt.show()\n</pre> # Plot partitions # ============================================================================== series = ['HUFL', 'HULL', 'MULL'] colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] fig, axs = plt.subplots(3, 1, figsize=(7, 5), sharex=True) for i, col in enumerate(series):     axs[i].plot(data[col], label=col, color=colors[i])     axs[i].legend(loc='lower left', fontsize=8)     axs[i].tick_params(axis='both', labelsize=8)     axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train     axs[i].axvline(pd.to_datetime(end_calibration), color='white', linestyle='--', linewidth=1)  # End validation  plt.tight_layout() plt.show() In\u00a0[18]: Copied! <pre># Create forecaster\n# ==============================================================================\nexog_features = [\n    'year', 'month_sin', 'month_cos', 'week_sin', 'week_cos', 'day_of_week_sin',\n    'day_of_week_cos', 'hour_sin', 'hour_cos'\n]\nlags = [1, 3, 11, 12, 13, 14, 15, 17, 23, 24, 25, 49, 73, 97, 145]\n\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = Ridge(random_state=15926),\n                 lags               = lags,\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = StandardScaler(),\n                 differentiation    = 1,\n                 binner_kwargs      = {'n_bins': 5}\n             )\n</pre> # Create forecaster # ============================================================================== exog_features = [     'year', 'month_sin', 'month_cos', 'week_sin', 'week_cos', 'day_of_week_sin',     'day_of_week_cos', 'hour_sin', 'hour_cos' ] lags = [1, 3, 11, 12, 13, 14, 15, 17, 23, 24, 25, 49, 73, 97, 145]  forecaster = ForecasterRecursiveMultiSeries(                  regressor          = Ridge(random_state=15926),                  lags               = lags,                  encoding           = 'ordinal',                  transformer_series = StandardScaler(),                  transformer_exog   = StandardScaler(),                  differentiation    = 1,                  binner_kwargs      = {'n_bins': 5}              ) In\u00a0[19]: Copied! <pre># Backtesting on test data using in-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         initial_train_size = len(data.loc[:end_calibration, :]),\n         steps              = 24,\n         differentiation    = 1,\n     )\n\nmetric_test, predictions_test = backtesting_forecaster_multiseries(\n                                    forecaster              = forecaster,\n                                    series                  = data.loc[:, series],\n                                    exog                    = data.loc[:, exog_features],\n                                    cv                      = cv,\n                                    metric                  = 'mean_absolute_error',\n                                    interval                = [10, 90],\n                                    interval_method         = \"bootstrapping\",\n                                    n_boot                  = 150,\n                                    use_in_sample_residuals = True,\n                                    use_binned_residuals    = True\n                                )\n</pre> # Backtesting on test data using in-sample residuals # ============================================================================== cv = TimeSeriesFold(          initial_train_size = len(data.loc[:end_calibration, :]),          steps              = 24,          differentiation    = 1,      )  metric_test, predictions_test = backtesting_forecaster_multiseries(                                     forecaster              = forecaster,                                     series                  = data.loc[:, series],                                     exog                    = data.loc[:, exog_features],                                     cv                      = cv,                                     metric                  = 'mean_absolute_error',                                     interval                = [10, 90],                                     interval_method         = \"bootstrapping\",                                     n_boot                  = 150,                                     use_in_sample_residuals = True,                                     use_binned_residuals    = True                                 ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> In\u00a0[20]: Copied! <pre># Coverage and area for each level\n# ==============================================================================\nfor level in predictions_test[\"level\"].unique():\n    predictions_level = (\n        predictions_test[predictions_test[\"level\"] == level]\n        .drop(columns=\"level\")\n        .copy()\n    )\n    coverage = calculate_coverage(\n        y_true=data_test[level],\n        lower_bound=predictions_level[\"lower_bound\"],\n        upper_bound=predictions_level[\"upper_bound\"],\n    )\n    area = (predictions_level[\"upper_bound\"] - predictions_level[\"lower_bound\"]).sum()\n    print(f\"{level} - Coverage: {round(100 * coverage, 2)} % - Area: {round(area, 2)}\")\n</pre> # Coverage and area for each level # ============================================================================== for level in predictions_test[\"level\"].unique():     predictions_level = (         predictions_test[predictions_test[\"level\"] == level]         .drop(columns=\"level\")         .copy()     )     coverage = calculate_coverage(         y_true=data_test[level],         lower_bound=predictions_level[\"lower_bound\"],         upper_bound=predictions_level[\"upper_bound\"],     )     area = (predictions_level[\"upper_bound\"] - predictions_level[\"lower_bound\"]).sum()     print(f\"{level} - Coverage: {round(100 * coverage, 2)} % - Area: {round(area, 2)}\") <pre>HUFL - Coverage: 96.24 % - Area: 11094.31\nHULL - Coverage: 97.58 % - Area: 6712.73\nMULL - Coverage: 96.77 % - Area: 5473.82\n</pre> <p>In this example, eventhough the intervals are calculated using in-sample residuals, the coverage is higher than the nominal coverage of 80%. Next, they are calibrated using the <code>ConformalIntervalCalibrator</code> transformer.</p> In\u00a0[21]: Copied! <pre># Backtesting on calibration set\n# ==============================================================================\ncv = TimeSeriesFold(\n         initial_train_size = len(data.loc[:end_train, :]),\n         steps              = 24,\n         differentiation    = 1,\n     )\n\nmetric_cal, predictions_cal = backtesting_forecaster_multiseries(\n                                  forecaster              = forecaster,\n                                  series                  = data.loc[:end_calibration, series],\n                                  exog                    = data.loc[:end_calibration, exog_features],\n                                  cv                      = cv,\n                                  metric                  = 'mean_absolute_error',\n                                  interval                = [10, 90],\n                                  interval_method         = \"bootstrapping\",\n                                  n_boot                  = 150,\n                                  use_in_sample_residuals = True,\n                                  use_binned_residuals    = True\n                              )\n</pre> # Backtesting on calibration set # ============================================================================== cv = TimeSeriesFold(          initial_train_size = len(data.loc[:end_train, :]),          steps              = 24,          differentiation    = 1,      )  metric_cal, predictions_cal = backtesting_forecaster_multiseries(                                   forecaster              = forecaster,                                   series                  = data.loc[:end_calibration, series],                                   exog                    = data.loc[:end_calibration, exog_features],                                   cv                      = cv,                                   metric                  = 'mean_absolute_error',                                   interval                = [10, 90],                                   interval_method         = \"bootstrapping\",                                   n_boot                  = 150,                                   use_in_sample_residuals = True,                                   use_binned_residuals    = True                               ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>  0%|          | 0/30 [00:00&lt;?, ?it/s]</pre> In\u00a0[22]: Copied! <pre># Create and fit ConformalIntervalCalibrator\n# ==============================================================================\ncalibrator = ConformalIntervalCalibrator(nominal_coverage=0.8)\ncalibrator.fit(\n    y_true = data_cal[series],\n    y_pred_interval = predictions_cal\n)\ncalibrator\n</pre> # Create and fit ConformalIntervalCalibrator # ============================================================================== calibrator = ConformalIntervalCalibrator(nominal_coverage=0.8) calibrator.fit(     y_true = data_cal[series],     y_pred_interval = predictions_cal ) calibrator Out[22]: ConformalIntervalCalibrator General Information <ul> <li>Nominal coverage: 0.8</li> <li>Coverage in fit data: {'HUFL': 0.9694444444444444, 'HULL': 0.9833333333333333, 'MULL': 0.9680555555555556}</li> <li>Symmetric interval: True</li> <li>Symmetric correction factor: {'HUFL': -2.7878517209861884, 'HULL': -1.9653641691183448, 'MULL': -1.4805341783220844}</li> <li>Asymmetric correction factor lower: {'HUFL': -2.090525169340902, 'HULL': -1.3830638561976496, 'MULL': -0.9309778691565327}</li> <li>Asymmetric correction factor upper: {'HUFL': -3.570236837689637, 'HULL': -2.5786524357616964, 'MULL': -1.975576082075848}</li> <li>Fitted series: ['HUFL', 'HULL', 'MULL']</li> </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>The correction factor of all levels is negative, which means that the prediction intervals are too wide and need to be narrowed to achieve the desired coverage probability.</p> <p> \u26a0 Warning </p> <p> <p>It is highly recommended to review the coverage observed in the calibration set, stored in the <code>fit_coverage_</code> attribute of <code>ConformalIntervalCalibrator</code> object. The assumption is that similar coverage will be observed in the test set, allowing the correction factor to be applied effectively.</p> <p>However, if the coverage differs between the calibration and test sets, the correction factor may not be appropriate. For instance, suppose the nominal coverage is 80%, but the calibration set achieves only 70%. In this case, the calibrator will learn a correction factor that expands the intervals to reach the desired 80% coverage. However, if the test set already has 90% coverage, applying the same correction factor would further widen the intervals instead of shrinking them, resulting in incorrect calibration.</p> <p>By reviewing the calibration coverage beforehand, you can verify whether the correction factor is valid for the test set, preventing potential miscalibrations.</p> </p> In\u00a0[23]: Copied! <pre># Calibrate prediction intervals of the test set\n# ==============================================================================\npredictions_test_calibrated = calibrator.transform(predictions_test)\npredictions_test_calibrated.head(3)\n</pre> # Calibrate prediction intervals of the test set # ============================================================================== predictions_test_calibrated = calibrator.transform(predictions_test) predictions_test_calibrated.head(3) Out[23]: level lower_bound upper_bound 2016-10-01 00:00:00 HUFL 35.454979 36.846566 2016-10-01 01:00:00 HUFL 34.322673 34.918745 2016-10-01 02:00:00 HUFL 32.518754 33.994147 In\u00a0[24]: Copied! <pre># Prediction intervals on test data after calibration\n# ==============================================================================\nfor level in predictions_test_calibrated[\"level\"].unique():\n    predictions_level = (\n        predictions_test_calibrated[predictions_test_calibrated[\"level\"] == level]\n        .drop(columns=\"level\")\n        .copy()\n    )\n    predictions_level['pred'] = predictions_test.loc[predictions_test['level'] == level, 'pred']\n        \n    plot_prediction_intervals(\n        predictions         = predictions_level,\n        y_true              = data_test[[level]],\n        target_variable     = level,\n        title               = level,\n        kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}\n    )\n    coverage = calculate_coverage(\n                   y_true      = data_test[level],\n                   lower_bound = predictions_level[\"lower_bound\"],\n                   upper_bound = predictions_level[\"upper_bound\"],\n               )\n    area = (predictions_level[\"upper_bound\"] - predictions_level[\"lower_bound\"]).sum()\n    print(f\"{level} - Coverage: {round(100 * coverage, 2)} % - Area: {round(area, 2)}\")\n</pre> # Prediction intervals on test data after calibration # ============================================================================== for level in predictions_test_calibrated[\"level\"].unique():     predictions_level = (         predictions_test_calibrated[predictions_test_calibrated[\"level\"] == level]         .drop(columns=\"level\")         .copy()     )     predictions_level['pred'] = predictions_test.loc[predictions_test['level'] == level, 'pred']              plot_prediction_intervals(         predictions         = predictions_level,         y_true              = data_test[[level]],         target_variable     = level,         title               = level,         kwargs_fill_between = {'color': 'gray', 'alpha': 0.3, 'zorder': 1}     )     coverage = calculate_coverage(                    y_true      = data_test[level],                    lower_bound = predictions_level[\"lower_bound\"],                    upper_bound = predictions_level[\"upper_bound\"],                )     area = (predictions_level[\"upper_bound\"] - predictions_level[\"lower_bound\"]).sum()     print(f\"{level} - Coverage: {round(100 * coverage, 2)} % - Area: {round(area, 2)}\") <pre>HUFL - Coverage: 79.84 % - Area: 7044.41\nHULL - Coverage: 77.69 % - Area: 3855.71\nMULL - Coverage: 77.42 % - Area: 3341.42\n</pre>"},{"location":"user_guides/probabilistic-forecasting-conformal-calibration.html#probabilistic-forecasting-conformal-calibration","title":"Probabilistic Forecasting: Conformal Calibration\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-conformal-calibration.html#calibrate-intervals-using-conformal-methods","title":"Calibrate intervals using conformal methods\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-conformal-calibration.html#calibration-of-single-series-models","title":"Calibration of single series models\u00b6","text":"<p>A <code>ForecasterRecursive</code> model is used to forecast prediction intervals with the bootstrapped residuals method. A <code>ConformalIntervalCalibrator</code> transformer is then used to calibrate the prediction intervals to ensure they have the desired coverage probability.</p>"},{"location":"user_guides/probabilistic-forecasting-conformal-calibration.html#calibration-for-global-models","title":"Calibration for global models\u00b6","text":"<p>Same process of calibration can be applied to global models, <code>ForecasterRecursiveMultiSeries</code>. In this case, the <code>ConformalIntervalCalibrator</code> transformer is fitted with the calibration intervals of multiple series.</p>"},{"location":"user_guides/probabilistic-forecasting-conformal-prediction.html","title":"Conformal predictions","text":"<p>Conformal prediction is a framework for constructing prediction intervals that are guaranteed to contain the true value with a specified probability (coverage probability). It works by combining the predictions of a point-forecasting model with its past residuals\u2014differences between previous predictions and actual values. These residuals help estimate the uncertainty in the forecast and determine the width of the prediction interval that is then added to the point forecast. Skforecast implements Split Conformal Prediction (SCP).</p> <p> Conformal regression turns point predictions into prediction intervals. Source: Introduction To Conformal Prediction With Python: A Short Guide For Quantifying Uncertainty Of Machine Learning Models by Christoph Molnar https://leanpub.com/conformal-prediction </p> <p>Conformal methods can also calibrate prediction intervals generated by other techniques, such as quantile regression or bootstrapped residuals. In this case, the conformal method adjusts the prediction intervals to ensure that they remain valid with respect to the coverage probability. Skforecast provides this functionality through the <code>ConformalIntervalCalibrator</code> transformer.</p> <p> \u26a0 Warning </p> <p>There are several well-established methods for conformal prediction, each with its own characteristics and assumptions. However, when applied to time series forecasting, their coverage guarantees are only valid for one-step-ahead predictions. For multi-step-ahead predictions, the coverage probability is not guaranteed. Skforecast implements Split Conformal Prediction (SCP) due to its simplicity and efficiency.</p> <p> \ud83d\udca1 Tip </p> <p>For more examples on how to use probabilistic forecasting, check out the following articles:</p> <ul> <li>              Probabilistic forecasting with machine learning          </li> <li>              Probabilistic forecasting: prediction intervals for multi-step time series forecasting          </li> <li>              Continuous Ranked Probability Score (CRPS) in probabilistic forecasting          </li> </ul> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme, plot_residuals, plot_prediction_intervals\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\nfrom skforecast.metrics import calculate_coverage\n\n# Configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('once')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd from skforecast.datasets import fetch_dataset  # Plots # ============================================================================== import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme, plot_residuals, plot_prediction_intervals  # Modelling and Forecasting # ============================================================================== from lightgbm import LGBMRegressor from skforecast.recursive import ForecasterRecursive from skforecast.preprocessing import RollingFeatures from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster from skforecast.metrics import calculate_coverage  # Configuration # ============================================================================== import warnings warnings.filterwarnings('once') In\u00a0[2]: Copied! <pre># Data\n# ==============================================================================\ndata = fetch_dataset(name=\"ett_m2_extended\")\ndata = data[[\n    \"OT\",\n    \"day_of_week_cos\",\n    \"day_of_week_sin\",\n    \"hour_cos\",\n    \"hour_sin\",\n    \"month_cos\",\n    \"month_sin\",\n    \"week_cos\",\n    \"week_sin\",\n    \"year\",\n]]\ndata.head(2)\n</pre> # Data # ============================================================================== data = fetch_dataset(name=\"ett_m2_extended\") data = data[[     \"OT\",     \"day_of_week_cos\",     \"day_of_week_sin\",     \"hour_cos\",     \"hour_sin\",     \"month_cos\",     \"month_sin\",     \"week_cos\",     \"week_sin\",     \"year\", ]] data.head(2) <pre>ett_m2_extended\n---------------\nData from an electricity transformer station was collected between July 2016 and\nJuly 2018 (2 years x 365 days x 24 hours x 4 intervals per hour = 70,080 data\npoints). Each data point consists of 8 features, including the date of the\npoint, the predictive value \"Oil Temperature (OT)\", and 6 different types of\nexternal power load features: High UseFul Load (HUFL), High UseLess Load (HULL),\nMiddle UseFul Load (MUFL), Middle UseLess Load (MULL), Low UseFul Load (LUFL),\nLow UseLess Load (LULL). Additional variables are created based on calendar\ninformation (year, month, week, day of the week, and hour). These variables have\nbeen encoded using the cyclical encoding technique (sin and cos transformations)\nto preserve the cyclical nature of the data.\nZhou, Haoyi &amp; Zhang, Shanghang &amp; Peng, Jieqi &amp; Zhang, Shuai &amp; Li, Jianxin &amp;\nXiong, Hui &amp; Zhang, Wancai. (2020). Informer: Beyond Efficient Transformer for\nLong Sequence Time-Series Forecasting.\n[10.48550/arXiv.2012.07436](https://arxiv.org/abs/2012.07436).\nhttps://github.com/zhouhaoyi/ETDataset\nShape of the dataset: (69680, 16)\n</pre> Out[2]: OT day_of_week_cos day_of_week_sin hour_cos hour_sin month_cos month_sin week_cos week_sin year date 2016-07-01 00:00:00 38.661999 -0.900969 -0.433884 1.0 0.0 -0.866025 -0.5 -1.0 1.224647e-16 2016 2016-07-01 00:15:00 38.223000 -0.900969 -0.433884 1.0 0.0 -0.866025 -0.5 -1.0 1.224647e-16 2016 In\u00a0[3]: Copied! <pre># Split train-calibration-test\n# ==============================================================================\nexog_features = data.columns.difference(['OT']).tolist()\nend_train = '2017-10-01 23:59:00'\nend_calibration = '2018-04-01 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_cal   = data.loc[end_train:end_calibration, :]\ndata_test  = data.loc[end_calibration:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates calibration: {data_cal.index.min()} --- {data_cal.index.max()}  (n={len(data_cal)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-calibration-test # ============================================================================== exog_features = data.columns.difference(['OT']).tolist() end_train = '2017-10-01 23:59:00' end_calibration = '2018-04-01 23:59:00' data_train = data.loc[: end_train, :] data_cal   = data.loc[end_train:end_calibration, :] data_test  = data.loc[end_calibration:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates calibration: {data_cal.index.min()} --- {data_cal.index.max()}  (n={len(data_cal)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 2016-07-01 00:00:00 --- 2017-10-01 23:45:00  (n=43968)\nDates calibration: 2017-10-02 00:00:00 --- 2018-04-01 23:45:00  (n=17472)\nDates test       : 2018-04-02 00:00:00 --- 2018-06-26 19:45:00  (n=8240)\n</pre> In\u00a0[4]: Copied! <pre># Plot partitions\n# ==============================================================================\nset_dark_theme()\nplt.rcParams['lines.linewidth'] = 0.5\nfig, ax = plt.subplots(figsize=(8, 3))\nax.plot(data_train['OT'], label='Train')\nax.plot(data_cal['OT'], label='Calibration')\nax.plot(data_test['OT'], label='Test')\nax.set_title('Oil Temperature')\nax.legend();\n</pre> # Plot partitions # ============================================================================== set_dark_theme() plt.rcParams['lines.linewidth'] = 0.5 fig, ax = plt.subplots(figsize=(8, 3)) ax.plot(data_train['OT'], label='Train') ax.plot(data_cal['OT'], label='Calibration') ax.plot(data_test['OT'], label='Test') ax.set_title('Oil Temperature') ax.legend(); In\u00a0[5]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nparams = {\n    \"max_depth\": 4,\n    \"verbose\": -1,\n    \"random_state\": 15926\n}\nlags = [1, 2, 3, 23, 24, 25, 47, 48, 49, 71, 72, 73]\nwindow_features = RollingFeatures(stats=[\"mean\"], window_sizes=24 * 3)\n\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(**params),\n                 lags            = lags,\n                 window_features = window_features,\n             )\n\nforecaster.fit(\n    y    = data.loc[:end_calibration, 'OT'],\n    exog = data.loc[:end_calibration, exog_features]\n)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== params = {     \"max_depth\": 4,     \"verbose\": -1,     \"random_state\": 15926 } lags = [1, 2, 3, 23, 24, 25, 47, 48, 49, 71, 72, 73] window_features = RollingFeatures(stats=[\"mean\"], window_sizes=24 * 3)  forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(**params),                  lags            = lags,                  window_features = window_features,              )  forecaster.fit(     y    = data.loc[:end_calibration, 'OT'],     exog = data.loc[:end_calibration, exog_features] ) forecaster Out[5]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [ 1  2  3 23 24 25 47 48 49 71 72 73]</li> <li>Window features: ['roll_mean_72']</li> <li>Window size: 73</li> <li>Series name: OT</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:08:00</li> <li>Last fit date: 2025-08-07 19:08:01</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     day_of_week_cos, day_of_week_sin, hour_cos, hour_sin, month_cos, month_sin, week_cos, week_sin, year                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('2016-07-01 00:00:00'), Timestamp('2018-04-01 23:45:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: 15min</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': 4, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 15926, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[6]: Copied! <pre># Backtesting on calibration data to obtain out-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24, \n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False\n     )\n\n_, predictions_cal = backtesting_forecaster(\n                         forecaster    = forecaster,\n                         y             = data.loc[:end_calibration, 'OT'],\n                         exog          = data.loc[:end_calibration, exog_features],\n                         cv            = cv,\n                         metric        = 'mean_absolute_error',\n                         n_jobs        = 'auto',\n                         verbose       = False,\n                         show_progress = True\n                     )\n</pre> # Backtesting on calibration data to obtain out-sample residuals # ============================================================================== cv = TimeSeriesFold(          steps              = 24,           initial_train_size = len(data.loc[:end_train]),          refit              = False      )  _, predictions_cal = backtesting_forecaster(                          forecaster    = forecaster,                          y             = data.loc[:end_calibration, 'OT'],                          exog          = data.loc[:end_calibration, exog_features],                          cv            = cv,                          metric        = 'mean_absolute_error',                          n_jobs        = 'auto',                          verbose       = False,                          show_progress = True                      ) <pre>  0%|          | 0/728 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre># Distribution of out-sample residuals\n# ==============================================================================\nresiduals = data.loc[predictions_cal.index, 'OT'] - predictions_cal['pred']\nprint(pd.Series(np.where(residuals &lt; 0, 'negative', 'positive')).value_counts())\nplt.rcParams.update({'font.size': 8})\n_ = plot_residuals(residuals=residuals, figsize=(7, 4))\n</pre> # Distribution of out-sample residuals # ============================================================================== residuals = data.loc[predictions_cal.index, 'OT'] - predictions_cal['pred'] print(pd.Series(np.where(residuals &lt; 0, 'negative', 'positive')).value_counts()) plt.rcParams.update({'font.size': 8}) _ = plot_residuals(residuals=residuals, figsize=(7, 4)) <pre>negative    12251\npositive     5221\nName: count, dtype: int64\n</pre> <p>With the <code>set_out_sample_residuals()</code> method, the out-sample residuals are stored in the forecaster object so that they can be used to calibrate the prediction intervals.</p> In\u00a0[8]: Copied! <pre># Store out-sample residuals in the forecaster\n# ==============================================================================\nforecaster.set_out_sample_residuals(\n    y_true = data.loc[predictions_cal.index, 'OT'], \n    y_pred = predictions_cal['pred']\n)\n</pre> # Store out-sample residuals in the forecaster # ============================================================================== forecaster.set_out_sample_residuals(     y_true = data.loc[predictions_cal.index, 'OT'],      y_pred = predictions_cal['pred'] ) <p>Now that the new residuals have been added to the forecaster, the prediction intervals can be calculated using <code>use_in_sample_residuals = False</code>.</p> In\u00a0[9]: Copied! <pre># Prediction intervals\n# ==============================================================================\nforecaster.predict_interval(\n    steps                   = 24,\n    exog                    = data.loc[end_calibration:, exog_features],\n    interval                = [10, 90],\n    method                  = 'conformal',\n    use_in_sample_residuals = False,\n    use_binned_residuals    = True\n)\n</pre> # Prediction intervals # ============================================================================== forecaster.predict_interval(     steps                   = 24,     exog                    = data.loc[end_calibration:, exog_features],     interval                = [10, 90],     method                  = 'conformal',     use_in_sample_residuals = False,     use_binned_residuals    = True ) Out[9]: pred lower_bound upper_bound 2018-04-02 00:00:00 32.024773 29.387492 34.662053 2018-04-02 00:15:00 31.853564 29.216283 34.490845 2018-04-02 00:30:00 31.695225 29.057944 34.332506 2018-04-02 00:45:00 31.442913 28.805632 34.080194 2018-04-02 01:00:00 31.390724 28.753443 34.028005 2018-04-02 01:15:00 31.137807 28.500526 33.775088 2018-04-02 01:30:00 30.926586 28.289305 33.563867 2018-04-02 01:45:00 30.793334 28.156053 33.430615 2018-04-02 02:00:00 30.544763 27.907482 33.182044 2018-04-02 02:15:00 30.544763 27.907482 33.182044 2018-04-02 02:30:00 30.569183 27.931902 33.206464 2018-04-02 02:45:00 30.585077 27.947796 33.222358 2018-04-02 03:00:00 30.589153 27.951873 33.226434 2018-04-02 03:15:00 30.589153 27.951873 33.226434 2018-04-02 03:30:00 30.594971 27.957691 33.232252 2018-04-02 03:45:00 30.613403 27.976122 33.250683 2018-04-02 04:00:00 30.613403 27.976122 33.250683 2018-04-02 04:15:00 30.609986 27.972705 33.247267 2018-04-02 04:30:00 30.625439 27.988158 33.262719 2018-04-02 04:45:00 30.625439 27.988158 33.262719 2018-04-02 05:00:00 30.637124 27.999843 33.274405 2018-04-02 05:15:00 30.636440 27.999159 33.273721 2018-04-02 05:30:00 30.647419 28.010138 33.284699 2018-04-02 05:45:00 30.647419 28.010138 33.284699 <p>It is also possible to use forecast prediction intervals within a backtesting loop.</p> In\u00a0[10]: Copied! <pre># Backtesting with prediction intervals in test data using out-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24, \n         initial_train_size = len(data.loc[:end_calibration]),\n         refit              = False\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster              = forecaster,\n                          y                       = data['OT'],\n                          exog                    = data[exog_features],\n                          cv                      = cv,\n                          metric                  = 'mean_absolute_error',\n                          interval                = [10, 90],  # 80% prediction interval\n                          interval_method         = 'conformal',\n                          use_in_sample_residuals = False,  # Use out-sample residuals\n                          use_binned_residuals    = True    # Adaptive conformal \n                      )\npredictions.head(5)\n</pre> # Backtesting with prediction intervals in test data using out-sample residuals # ============================================================================== cv = TimeSeriesFold(          steps              = 24,           initial_train_size = len(data.loc[:end_calibration]),          refit              = False      )  metric, predictions = backtesting_forecaster(                           forecaster              = forecaster,                           y                       = data['OT'],                           exog                    = data[exog_features],                           cv                      = cv,                           metric                  = 'mean_absolute_error',                           interval                = [10, 90],  # 80% prediction interval                           interval_method         = 'conformal',                           use_in_sample_residuals = False,  # Use out-sample residuals                           use_binned_residuals    = True    # Adaptive conformal                        ) predictions.head(5) <pre>  0%|          | 0/344 [00:00&lt;?, ?it/s]</pre> Out[10]: pred lower_bound upper_bound 2018-04-02 00:00:00 32.024773 29.387492 34.662053 2018-04-02 00:15:00 31.853564 29.216283 34.490845 2018-04-02 00:30:00 31.695225 29.057944 34.332506 2018-04-02 00:45:00 31.442913 28.805632 34.080194 2018-04-02 01:00:00 31.390724 28.753443 34.028005 <p> \u270e Note </p> <p>Two arguments control the use of residuals in <code>predict()</code> and <code>backtesting_forecaster()</code>:</p> <ul> <li><p><code>use_in_sample_residuals</code>: If <code>True</code>, the in-sample residuals are used to compute the prediction intervals. Since these residuals are obtained from the training set, they are always available, but usually lead to overoptimistic intervals. If <code>False</code>, the out-sample residuals (calibration) are used to calculate the prediction intervals. These residuals are obtained from the validation/calibration set and are only available if the <code>set_out_sample_residuals()</code> method has been called. It is recommended to use out-sample residuals to achieve the desired coverage.</p> </li> <li><p><code>use_binned_residuals</code>: If <code>False</code>, a single correction factor is applied to all predictions during conformalization. If <code>True</code>, the conformalization process uses a correction factor that depends on the bin where the prediction falls. This can be thought as a type of Adaptive Conformal Predictions and it can lead to more accurate prediction intervals since the correction factor is conditioned on the region of the prediction space.</p> </li> </ul> In\u00a0[11]: Copied! <pre># Plot intervals\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_test,\n    target_variable     = \"OT\",\n    title               = \"Predicted intervals\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.4, 'zorder': 1}\n)\n</pre> # Plot intervals # ============================================================================== plot_prediction_intervals(     predictions         = predictions,     y_true              = data_test,     target_variable     = \"OT\",     title               = \"Predicted intervals\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.4, 'zorder': 1} ) In\u00a0[12]: Copied! <pre># Plot intervals with zoom ['2018-05-01', '2018-05-15']\n# ==============================================================================\nplot_prediction_intervals(\n    predictions         = predictions,\n    y_true              = data_test,\n    target_variable     = \"OT\",\n    initial_x_zoom      = ['2018-05-01', '2018-05-15'],\n    title               = \"Predicted intervals\",\n    kwargs_fill_between = {'color': 'gray', 'alpha': 0.4, 'zorder': 1}\n);\n</pre> # Plot intervals with zoom ['2018-05-01', '2018-05-15'] # ============================================================================== plot_prediction_intervals(     predictions         = predictions,     y_true              = data_test,     target_variable     = \"OT\",     initial_x_zoom      = ['2018-05-01', '2018-05-15'],     title               = \"Predicted intervals\",     kwargs_fill_between = {'color': 'gray', 'alpha': 0.4, 'zorder': 1} ); In\u00a0[13]: Copied! <pre># Predicted interval coverage (on test data)\n# ==============================================================================\ncoverage = calculate_coverage(\n               y_true      = data.loc[end_calibration:, 'OT'],\n               lower_bound = predictions[\"lower_bound\"], \n               upper_bound = predictions[\"upper_bound\"]\n           )\nprint(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")\n\n# Area of the interval\n# ==============================================================================\narea = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum()\nprint(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Predicted interval coverage (on test data) # ============================================================================== coverage = calculate_coverage(                y_true      = data.loc[end_calibration:, 'OT'],                lower_bound = predictions[\"lower_bound\"],                 upper_bound = predictions[\"upper_bound\"]            ) print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")  # Area of the interval # ============================================================================== area = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum() print(f\"Area of the interval: {round(area, 2)}\") <pre>Predicted interval coverage: 79.32 %\nArea of the interval: 38409.35\n</pre> <p>The prediction intervals generated using conformal prediction achieve an empirical coverage very close to the nominal coverage of 80%.</p> <p> \u26a0 Warning </p> <p> Probabilistic forecasting in production <p>The correct estimation of prediction intervals with conformal methods depends on the residuals being representative of future errors. For this reason, calibration residuals should be used. However, the dynamics of the series and models can change over time, so it is important to monitor and regularly update the residuals. It can be done easily using the <code>set_out_sample_residuals()</code> method.</p> </p>"},{"location":"user_guides/probabilistic-forecasting-conformal-prediction.html#probabilistic-forecasting-conformal-prediction","title":"Probabilistic Forecasting: Conformal Prediction\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-conformal-prediction.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-conformal-prediction.html#calibration-residuals","title":"Calibration residuals\u00b6","text":"<p>To address the issue of overoptimistic intervals, it is recomended to use out-sample residuals. These are residuals from a calibration set, which contains data not seen during training. These residuals can be obtained through backtesting.</p>"},{"location":"user_guides/probabilistic-forecasting-global-models.html","title":"Probabilistic global models","text":"<p>Skforecast allows to apply all its implemented probabilistic forecasting methods (bootstrapping, conformal prediction and quantile regression) to global models. This means that the model is trained with all the available time series and the forecast is made for all the time series.</p> <p>For detailed information about the available probabilistic forecasting methods, see the following user guides:</p> <ul> <li><p>Probabilistic forecasting: Overview</p> </li> <li><p>Probabilistic forecasting: Bootstrapping</p> </li> <li><p>Probabilistic forecasting: Conformal prediction</p> </li> <li><p>Probabilistic forecasting: Conformal Calibration</p> </li> <li><p>Probabilistic forecasting: Quantile regression</p> </li> </ul> <p> \ud83d\udca1 Tip </p> <p> When predicting multiple series, scaling to hundreds or thousands of series, computational time can be a bottleneck. For these cases, using the conformal framework can be a good option, as it is faster than the bootstrapping and quantile regression methods.  <p>In case of using bootstrapping, note that if <code>use_binned_residuals</code> is set to <code>True</code>, the computational time will be even higher.</p> </p> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme, plot_prediction_intervals\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom itertools import cycle\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster_multiseries\nfrom skforecast.metrics import calculate_coverage\nfrom skforecast.plot import calculate_lag_autocorrelation\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('once')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd from skforecast.datasets import fetch_dataset  # Plots # ============================================================================== import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme, plot_prediction_intervals from statsmodels.graphics.tsaplots import plot_pacf from itertools import cycle  # Modelling and Forecasting # ============================================================================== from lightgbm import LGBMRegressor from skforecast.recursive import ForecasterRecursiveMultiSeries from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster_multiseries from skforecast.metrics import calculate_coverage from skforecast.plot import calculate_lag_autocorrelation from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer  # Configuration # ============================================================================== import warnings warnings.filterwarnings('once') In\u00a0[2]: Copied! <pre># Data\n# ==============================================================================\ndata = fetch_dataset(name=\"bdg2_hourly_sample\")\ndata = data.loc['2016-01-01 00:00:00' : '2016-09-30 00:00:00', :]\ndata.head(2)\n</pre> # Data # ============================================================================== data = fetch_dataset(name=\"bdg2_hourly_sample\") data = data.loc['2016-01-01 00:00:00' : '2016-09-30 00:00:00', :] data.head(2) <pre>bdg2_hourly_sample\n------------------\nDaily energy consumption data of two buildings sampled from the The Building\nData Genome Project 2. https://github.com/buds-lab/building-data-genome-\nproject-2\nMiller, C., Kathirgamanathan, A., Picchetti, B. et al. The Building Data Genome\nProject 2, energy meter data from the ASHRAE Great Energy Predictor III\ncompetition. Sci Data 7, 368 (2020). https://doi.org/10.1038/s41597-020-00712-x\nShape of the dataset: (6553, 2)\n</pre> Out[2]: building_1 building_2 timestamp 2016-01-01 00:00:00 186.532 219.27 2016-01-01 01:00:00 186.532 219.27 In\u00a0[3]: Copied! <pre># Calendar features\n# ==============================================================================\ndata['day_of_week'] = data.index.dayofweek\ndata['hour'] = data.index.hour\n\ntransformer = ColumnTransformer(\n    transformers=[\n        ('one_hot_encoder', OneHotEncoder(sparse_output=False), ['day_of_week', 'hour'])\n    ],\n    remainder='passthrough',\n    verbose_feature_names_out=False\n).set_output(transform='pandas')\n\ndata = transformer.fit_transform(data)\ndata.head()\n</pre> # Calendar features # ============================================================================== data['day_of_week'] = data.index.dayofweek data['hour'] = data.index.hour  transformer = ColumnTransformer(     transformers=[         ('one_hot_encoder', OneHotEncoder(sparse_output=False), ['day_of_week', 'hour'])     ],     remainder='passthrough',     verbose_feature_names_out=False ).set_output(transform='pandas')  data = transformer.fit_transform(data) data.head() Out[3]: day_of_week_0 day_of_week_1 day_of_week_2 day_of_week_3 day_of_week_4 day_of_week_5 day_of_week_6 hour_0 hour_1 hour_2 ... hour_16 hour_17 hour_18 hour_19 hour_20 hour_21 hour_22 hour_23 building_1 building_2 timestamp 2016-01-01 00:00:00 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 186.532 219.270 2016-01-01 01:00:00 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 186.532 219.270 2016-01-01 02:00:00 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 183.479 218.348 2016-01-01 03:00:00 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 184.303 220.003 2016-01-01 04:00:00 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 183.657 217.731 <p>5 rows \u00d7 33 columns</p> In\u00a0[4]: Copied! <pre># Plot time series\n# ==============================================================================\nset_dark_theme()\nplt.rcParams['lines.linewidth'] = 0.5\ncolors = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\nfig, axs = plt.subplots(2, 1, figsize=(7, 4), sharex=True)\nseries = ['building_1', 'building_2']\nfor i, col in enumerate(series):\n    axs[i].plot(data[col], label=col, color=next(colors))\n    axs[i].legend(loc='lower left', fontsize=8)\n    axs[i].tick_params(axis='both', labelsize=8)\n\nplt.tight_layout()\n</pre> # Plot time series # ============================================================================== set_dark_theme() plt.rcParams['lines.linewidth'] = 0.5 colors = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color']) fig, axs = plt.subplots(2, 1, figsize=(7, 4), sharex=True) series = ['building_1', 'building_2'] for i, col in enumerate(series):     axs[i].plot(data[col], label=col, color=next(colors))     axs[i].legend(loc='lower left', fontsize=8)     axs[i].tick_params(axis='both', labelsize=8)  plt.tight_layout() In\u00a0[5]: Copied! <pre># Partial autocorrelation values and plots\n# ==============================================================================\nn_lags = 7 * 24\npacf_df = []\n\nfig, axs = plt.subplots(2, 1, figsize=(6, 3))\naxs = axs.ravel()\nfor i, col in enumerate(series):\n    pacf_values = calculate_lag_autocorrelation(data[col], n_lags=n_lags)\n    pacf_values['variable'] = col\n    pacf_df.append(pacf_values)\n    \n    plot_pacf(data[col], lags=n_lags, ax=axs[i])\n    axs[i].set_title(col, fontsize=10)\n    axs[i].set_ylim(-0.5, 1.1)\n\nplt.tight_layout()\n</pre> # Partial autocorrelation values and plots # ============================================================================== n_lags = 7 * 24 pacf_df = []  fig, axs = plt.subplots(2, 1, figsize=(6, 3)) axs = axs.ravel() for i, col in enumerate(series):     pacf_values = calculate_lag_autocorrelation(data[col], n_lags=n_lags)     pacf_values['variable'] = col     pacf_df.append(pacf_values)          plot_pacf(data[col], lags=n_lags, ax=axs[i])     axs[i].set_title(col, fontsize=10)     axs[i].set_ylim(-0.5, 1.1)  plt.tight_layout() In\u00a0[6]: Copied! <pre># Top n lags with highest absolute partial autocorrelation per variable\n# ==============================================================================\nn = 10\ntop_lags = set()\nfor pacf_values in pacf_df:\n    variable = pacf_values['variable'].iloc[0]\n    lags = pacf_values.nlargest(n, 'partial_autocorrelation_abs')['lag'].sort_values().tolist()\n    top_lags.update(lags)\n    print(f\"{variable}: {lags}\")\n\ntop_lags = list(top_lags)\ntop_lags.sort()\nprint(f\"\\nAll lags: {top_lags}\")\n</pre> # Top n lags with highest absolute partial autocorrelation per variable # ============================================================================== n = 10 top_lags = set() for pacf_values in pacf_df:     variable = pacf_values['variable'].iloc[0]     lags = pacf_values.nlargest(n, 'partial_autocorrelation_abs')['lag'].sort_values().tolist()     top_lags.update(lags)     print(f\"{variable}: {lags}\")  top_lags = list(top_lags) top_lags.sort() print(f\"\\nAll lags: {top_lags}\") <pre>building_1: [1, 2, 3, 15, 16, 19, 24, 25, 136, 145]\nbuilding_2: [1, 2, 3, 20, 21, 22, 25, 26, 145, 166]\n\nAll lags: [1, 2, 3, 15, 16, 19, 20, 21, 22, 24, 25, 26, 136, 145, 166]\n</pre> <p>The target series exhibit similar dynamics with several lagged correlations that can be used as predictors.</p> In\u00a0[7]: Copied! <pre># Split train-validation-test\n# ==============================================================================\nend_train = '2016-07-01 23:59:00'\nend_validation = '2016-09-01 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_val   = data.loc[end_train:end_validation, :]\ndata_test  = data.loc[end_validation:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-validation-test # ============================================================================== end_train = '2016-07-01 23:59:00' end_validation = '2016-09-01 23:59:00' data_train = data.loc[: end_train, :] data_val   = data.loc[end_train:end_validation, :] data_test  = data.loc[end_validation:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 2016-01-01 00:00:00 --- 2016-07-01 23:00:00  (n=4392)\nDates validation : 2016-07-02 00:00:00 --- 2016-09-01 23:00:00  (n=1488)\nDates test       : 2016-09-02 00:00:00 --- 2016-09-30 00:00:00  (n=673)\n</pre> In\u00a0[8]: Copied! <pre># Plot partitions\n# ==============================================================================\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\nfig, axs = plt.subplots(2, 1, figsize=(7, 5), sharex=True)\nfor i, col in enumerate(series):\n    axs[i].plot(data[col], label=col, color=colors[i])\n    axs[i].legend(loc='lower left', fontsize=8)\n    axs[i].tick_params(axis='both', labelsize=8)\n    axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train\n    axs[i].axvline(pd.to_datetime(end_validation), color='white', linestyle='--', linewidth=1)  # End validation\n\nplt.tight_layout()\n</pre> # Plot partitions # ============================================================================== colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] fig, axs = plt.subplots(2, 1, figsize=(7, 5), sharex=True) for i, col in enumerate(series):     axs[i].plot(data[col], label=col, color=colors[i])     axs[i].legend(loc='lower left', fontsize=8)     axs[i].tick_params(axis='both', labelsize=8)     axs[i].axvline(pd.to_datetime(end_train), color='white', linestyle='--', linewidth=1)  # End train     axs[i].axvline(pd.to_datetime(end_validation), color='white', linestyle='--', linewidth=1)  # End validation  plt.tight_layout() In\u00a0[9]: Copied! <pre># Create forecaster\n# ==============================================================================\nexog_features = data.columns[data.columns.str.contains('day_of_|hour_')].tolist()\n\nparams = {\n    \"n_estimators\": 300,\n    \"learning_rate\": 0.05,\n    \"max_depth\": 5,\n}\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor = LGBMRegressor(random_state=15926, verbose=-1, **params),\n                 lags      = top_lags,\n                 encoding  = 'ordinal',\n             )\n</pre> # Create forecaster # ============================================================================== exog_features = data.columns[data.columns.str.contains('day_of_|hour_')].tolist()  params = {     \"n_estimators\": 300,     \"learning_rate\": 0.05,     \"max_depth\": 5, } forecaster = ForecasterRecursiveMultiSeries(                  regressor = LGBMRegressor(random_state=15926, verbose=-1, **params),                  lags      = top_lags,                  encoding  = 'ordinal',              ) In\u00a0[10]: Copied! <pre># Backtesting on validation data to obtain out-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         initial_train_size = len(data.loc[:end_train, :]),\n         steps              = 24,\n     )\n\nmetric_val, predictions_val = backtesting_forecaster_multiseries(\n                                  forecaster = forecaster,\n                                  series     = data.loc[:end_validation, series],\n                                  exog       = data.loc[:end_validation, exog_features],\n                                  cv         = cv,\n                                  metric     = 'mean_absolute_error'\n                              )\n</pre> # Backtesting on validation data to obtain out-sample residuals # ============================================================================== cv = TimeSeriesFold(          initial_train_size = len(data.loc[:end_train, :]),          steps              = 24,      )  metric_val, predictions_val = backtesting_forecaster_multiseries(                                   forecaster = forecaster,                                   series     = data.loc[:end_validation, series],                                   exog       = data.loc[:end_validation, exog_features],                                   cv         = cv,                                   metric     = 'mean_absolute_error'                               ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>  0%|          | 0/62 [00:00&lt;?, ?it/s]</pre> In\u00a0[11]: Copied! <pre># Plot predictions on validation data\n# ==============================================================================\nfig, axs = plt.subplots(2, 1, figsize=(7, 5.5), sharex=True, sharey=False)\nfor i, level in enumerate(predictions_val['level'].unique()):\n    predictions_val_level = predictions_val.loc[predictions_val['level'] == level, 'pred']\n    data.loc[end_train:end_validation, level].plot(ax=axs[i], label='Real value')\n    predictions_val_level.plot(ax=axs[i], label='prediction')\n    axs[i].set_xlabel(\"\")\n    axs[i].set_title(level)\n    axs[i].legend(loc='lower right', fontsize=7)\n\nplt.tight_layout()\n</pre> # Plot predictions on validation data # ============================================================================== fig, axs = plt.subplots(2, 1, figsize=(7, 5.5), sharex=True, sharey=False) for i, level in enumerate(predictions_val['level'].unique()):     predictions_val_level = predictions_val.loc[predictions_val['level'] == level, 'pred']     data.loc[end_train:end_validation, level].plot(ax=axs[i], label='Real value')     predictions_val_level.plot(ax=axs[i], label='prediction')     axs[i].set_xlabel(\"\")     axs[i].set_title(level)     axs[i].legend(loc='lower right', fontsize=7)  plt.tight_layout() In\u00a0[12]: Copied! <pre># Out-sample residuals distribution\n# ==============================================================================\nfig, axs = plt.subplots(2, 1, figsize=(7, 4), sharex=True, sharey=False)\nfor i, level in enumerate(predictions_val[\"level\"].unique()):\n    residuals = (\n        data.loc[predictions_val.index, level]\n        - predictions_val.loc[predictions_val[\"level\"] == level, \"pred\"]\n    )\n    residuals.plot(ax=axs[i], label=f\"Residuals {level}\")\n    axs[i].legend(loc=\"upper right\", fontsize=7)\n\nfig.tight_layout()\n</pre> # Out-sample residuals distribution # ============================================================================== fig, axs = plt.subplots(2, 1, figsize=(7, 4), sharex=True, sharey=False) for i, level in enumerate(predictions_val[\"level\"].unique()):     residuals = (         data.loc[predictions_val.index, level]         - predictions_val.loc[predictions_val[\"level\"] == level, \"pred\"]     )     residuals.plot(ax=axs[i], label=f\"Residuals {level}\")     axs[i].legend(loc=\"upper right\", fontsize=7)  fig.tight_layout() In\u00a0[13]: Copied! <pre># Store out-sample residuals in the forecaster\n# ==============================================================================\nforecaster.fit(\n    series = data.loc[:end_train, series],\n    exog   = data.loc[:end_train, exog_features]\n)\nforecaster.set_out_sample_residuals(\n    y_true = {k: data.loc[predictions_val.index.unique(), k] for k in series}, \n    y_pred = {k: v for k, v in predictions_val.groupby('level')['pred']}\n)\n</pre> # Store out-sample residuals in the forecaster # ============================================================================== forecaster.fit(     series = data.loc[:end_train, series],     exog   = data.loc[:end_train, exog_features] ) forecaster.set_out_sample_residuals(     y_true = {k: data.loc[predictions_val.index.unique(), k] for k in series},      y_pred = {k: v for k, v in predictions_val.groupby('level')['pred']} ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <p> \u270e Note </p> <p>When new residuals are stored in the forecaster, the residuals are binned according to the size of the predictions to which they correspond. Later, in the bootstrapping process, the residuals are sampled from the appropriate bin to ensure that the distribution of the residuals is consistent with the predictions. This process allows for more accurate prediction intervals because the residuals are more closely aligned with the predictions to which they correspond, resulting in better coverage with narrower intervals. For more information about how residuals are used in interval estimation visit Probabilistic forecasting: Bootstrapped Residuals.</p> In\u00a0[14]: Copied! <pre># Intervals of the residual bins (conditioned on predicted values) for each level\n# ==============================================================================\nfrom pprint import pprint\n\nfor k, v in forecaster.binner_intervals_.items():\n    print(k)\n    pprint(v)\n    print(\"\")\n</pre> # Intervals of the residual bins (conditioned on predicted values) for each level # ============================================================================== from pprint import pprint  for k, v in forecaster.binner_intervals_.items():     print(k)     pprint(v)     print(\"\") <pre>building_1\n{0: (162.55654558304457, 188.36664987311164),\n 1: (188.36664987311164, 192.69194794958057),\n 2: (192.69194794958057, 196.33152734653578),\n 3: (196.33152734653578, 200.63335799466273),\n 4: (200.63335799466273, 205.69014402995123),\n 5: (205.69014402995123, 212.73488176715347),\n 6: (212.73488176715347, 223.7347135798683),\n 7: (223.7347135798683, 247.49425709088368),\n 8: (247.49425709088368, 257.45057166569234),\n 9: (257.45057166569234, 292.2588224363378)}\n\nbuilding_2\n{0: (191.2489257150324, 221.78927964090707),\n 1: (221.78927964090707, 227.7092976710155),\n 2: (227.7092976710155, 231.21030346461032),\n 3: (231.21030346461032, 234.22923980033235),\n 4: (234.22923980033235, 237.49248298694857),\n 5: (237.49248298694857, 241.4080869629871),\n 6: (241.4080869629871, 260.62031922270853),\n 7: (260.62031922270853, 288.5886870651536),\n 8: (288.5886870651536, 299.908273995279),\n 9: (299.908273995279, 317.16060597117587)}\n\n_unknown_level\n{0: (162.55654558304457, 192.66325275631186),\n 1: (192.66325275631186, 200.60633333417312),\n 2: (200.60633333417312, 211.4832393790082),\n 3: (211.4832393790082, 222.46785614535224),\n 4: (222.46785614535224, 230.27647724100552),\n 5: (230.27647724100552, 235.8292109781876),\n 6: (235.8292109781876, 242.7889817619577),\n 7: (242.7889817619577, 258.04911594281583),\n 8: (258.04911594281583, 288.66360939645296),\n 9: (288.66360939645296, 317.16060597117587)}\n\n</pre> In\u00a0[15]: Copied! <pre># Distribution of the residual by bin and level\n# ==============================================================================\nfig, axs = plt.subplots(2, 2, figsize=(7, 5), sharex=True, sharey=True)\naxs = axs.ravel()\nfor i, level in enumerate(forecaster.out_sample_residuals_by_bin_):\n    out_sample_residuals_by_bin_df = pd.DataFrame(\n        dict(\n            [(k, pd.Series(v))\n            for k, v in forecaster.out_sample_residuals_by_bin_[level].items()]\n        )\n    )\n    flierprops = dict(marker='o', markerfacecolor='gray', markersize=5, linestyle='none')\n    out_sample_residuals_by_bin_df.boxplot(ax=axs[i], flierprops=flierprops)\n    axs[i].set_title(level, fontsize=10)\n\nfig.suptitle(\"Distribution of residuals by bin\", fontsize=12)\nfig.tight_layout()\n</pre> # Distribution of the residual by bin and level # ============================================================================== fig, axs = plt.subplots(2, 2, figsize=(7, 5), sharex=True, sharey=True) axs = axs.ravel() for i, level in enumerate(forecaster.out_sample_residuals_by_bin_):     out_sample_residuals_by_bin_df = pd.DataFrame(         dict(             [(k, pd.Series(v))             for k, v in forecaster.out_sample_residuals_by_bin_[level].items()]         )     )     flierprops = dict(marker='o', markerfacecolor='gray', markersize=5, linestyle='none')     out_sample_residuals_by_bin_df.boxplot(ax=axs[i], flierprops=flierprops)     axs[i].set_title(level, fontsize=10)  fig.suptitle(\"Distribution of residuals by bin\", fontsize=12) fig.tight_layout() <p> \u270e Note </p> <p>Two arguments control the use of residuals in <code>predict()</code> and <code>backtesting_forecaster()</code>:</p> <ul> <li><p><code>use_in_sample_residuals</code>: If <code>True</code>, the in-sample residuals are used to compute the prediction intervals. Since these residuals are obtained from the training set, they are always available, but usually lead to overoptimistic intervals. If <code>False</code>, the out-sample residuals (calibration) are used to calculate the prediction intervals. These residuals are obtained from the validation/calibration set and are only available if the <code>set_out_sample_residuals()</code> method has been called. It is recommended to use out-sample residuals to achieve the desired coverage.</p> </li> <li><p><code>use_binned_residuals</code>: If <code>False</code>, a single correction factor is applied to all predictions during conformalization. If <code>True</code>, the conformalization process uses a correction factor that depends on the bin where the prediction falls. This can be thought as a type of Adaptive Conformal Predictions and it can lead to more accurate prediction intervals since the correction factor is conditioned on the region of the prediction space.</p> </li> </ul> In\u00a0[16]: Copied! <pre># Backtesting with conformal prediction intervals\n# ==============================================================================\ncv = TimeSeriesFold(\n         initial_train_size = len(data.loc[:end_validation, :]),\n         steps              = 24,\n     )\n\nmetric, predictions_test = backtesting_forecaster_multiseries(\n                               forecaster              = forecaster,\n                               series                  = data[series],\n                               exog                    = data[exog_features],\n                               cv                      = cv,\n                               metric                  = 'mean_absolute_error',\n                               interval                = [10, 90],\n                               interval_method         = \"conformal\",\n                               use_in_sample_residuals = False,  # Use out-sample residuals\n                               use_binned_residuals    = True    # Adaptative intervals\n                           )\npredictions_test\n</pre> # Backtesting with conformal prediction intervals # ============================================================================== cv = TimeSeriesFold(          initial_train_size = len(data.loc[:end_validation, :]),          steps              = 24,      )  metric, predictions_test = backtesting_forecaster_multiseries(                                forecaster              = forecaster,                                series                  = data[series],                                exog                    = data[exog_features],                                cv                      = cv,                                metric                  = 'mean_absolute_error',                                interval                = [10, 90],                                interval_method         = \"conformal\",                                use_in_sample_residuals = False,  # Use out-sample residuals                                use_binned_residuals    = True    # Adaptative intervals                            ) predictions_test <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>  0%|          | 0/29 [00:00&lt;?, ?it/s]</pre> Out[16]: level pred lower_bound upper_bound 2016-09-02 00:00:00 building_1 198.654360 189.636937 207.671784 2016-09-02 00:00:00 building_2 204.937493 195.752042 214.122945 2016-09-02 01:00:00 building_1 195.443918 186.426494 204.461342 2016-09-02 01:00:00 building_2 202.337314 193.151862 211.522766 2016-09-02 02:00:00 building_1 192.120380 184.568154 199.672607 ... ... ... ... ... 2016-09-29 22:00:00 building_2 214.340279 205.154827 223.525730 2016-09-29 23:00:00 building_1 203.202476 193.458915 212.946037 2016-09-29 23:00:00 building_2 211.105410 201.919958 220.290861 2016-09-30 00:00:00 building_1 200.085366 190.341805 209.828928 2016-09-30 00:00:00 building_2 204.815233 195.629781 214.000684 <p>1346 rows \u00d7 4 columns</p> In\u00a0[17]: Copied! <pre># Plot intervals and calculate coverage for each level\n# ==============================================================================\nfor level in predictions_test['level'].unique():\n    print(f\"\\nLevel: {level}\")\n    predictions_level = (\n        predictions_test[predictions_test[\"level\"] == level]\n        .drop(columns=\"level\")\n        .copy()\n    )\n    \n    # Plot intervals\n    plt.rcParams['lines.linewidth'] = 1\n    fig, ax = plt.subplots(figsize=(7, 3))\n    plot_prediction_intervals(\n        predictions     = predictions_level,\n        y_true          = data_test[[level]],\n        target_variable = level,\n        initial_x_zoom  = None,\n        title           = \"Prediction intervals\",\n        xaxis_title     = \"\",\n        yaxis_title     = level,\n        ax              = ax\n    )\n    plt.gca().legend(loc='upper left')\n    fill_between_obj = ax.collections[0]\n    fill_between_obj.set_facecolor('white')\n    fill_between_obj.set_alpha(0.3)\n\n    # Predicted interval coverage (on test data)\n    coverage = calculate_coverage(\n                   y_true      = data_test[level],\n                   lower_bound = predictions_level[\"lower_bound\"], \n                   upper_bound = predictions_level[\"upper_bound\"]\n               )\n    print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")\n\n    # Area of the interval\n    area = (predictions_level[\"upper_bound\"] - predictions_level[\"lower_bound\"]).sum()\n    print(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Plot intervals and calculate coverage for each level # ============================================================================== for level in predictions_test['level'].unique():     print(f\"\\nLevel: {level}\")     predictions_level = (         predictions_test[predictions_test[\"level\"] == level]         .drop(columns=\"level\")         .copy()     )          # Plot intervals     plt.rcParams['lines.linewidth'] = 1     fig, ax = plt.subplots(figsize=(7, 3))     plot_prediction_intervals(         predictions     = predictions_level,         y_true          = data_test[[level]],         target_variable = level,         initial_x_zoom  = None,         title           = \"Prediction intervals\",         xaxis_title     = \"\",         yaxis_title     = level,         ax              = ax     )     plt.gca().legend(loc='upper left')     fill_between_obj = ax.collections[0]     fill_between_obj.set_facecolor('white')     fill_between_obj.set_alpha(0.3)      # Predicted interval coverage (on test data)     coverage = calculate_coverage(                    y_true      = data_test[level],                    lower_bound = predictions_level[\"lower_bound\"],                     upper_bound = predictions_level[\"upper_bound\"]                )     print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")      # Area of the interval     area = (predictions_level[\"upper_bound\"] - predictions_level[\"lower_bound\"]).sum()     print(f\"Area of the interval: {round(area, 2)}\") <pre>\nLevel: building_1\nPredicted interval coverage: 83.8 %\nArea of the interval: 12929.54\n\nLevel: building_2\nPredicted interval coverage: 78.75 %\nArea of the interval: 13614.4\n</pre> In\u00a0[18]: Copied! <pre># Backtesting with prediction intervals using bootstrapping\n# ==============================================================================\ncv = TimeSeriesFold(\n         initial_train_size = len(data.loc[:end_validation, :]),\n         steps              = 24,\n     )\n\nmetric, predictions_test = backtesting_forecaster_multiseries(\n                               forecaster              = forecaster,\n                               series                  = data[series],\n                               exog                    = data[exog_features],\n                               cv                      = cv,\n                               metric                  = 'mean_absolute_error',\n                               interval                = [10, 90],\n                               interval_method         = \"bootstrapping\",\n                               n_boot                  = 150,\n                               use_in_sample_residuals = False,  # Use out-sample residuals\n                               use_binned_residuals    = True    # Residuals conditioned on predicted values\n                           )\npredictions_test\n</pre> # Backtesting with prediction intervals using bootstrapping # ============================================================================== cv = TimeSeriesFold(          initial_train_size = len(data.loc[:end_validation, :]),          steps              = 24,      )  metric, predictions_test = backtesting_forecaster_multiseries(                                forecaster              = forecaster,                                series                  = data[series],                                exog                    = data[exog_features],                                cv                      = cv,                                metric                  = 'mean_absolute_error',                                interval                = [10, 90],                                interval_method         = \"bootstrapping\",                                n_boot                  = 150,                                use_in_sample_residuals = False,  # Use out-sample residuals                                use_binned_residuals    = True    # Residuals conditioned on predicted values                            ) predictions_test <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>  0%|          | 0/29 [00:00&lt;?, ?it/s]</pre> Out[18]: level pred lower_bound upper_bound 2016-09-02 00:00:00 building_1 198.654360 193.703464 209.075452 2016-09-02 00:00:00 building_2 204.937493 194.825202 212.585027 2016-09-02 01:00:00 building_1 195.443918 186.221740 209.323422 2016-09-02 01:00:00 building_2 202.337314 188.617664 213.909010 2016-09-02 02:00:00 building_1 192.120380 179.659126 206.835901 ... ... ... ... ... 2016-09-29 22:00:00 building_2 214.340279 198.380518 237.528369 2016-09-29 23:00:00 building_1 203.202476 192.659555 258.594803 2016-09-29 23:00:00 building_2 211.105410 193.229093 238.896552 2016-09-30 00:00:00 building_1 200.085366 188.294833 208.383293 2016-09-30 00:00:00 building_2 204.815233 193.892343 214.680595 <p>1346 rows \u00d7 4 columns</p> In\u00a0[19]: Copied! <pre># Plot intervals and calculate coverage for each level\n# ==============================================================================\nfor level in predictions_test[\"level\"].unique():\n    print(f\"\\nLevel: {level}\")\n    predictions_level = (\n        predictions_test[predictions_test[\"level\"] == level]\n        .drop(columns=\"level\")\n        .copy()\n    )\n\n    # Plot intervals\n    plt.rcParams[\"lines.linewidth\"] = 1\n    fig, ax = plt.subplots(figsize=(7, 3))\n    plot_prediction_intervals(\n        predictions     = predictions_level,\n        y_true          = data_test[[level]],\n        target_variable = level,\n        initial_x_zoom  = None,\n        title           = \"Prediction intervals\",\n        xaxis_title     = \"\",\n        yaxis_title     = level,\n        ax              = ax\n    )\n    plt.gca().legend(loc=\"upper left\", fontsize=7)\n    fill_between_obj = ax.collections[0]\n    fill_between_obj.set_facecolor(\"white\")\n    fill_between_obj.set_alpha(0.3)\n\n    # Predicted interval coverage (on test data)\n    coverage = calculate_coverage(\n                   y_true      = data_test[level],\n                   lower_bound = predictions_level[\"lower_bound\"], \n                   upper_bound = predictions_level[\"upper_bound\"]\n               )\n    print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")\n\n    # Area of the interval\n    area = (predictions_level[\"upper_bound\"] - predictions_level[\"lower_bound\"]).sum()\n    print(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Plot intervals and calculate coverage for each level # ============================================================================== for level in predictions_test[\"level\"].unique():     print(f\"\\nLevel: {level}\")     predictions_level = (         predictions_test[predictions_test[\"level\"] == level]         .drop(columns=\"level\")         .copy()     )      # Plot intervals     plt.rcParams[\"lines.linewidth\"] = 1     fig, ax = plt.subplots(figsize=(7, 3))     plot_prediction_intervals(         predictions     = predictions_level,         y_true          = data_test[[level]],         target_variable = level,         initial_x_zoom  = None,         title           = \"Prediction intervals\",         xaxis_title     = \"\",         yaxis_title     = level,         ax              = ax     )     plt.gca().legend(loc=\"upper left\", fontsize=7)     fill_between_obj = ax.collections[0]     fill_between_obj.set_facecolor(\"white\")     fill_between_obj.set_alpha(0.3)      # Predicted interval coverage (on test data)     coverage = calculate_coverage(                    y_true      = data_test[level],                    lower_bound = predictions_level[\"lower_bound\"],                     upper_bound = predictions_level[\"upper_bound\"]                )     print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")      # Area of the interval     area = (predictions_level[\"upper_bound\"] - predictions_level[\"lower_bound\"]).sum()     print(f\"Area of the interval: {round(area, 2)}\") <pre>\nLevel: building_1\nPredicted interval coverage: 93.02 %\nArea of the interval: 32018.44\n\nLevel: building_2\nPredicted interval coverage: 95.69 %\nArea of the interval: 29612.23\n</pre> <p>The resulting intervals are overly conservative, they have an empirical coverage higher than expected (80%).</p> In\u00a0[20]: Copied! <pre># Predictions for an known series\n# ==============================================================================\n# Simulate last_window of the new series\nlen_last_window = forecaster.max_lag\nlast_window = pd.DataFrame({\n    'new_series': data_val['building_1'].iloc[-len_last_window:] + np.random.normal(0, 0.1, len_last_window)\n})\n\nforecaster.predict_interval(\n    steps                   = 24,\n    last_window             = last_window,\n    exog                    = data_test[exog_features],\n    method                  = 'conformal',\n    interval                = [10, 90],\n    use_in_sample_residuals = False\n)\n</pre> # Predictions for an known series # ============================================================================== # Simulate last_window of the new series len_last_window = forecaster.max_lag last_window = pd.DataFrame({     'new_series': data_val['building_1'].iloc[-len_last_window:] + np.random.normal(0, 0.1, len_last_window) })  forecaster.predict_interval(     steps                   = 24,     last_window             = last_window,     exog                    = data_test[exog_features],     method                  = 'conformal',     interval                = [10, 90],     use_in_sample_residuals = False ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 UnknownLevelWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `levels` {'new_series'} were not included in training. Unknown levels are encoded as \u2502\n\u2502 NaN, which may cause the prediction to fail if the regressor does not accept NaN     \u2502\n\u2502 values.                                                                              \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : UnknownLevelWarning                                                       \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:910                                                              \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=UnknownLevelWarning)             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 UnknownLevelWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 `levels` {'new_series'} are not present in                                           \u2502\n\u2502 `forecaster.out_sample_residuals_by_bin_`. A random sample of the residuals from     \u2502\n\u2502 other levels will be used. This can lead to inaccurate intervals for the unknown     \u2502\n\u2502 levels. Otherwise, Use the `set_out_sample_residuals()` method before predicting to  \u2502\n\u2502 set the residuals for these levels.                                                  \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : UnknownLevelWarning                                                       \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:1304                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=UnknownLevelWarning)             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[20]: level pred lower_bound upper_bound 2016-09-02 00:00:00 new_series 199.261711 190.630329 207.893093 2016-09-02 01:00:00 new_series 196.635219 188.003837 205.266601 2016-09-02 02:00:00 new_series 193.146228 184.514846 201.777610 2016-09-02 03:00:00 new_series 191.096731 182.998267 199.195196 2016-09-02 04:00:00 new_series 190.758139 182.659675 198.856604 2016-09-02 05:00:00 new_series 190.190634 182.092170 198.289099 2016-09-02 06:00:00 new_series 191.960193 183.861728 200.058657 2016-09-02 07:00:00 new_series 200.684158 191.357070 210.011246 2016-09-02 08:00:00 new_series 220.888254 211.254333 230.522175 2016-09-02 09:00:00 new_series 239.945580 225.287244 254.603916 2016-09-02 10:00:00 new_series 248.945741 234.623388 263.268094 2016-09-02 11:00:00 new_series 253.711857 239.389503 268.034210 2016-09-02 12:00:00 new_series 254.023096 239.700742 268.345449 2016-09-02 13:00:00 new_series 254.873665 240.551312 269.196018 2016-09-02 14:00:00 new_series 255.210915 240.888562 269.533269 2016-09-02 15:00:00 new_series 252.259916 237.937562 266.582269 2016-09-02 16:00:00 new_series 248.189977 233.867624 262.512330 2016-09-02 17:00:00 new_series 237.257947 222.599611 251.916283 2016-09-02 18:00:00 new_series 227.134771 216.891571 237.377972 2016-09-02 19:00:00 new_series 216.010340 206.376419 225.644261 2016-09-02 20:00:00 new_series 211.473445 202.146356 220.800533 2016-09-02 21:00:00 new_series 210.180947 200.853858 219.508035 2016-09-02 22:00:00 new_series 207.999901 198.672813 217.326990 2016-09-02 23:00:00 new_series 204.757490 195.430402 214.084578 <p> \u26a0 Warning </p> <p> <p>The correct estimation of prediction intervals with conformal methods depends on the residuals being representative of future errors. For this reason, calibration residuals should be used. However, the dynamics of the series and models can change over time, so it is important to monitor and regularly update the residuals. It can be done easily using the <code>set_out_sample_residuals()</code> method.</p> </p>"},{"location":"user_guides/probabilistic-forecasting-global-models.html#probabilistic-forecasting-global-models","title":"Probabilistic Forecasting: Global Models\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-global-models.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-global-models.html#intervals-using-conformal-prediction","title":"Intervals using conformal prediction\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-global-models.html#intervals-using-bootstrapped-residuals","title":"Intervals using bootstrapped residuals\u00b6","text":"<p>The same process is repeated but this time using the conformal method instead of bootstrapping.</p>"},{"location":"user_guides/probabilistic-forecasting-global-models.html#forecasting-intervals-for-unknown-series","title":"Forecasting intervals for unknown series\u00b6","text":"<p>The <code>ForecasterRecursiveMultiseries</code> class allows the forecasting of series not seen during training, with the only requirement that at least <code>window_size</code> observations of the new series are available. To generate probabilistic predictions, the forecaster uses a random sample of the residuals of all known series.</p>"},{"location":"user_guides/probabilistic-forecasting-global-models.html#probabilistic-forecasting-in-production","title":"Probabilistic forecasting in production\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-metrics.html","title":"Metrics in probabilistic forecasting","text":"<p>In point estimate forecasting, the model outputs a single value that ideally represents the most likely value of the time series at future steps. In this scenario, the quality of the predictions can be assessed by comparing the predicted value with the true value of the series. Examples of metrics used for this purpose includes Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).</p> <p>In probabilistic forecasting, however, the model does not produce a single value, but rather a representation of the entire distribution of possible predicted values. In practice, this is often represented by a sample of the underlying distribution (e.g. 50 possible predicted values) or by specific quantiles that capture most of the information in the distribution. This approach provides richer insights by allowing the creation of prediction intervals - ranges within which the true value is likely to fall.</p> <p>In this context, the quality of the predictions cannot be assessed using the same metrics as in point estimate forecasting. Instead, we need to use metrics that are specifically designed to evaluate the quality of probabilistic forecasts.</p> <p>This notebook explores some metrics that can be used to assess the quality of probabilistic forecasts:</p> <ul> <li><p>Coverage: The proportion of the true values that fall within the prediction intervals.</p> </li> <li><p>Interval width: The average width of the prediction intervals.</p> </li> <li><p>Interval area: The area covered by the prediction intervals.</p> </li> <li><p>Continuous Ranked Probability Score (CRPS): A scoring rule that measures the distance between the predicted cumulative distribution function and the true cumulative distribution function.</p> </li> </ul> <p>In general, the goal is to produce prediction intervals that are as narrow as possible while still capturing the true values with the desired probability. This is a trade-off between the width of the intervals and the coverage of the true values.</p> <p> \ud83d\udca1 Tip </p> <p>For more examples on how to use probabilistic forecasting, check out the following articles:</p> <ul> <li>              Probabilistic forecasting with machine learning          </li> <li>              Probabilistic forecasting: prediction intervals for multi-step time series forecasting          </li> <li>              Continuous Ranked Probability Score (CRPS) in probabilistic forecasting          </li> </ul> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme\nfrom skforecast.plot import plot_prediction_intervals\nfrom skforecast.plot import plot_residuals\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom feature_engine.datetime import DatetimeFeatures\nfrom feature_engine.creation import CyclicalFeatures\nfrom feature_engine.timeseries.forecasting import LagFeatures\nfrom feature_engine.timeseries.forecasting import WindowFeatures\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.metrics import calculate_coverage, crps_from_quantiles\n\n# Warnings configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('once')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd from skforecast.datasets import fetch_dataset  # Plots # ============================================================================== import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme from skforecast.plot import plot_prediction_intervals from skforecast.plot import plot_residuals  # Modelling and Forecasting # ============================================================================== from sklearn.linear_model import Ridge from sklearn.pipeline import make_pipeline from feature_engine.datetime import DatetimeFeatures from feature_engine.creation import CyclicalFeatures from feature_engine.timeseries.forecasting import LagFeatures from feature_engine.timeseries.forecasting import WindowFeatures from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster from skforecast.preprocessing import RollingFeatures from skforecast.metrics import calculate_coverage, crps_from_quantiles  # Warnings configuration # ============================================================================== import warnings warnings.filterwarnings('once') In\u00a0[2]: Copied! <pre># Load data\n# ==============================================================================\ndata = fetch_dataset('ett_m2')\ndata = data.resample(rule=\"1h\", closed=\"left\", label=\"right\").mean()\ndata.head(3)\n</pre> # Load data # ============================================================================== data = fetch_dataset('ett_m2') data = data.resample(rule=\"1h\", closed=\"left\", label=\"right\").mean() data.head(3) <pre>ett_m2\n------\nData from an electricity transformer station was collected between July 2016 and\nJuly 2018 (2 years x 365 days x 24 hours x 4 intervals per hour = 70,080 data\npoints). Each data point consists of 8 features, including the date of the\npoint, the predictive value \"Oil Temperature (OT)\", and 6 different types of\nexternal power load features: High UseFul Load (HUFL), High UseLess Load (HULL),\nMiddle UseFul Load (MUFL), Middle UseLess Load (MULL), Low UseFul Load (LUFL),\nLow UseLess Load (LULL).\nZhou, Haoyi &amp; Zhang, Shanghang &amp; Peng, Jieqi &amp; Zhang, Shuai &amp; Li, Jianxin &amp;\nXiong, Hui &amp; Zhang, Wancai. (2020). Informer: Beyond Efficient Transformer for\nLong Sequence Time-Series Forecasting.\n[10.48550/arXiv.2012.07436](https://arxiv.org/abs/2012.07436).\nhttps://github.com/zhouhaoyi/ETDataset\nShape of the dataset: (69680, 7)\n</pre> Out[2]: HUFL HULL MUFL MULL LUFL LULL OT date 2016-07-01 01:00:00 38.784501 10.88975 34.753500 8.55100 4.12575 1.26050 37.83825 2016-07-01 02:00:00 36.041249 9.44475 32.696001 7.13700 3.59025 0.62900 36.84925 2016-07-01 03:00:00 38.240000 11.41350 35.343501 9.10725 3.06000 0.31175 35.91575 In\u00a0[3]: Copied! <pre># Split train-validation-test\n# ==============================================================================\nend_train = '2017-10-01 23:59:00'\nend_validation = '2018-04-03 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_val   = data.loc[end_train:end_validation, :]\ndata_test  = data.loc[end_validation:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-validation-test # ============================================================================== end_train = '2017-10-01 23:59:00' end_validation = '2018-04-03 23:59:00' data_train = data.loc[: end_train, :] data_val   = data.loc[end_train:end_validation, :] data_test  = data.loc[end_validation:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 2016-07-01 01:00:00 --- 2017-10-01 23:00:00  (n=10991)\nDates validation : 2017-10-02 00:00:00 --- 2018-04-03 23:00:00  (n=4416)\nDates test       : 2018-04-04 00:00:00 --- 2018-06-26 20:00:00  (n=2013)\n</pre> In\u00a0[4]: Copied! <pre># Plot partitions of the target series\n# ==============================================================================\nset_dark_theme()\nplt.rcParams['lines.linewidth'] = 0.5\nfig, ax = plt.subplots(figsize=(8, 3))\nax.plot(data_train['OT'], label='Train')\nax.plot(data_val['OT'], label='Validation')\nax.plot(data_test['OT'], label='Test')\nax.set_title('Oil Temperature')\nax.legend();\n</pre> # Plot partitions of the target series # ============================================================================== set_dark_theme() plt.rcParams['lines.linewidth'] = 0.5 fig, ax = plt.subplots(figsize=(8, 3)) ax.plot(data_train['OT'], label='Train') ax.plot(data_val['OT'], label='Validation') ax.plot(data_test['OT'], label='Test') ax.set_title('Oil Temperature') ax.legend(); In\u00a0[5]: Copied! <pre># Plot partitions after differencing\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(8, 3))\nax.plot(data_train['OT'].diff(1), label='Train')\nax.plot(data_val['OT'].diff(1), label='Validation')\nax.plot(data_test['OT'].diff(1), label='Test')\nax.set_title('Differenced Oil Temperature')\nax.legend();\n</pre> # Plot partitions after differencing # ============================================================================== fig, ax = plt.subplots(figsize=(8, 3)) ax.plot(data_train['OT'].diff(1), label='Train') ax.plot(data_val['OT'].diff(1), label='Validation') ax.plot(data_test['OT'].diff(1), label='Test') ax.set_title('Differenced Oil Temperature') ax.legend(); In\u00a0[6]: Copied! <pre># Calendar features\n# ==============================================================================\nfeatures_to_extract = [\n    'year',\n    'month',\n    'week',\n    'day_of_week',\n    'hour'\n]\ncalendar_transformer = DatetimeFeatures(\n    variables           = 'index',\n    features_to_extract = features_to_extract,\n    drop_original       = False,\n)\n\n# Lags of exogenous variables\n# ==============================================================================\nlag_transformer = LagFeatures(\n    variables = [\"HUFL\", \"MUFL\", \"MULL\", \"HULL\", \"LUFL\", \"LULL\"],\n    periods   = [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 42]\n)\n\n# Rolling features for exogenous variables\n# ==============================================================================\nwf_transformer = WindowFeatures(\n    variables = [\"HUFL\", \"MUFL\", \"MULL\", \"HULL\", \"LUFL\", \"LULL\"],\n    window    = [\"1D\", \"7D\"],\n    functions = [\"mean\", \"max\", \"min\"],\n    freq      = \"1h\",\n)\n\n# Cyclical encoding of calendar features\n# ==============================================================================\nfeatures_to_encode = [\n    \"month\",\n    \"week\",\n    \"day_of_week\",\n    \"hour\",\n]\nmax_values = {\n    \"month\": 12,\n    \"week\": 52,\n    \"day_of_week\": 7,\n    \"hour\": 24,\n}\ncyclical_encoder = CyclicalFeatures(\n                       variables     = features_to_encode,\n                       max_values    = max_values,\n                       drop_original = True\n                   )\n\nexog_transformer = make_pipeline(\n                       calendar_transformer,\n                       lag_transformer,\n                       wf_transformer,\n                       cyclical_encoder\n                   )\ndisplay(exog_transformer)\n\ndata = exog_transformer.fit_transform(data)\n# Remove rows with NaNs created by lag features\ndata = data.dropna()\ndisplay(data.head(3))\n</pre> # Calendar features # ============================================================================== features_to_extract = [     'year',     'month',     'week',     'day_of_week',     'hour' ] calendar_transformer = DatetimeFeatures(     variables           = 'index',     features_to_extract = features_to_extract,     drop_original       = False, )  # Lags of exogenous variables # ============================================================================== lag_transformer = LagFeatures(     variables = [\"HUFL\", \"MUFL\", \"MULL\", \"HULL\", \"LUFL\", \"LULL\"],     periods   = [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 42] )  # Rolling features for exogenous variables # ============================================================================== wf_transformer = WindowFeatures(     variables = [\"HUFL\", \"MUFL\", \"MULL\", \"HULL\", \"LUFL\", \"LULL\"],     window    = [\"1D\", \"7D\"],     functions = [\"mean\", \"max\", \"min\"],     freq      = \"1h\", )  # Cyclical encoding of calendar features # ============================================================================== features_to_encode = [     \"month\",     \"week\",     \"day_of_week\",     \"hour\", ] max_values = {     \"month\": 12,     \"week\": 52,     \"day_of_week\": 7,     \"hour\": 24, } cyclical_encoder = CyclicalFeatures(                        variables     = features_to_encode,                        max_values    = max_values,                        drop_original = True                    )  exog_transformer = make_pipeline(                        calendar_transformer,                        lag_transformer,                        wf_transformer,                        cyclical_encoder                    ) display(exog_transformer)  data = exog_transformer.fit_transform(data) # Remove rows with NaNs created by lag features data = data.dropna() display(data.head(3)) <pre>Pipeline(steps=[('datetimefeatures',\n                 DatetimeFeatures(drop_original=False,\n                                  features_to_extract=['year', 'month', 'week',\n                                                       'day_of_week', 'hour'],\n                                  variables='index')),\n                ('lagfeatures',\n                 LagFeatures(periods=[1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14,\n                                      15, 16, 17, 18, 19, 20, 21, 23, 24, 42],\n                             variables=['HUFL', 'MUFL', 'MULL', 'HULL', 'LUFL',\n                                        'LULL'])),\n                ('windowfeatures',\n                 WindowFeatures(freq='1h', functions=['mean', 'max', 'min'],\n                                variables=['HUFL', 'MUFL', 'MULL', 'HULL',\n                                           'LUFL', 'LULL'],\n                                window=['1D', '7D'])),\n                ('cyclicalfeatures',\n                 CyclicalFeatures(drop_original=True,\n                                  max_values={'day_of_week': 7, 'hour': 24,\n                                              'month': 12, 'week': 52},\n                                  variables=['month', 'week', 'day_of_week',\n                                             'hour']))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted Parameters steps\u00a0 [('datetimefeatures', ...), ('lagfeatures', ...), ...] transform_input\u00a0 None memory\u00a0 None verbose\u00a0 False DatetimeFeatures Parameters variables\u00a0 'index' features_to_extract\u00a0 ['year', 'month', ...] drop_original\u00a0 False missing_values\u00a0 'raise' dayfirst\u00a0 False yearfirst\u00a0 False utc\u00a0 None format\u00a0 None LagFeatures Parameters variables\u00a0 ['HUFL', 'MUFL', ...] periods\u00a0 [1, 2, ...] freq\u00a0 None fill_value\u00a0 None sort_index\u00a0 True missing_values\u00a0 'raise' drop_original\u00a0 False drop_na\u00a0 False WindowFeatures Parameters variables\u00a0 ['HUFL', 'MUFL', ...] window\u00a0 ['1D', '7D'] min_periods\u00a0 None functions\u00a0 ['mean', 'max', ...] periods\u00a0 1 freq\u00a0 '1h' sort_index\u00a0 True missing_values\u00a0 'raise' drop_original\u00a0 False drop_na\u00a0 False CyclicalFeatures Parameters variables\u00a0 ['month', 'week', ...] max_values\u00a0 {'day_of_week': 7, 'hour': 24, 'month': 12, 'week': 52} drop_original\u00a0 True HUFL HULL MUFL MULL LUFL LULL OT year HUFL_lag_1 MUFL_lag_1 ... LULL_window_7D_max LULL_window_7D_min month_sin month_cos week_sin week_cos day_of_week_sin day_of_week_cos hour_sin hour_cos date 2016-07-02 19:00:00 33.6330 9.08900 29.874751 6.95600 3.753 0.61025 28.719500 2016 32.5440 29.017001 ... 1.2605 0.0 -0.5 -0.866025 1.224647e-16 -1.0 -0.974928 -0.222521 -0.965926 0.258819 2016-07-02 20:00:00 31.7065 7.72750 27.884500 5.63600 3.753 0.67425 29.103875 2016 33.6330 29.874751 ... 1.2605 0.0 -0.5 -0.866025 1.224647e-16 -1.0 -0.974928 -0.222521 -0.866025 0.500000 2016-07-02 21:00:00 31.8110 7.81125 27.362000 5.18025 4.435 1.42075 29.598500 2016 31.7065 27.884500 ... 1.2605 0.0 -0.5 -0.866025 1.224647e-16 -1.0 -0.974928 -0.222521 -0.707107 0.707107 <p>3 rows \u00d7 184 columns</p> In\u00a0[7]: Copied! <pre># Exogenous features\n# ==============================================================================\nlags = [1, 2, 3, 4, 5, 6, 9, 12, 15, 17, 20, 23, 24, 42]\nexog_features = [\n    'HUFL', 'HUFL_lag_1', 'HUFL_lag_12', 'HUFL_lag_13', 'HUFL_lag_15', 'HUFL_lag_19',\n    'HUFL_lag_2', 'HUFL_lag_20', 'HUFL_lag_23', 'HUFL_lag_4', 'HUFL_lag_5', 'HUFL_lag_9',\n    'HUFL_window_1D_mean', 'HULL', 'HULL_lag_1', 'HULL_lag_12', 'HULL_lag_14',\n    'HULL_lag_15', 'HULL_lag_2', 'HULL_lag_20', 'HULL_lag_21', 'HULL_lag_23',\n    'HULL_lag_3', 'HULL_lag_4', 'HULL_window_1D_mean', 'LUFL', 'LUFL_lag_1',\n    'LUFL_lag_10', 'LUFL_lag_15', 'LUFL_lag_19', 'LUFL_lag_2', 'LUFL_lag_20',\n    'LUFL_lag_23', 'LUFL_lag_3', 'LUFL_lag_4', 'LUFL_lag_5', 'LUFL_window_1D_mean',\n    'LULL', 'LULL_lag_1', 'LULL_lag_12', 'LULL_lag_13', 'LULL_lag_14', 'LULL_lag_18',\n    'LULL_lag_19', 'LULL_lag_20', 'LULL_lag_21', 'LULL_lag_23', 'LULL_lag_24',\n    'LULL_lag_4', 'LULL_lag_5', 'LULL_lag_6', 'LULL_window_1D_max', 'LULL_window_1D_min',\n    'MUFL', 'MUFL_lag_1', 'MUFL_lag_11', 'MUFL_lag_12', 'MUFL_lag_13', 'MUFL_lag_15',\n    'MUFL_lag_2', 'MUFL_lag_20', 'MUFL_lag_23', 'MUFL_lag_4', 'MUFL_lag_9',\n    'MUFL_window_1D_mean', 'MULL', 'MULL_lag_1', 'MULL_lag_11', 'MULL_lag_12',\n    'MULL_lag_13', 'MULL_lag_14', 'MULL_lag_15', 'MULL_lag_17', 'MULL_lag_19',\n    'MULL_lag_2', 'MULL_lag_20', 'MULL_lag_3', 'MULL_lag_4', 'MULL_lag_5', 'MULL_lag_9',\n    'MULL_window_1D_mean', 'MULL_window_1D_min', 'MULL_window_7D_mean', 'day_of_week_sin',\n    'hour_cos', 'hour_sin', 'month_cos', 'month_sin', 'week_cos', 'week_sin', 'year'\n]\n</pre> # Exogenous features # ============================================================================== lags = [1, 2, 3, 4, 5, 6, 9, 12, 15, 17, 20, 23, 24, 42] exog_features = [     'HUFL', 'HUFL_lag_1', 'HUFL_lag_12', 'HUFL_lag_13', 'HUFL_lag_15', 'HUFL_lag_19',     'HUFL_lag_2', 'HUFL_lag_20', 'HUFL_lag_23', 'HUFL_lag_4', 'HUFL_lag_5', 'HUFL_lag_9',     'HUFL_window_1D_mean', 'HULL', 'HULL_lag_1', 'HULL_lag_12', 'HULL_lag_14',     'HULL_lag_15', 'HULL_lag_2', 'HULL_lag_20', 'HULL_lag_21', 'HULL_lag_23',     'HULL_lag_3', 'HULL_lag_4', 'HULL_window_1D_mean', 'LUFL', 'LUFL_lag_1',     'LUFL_lag_10', 'LUFL_lag_15', 'LUFL_lag_19', 'LUFL_lag_2', 'LUFL_lag_20',     'LUFL_lag_23', 'LUFL_lag_3', 'LUFL_lag_4', 'LUFL_lag_5', 'LUFL_window_1D_mean',     'LULL', 'LULL_lag_1', 'LULL_lag_12', 'LULL_lag_13', 'LULL_lag_14', 'LULL_lag_18',     'LULL_lag_19', 'LULL_lag_20', 'LULL_lag_21', 'LULL_lag_23', 'LULL_lag_24',     'LULL_lag_4', 'LULL_lag_5', 'LULL_lag_6', 'LULL_window_1D_max', 'LULL_window_1D_min',     'MUFL', 'MUFL_lag_1', 'MUFL_lag_11', 'MUFL_lag_12', 'MUFL_lag_13', 'MUFL_lag_15',     'MUFL_lag_2', 'MUFL_lag_20', 'MUFL_lag_23', 'MUFL_lag_4', 'MUFL_lag_9',     'MUFL_window_1D_mean', 'MULL', 'MULL_lag_1', 'MULL_lag_11', 'MULL_lag_12',     'MULL_lag_13', 'MULL_lag_14', 'MULL_lag_15', 'MULL_lag_17', 'MULL_lag_19',     'MULL_lag_2', 'MULL_lag_20', 'MULL_lag_3', 'MULL_lag_4', 'MULL_lag_5', 'MULL_lag_9',     'MULL_window_1D_mean', 'MULL_window_1D_min', 'MULL_window_7D_mean', 'day_of_week_sin',     'hour_cos', 'hour_sin', 'month_cos', 'month_sin', 'week_cos', 'week_sin', 'year' ] <p> \u270e Note </p> <p>The lags, features and hyperparameters used in this document were selected after a hyperparameter optimization and feature selection process. For a more detailed visit the full document Probabilistic forecasting: prediction intervals for multi-step time series forecasting.</p> In\u00a0[8]: Copied! <pre># Create forecaster\n# ==============================================================================\nwindow_features = RollingFeatures(stats=['mean', 'min', 'max'], window_sizes=24)\nforecaster = ForecasterRecursive(\n                 regressor       = Ridge(random_state=15926, alpha=1.1075),\n                 lags            = lags,\n                 window_features = window_features,\n                 differentiation = 1,\n                 binner_kwargs   = {'n_bins': 10}\n             )\n</pre> # Create forecaster # ============================================================================== window_features = RollingFeatures(stats=['mean', 'min', 'max'], window_sizes=24) forecaster = ForecasterRecursive(                  regressor       = Ridge(random_state=15926, alpha=1.1075),                  lags            = lags,                  window_features = window_features,                  differentiation = 1,                  binner_kwargs   = {'n_bins': 10}              ) In\u00a0[9]: Copied! <pre># Backtesting on validation data to obtain out-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         initial_train_size = len(data.loc[:end_train, :]),\n         steps              = 24,  # all hours of next day\n         differentiation    = 1,\n     )\n\nmetric_val, predictions_val = backtesting_forecaster(\n                                  forecaster = forecaster,\n                                  y          = data.loc[:end_validation, 'OT'],\n                                  exog       = data.loc[:end_validation, exog_features],\n                                  cv         = cv,\n                                  metric     = 'mean_absolute_error'\n                              )\n\ndisplay(metric_val)\nfig, ax = plt.subplots(figsize=(8, 3))\ndata.loc[end_train:end_validation, 'OT'].plot(ax=ax, label='real value')\npredictions_val['pred'].plot(ax=ax, label='prediction')\nax.set_title(\"Backtesting on validation data\")\nax.legend();\n</pre> # Backtesting on validation data to obtain out-sample residuals # ============================================================================== cv = TimeSeriesFold(          initial_train_size = len(data.loc[:end_train, :]),          steps              = 24,  # all hours of next day          differentiation    = 1,      )  metric_val, predictions_val = backtesting_forecaster(                                   forecaster = forecaster,                                   y          = data.loc[:end_validation, 'OT'],                                   exog       = data.loc[:end_validation, exog_features],                                   cv         = cv,                                   metric     = 'mean_absolute_error'                               )  display(metric_val) fig, ax = plt.subplots(figsize=(8, 3)) data.loc[end_train:end_validation, 'OT'].plot(ax=ax, label='real value') predictions_val['pred'].plot(ax=ax, label='prediction') ax.set_title(\"Backtesting on validation data\") ax.legend(); <pre>  0%|          | 0/184 [00:00&lt;?, ?it/s]</pre> mean_absolute_error 0 2.382001 In\u00a0[10]: Copied! <pre># Out-sample residuals distribution\n# ==============================================================================\nresiduals = data.loc[predictions_val.index, 'OT'] - predictions_val['pred']\nprint(pd.Series(np.where(residuals &lt; 0, 'negative', 'positive')).value_counts())\nplt.rcParams.update({'font.size': 8})\n_ = plot_residuals(residuals=residuals, figsize=(7, 4))\n</pre> # Out-sample residuals distribution # ============================================================================== residuals = data.loc[predictions_val.index, 'OT'] - predictions_val['pred'] print(pd.Series(np.where(residuals &lt; 0, 'negative', 'positive')).value_counts()) plt.rcParams.update({'font.size': 8}) _ = plot_residuals(residuals=residuals, figsize=(7, 4)) <pre>positive    2462\nnegative    1954\nName: count, dtype: int64\n</pre> In\u00a0[11]: Copied! <pre># Store out-sample residuals in the forecaster\n# ==============================================================================\nforecaster.fit(y=data.loc[:end_train, 'OT'], exog=data.loc[:end_train, exog_features])\nforecaster.set_out_sample_residuals(\n    y_true = data.loc[predictions_val.index, 'OT'], \n    y_pred = predictions_val['pred']\n)\n</pre> # Store out-sample residuals in the forecaster # ============================================================================== forecaster.fit(y=data.loc[:end_train, 'OT'], exog=data.loc[:end_train, exog_features]) forecaster.set_out_sample_residuals(     y_true = data.loc[predictions_val.index, 'OT'],      y_pred = predictions_val['pred'] ) In\u00a0[12]: Copied! <pre># Backtesting with prediction intervals in test data using out-sample residuals\n# ==============================================================================\ncv = TimeSeriesFold(\n         initial_train_size = len(data.loc[:end_validation, :]),\n         steps              = 24,  # all hours of next day\n         differentiation    = 1\n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster              = forecaster,\n                          y                       = data['OT'],\n                          exog                    = data[exog_features],\n                          cv                      = cv,\n                          metric                  = 'mean_absolute_error',\n                          interval                = [10, 90],  # 80% prediction interval\n                          interval_method         = 'bootstrapping',\n                          n_boot                  = 150,\n                          use_in_sample_residuals = False,  # Use out-sample residuals\n                          use_binned_residuals    = True\n                      )\ndisplay(metric)\npredictions.head(5)\n</pre> # Backtesting with prediction intervals in test data using out-sample residuals # ============================================================================== cv = TimeSeriesFold(          initial_train_size = len(data.loc[:end_validation, :]),          steps              = 24,  # all hours of next day          differentiation    = 1      )  metric, predictions = backtesting_forecaster(                           forecaster              = forecaster,                           y                       = data['OT'],                           exog                    = data[exog_features],                           cv                      = cv,                           metric                  = 'mean_absolute_error',                           interval                = [10, 90],  # 80% prediction interval                           interval_method         = 'bootstrapping',                           n_boot                  = 150,                           use_in_sample_residuals = False,  # Use out-sample residuals                           use_binned_residuals    = True                       ) display(metric) predictions.head(5) <pre>  0%|          | 0/84 [00:00&lt;?, ?it/s]</pre> mean_absolute_error 0 2.872448 Out[12]: pred lower_bound upper_bound 2018-04-04 00:00:00 32.748221 32.293918 33.421547 2018-04-04 01:00:00 31.947644 31.428145 33.195005 2018-04-04 02:00:00 31.052970 30.153244 32.860915 2018-04-04 03:00:00 30.109002 28.482297 32.571251 2018-04-04 04:00:00 29.229875 27.802629 32.305870 In\u00a0[13]: Copied! <pre># Plot intervals\n# ==============================================================================\nplt.rcParams['lines.linewidth'] = 1\nfig, ax = plt.subplots(figsize=(9, 4))\nplot_prediction_intervals(\n    predictions     = predictions,\n    y_true          = data_test,\n    target_variable = \"OT\",\n    initial_x_zoom  = None,\n    title           = \"Prediction interval in test data\",\n    xaxis_title     = \"Date time\",\n    yaxis_title     = \"OT\",\n    ax              = ax\n)\nfill_between_obj = ax.collections[0]\nfill_between_obj.set_facecolor('white')\nfill_between_obj.set_alpha(0.3)\n</pre> # Plot intervals # ============================================================================== plt.rcParams['lines.linewidth'] = 1 fig, ax = plt.subplots(figsize=(9, 4)) plot_prediction_intervals(     predictions     = predictions,     y_true          = data_test,     target_variable = \"OT\",     initial_x_zoom  = None,     title           = \"Prediction interval in test data\",     xaxis_title     = \"Date time\",     yaxis_title     = \"OT\",     ax              = ax ) fill_between_obj = ax.collections[0] fill_between_obj.set_facecolor('white') fill_between_obj.set_alpha(0.3) In\u00a0[14]: Copied! <pre># Empirical interval coverage (on test data)\n# ==============================================================================\ncoverage = calculate_coverage(\n               y_true      = data.loc[end_validation:, 'OT'],\n               lower_bound = predictions[\"lower_bound\"], \n               upper_bound = predictions[\"upper_bound\"]\n           )\nprint(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")\n\n# Mean widht and ara of the interval\n# ==============================================================================\narea = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum()\nmean_width = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).mean()\nprint(f\"Area of the interval: {round(area, 2)}\")\nprint(f\"Mean width of the interval: {round(mean_width, 2)}\")\n</pre> # Empirical interval coverage (on test data) # ============================================================================== coverage = calculate_coverage(                y_true      = data.loc[end_validation:, 'OT'],                lower_bound = predictions[\"lower_bound\"],                 upper_bound = predictions[\"upper_bound\"]            ) print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")  # Mean widht and ara of the interval # ============================================================================== area = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).sum() mean_width = (predictions[\"upper_bound\"] - predictions[\"lower_bound\"]).mean() print(f\"Area of the interval: {round(area, 2)}\") print(f\"Mean width of the interval: {round(mean_width, 2)}\") <pre>Predicted interval coverage: 80.87 %\nArea of the interval: 19873.9\nMean width of the interval: 9.87\n</pre> <p>The <code>backtesting_forecaster</code> function not only allows to estimate a single prediction interval, but also to estimate multiple quantiles (percentiles) from which multiple prediction intervals can be constructed. This is useful to evaluate the quality of the prediction intervals for a range of probabilities.</p> <p>Several percentiles are predicted and from these, prediction intervals are estimated for different nominal coverage levels - 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90% and 95% - and their actual coverage is assessed.</p> In\u00a0[15]: Copied! <pre># Prediction intervals for different nominal coverages\n# ==============================================================================\nquantiles = [2.5, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 97.5]\nintervals = [[2.5, 97.5], [5, 95], [10, 90], [15, 85], [20, 80], [30, 70], [35, 65], [40, 60], [45, 55]]\nobserved_coverages = []\nobserved_areas = []\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster              = forecaster,\n                          y                       = data['OT'],\n                          exog                    = data[exog_features],\n                          cv                      = cv,\n                          metric                  = 'mean_absolute_error',\n                          interval                = quantiles,\n                          interval_method         = 'bootstrapping',\n                          n_boot                  = 150,\n                          use_in_sample_residuals = False,  # Use out-sample residuals\n                          use_binned_residuals    = True\n                      )\npredictions.head()\n</pre> # Prediction intervals for different nominal coverages # ============================================================================== quantiles = [2.5, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 97.5] intervals = [[2.5, 97.5], [5, 95], [10, 90], [15, 85], [20, 80], [30, 70], [35, 65], [40, 60], [45, 55]] observed_coverages = [] observed_areas = []  metric, predictions = backtesting_forecaster(                           forecaster              = forecaster,                           y                       = data['OT'],                           exog                    = data[exog_features],                           cv                      = cv,                           metric                  = 'mean_absolute_error',                           interval                = quantiles,                           interval_method         = 'bootstrapping',                           n_boot                  = 150,                           use_in_sample_residuals = False,  # Use out-sample residuals                           use_binned_residuals    = True                       ) predictions.head() <pre>  0%|          | 0/84 [00:00&lt;?, ?it/s]</pre> Out[15]: pred p_2.5 p_5 p_10 p_15 p_20 p_25 p_30 p_35 p_40 ... p_55 p_60 p_65 p_70 p_75 p_80 p_85 p_90 p_95 p_97.5 2018-04-04 00:00:00 32.748221 31.977111 32.079720 32.293918 32.462264 32.535513 32.571749 32.652022 32.785265 32.843000 ... 33.025805 33.070953 33.132935 33.199885 33.255073 33.293658 33.330013 33.421547 33.546763 33.805721 2018-04-04 01:00:00 31.947644 30.611061 31.185507 31.428145 31.587988 31.717603 31.866081 31.984188 32.070315 32.210793 ... 32.519054 32.659853 32.791033 32.868997 32.921711 32.993290 33.073593 33.195005 33.308497 33.484556 2018-04-04 02:00:00 31.052970 28.613107 29.604880 30.153244 30.325608 30.673197 30.913207 31.189854 31.402368 31.564998 ... 32.020048 32.096509 32.183147 32.299333 32.451905 32.588603 32.656948 32.860915 33.158009 33.299128 2018-04-04 03:00:00 30.109002 27.524392 28.040126 28.482297 29.131206 29.614509 29.989295 30.330817 30.542676 30.822563 ... 31.413216 31.624016 31.760533 31.880495 31.930950 32.091862 32.389500 32.571251 32.965276 33.314962 2018-04-04 04:00:00 29.229875 26.708860 27.127498 27.802629 28.255452 28.937064 29.265236 29.381942 29.786477 30.019734 ... 30.858493 30.988576 31.179325 31.407085 31.570523 31.820700 31.959443 32.305870 32.768797 33.306518 <p>5 rows \u00d7 22 columns</p> In\u00a0[16]: Copied! <pre># Calculate coverage and area for each interval\n# ==============================================================================\nfor interval in intervals:\n    observed_coverage = calculate_coverage(\n        y_true      = data.loc[end_validation:, 'OT'],\n        lower_bound = predictions[f\"p_{interval[0]}\"], \n        upper_bound = predictions[f\"p_{interval[1]}\"]\n    )\n    observed_area = (predictions[f\"p_{interval[1]}\"] - predictions[f\"p_{interval[0]}\"]).sum()\n    observed_coverages.append(100 * observed_coverage)\n    observed_areas.append(observed_area)\n\nresults = pd.DataFrame({\n              'Interval': intervals,\n              'Nominal coverage': [interval[1] - interval[0] for interval in intervals],\n              'Observed coverage': observed_coverages,\n              'Area': observed_areas\n          })\nresults.round(1)\n</pre> # Calculate coverage and area for each interval # ============================================================================== for interval in intervals:     observed_coverage = calculate_coverage(         y_true      = data.loc[end_validation:, 'OT'],         lower_bound = predictions[f\"p_{interval[0]}\"],          upper_bound = predictions[f\"p_{interval[1]}\"]     )     observed_area = (predictions[f\"p_{interval[1]}\"] - predictions[f\"p_{interval[0]}\"]).sum()     observed_coverages.append(100 * observed_coverage)     observed_areas.append(observed_area)  results = pd.DataFrame({               'Interval': intervals,               'Nominal coverage': [interval[1] - interval[0] for interval in intervals],               'Observed coverage': observed_coverages,               'Area': observed_areas           }) results.round(1) Out[16]: Interval Nominal coverage Observed coverage Area 0 [2.5, 97.5] 95.0 93.6 32010.8 1 [5, 95] 90.0 90.1 26152.9 2 [10, 90] 80.0 80.9 19873.9 3 [15, 85] 70.0 71.7 15862.6 4 [20, 80] 60.0 63.7 12775.7 5 [30, 70] 40.0 44.0 7826.7 6 [35, 65] 30.0 32.9 5708.6 7 [40, 60] 20.0 22.9 3726.1 8 [45, 55] 10.0 11.1 1834.2 <p>Finally, the Continuous Ranked Probability Score (CRPS) is calculated for each of the prediction using the function <code>crps_from_quantiles</code> and averaged to obtain a single value that summarizes the quality of the prediction intervals.</p> In\u00a0[17]: Copied! <pre># Average CRPS\n# ==============================================================================\n\n# Collapse predictions to a single column\npredicted_q = predictions.iloc[:, 1:].apply(lambda row: np.array(row), axis=1).to_frame(name='predicted_q')\npredicted_q = pd.concat([data.loc[end_validation:, 'OT'], predicted_q], axis=1)\n\n# Calculate CRPS for each row\npredicted_q['crps'] = predicted_q.apply(\n    lambda row: crps_from_quantiles(y_true=row['OT'], pred_quantiles=row['predicted_q'], quantile_levels=np.array(quantiles)), axis=1\n)\n\ncrps = predicted_q['crps'].mean()\nprint(f\"Average CRPS: {round(crps, 2)}\")\npredicted_q.head(3)\n</pre> # Average CRPS # ==============================================================================  # Collapse predictions to a single column predicted_q = predictions.iloc[:, 1:].apply(lambda row: np.array(row), axis=1).to_frame(name='predicted_q') predicted_q = pd.concat([data.loc[end_validation:, 'OT'], predicted_q], axis=1)  # Calculate CRPS for each row predicted_q['crps'] = predicted_q.apply(     lambda row: crps_from_quantiles(y_true=row['OT'], pred_quantiles=row['predicted_q'], quantile_levels=np.array(quantiles)), axis=1 )  crps = predicted_q['crps'].mean() print(f\"Average CRPS: {round(crps, 2)}\") predicted_q.head(3) <pre>Average CRPS: 62518.41\n</pre> Out[17]: OT predicted_q crps 2018-04-04 00:00:00 32.674500 [31.977110647741327, 32.079720381566105, 32.29... 6276.813891 2018-04-04 01:00:00 31.575750 [30.61106102395088, 31.18550716728504, 31.4281... 7141.667190 2018-04-04 02:00:00 29.763125 [28.61310741921978, 29.60488040659742, 30.1532... 10335.665443"},{"location":"user_guides/probabilistic-forecasting-metrics.html#metrics-in-probabilistic-forecasting","title":"Metrics in probabilistic forecasting\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-metrics.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-metrics.html#probabilistic-forecasting","title":"Probabilistic forecasting\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-metrics.html#prediction-of-multiple-intervals","title":"Prediction of multiple intervals\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-overview.html","title":"Overview","text":"<p> \u26a0 Warning </p> <p>There are several well-established methods for conformal prediction, each with its own characteristics and assumptions. However, when applied to time series forecasting, their coverage guarantees are only valid for one-step-ahead predictions. For multi-step-ahead predictions, the coverage probability is not guaranteed. Skforecast implements Split Conformal Prediction (SCP) due to its simplicity and efficiency.</p> <p> \u26a0 Warning </p> <p>As Rob J Hyndman explains in his blog, in real-world problems, almost all prediction intervals are too narrow. For example, nominal 95% intervals may only provide coverage between 71% and 87%. This is a well-known phenomenon and arises because they do not account for all sources of uncertainty. With forecasting models, there are at least four sources of uncertainty:</p> <ul> <li>The random error term</li> <li>The parameter estimates</li> <li>The choice of model for the historical data</li> <li>The continuation of the historical data generating process into the future</li> </ul> <p>When producing prediction intervals for time series models, generally only the first of these sources is taken into account. Therefore, it is advisable to use test data to validate the empirical coverage of the interval and not only rely on the expected one.</p> <p> \ud83d\udca1 Tip </p> <p>For more examples on how to use probabilistic forecasting, check out the following articles:</p> <ul> <li>              Probabilistic forecasting with machine learning          </li> <li>              Probabilistic forecasting: prediction intervals for multi-step time series forecasting          </li> <li>              Continuous Ranked Probability Score (CRPS) in probabilistic forecasting          </li> </ul>"},{"location":"user_guides/probabilistic-forecasting-overview.html#probabilistic-forecasting","title":"Probabilistic forecasting\u00b6","text":"<p>When trying to predict future values, most forecasting models try to predict what will be the most likely value. This is called point-forecasting. Although knowing the expected value of a time series in advance is useful in almost any business case, this type of prediction does not provide any information about the confidence of the model or the uncertainty of the prediction.</p> <p>Probabilistic forecasting, as opposed to point-forecasting, is a family of techniques that allow the prediction of the expected distribution of the outcome rather than a single future value. This type of forecasting provides much richer information because it allows the creation of prediction intervals, the range of likely values where the true value may fall. More formally, a prediction interval defines the interval within which the true value of the response variable is expected to be found with a given probability.</p> <p>Skforecast implements several methods for probabilistic forecasting.</p>"},{"location":"user_guides/probabilistic-forecasting-overview.html#bootstrapped-residuals","title":"Bootstrapped residuals\u00b6","text":"<p>Bootstrapping is a statistical technique that allows for estimating the distribution of a statistic by resampling the data with replacement. In the context of forecasting, bootstrapping the residuals of a model allows for estimating the distribution of the errors, which can be used to create prediction intervals. Four methods are available in Skforecast for bootstrapping the residuals:</p> <ul> <li><p><code>predict_bootstrapping</code>: this method generates multiple forecasting predictions through a bootstrapping process. By sampling from a collection of past observed errors (the residuals), each bootstrapping iteration generates a different set of predictions. The output is a <code>pandas DataFrame</code> with one row for each predicted step and one column for each bootstrapping iteration.</p> </li> <li><p><code>predict_interval(method='bootstrapping')</code>: this method estimates quantile prediction intervals using the values generated with <code>predict_bootstrapping</code>.</p> </li> <li><p><code>predict_quantiles</code>: this method estimates a list of quantile predictions using the values generated with <code>predict_bootstrapping</code>.</p> </li> <li><p><code>predict_dist</code>: this method fits a parametric distribution using the values generated with <code>predict_bootstrapping</code>. Any of the continuous distributions available in scipy.stats can be used.</p> </li> </ul> <p>The four methods can use in-sample residuals (default) or out-sample residuals. In both cases, the residuals can be conditioned on the predicted value to try to account for the existence of a correlation between the predicted values and the residuals.</p> <p>Discover how to use these methods in Probabilistic forecasting with bootstrapped residuals.</p>"},{"location":"user_guides/probabilistic-forecasting-overview.html#conformal-prediction","title":"Conformal prediction\u00b6","text":"<p>Conformal prediction is a framework for constructing prediction intervals that are guaranteed to contain the true value with a specified probability (coverage probability). It works by combining the predictions of a point-forecasting model with its past residuals\u2014differences between previous predictions and actual values. These residuals help estimate the uncertainty in the forecast and determine the width of the prediction interval that is then added to the point forecast. Skforecast implements Split Conformal Prediction (SCP) through the <code>predict_interval(method='conformal')</code> method.</p> <p>Discover how to use conformal methods in Probabilistic forecasting with conformal prediction.</p> <p>Conformal methods can also calibrate prediction intervals generated by other techniques, such as quantile regression or bootstrapped residuals. In this case, the conformal method adjusts the prediction intervals to ensure that they remain valid with respect to the coverage probability. Skforecast provides this functionality through the <code>ConformalIntervalCalibrator</code> transformer.</p> <p>Discover how to perform this calibration in Probabilistic forecasting, conformal calibration.</p>"},{"location":"user_guides/probabilistic-forecasting-overview.html#quantile-regression","title":"Quantile regression\u00b6","text":"<p>Quantile regression is a technique for estimating the conditional quantiles of a response variable. By combining the predictions of two quantile regressors, an interval can be constructed, with each model estimating one of the bounds of the interval. For example, models trained on $Q = 0.1$ and $Q = 0.9$ produce an 80% prediction interval ($90\\% - 10\\% = 80\\%$).</p> <p>If a machine learning algorithm capable of modeling quantiles is used as the <code>regressor</code> in a forecaster, the <code>predict</code> method will return predictions for a specified quantile. By creating two forecasters, each configured with a different quantile, their predictions can be combined to generate a prediction interval.</p> <p>Discover how to use this method in Probabilistic forecasting with quantile regression.</p>"},{"location":"user_guides/probabilistic-forecasting-overview.html#which-method-to-use","title":"Which Method to Use?\u00b6","text":"<p>There is no definitive answer to this question, as the resulting coverage may vary depending on the dataset and regressor. However, some general guidelines can be followed:</p> <ul> <li><p>Bootstrapped residuals: This method achieves good results in most cases, especially when using residuals conditioned on the predicted value (<code>use_binned_residuals = True</code>). However, it is computationally expensive, especially when using a large number of bootstrapping iterations, and may not scale well to large datasets or multiple time series.</p> </li> <li><p>Conformal Prediction: Similar results to bootstrapping, but more computationally efficient (fast).</p> </li> <li><p>Quantile regression: An appropriate choice when the regressor is capable of modeling quantiles. However, two Forecasters must be trained \u2014 one for each quantile \u2014 which can be computationally expensive.</p> </li> </ul> <p>None of these methods guarantee coverage probability for multi-step predictions. Therefore, it is strongly recommended to validate the empirical coverage obtained with the chosen method. If the coverage is not satisfactory, the prediction intervals can be calibrated using the <code>ConformalIntervalCalibrator</code> transformer.</p> <p>For bootstrapped and conformal methods, it is recommended to use out-sample residuals (known as calibration residuals in conformal literature) as they provide a more realistic estimate of the prediction uncertainty (<code>use_in_sample_residuals = False</code>). If in-sample residuals are used, the prediction intervals may be too narrow, resulting in low coverage.</p>"},{"location":"user_guides/probabilistic-forecasting-quantile-regression.html","title":"Quantile forecasting","text":"<p>Quantile regression is a technique for estimating the conditional quantiles of a response variable. By combining the predictions of two quantile regressors, an interval can be constructed, where each model estimates one of the interval\u2019s boundaries. For example, models trained for $Q = 0.1$ and $Q = 0.9$ produce an 80% prediction interval ($90\\% - 10\\% = 80\\%$).</p> <p>If a machine learning algorithm capable of modeling quantiles is used as the <code>regressor</code> in a forecaster, the <code>predict</code> method will return predictions for a specified quantile. By creating two forecasters, each configured with a different quantile, their predictions can be combined to generate a prediction interval.</p> <p>As opposed to linear regression, which is intended to estimate the conditional mean of the response variable given certain values of the predictor variables, quantile regression aims at estimating the conditional quantiles of the response variable. For a continuous distribution function, the $\\alpha$-quantile $Q_{\\alpha}(x)$ is defined such that the probability of $Y$ being smaller than $Q_{\\alpha}(x)$ is, for a given $X=x$, equal to $\\alpha$. For example, 36% of the population values are lower than the quantile  $Q=0.36$. The most known quantile is the 50%-quantile, more commonly called the median.</p> <p>By combining the predictions of two quantile regressors, it is possible to build an interval. Each model estimates one of the limits of the interval. For example, the models obtained for $Q = 0.1$ and $Q = 0.9$ produce an 80% prediction interval (90% - 10% = 80%).</p> <p>Several machine learning algorithms are capable of modeling quantiles. Some of them are:</p> <ul> <li><p>LightGBM</p> </li> <li><p>XGBoost</p> </li> <li><p>CatBoost</p> </li> <li><p>Scikit-learn HistGradientBoostingRegressor</p> </li> <li><p>Scikit-learn QuantileRegressor</p> </li> <li><p>skranger quantile RandomForest</p> </li> </ul> <p>Just as the squared-error loss function is used to train models that predict the mean value, a specific loss function is needed in order to train models that predict quantiles. The most common metric used for quantile regression is calles quantile loss  or pinball loss:</p> <p>$$\\text{pinball}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1}  \\alpha \\max(y_i - \\hat{y}_i, 0) + (1 - \\alpha) \\max(\\hat{y}_i - y_i, 0)$$</p> <p>where $\\alpha$ is the target quantile, $y$ the real value and $\\hat{y}$ the quantile prediction.</p> <p>It can be seen that loss differs depending on the evaluated quantile. The higher the quantile, the more the loss function penalizes underestimates, and the less it penalizes overestimates. As with MSE and MAE, the goal is to minimize its values (the lower loss, the better).</p> <p>Two disadvantages of quantile regression, compared to the bootstrap approach to prediction intervals, are that each quantile needs its regressor and quantile regression is not available for all types of regression models. However, once the models are trained, the inference is much faster since no iterative process is needed.</p> <p>This type of prediction intervals can be easily estimated using ForecasterDirect and ForecasterDirectMultiVariate models.</p> <p> \ud83d\udca1 Tip </p> <p>For more examples on how to use probabilistic forecasting, check out the following articles:</p> <ul> <li>              Probabilistic forecasting with machine learning          </li> <li>              Probabilistic forecasting: prediction intervals for multi-step time series forecasting          </li> <li>              Continuous Ranked Probability Score (CRPS) in probabilistic forecasting          </li> </ul> <p> \u26a0 Warning </p> <p>Forecasters of type <code>ForecasterDirect</code> are slower than <code>ForecasterRecursive</code> because they require training one model per step. Although they can achieve better performance, their scalability is an important limitation when many steps need to be predicted.</p> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom skforecast.direct import ForecasterDirect\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import bayesian_search_forecaster\nfrom skforecast.metrics import calculate_coverage\nfrom skforecast.metrics import create_mean_pinball_loss\n\n# Configuration\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('once')\n</pre> # Data processing # ============================================================================== import pandas as pd from skforecast.datasets import fetch_dataset  # Plots # ============================================================================== import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme  # Modelling and Forecasting # ============================================================================== from lightgbm import LGBMRegressor from skforecast.direct import ForecasterDirect from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import bayesian_search_forecaster from skforecast.metrics import calculate_coverage from skforecast.metrics import create_mean_pinball_loss  # Configuration # ============================================================================== import warnings warnings.filterwarnings('once') In\u00a0[2]: Copied! <pre># Data\n# ==============================================================================\ndata = fetch_dataset(name=\"ett_m2_extended\")\ndata = data[[\n    \"OT\",\n    \"day_of_week_cos\",\n    \"day_of_week_sin\",\n    \"hour_cos\",\n    \"hour_sin\",\n    \"month_cos\",\n    \"month_sin\",\n    \"week_cos\",\n    \"week_sin\",\n    \"year\",\n]]\ndata.head(2)\n</pre> # Data # ============================================================================== data = fetch_dataset(name=\"ett_m2_extended\") data = data[[     \"OT\",     \"day_of_week_cos\",     \"day_of_week_sin\",     \"hour_cos\",     \"hour_sin\",     \"month_cos\",     \"month_sin\",     \"week_cos\",     \"week_sin\",     \"year\", ]] data.head(2) <pre>ett_m2_extended\n---------------\nData from an electricity transformer station was collected between July 2016 and\nJuly 2018 (2 years x 365 days x 24 hours x 4 intervals per hour = 70,080 data\npoints). Each data point consists of 8 features, including the date of the\npoint, the predictive value \"Oil Temperature (OT)\", and 6 different types of\nexternal power load features: High UseFul Load (HUFL), High UseLess Load (HULL),\nMiddle UseFul Load (MUFL), Middle UseLess Load (MULL), Low UseFul Load (LUFL),\nLow UseLess Load (LULL). Additional variables are created based on calendar\ninformation (year, month, week, day of the week, and hour). These variables have\nbeen encoded using the cyclical encoding technique (sin and cos transformations)\nto preserve the cyclical nature of the data.\nZhou, Haoyi &amp; Zhang, Shanghang &amp; Peng, Jieqi &amp; Zhang, Shuai &amp; Li, Jianxin &amp;\nXiong, Hui &amp; Zhang, Wancai. (2020). Informer: Beyond Efficient Transformer for\nLong Sequence Time-Series Forecasting.\n[10.48550/arXiv.2012.07436](https://arxiv.org/abs/2012.07436).\nhttps://github.com/zhouhaoyi/ETDataset\nShape of the dataset: (69680, 16)\n</pre> Out[2]: OT day_of_week_cos day_of_week_sin hour_cos hour_sin month_cos month_sin week_cos week_sin year date 2016-07-01 00:00:00 38.661999 -0.900969 -0.433884 1.0 0.0 -0.866025 -0.5 -1.0 1.224647e-16 2016 2016-07-01 00:15:00 38.223000 -0.900969 -0.433884 1.0 0.0 -0.866025 -0.5 -1.0 1.224647e-16 2016 In\u00a0[3]: Copied! <pre># Split train-validation-test\n# ==============================================================================\nexog_features = data.columns.difference(['OT']).tolist()\nend_train = '2017-10-01 23:59:00'\nend_validation = '2018-04-03 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_val   = data.loc[end_train:end_validation, :]\ndata_test  = data.loc[end_validation:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-validation-test # ============================================================================== exog_features = data.columns.difference(['OT']).tolist() end_train = '2017-10-01 23:59:00' end_validation = '2018-04-03 23:59:00' data_train = data.loc[: end_train, :] data_val   = data.loc[end_train:end_validation, :] data_test  = data.loc[end_validation:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 2016-07-01 00:00:00 --- 2017-10-01 23:45:00  (n=43968)\nDates validation : 2017-10-02 00:00:00 --- 2018-04-03 23:45:00  (n=17664)\nDates test       : 2018-04-04 00:00:00 --- 2018-06-26 19:45:00  (n=8048)\n</pre> In\u00a0[4]: Copied! <pre># Plot partitions\n# ==============================================================================\nset_dark_theme()\nplt.rcParams['lines.linewidth'] = 0.5\nfig, ax = plt.subplots(figsize=(8, 3))\nax.plot(data_train['OT'], label='Train')\nax.plot(data_val['OT'], label='Validation')\nax.plot(data_test['OT'], label='Test')\nax.set_title('Oil Temperature')\nax.legend();\n</pre> # Plot partitions # ============================================================================== set_dark_theme() plt.rcParams['lines.linewidth'] = 0.5 fig, ax = plt.subplots(figsize=(8, 3)) ax.plot(data_train['OT'], label='Train') ax.plot(data_val['OT'], label='Validation') ax.plot(data_test['OT'], label='Test') ax.set_title('Oil Temperature') ax.legend(); <p>Two quantile forecasters are trained, one for the 10% quantile and another for the 90% quantile. The predictions of both forecasters can be combined to generate a prediction interval of 80% coverage.</p> In\u00a0[5]: Copied! <pre># Create forecasters: one for each bound of the interval\n# ==============================================================================\n# The forecasters obtained for alpha=0.1 and alpha=0.9 produce a 80% confidence\n# interval (90% - 10% = 80%).\n\n# Forecaster for quantile 10%\nforecaster_q10 = ForecasterDirect(\n                     regressor = LGBMRegressor(\n                                     objective = 'quantile',\n                                     metric    = 'quantile',\n                                     alpha     = 0.1,\n                                     verbose   = -1\n                                 ),\n                     steps = 24,\n                     lags  = [1, 2, 3, 23, 24, 25, 47, 48, 49, 71, 72, 73]\n                 )\n                  \n# Forecaster for quantile 90%\nforecaster_q90 = ForecasterDirect(\n                     regressor = LGBMRegressor(\n                                     objective = 'quantile',\n                                     metric    = 'quantile',\n                                     alpha     = 0.9,\n                                     verbose   = -1\n                                 ),\n                     steps = 24,\n                     lags  = [1, 2, 3, 23, 24, 25, 47, 48, 49, 71, 72, 73]\n                 )\n</pre> # Create forecasters: one for each bound of the interval # ============================================================================== # The forecasters obtained for alpha=0.1 and alpha=0.9 produce a 80% confidence # interval (90% - 10% = 80%).  # Forecaster for quantile 10% forecaster_q10 = ForecasterDirect(                      regressor = LGBMRegressor(                                      objective = 'quantile',                                      metric    = 'quantile',                                      alpha     = 0.1,                                      verbose   = -1                                  ),                      steps = 24,                      lags  = [1, 2, 3, 23, 24, 25, 47, 48, 49, 71, 72, 73]                  )                    # Forecaster for quantile 90% forecaster_q90 = ForecasterDirect(                      regressor = LGBMRegressor(                                      objective = 'quantile',                                      metric    = 'quantile',                                      alpha     = 0.9,                                      verbose   = -1                                  ),                      steps = 24,                      lags  = [1, 2, 3, 23, 24, 25, 47, 48, 49, 71, 72, 73]                  ) <p>Next, a bayesian search is performed to find the best hyperparameters for the quantile regressors. When validating a quantile regression model, it is important to use a metric that is coherent with the quantile being evaluated. In this case, the pinball loss is used. Skforecast provides the function <code>create_mean_pinball_loss</code> calculate the pinball loss for a given quantile.</p> In\u00a0[6]: Copied! <pre># Bayesian search of hyper-parameters and lags for each quantile forecaster\n# ==============================================================================\ndef search_space(trial):\n    search_space  = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),\n        'max_depth': trial.suggest_int('max_depth', 3, 10, step=1),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1)\n    }\n\n    return search_space\n\ncv = TimeSeriesFold(\n        steps              = 24,\n        initial_train_size = len(data[:end_train]),\n        refit              = False,\n    )\n\nresults_grid_q10 = bayesian_search_forecaster(\n                       forecaster   = forecaster_q10,\n                       y            = data.loc[:end_validation, 'OT'],\n                       cv           = cv,\n                       metric       = create_mean_pinball_loss(alpha=0.1),\n                       search_space = search_space,\n                       n_trials     = 10,\n                       return_best  = True\n                   )\n\nresults_grid_q90 = bayesian_search_forecaster(\n                       forecaster   = forecaster_q90,\n                       y            = data.loc[:end_validation, 'OT'],\n                       cv           = cv,\n                       metric       = create_mean_pinball_loss(alpha=0.9),\n                       search_space = search_space,\n                       n_trials     = 10,\n                       return_best  = True\n                   )\n</pre> # Bayesian search of hyper-parameters and lags for each quantile forecaster # ============================================================================== def search_space(trial):     search_space  = {         'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),         'max_depth': trial.suggest_int('max_depth', 3, 10, step=1),         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1)     }      return search_space  cv = TimeSeriesFold(         steps              = 24,         initial_train_size = len(data[:end_train]),         refit              = False,     )  results_grid_q10 = bayesian_search_forecaster(                        forecaster   = forecaster_q10,                        y            = data.loc[:end_validation, 'OT'],                        cv           = cv,                        metric       = create_mean_pinball_loss(alpha=0.1),                        search_space = search_space,                        n_trials     = 10,                        return_best  = True                    )  results_grid_q90 = bayesian_search_forecaster(                        forecaster   = forecaster_q90,                        y            = data.loc[:end_validation, 'OT'],                        cv           = cv,                        metric       = create_mean_pinball_loss(alpha=0.9),                        search_space = search_space,                        n_trials     = 10,                        return_best  = True                    ) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3 23 24 25 47 48 49 71 72 73] \n  Parameters: {'n_estimators': 250, 'max_depth': 3, 'learning_rate': 0.04040638127771271}\n  Backtesting metric: 0.41020051875078006\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3 23 24 25 47 48 49 71 72 73] \n  Parameters: {'n_estimators': 450, 'max_depth': 8, 'learning_rate': 0.061491327557080706}\n  Backtesting metric: 0.350142766138876\n</pre> In\u00a0[7]: Copied! <pre># Backtesting on test data\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 24, \n         initial_train_size = len(data.loc[:end_validation]),\n         refit              = False\n     )\n\nmetric_q10, predictions_q10 = backtesting_forecaster(\n                                  forecaster = forecaster_q10,\n                                  y          = data['OT'],\n                                  cv         = cv,\n                                  metric     = create_mean_pinball_loss(alpha=0.1)\n                              )\n\nmetric_q90, predictions_q90 = backtesting_forecaster(\n                                  forecaster = forecaster_q90,\n                                  y          = data['OT'],\n                                  cv         = cv,\n                                  metric     = create_mean_pinball_loss(alpha=0.9)\n                              )\n\npred_intervals = pd.concat([predictions_q10, predictions_q90], axis=1)\npred_intervals.columns = ['lower_bound', 'upper_bound']\npred_intervals.head()\n</pre> # Backtesting on test data # ============================================================================== cv = TimeSeriesFold(          steps              = 24,           initial_train_size = len(data.loc[:end_validation]),          refit              = False      )  metric_q10, predictions_q10 = backtesting_forecaster(                                   forecaster = forecaster_q10,                                   y          = data['OT'],                                   cv         = cv,                                   metric     = create_mean_pinball_loss(alpha=0.1)                               )  metric_q90, predictions_q90 = backtesting_forecaster(                                   forecaster = forecaster_q90,                                   y          = data['OT'],                                   cv         = cv,                                   metric     = create_mean_pinball_loss(alpha=0.9)                               )  pred_intervals = pd.concat([predictions_q10, predictions_q90], axis=1) pred_intervals.columns = ['lower_bound', 'upper_bound'] pred_intervals.head() <pre>  0%|          | 0/336 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/336 [00:00&lt;?, ?it/s]</pre> Out[7]: lower_bound upper_bound 2018-04-04 00:00:00 31.848286 32.268329 2018-04-04 00:15:00 31.563152 32.150039 2018-04-04 00:30:00 31.269266 32.271917 2018-04-04 00:45:00 31.081806 32.139591 2018-04-04 01:00:00 30.670913 32.049578 In\u00a0[8]: Copied! <pre># Plot interval\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[end_validation:, 'OT'].plot(ax=ax, label='Real value', color='orange')\nax.fill_between(\n    data.loc[end_validation:].index,\n    pred_intervals['lower_bound'],\n    pred_intervals['upper_bound'],\n    color = 'gray',\n    alpha = 0.6,\n    zorder = 1,\n    label = '80% prediction interval'\n)\nax.set_xlabel('')\nax.set_title(\"Predicted intervals\")\nax.legend();\n</pre> # Plot interval # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[end_validation:, 'OT'].plot(ax=ax, label='Real value', color='orange') ax.fill_between(     data.loc[end_validation:].index,     pred_intervals['lower_bound'],     pred_intervals['upper_bound'],     color = 'gray',     alpha = 0.6,     zorder = 1,     label = '80% prediction interval' ) ax.set_xlabel('') ax.set_title(\"Predicted intervals\") ax.legend(); In\u00a0[9]: Copied! <pre># Plot intervals with zoom ['2018-05-01', '2018-05-15']\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata.loc[end_validation:, 'OT'].plot(ax=ax, label='Real value', color='orange')\nax.fill_between(\n    data.loc[end_validation:].index,\n    pred_intervals['lower_bound'],\n    pred_intervals['upper_bound'],\n    color = 'gray',\n    alpha = 0.6,\n    zorder = 1,\n    label = '80% prediction interval'\n)\nax.set_xlabel('')\nax.set_title(\"Predicted intervals (zoom in)\")\nax.set_xlim(['2018-05-01', '2018-05-15']);\n</pre> # Plot intervals with zoom ['2018-05-01', '2018-05-15'] # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data.loc[end_validation:, 'OT'].plot(ax=ax, label='Real value', color='orange') ax.fill_between(     data.loc[end_validation:].index,     pred_intervals['lower_bound'],     pred_intervals['upper_bound'],     color = 'gray',     alpha = 0.6,     zorder = 1,     label = '80% prediction interval' ) ax.set_xlabel('') ax.set_title(\"Predicted intervals (zoom in)\") ax.set_xlim(['2018-05-01', '2018-05-15']); In\u00a0[10]: Copied! <pre># Predicted interval coverage (on test data)\n# ==============================================================================\ncoverage = calculate_coverage(\n               y_true      = data.loc[end_validation:, 'OT'],\n               lower_bound = predictions_q10[\"pred\"], \n               upper_bound = predictions_q90[\"pred\"]\n           )\nprint(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")\n\n# Area of the interval\n# ==============================================================================\narea = (predictions_q90[\"pred\"] - predictions_q10[\"pred\"]).sum()\nprint(f\"Area of the interval: {round(area, 2)}\")\n</pre> # Predicted interval coverage (on test data) # ============================================================================== coverage = calculate_coverage(                y_true      = data.loc[end_validation:, 'OT'],                lower_bound = predictions_q10[\"pred\"],                 upper_bound = predictions_q90[\"pred\"]            ) print(f\"Predicted interval coverage: {round(100 * coverage, 2)} %\")  # Area of the interval # ============================================================================== area = (predictions_q90[\"pred\"] - predictions_q10[\"pred\"]).sum() print(f\"Area of the interval: {round(area, 2)}\") <pre>Predicted interval coverage: 77.37 %\nArea of the interval: 38980.18\n</pre> <p>The prediction intervals generated by quantile regression achieve an empirical coverage close to the nominal coverage of 80%.</p>"},{"location":"user_guides/probabilistic-forecasting-quantile-regression.html#probabilistic-forecasting-quantile-regression","title":"Probabilistic Forecasting: Quantile Regression\u00b6","text":""},{"location":"user_guides/probabilistic-forecasting-quantile-regression.html#predictions-backtesting","title":"Predictions (backtesting)\u00b6","text":"<p>Once the quantile forecasters are trained, they can be used to predict each of the bounds of the forecasting interval.</p>"},{"location":"user_guides/rolling_skewness.html","title":"Rolling skewness","text":"In\u00a0[\u00a0]: Copied! <pre># Custom class to create rolling skewness features\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import skew\n</pre> # Custom class to create rolling skewness features # ============================================================================== import numpy as np import pandas as pd from scipy.stats import skew In\u00a0[\u00a0]: Copied! <pre>class RollingSkewness():\n    \"\"\"\n    Custom class to create rolling skewness features.\n    \"\"\"\n\n    def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):\n        \n        if not isinstance(window_sizes, list):\n            window_sizes = [window_sizes]\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \n        rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')\n        rolling_skewness = rolling_obj.skew()\n        rolling_skewness = pd.DataFrame({\n                               self.features_names: rolling_skewness\n                           }).dropna()\n\n        return rolling_skewness\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \n        X = X[~np.isnan(X)]\n        if len(X) &gt; 0:\n            rolling_skewness = np.array([skew(X, bias=False)])\n        else:\n            rolling_skewness = np.nan\n        \n        return rolling_skewness\n</pre> class RollingSkewness():     \"\"\"     Custom class to create rolling skewness features.     \"\"\"      def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):                  if not isinstance(window_sizes, list):             window_sizes = [window_sizes]         self.window_sizes = window_sizes         self.features_names = features_names      def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:                  rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')         rolling_skewness = rolling_obj.skew()         rolling_skewness = pd.DataFrame({                                self.features_names: rolling_skewness                            }).dropna()          return rolling_skewness      def transform(self, X: np.ndarray) -&gt; np.ndarray:                  X = X[~np.isnan(X)]         if len(X) &gt; 0:             rolling_skewness = np.array([skew(X, bias=False)])         else:             rolling_skewness = np.nan                  return rolling_skewness In\u00a0[\u00a0]: Copied! <pre>class RollingSkewnessMultiSeries():\n    \"\"\"\n    Custom class to create rolling skewness features for multiple series.\n    \"\"\"\n\n    def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):\n        \n        if not isinstance(window_sizes, list):\n            window_sizes = [window_sizes]\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \n        rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')\n        rolling_skewness = rolling_obj.skew()\n        rolling_skewness = pd.DataFrame({\n                               self.features_names: rolling_skewness\n                           }).dropna()\n\n        return rolling_skewness\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \n        X_dim = X.ndim\n        if X_dim == 1:\n            n_series = 1  # Only one series\n            X = X.reshape(-1, 1)\n        else:\n            n_series = X.shape[1]  # Series (levels) to be predicted (present in last_window)\n        \n        n_stats = 1  # Only skewness is calculated\n        rolling_skewness = np.full(\n            shape=(n_series, n_stats), fill_value=np.nan, dtype=float\n        )\n        for i in range(n_series):\n            if len(X) &gt; 0:\n                rolling_skewness[i, :] = skew(X[:, i], bias=False)\n            else:\n                rolling_skewness[i, :] = np.nan      \n\n        if X_dim == 1:\n            rolling_skewness = rolling_skewness.flatten()  \n        \n        return rolling_skewness\n</pre> class RollingSkewnessMultiSeries():     \"\"\"     Custom class to create rolling skewness features for multiple series.     \"\"\"      def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):                  if not isinstance(window_sizes, list):             window_sizes = [window_sizes]         self.window_sizes = window_sizes         self.features_names = features_names      def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:                  rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')         rolling_skewness = rolling_obj.skew()         rolling_skewness = pd.DataFrame({                                self.features_names: rolling_skewness                            }).dropna()          return rolling_skewness      def transform(self, X: np.ndarray) -&gt; np.ndarray:                  X_dim = X.ndim         if X_dim == 1:             n_series = 1  # Only one series             X = X.reshape(-1, 1)         else:             n_series = X.shape[1]  # Series (levels) to be predicted (present in last_window)                  n_stats = 1  # Only skewness is calculated         rolling_skewness = np.full(             shape=(n_series, n_stats), fill_value=np.nan, dtype=float         )         for i in range(n_series):             if len(X) &gt; 0:                 rolling_skewness[i, :] = skew(X[:, i], bias=False)             else:                 rolling_skewness[i, :] = np.nan                if X_dim == 1:             rolling_skewness = rolling_skewness.flatten()                    return rolling_skewness"},{"location":"user_guides/save-load-forecaster.html","title":"Save and load forecaster","text":"<p> \u270e Note </p> <p>Learn how to use forecaster models in production.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\nfrom skforecast.utils import save_forecaster\nfrom skforecast.utils import load_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.recursive import ForecasterRecursiveMultiSeries from skforecast.utils import save_forecaster from skforecast.utils import load_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0}\n)\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('MS')\n</pre> # Download data # ============================================================================== data = fetch_dataset(     name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"date\"], \"header\": 0} ) data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('MS') <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n</pre> In\u00a0[3]: Copied! <pre># Create and train forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor     = RandomForestRegressor(random_state=123),\n                 lags          = 5,\n                 forecaster_id = \"forecaster_001\"\n             )\n\nforecaster.fit(y=data['y'])\nforecaster.predict(steps=3)\n</pre> # Create and train forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor     = RandomForestRegressor(random_state=123),                  lags          = 5,                  forecaster_id = \"forecaster_001\"              )  forecaster.fit(y=data['y']) forecaster.predict(steps=3) Out[3]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[4]: Copied! <pre># Save model\n# ==============================================================================\nsave_forecaster(forecaster, file_name='forecaster_001.joblib', verbose=False)\n</pre> # Save model # ============================================================================== save_forecaster(forecaster, file_name='forecaster_001.joblib', verbose=False) In\u00a0[5]: Copied! <pre># Load model\n# ==============================================================================\nforecaster_loaded = load_forecaster('forecaster_001.joblib', verbose=True)\n</pre> # Load model # ============================================================================== forecaster_loaded = load_forecaster('forecaster_001.joblib', verbose=True) <pre>=================== \nForecasterRecursive \n=================== \nRegressor: RandomForestRegressor \nLags: [1 2 3 4 5] \nWindow features: None \nWindow size: 5 \nSeries name: y \nExogenous included: False \nExogenous names: None \nTransformer for y: None \nTransformer for exog: None \nWeight function included: False \nDifferentiation order: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: \n    {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth':\n    None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None,\n    'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2,\n    'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100,\n    'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0,\n    'warm_start': False} \nfit_kwargs: {} \nCreation date: 2025-08-07 19:29:39 \nLast fit date: 2025-08-07 19:29:39 \nSkforecast version: 0.17.0 \nPython version: 3.12.11 \nForecaster id: forecaster_001 \n\n</pre> In\u00a0[6]: Copied! <pre># Predict\n# ==============================================================================\nforecaster_loaded.predict(steps=3)\n</pre> # Predict # ============================================================================== forecaster_loaded.predict(steps=3) Out[6]: <pre>2008-07-01    0.714526\n2008-08-01    0.789144\n2008-09-01    0.818433\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[7]: Copied! <pre># Forecaster identifier\n# ==============================================================================\nforecaster.forecaster_id\n</pre> # Forecaster identifier # ============================================================================== forecaster.forecaster_id Out[7]: <pre>'forecaster_001'</pre> In\u00a0[8]: Copied! <pre># Custom class to create rolling skewness features\n# ==============================================================================\nfrom scipy.stats import skew\n\n\nclass RollingSkewness():\n    \"\"\"\n    Custom class to create rolling skewness features.\n    \"\"\"\n\n    def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):\n        \n        if not isinstance(window_sizes, list):\n            window_sizes = [window_sizes]\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \n        rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')\n        rolling_skewness = rolling_obj.skew()\n        rolling_skewness = pd.DataFrame({\n                               self.features_names: rolling_skewness\n                           }).dropna()\n\n        return rolling_skewness\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \n        X = X[~np.isnan(X)]\n        if len(X) &gt; 0:\n            rolling_skewness = np.array([skew(X, bias=False)])\n        else:\n            rolling_skewness = np.array([np.nan])\n        \n        return rolling_skewness\n</pre> # Custom class to create rolling skewness features # ============================================================================== from scipy.stats import skew   class RollingSkewness():     \"\"\"     Custom class to create rolling skewness features.     \"\"\"      def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):                  if not isinstance(window_sizes, list):             window_sizes = [window_sizes]         self.window_sizes = window_sizes         self.features_names = features_names      def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:                  rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')         rolling_skewness = rolling_obj.skew()         rolling_skewness = pd.DataFrame({                                self.features_names: rolling_skewness                            }).dropna()          return rolling_skewness      def transform(self, X: np.ndarray) -&gt; np.ndarray:                  X = X[~np.isnan(X)]         if len(X) &gt; 0:             rolling_skewness = np.array([skew(X, bias=False)])         else:             rolling_skewness = np.array([np.nan])                  return rolling_skewness In\u00a0[9]: Copied! <pre># Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is between 2004-01-01 and 2005-01-01.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2004-01-01') &amp; (index &lt;= '2005-01-01'),\n                   0,\n                   1\n              )\n\n    return weights\n</pre> # Custom function to create weights # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between 2004-01-01 and 2005-01-01.     \"\"\"     weights = np.where(                   (index &gt;= '2004-01-01') &amp; (index &lt;= '2005-01-01'),                    0,                    1               )      return weights In\u00a0[10]: Copied! <pre># Create and train forecaster\n# ==============================================================================\nwindow_features = RollingSkewness(window_sizes=3)\n\nforecaster = ForecasterRecursive(\n                 regressor       = RandomForestRegressor(random_state=123),\n                 lags            = 3,\n                 window_features = window_features,\n                 weight_func     = custom_weights,\n                 forecaster_id   = \"forecaster_custom_features\"\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and train forecaster # ============================================================================== window_features = RollingSkewness(window_sizes=3)  forecaster = ForecasterRecursive(                  regressor       = RandomForestRegressor(random_state=123),                  lags            = 3,                  window_features = window_features,                  weight_func     = custom_weights,                  forecaster_id   = \"forecaster_custom_features\"              )  forecaster.fit(y=data['y']) <p> \u26a0 Warning </p> <p>The <code>save_forecaster</code> function will save the functions used to create the weights as a module (<code>custom_weights.py</code>). But the classes used to create the window features will not be saved. Therefore, you must ensure that these classes are available in the environment where the Forecaster is loaded.</p> In\u00a0[11]: Copied! <pre># Save model and custom function\n# ==============================================================================\nsave_forecaster(\n    forecaster, \n    file_name = 'forecaster_custom_features.joblib', \n    save_custom_functions = True, \n    verbose = False\n)\n</pre> # Save model and custom function # ============================================================================== save_forecaster(     forecaster,      file_name = 'forecaster_custom_features.joblib',      save_custom_functions = True,      verbose = False ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 SaveLoadSkforecastWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The Forecaster includes custom user-defined classes in the `window_features`         \u2502\n\u2502 argument. These classes are not saved automatically when saving the Forecaster.      \u2502\n\u2502 Please ensure you save these classes manually and import them before loading the     \u2502\n\u2502 Forecaster.                                                                          \u2502\n\u2502     Custom classes: RollingSkewness                                                  \u2502\n\u2502 Visit the documentation for more information:                                        \u2502\n\u2502 https://skforecast.org/latest/user_guides/save-load-forecaster.html#saving-and-loadi \u2502\n\u2502 ng-a-forecaster-model-with-custom-features                                           \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : SaveLoadSkforecastWarning                                                 \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:1996                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=SaveLoadSkforecastWarning)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <p>At this point, the <code>RollingSkewness</code> class is manually saved in a file called <code>rolling_skewness.py</code>. This file must be available in the environment where the Forecaster is loaded.</p> In\u00a0[12]: Copied! <pre># Load model and custom function\n# ==============================================================================\nfrom rolling_skewness import RollingSkewness  # This file has to be generated manually\nfrom custom_weights import custom_weights\n\nforecaster_loaded = load_forecaster('forecaster_custom_features.joblib', verbose=True)\n</pre> # Load model and custom function # ============================================================================== from rolling_skewness import RollingSkewness  # This file has to be generated manually from custom_weights import custom_weights  forecaster_loaded = load_forecaster('forecaster_custom_features.joblib', verbose=True) <pre>=================== \nForecasterRecursive \n=================== \nRegressor: RandomForestRegressor \nLags: [1 2 3] \nWindow features: ['rolling_skewness'] \nWindow size: 3 \nSeries name: y \nExogenous included: False \nExogenous names: None \nTransformer for y: None \nTransformer for exog: None \nWeight function included: True \nDifferentiation order: None \nTraining range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \nTraining index type: DatetimeIndex \nTraining index frequency: MS \nRegressor parameters: \n    {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth':\n    None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None,\n    'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2,\n    'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100,\n    'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0,\n    'warm_start': False} \nfit_kwargs: {} \nCreation date: 2025-08-07 19:29:40 \nLast fit date: 2025-08-07 19:29:40 \nSkforecast version: 0.17.0 \nPython version: 3.12.11 \nForecaster id: forecaster_custom_features \n\n</pre> In\u00a0[13]: Copied! <pre># Predict using loaded forecaster\n# ==============================================================================\nforecaster_loaded.predict(steps=5)\n</pre> # Predict using loaded forecaster # ============================================================================== forecaster_loaded.predict(steps=5) Out[13]: <pre>2008-07-01    0.808125\n2008-08-01    0.859447\n2008-09-01    0.933751\n2008-10-01    0.950768\n2008-11-01    0.914137\nFreq: MS, Name: pred, dtype: float64</pre> <p>When using a <code>ForecasterRecursiveMultiSeries</code>, the <code>save_forecaster</code> function will save a different module for each of the functions used to create the weights.</p> In\u00a0[14]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\ndata.head()\n</pre> # Data download # ============================================================================== data = fetch_dataset(name=\"items_sales\") data.head() <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> Out[14]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[15]: Copied! <pre># Custom function to create weights for each item\n# ==============================================================================\ndef custom_weights_item_1(index):\n    \"\"\"\n    Return 0 if index is between 2012-01-01 and 2012-06-01.\n    \"\"\"\n    weights = np.where(\n        (index &gt;= '2012-01-01') &amp; (index &lt;= '2012-06-01'), 0, 1\n    )\n\n    return weights\n\ndef custom_weights_item_2(index):\n    \"\"\"\n    Return 0 if index is between 2012-04-01 and 2013-01-01.\n    \"\"\"\n    weights = np.where(\n        (index &gt;= '2012-04-01') &amp; (index &lt;= '2013-01-01'), 0, 1\n    )\n\n    return weights\n\ndef custom_weights_item_3(index):\n    \"\"\"\n    Return 0 if index is between 2012-06-01 and 2013-01-01.\n    \"\"\"\n    weights = np.where(\n        (index &gt;= '2012-06-01') &amp; (index &lt;= '2013-01-01'), 0, 1\n    )\n\n    return weights\n</pre> # Custom function to create weights for each item # ============================================================================== def custom_weights_item_1(index):     \"\"\"     Return 0 if index is between 2012-01-01 and 2012-06-01.     \"\"\"     weights = np.where(         (index &gt;= '2012-01-01') &amp; (index &lt;= '2012-06-01'), 0, 1     )      return weights  def custom_weights_item_2(index):     \"\"\"     Return 0 if index is between 2012-04-01 and 2013-01-01.     \"\"\"     weights = np.where(         (index &gt;= '2012-04-01') &amp; (index &lt;= '2013-01-01'), 0, 1     )      return weights  def custom_weights_item_3(index):     \"\"\"     Return 0 if index is between 2012-06-01 and 2013-01-01.     \"\"\"     weights = np.where(         (index &gt;= '2012-06-01') &amp; (index &lt;= '2013-01-01'), 0, 1     )      return weights In\u00a0[16]: Copied! <pre># Custom class to create rolling skewness features (multi-series)\n# ==============================================================================\nfrom scipy.stats import skew\n\n\nclass RollingSkewnessMultiSeries():\n    \"\"\"\n    Custom class to create rolling skewness features for multiple series.\n    \"\"\"\n\n    def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):\n        \n        if not isinstance(window_sizes, list):\n            window_sizes = [window_sizes]\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \n        rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')\n        rolling_skewness = rolling_obj.skew()\n        rolling_skewness = pd.DataFrame({\n                               self.features_names: rolling_skewness\n                           }).dropna()\n\n        return rolling_skewness\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \n        X_dim = X.ndim\n        if X_dim == 1:\n            n_series = 1  # Only one series\n            X = X.reshape(-1, 1)\n        else:\n            n_series = X.shape[1]  # Series (levels) to be predicted (present in last_window)\n        \n        n_stats = 1  # Only skewness is calculated\n        rolling_skewness = np.full(\n            shape=(n_series, n_stats), fill_value=np.nan, dtype=float\n        )\n        for i in range(n_series):\n            if len(X) &gt; 0:\n                rolling_skewness[i, :] = skew(X[:, i], bias=False)\n            else:\n                rolling_skewness[i, :] = np.nan      \n\n        if X_dim == 1:\n            rolling_skewness = rolling_skewness.flatten()  \n        \n        return rolling_skewness\n</pre> # Custom class to create rolling skewness features (multi-series) # ============================================================================== from scipy.stats import skew   class RollingSkewnessMultiSeries():     \"\"\"     Custom class to create rolling skewness features for multiple series.     \"\"\"      def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):                  if not isinstance(window_sizes, list):             window_sizes = [window_sizes]         self.window_sizes = window_sizes         self.features_names = features_names      def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:                  rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')         rolling_skewness = rolling_obj.skew()         rolling_skewness = pd.DataFrame({                                self.features_names: rolling_skewness                            }).dropna()          return rolling_skewness      def transform(self, X: np.ndarray) -&gt; np.ndarray:                  X_dim = X.ndim         if X_dim == 1:             n_series = 1  # Only one series             X = X.reshape(-1, 1)         else:             n_series = X.shape[1]  # Series (levels) to be predicted (present in last_window)                  n_stats = 1  # Only skewness is calculated         rolling_skewness = np.full(             shape=(n_series, n_stats), fill_value=np.nan, dtype=float         )         for i in range(n_series):             if len(X) &gt; 0:                 rolling_skewness[i, :] = skew(X[:, i], bias=False)             else:                 rolling_skewness[i, :] = np.nan                if X_dim == 1:             rolling_skewness = rolling_skewness.flatten()                    return rolling_skewness <p> \u26a0 Warning </p> <p>When <code>weight_func</code> is a <code>dict</code> and does not contain any of the series, for instance:</p> <pre># Weights are not included for item_2\nweight_func_dict = {\n    'item_1': custom_weights_item_1,\n    'item_3': custom_weights_item_3\n}\n</pre> <p>You must create a function that returns all 1's as weights of that series.</p> <pre>def custom_weights_all_1(index):\n    \"\"\"\n    Return 1 for all elements in the index.\n    \"\"\"\n    weights = np.ones(len(index))\n    return weights\n\n# item_2 dummy weights\nweight_func_dict = {\n    'item_1': custom_weights_item_1,\n    'item_2': custom_weights_all_1,\n    'item_3': custom_weights_item_3\n}\n</pre> In\u00a0[17]: Copied! <pre># Create and train ForecasterRecursiveMultiSeries\n# ==============================================================================\nwindow_features = RollingSkewnessMultiSeries(window_sizes=3)\nweight_func_dict = {\n    'item_1': custom_weights_item_1,\n    'item_2': custom_weights_item_2,\n    'item_3': custom_weights_item_3\n}\n\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor       = RandomForestRegressor(random_state=123),\n                 lags            = 3,\n                 window_features = window_features,\n                 encoding        = 'ordinal',\n                 weight_func     = weight_func_dict\n             )\n\nforecaster.fit(series=data)\nforecaster\n</pre> # Create and train ForecasterRecursiveMultiSeries # ============================================================================== window_features = RollingSkewnessMultiSeries(window_sizes=3) weight_func_dict = {     'item_1': custom_weights_item_1,     'item_2': custom_weights_item_2,     'item_3': custom_weights_item_3 }  forecaster = ForecasterRecursiveMultiSeries(                  regressor       = RandomForestRegressor(random_state=123),                  lags            = 3,                  window_features = window_features,                  encoding        = 'ordinal',                  weight_func     = weight_func_dict              )  forecaster.fit(series=data) forecaster <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[17]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: RandomForestRegressor</li> <li>Lags: [1 2 3]</li> <li>Window features: ['rolling_skewness']</li> <li>Window size: 3</li> <li>Series encoding: ordinal</li> <li>Exogenous included: False</li> <li>Weight function included: True</li> <li>Series weights: None</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:29:41</li> <li>Last fit date: 2025-08-07 19:29:44</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): item_1, item_2, item_3</li> <li>Training range: 'item_1': ['2012-01-01', '2015-01-01'], 'item_2': ['2012-01-01', '2015-01-01'], 'item_3': ['2012-01-01', '2015-01-01']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[18]: Copied! <pre># Save model and custom function\n# ==============================================================================\nsave_forecaster(\n    forecaster, \n    file_name = 'forecaster_multiseries_custom_features.joblib', \n    save_custom_functions = True, \n    verbose = False\n)\n</pre> # Save model and custom function # ============================================================================== save_forecaster(     forecaster,      file_name = 'forecaster_multiseries_custom_features.joblib',      save_custom_functions = True,      verbose = False ) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 SaveLoadSkforecastWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The Forecaster includes custom user-defined classes in the `window_features`         \u2502\n\u2502 argument. These classes are not saved automatically when saving the Forecaster.      \u2502\n\u2502 Please ensure you save these classes manually and import them before loading the     \u2502\n\u2502 Forecaster.                                                                          \u2502\n\u2502     Custom classes: RollingSkewnessMultiSeries                                       \u2502\n\u2502 Visit the documentation for more information:                                        \u2502\n\u2502 https://skforecast.org/latest/user_guides/save-load-forecaster.html#saving-and-loadi \u2502\n\u2502 ng-a-forecaster-model-with-custom-features                                           \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : SaveLoadSkforecastWarning                                                 \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:1996                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=SaveLoadSkforecastWarning)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[19]: Copied! <pre># Load model and custom function\n# ==============================================================================\nfrom rolling_skewness import RollingSkewnessMultiSeries  # This file has to be generated manually\nfrom custom_weights_item_1 import custom_weights_item_1\nfrom custom_weights_item_2 import custom_weights_item_2\nfrom custom_weights_item_3 import custom_weights_item_3\n\nforecaster_loaded = load_forecaster(\n    'forecaster_multiseries_custom_features.joblib', verbose=True\n)\n</pre> # Load model and custom function # ============================================================================== from rolling_skewness import RollingSkewnessMultiSeries  # This file has to be generated manually from custom_weights_item_1 import custom_weights_item_1 from custom_weights_item_2 import custom_weights_item_2 from custom_weights_item_3 import custom_weights_item_3  forecaster_loaded = load_forecaster(     'forecaster_multiseries_custom_features.joblib', verbose=True ) <pre>============================== \nForecasterRecursiveMultiSeries \n============================== \nRegressor: RandomForestRegressor \nLags: [1 2 3] \nWindow features: ['rolling_skewness'] \nWindow size: 3 \nSeries encoding: ordinal \nSeries names (levels): item_1, item_2, item_3 \nExogenous included: False \nExogenous names: None \nTransformer for series: None \nTransformer for exog: None \nWeight function included: True \nSeries weights: None \nDifferentiation order: None \nTraining range: \n    'item_1': ['2012-01-01', '2015-01-01'], 'item_2': ['2012-01-01', '2015-01-01'],\n    'item_3': ['2012-01-01', '2015-01-01'] \nTraining index type: DatetimeIndex \nTraining index frequency: D \nRegressor parameters: \n    {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth':\n    None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None,\n    'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2,\n    'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100,\n    'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0,\n    'warm_start': False} \nfit_kwargs: {} \nCreation date: 2025-08-07 19:29:41 \nLast fit date: 2025-08-07 19:29:44 \nSkforecast version: 0.17.0 \nPython version: 3.12.11 \nForecaster id: None \n\n</pre> In\u00a0[20]: Copied! <pre># Predict using loaded forecaster\n# ==============================================================================\nforecaster_loaded.predict(steps=5, levels=None)  # Predict all levels\n</pre> # Predict using loaded forecaster # ============================================================================== forecaster_loaded.predict(steps=5, levels=None)  # Predict all levels Out[20]: level pred 2015-01-02 item_1 14.818313 2015-01-02 item_2 17.954045 2015-01-02 item_3 19.676498 2015-01-03 item_1 14.961743 2015-01-03 item_2 17.530592 2015-01-03 item_3 19.207165 2015-01-04 item_1 18.349711 2015-01-04 item_2 17.792810 2015-01-04 item_3 19.919855 2015-01-05 item_1 18.639790 2015-01-05 item_2 18.447346 2015-01-05 item_3 22.158983 2015-01-06 item_1 17.254107 2015-01-06 item_2 19.599428 2015-01-06 item_3 22.687187"},{"location":"user_guides/save-load-forecaster.html#save-and-load-forecasters","title":"Save and load forecasters\u00b6","text":"<p>Skforecast models can be easily saved and loaded from disk using the joblib library. Two handy functions, <code>save_forecaster</code> and <code>load_forecaster</code> are available to streamline this process. See below for a simple example.</p> <p>A <code>forecaster_id</code> has been included when initializing the Forecaster, this may help to identify the target of the model.</p>"},{"location":"user_guides/save-load-forecaster.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/save-load-forecaster.html#save-and-load-forecaster-model","title":"Save and load forecaster model\u00b6","text":""},{"location":"user_guides/save-load-forecaster.html#saving-and-loading-a-forecaster-model-with-custom-features","title":"Saving and Loading a Forecaster Model with Custom Features\u00b6","text":"<p>Sometimes external objects are needed when creating a Forecaster. For example:</p> <ul> <li><p>Custom class to create window and custom features.</p> </li> <li><p>A function to reduce the impact of some dates on the model, Weighted Time Series Forecasting.</p> </li> </ul> <p>For your code to work properly, these functions must be available in the environment where the Forecaster is loaded.</p>"},{"location":"user_guides/save-load-forecaster.html#forecasterrecursivemultiseries","title":"ForecasterRecursiveMultiSeries\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html","title":"Skforecast in GPU","text":"<p>Traditionally, machine learning algorithms have been executed on CPUs (Central Processing Units)\u2014general-purpose processors capable of handling a wide variety of tasks. However, CPUs are not optimized for the highly parallelized matrix operations that many machine learning algorithms rely on, often resulting in slower training times and limited scalability. In contrast, GPUs (Graphics Processing Units) are specifically designed for parallel processing, capable of performing thousands of simultaneous mathematical operations. This makes them particularly well-suited for training and deploying large-scale machine learning models.</p> <p>Several popular machine learning libraries have implemented GPU acceleration, including XGBoost, LightGBM, CatBoost and CuML. By leveraging GPU capabilities, these libraries can dramatically reduce training times and enhance scalability. The following sections demonstrate how execute skforecast with GPU acceleration to build efficient and powerful forecasting models.</p> <p> \u270e Note </p> <p> <p>The performance advantage of using a GPU depends heavily on the specific task and the size of the dataset. Generally, GPU acceleration offers the greatest benefits when working with large datasets and complex models, where its parallel processing capabilities can significantly reduce training times.</p> <p>In recursive forecasting (<code>ForecasterRecursive</code> and <code>ForecasterRecursiveMultiseries</code>) the prediction phase must be executed sequentially since each time step depends on the previous prediction. This inherent dependency prevents parallelization during inference, which explains why model fitting is substantially faster on a GPU, while prediction can actually be slower compared to using a CPU. To overcome this limitation, skforecast automatically switches the regressor to use the CPU for prediction, even if it was trained on a GPU.</p> <p>In contrast, direct forecasters (<code>ForecasterDirect</code>, <code>ForecasterDirectMultivariate</code>) do not rely on previous predictions during inference. This lack of dependency allows both training and prediction to fully benefit from GPU acceleration.</p> </p> <p> \u270e Note </p> <p> <p>Despite the significant advantages offered by GPUs (specifically Nvidia GPUs) in accelerating machine learning computations, access to them is often limited due to high costs or other practical constraints. Fortunatelly, Google Colaboratory (Colab), a free Jupyter notebook environment, allows users to run Python code in the cloud, with access to powerful hardware resources such as GPUs. This makes it an excellent platform for experimenting with machine learning models, especially those that require intensive computations. The following links provide access to Google Colab notebooks that demonstrate how to use skforecast with GPU acceleration.</p> </p> <ul> <li>Skforecast in GPU: XGBoost</li> <li>Skforecast in GPU: LightGBM</li> <li>Skforecast in GPU: CatBoost</li> <li>Skforecast in GPU: Rapids cuML</li> </ul> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nimport torch\nimport psutil\nimport xgboost\nfrom xgboost import XGBRegressor\nimport lightgbm\nfrom lightgbm import LGBMRegressor\nimport catboost\nfrom catboost import CatBoostRegressor\nimport warnings\n\nimport skforecast\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import backtesting_forecaster, TimeSeriesFold\n\nprint(f\"skforecast version : {skforecast.__version__}\")\nprint(f\"xgboost version    : {xgboost.__version__}\")\nprint(f\"lightgbm version   : {lightgbm.__version__}\")\nprint(f\"catboost version   : {catboost.__version__}\")\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd import torch import psutil import xgboost from xgboost import XGBRegressor import lightgbm from lightgbm import LGBMRegressor import catboost from catboost import CatBoostRegressor import warnings  import skforecast from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import backtesting_forecaster, TimeSeriesFold  print(f\"skforecast version : {skforecast.__version__}\") print(f\"xgboost version    : {xgboost.__version__}\") print(f\"lightgbm version   : {lightgbm.__version__}\") print(f\"catboost version   : {catboost.__version__}\") <pre>skforecast version : 0.16.0\nxgboost version    : 2.1.2\nlightgbm version   : 4.5.0\ncatboost version   : 1.2.8\n</pre> In\u00a0[2]: Copied! <pre># Print information abput the GPU and CPU\n# ==============================================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated :', round(torch.cuda.memory_allocated(0) / 1024**3, 1), 'GB')\n    print('Reserved  :', round(torch.cuda.memory_reserved(0) / 1024**3, 1), 'GB')\n\nprint(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\")\n</pre> # Print information abput the GPU and CPU # ============================================================================== device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print('Using device:', device)  if device.type == 'cuda':     print(torch.cuda.get_device_name(0))     print('Memory Usage:')     print('Allocated :', round(torch.cuda.memory_allocated(0) / 1024**3, 1), 'GB')     print('Reserved  :', round(torch.cuda.memory_reserved(0) / 1024**3, 1), 'GB')  print(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\") <pre>Using device: cuda\nNVIDIA T1200 Laptop GPU\nMemory Usage:\nAllocated : 0.0 GB\nReserved  : 0.0 GB\nCPU RAM Free: 13.62 GB\n</pre> In\u00a0[3]: Copied! <pre># Data\n# ==============================================================================\nn = 1_000_000\ndata = pd.Series(\n    data  = np.random.normal(size=n), \n    index = pd.date_range(start=\"1990-01-01\", periods=n, freq=\"h\"),\n    name  = 'y'\n)\ndata.head(2)\n</pre> # Data # ============================================================================== n = 1_000_000 data = pd.Series(     data  = np.random.normal(size=n),      index = pd.date_range(start=\"1990-01-01\", periods=n, freq=\"h\"),     name  = 'y' ) data.head(2) Out[3]: <pre>1990-01-01 00:00:00    1.500408\n1990-01-01 01:00:00   -1.112868\nFreq: h, Name: y, dtype: float64</pre> <p>To run an XGBoost model (version 2.0 or higher) on a GPU, set the argument device='cuda' during initialization.</p> In\u00a0[4]: Copied! <pre># Suppress warnings\n# ==============================================================================\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*Falling back to prediction using DMatrix.*\",\n    category=UserWarning,\n    module=\"xgboost\"\n)\n</pre> # Suppress warnings # ============================================================================== warnings.filterwarnings(     \"ignore\",     message=\".*Falling back to prediction using DMatrix.*\",     category=UserWarning,     module=\"xgboost\" ) In\u00a0[5]: Copied! <pre># Create and train forecaster with a XGBRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = XGBRegressor(\n                                 n_estimators = 1000,\n                                 device       = 'cuda',\n                                 verbosity    = 1\n                             ),\n                 lags = 50\n             )\n\nstart_time = pd.Timestamp.now()\nforecaster.fit(y=data)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Training time using GPU: {elapsed_time}\")\n\n# Predict\n# ==============================================================================\nstart_time = pd.Timestamp.now()\nforecaster.predict(steps=100)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Prediction time using GPU: {elapsed_time}\")\n\n# Backtesting\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 100,\n         initial_train_size = 990_000,\n         refit              = False,\n         verbose            = False\n     )\nstart_time = pd.Timestamp.now()\n_ = backtesting_forecaster(\n        forecaster = forecaster,\n        y          = data,\n        cv         = cv,\n        metric     = 'mean_absolute_error'\n\n    )\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Backtesting time using GPU: {elapsed_time}\")\n</pre> # Create and train forecaster with a XGBRegressor using GPU # ============================================================================== forecaster = ForecasterRecursive(                  regressor = XGBRegressor(                                  n_estimators = 1000,                                  device       = 'cuda',                                  verbosity    = 1                              ),                  lags = 50              )  start_time = pd.Timestamp.now() forecaster.fit(y=data) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Training time using GPU: {elapsed_time}\")  # Predict # ============================================================================== start_time = pd.Timestamp.now() forecaster.predict(steps=100) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Prediction time using GPU: {elapsed_time}\")  # Backtesting # ============================================================================== cv = TimeSeriesFold(          steps              = 100,          initial_train_size = 990_000,          refit              = False,          verbose            = False      ) start_time = pd.Timestamp.now() _ = backtesting_forecaster(         forecaster = forecaster,         y          = data,         cv         = cv,         metric     = 'mean_absolute_error'      ) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Backtesting time using GPU: {elapsed_time}\") <pre>Training time using GPU: 0 days 00:00:28.760755\nPrediction time using GPU: 0 days 00:00:00.095367\n</pre> <pre>  0%|          | 0/100 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting time using GPU: 0 days 00:00:43.120299\n</pre> In\u00a0[6]: Copied! <pre># Create and train forecaster with a XGBRegressor using CPU\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = XGBRegressor(n_estimators=1000),\n                 lags      = 50\n             )\n\nstart_time = pd.Timestamp.now()\nforecaster.fit(y=data)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Training time using CPU: {elapsed_time}\")\n\n# Predict\n# ==============================================================================\nstart_time = pd.Timestamp.now()\nforecaster.predict(steps=100)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Prediction time using CPU: {elapsed_time}\")\n\n# Backtesting\n# ==============================================================================\nstart_time = pd.Timestamp.now()\n_ = backtesting_forecaster(\n        forecaster = forecaster,\n        y          = data,\n        cv         = cv,\n        metric     = 'mean_absolute_error'\n\n    )\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Backtesting time using CPU: {elapsed_time}\")\n</pre> # Create and train forecaster with a XGBRegressor using CPU # ============================================================================== forecaster = ForecasterRecursive(                  regressor = XGBRegressor(n_estimators=1000),                  lags      = 50              )  start_time = pd.Timestamp.now() forecaster.fit(y=data) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Training time using CPU: {elapsed_time}\")  # Predict # ============================================================================== start_time = pd.Timestamp.now() forecaster.predict(steps=100) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Prediction time using CPU: {elapsed_time}\")  # Backtesting # ============================================================================== start_time = pd.Timestamp.now() _ = backtesting_forecaster(         forecaster = forecaster,         y          = data,         cv         = cv,         metric     = 'mean_absolute_error'      ) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Backtesting time using CPU: {elapsed_time}\") <pre>Training time using CPU: 0 days 00:00:44.323256\nPrediction time using CPU: 0 days 00:00:00.143112\n</pre> <pre>  0%|          | 0/100 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting time using CPU: 0 days 00:00:48.289869\n</pre> In\u00a0[7]: Copied! <pre># Suppress warnings\n# ==============================================================================\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\",\n    category=FutureWarning,\n    module=\"sklearn.utils.deprecation\"\n)\n</pre> # Suppress warnings # ============================================================================== warnings.filterwarnings(     \"ignore\",     message=\"'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\",     category=FutureWarning,     module=\"sklearn.utils.deprecation\" ) <p>When using Google colab, run the following in a notebook cell to ensure LightGBM can utilize the NVIDIA GPU when executing in google colab.</p> <pre>!mkdir -p /etc/OpenCL/vendors &amp;&amp; echo \"libnvidia-opencl.so.1\" &gt; /etc/OpenCL/vendors/nvidia.icd\n</pre> In\u00a0[8]: Copied! <pre># Create and train forecaster with a LGBMRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(n_estimators=1000, device='gpu', verbose=-1),\n                 lags      = 50\n             )\n\nstart_time = pd.Timestamp.now()\nforecaster.fit(y=data)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Training time using GPU: {elapsed_time}\")\n\n# Predict\n# ==============================================================================\nstart_time = pd.Timestamp.now()\nforecaster.predict(steps=100)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Prediction time using GPU: {elapsed_time}\")\n\n# Backtesting\n# ==============================================================================\nstart_time = pd.Timestamp.now()\n_ = backtesting_forecaster(\n        forecaster = forecaster,\n        y          = data,\n        cv         = cv,\n        metric     = 'mean_absolute_error'\n\n    )\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Backtesting time using GPU: {elapsed_time}\")\n</pre> # Create and train forecaster with a LGBMRegressor using GPU # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(n_estimators=1000, device='gpu', verbose=-1),                  lags      = 50              )  start_time = pd.Timestamp.now() forecaster.fit(y=data) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Training time using GPU: {elapsed_time}\")  # Predict # ============================================================================== start_time = pd.Timestamp.now() forecaster.predict(steps=100) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Prediction time using GPU: {elapsed_time}\")  # Backtesting # ============================================================================== start_time = pd.Timestamp.now() _ = backtesting_forecaster(         forecaster = forecaster,         y          = data,         cv         = cv,         metric     = 'mean_absolute_error'      ) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Backtesting time using GPU: {elapsed_time}\") <pre>Training time using GPU: 0 days 00:00:21.657198\nPrediction time using GPU: 0 days 00:00:00.059898\n</pre> <pre>  0%|          | 0/100 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting time using GPU: 0 days 00:00:24.554291\n</pre> In\u00a0[9]: Copied! <pre># Create and train forecaster with a LGBMRegressor using CPU\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = LGBMRegressor(n_estimators=1000, device='cpu', verbose=-1),\n                 lags      = 50\n             )\n\nstart_time = pd.Timestamp.now()\nforecaster.fit(y=data)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Training time using CPU: {elapsed_time}\")\n\n# Predict\n# ==============================================================================\nstart_time = pd.Timestamp.now()\nforecaster.predict(steps=100)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Prediction time using CPU: {elapsed_time}\")\n\n# Backtesting\n# ==============================================================================\nstart_time = pd.Timestamp.now()\n_ = backtesting_forecaster(\n        forecaster = forecaster,\n        y          = data,\n        cv         = cv,\n        metric     = 'mean_absolute_error'\n\n    )\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Backtesting time using CPU: {elapsed_time}\")\n</pre> # Create and train forecaster with a LGBMRegressor using CPU # ============================================================================== forecaster = ForecasterRecursive(                  regressor = LGBMRegressor(n_estimators=1000, device='cpu', verbose=-1),                  lags      = 50              )  start_time = pd.Timestamp.now() forecaster.fit(y=data) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Training time using CPU: {elapsed_time}\")  # Predict # ============================================================================== start_time = pd.Timestamp.now() forecaster.predict(steps=100) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Prediction time using CPU: {elapsed_time}\")  # Backtesting # ============================================================================== start_time = pd.Timestamp.now() _ = backtesting_forecaster(         forecaster = forecaster,         y          = data,         cv         = cv,         metric     = 'mean_absolute_error'      ) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Backtesting time using CPU: {elapsed_time}\") <pre>Training time using CPU: 0 days 00:00:27.196106\nPrediction time using CPU: 0 days 00:00:00.044868\n</pre> <pre>  0%|          | 0/100 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting time using CPU: 0 days 00:00:25.313712\n</pre> In\u00a0[10]: Copied! <pre># Create and train forecaster with a CatBoostRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = CatBoostRegressor(n_estimators=1000, task_type='GPU', silent=True, allow_writing_files=False),\n                 lags      = 50\n             )\n\nstart_time = pd.Timestamp.now()\nforecaster.fit(y=data)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Training time using GPU: {elapsed_time}\")\n\n# Predict\n# ==============================================================================\nstart_time = pd.Timestamp.now()\nforecaster.predict(steps=100)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Prediction time using GPU: {elapsed_time}\")\n\n# Backtesting\n# ==============================================================================\nstart_time = pd.Timestamp.now()\n_ = backtesting_forecaster(\n        forecaster = forecaster,\n        y          = data,\n        cv         = cv,\n        metric     = 'mean_absolute_error'\n\n    )\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Backtesting time using GPU: {elapsed_time}\")\n</pre> # Create and train forecaster with a CatBoostRegressor using GPU # ============================================================================== forecaster = ForecasterRecursive(                  regressor = CatBoostRegressor(n_estimators=1000, task_type='GPU', silent=True, allow_writing_files=False),                  lags      = 50              )  start_time = pd.Timestamp.now() forecaster.fit(y=data) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Training time using GPU: {elapsed_time}\")  # Predict # ============================================================================== start_time = pd.Timestamp.now() forecaster.predict(steps=100) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Prediction time using GPU: {elapsed_time}\")  # Backtesting # ============================================================================== start_time = pd.Timestamp.now() _ = backtesting_forecaster(         forecaster = forecaster,         y          = data,         cv         = cv,         metric     = 'mean_absolute_error'      ) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Backtesting time using GPU: {elapsed_time}\") <pre>Training time using GPU: 0 days 00:00:26.810713\nPrediction time using GPU: 0 days 00:00:00.109821\n</pre> <pre>  0%|          | 0/100 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting time using GPU: 0 days 00:00:30.199580\n</pre> In\u00a0[11]: Copied! <pre># Create and train forecaster with a CatBoostRegressor using CPU\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = CatBoostRegressor(n_estimators=1000, task_type='CPU', silent=True, allow_writing_files=False),\n                 lags      = 50\n             )\n\nstart_time = pd.Timestamp.now()\nforecaster.fit(y=data)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Training time using CPU: {elapsed_time}\")\n\n# Predict\n# ==============================================================================\nstart_time = pd.Timestamp.now()\nforecaster.predict(steps=100)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Prediction time using CPU: {elapsed_time}\")\n\n# Backtesting\n# ==============================================================================\nstart_time = pd.Timestamp.now()\n_ = backtesting_forecaster(\n        forecaster = forecaster,\n        y          = data,\n        cv         = cv,\n        metric     = 'mean_absolute_error'\n\n    )\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Backtesting time using CPU: {elapsed_time}\")\n</pre> # Create and train forecaster with a CatBoostRegressor using CPU # ============================================================================== forecaster = ForecasterRecursive(                  regressor = CatBoostRegressor(n_estimators=1000, task_type='CPU', silent=True, allow_writing_files=False),                  lags      = 50              )  start_time = pd.Timestamp.now() forecaster.fit(y=data) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Training time using CPU: {elapsed_time}\")  # Predict # ============================================================================== start_time = pd.Timestamp.now() forecaster.predict(steps=100) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Prediction time using CPU: {elapsed_time}\")  # Backtesting # ============================================================================== start_time = pd.Timestamp.now() _ = backtesting_forecaster(         forecaster = forecaster,         y          = data,         cv         = cv,         metric     = 'mean_absolute_error'      ) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Backtesting time using CPU: {elapsed_time}\") <pre>Training time using CPU: 0 days 00:01:13.747194\nPrediction time using CPU: 0 days 00:00:00.101679\n</pre> <pre>  0%|          | 0/100 [00:00&lt;?, ?it/s]</pre> <pre>Backtesting time using CPU: 0 days 00:01:06.052743\n</pre> In\u00a0[\u00a0]: Copied! <pre># Libraries\n# ==============================================================================\nimport cuml\nfrom sklearn.ensemble import RandomForestRegressor\n\nprint(f\"skforecast version : {skforecast.__version__}\")\nprint(f\"cuml version       : {cuml.__version__}\")\n</pre> # Libraries # ============================================================================== import cuml from sklearn.ensemble import RandomForestRegressor  print(f\"skforecast version : {skforecast.__version__}\") print(f\"cuml version       : {cuml.__version__}\") In\u00a0[\u00a0]: Copied! <pre># Create and train forecaster with a RandomForestRegressor using GPU\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                regressor = cuml.ensemble.RandomForestRegressor(\n                                n_estimators=200,\n                                max_depth=5,\n                            ),\n                lags = 20\n             )\n\nstart_time = pd.Timestamp.now()\nforecaster.fit(y=data)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Training time using GPU: {elapsed_time}\")\n\n# Predict\n# ==============================================================================\nstart_time = pd.Timestamp.now()\nforecaster.predict(steps=100)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Prediction time using GPU: {elapsed_time}\")\n\n# Backtesting\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 100,\n         initial_train_size = 90_000,\n         refit              = False,\n         verbose            = False\n     )\nstart_time = pd.Timestamp.now()\n_ = backtesting_forecaster(\n        forecaster = forecaster,\n        y          = data,\n        cv         = cv,\n        metric     = 'mean_absolute_error'\n\n    )\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Backtesting time using GPU: {elapsed_time}\")\n</pre> # Create and train forecaster with a RandomForestRegressor using GPU # ============================================================================== forecaster = ForecasterRecursive(                 regressor = cuml.ensemble.RandomForestRegressor(                                 n_estimators=200,                                 max_depth=5,                             ),                 lags = 20              )  start_time = pd.Timestamp.now() forecaster.fit(y=data) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Training time using GPU: {elapsed_time}\")  # Predict # ============================================================================== start_time = pd.Timestamp.now() forecaster.predict(steps=100) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Prediction time using GPU: {elapsed_time}\")  # Backtesting # ============================================================================== cv = TimeSeriesFold(          steps              = 100,          initial_train_size = 90_000,          refit              = False,          verbose            = False      ) start_time = pd.Timestamp.now() _ = backtesting_forecaster(         forecaster = forecaster,         y          = data,         cv         = cv,         metric     = 'mean_absolute_error'      ) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Backtesting time using GPU: {elapsed_time}\") In\u00a0[\u00a0]: Copied! <pre># Create and train forecaster with a RandomForestRegressor using CPU\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                RandomForestRegressor(n_estimators=200, max_depth=5),\n                lags = 20\n             )\n\nstart_time = pd.Timestamp.now()\nforecaster.fit(y=data)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Training time using CPU: {elapsed_time}\")\n\n# Predict\n# ==============================================================================\nstart_time = pd.Timestamp.now()\nforecaster.predict(steps=100)\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Prediction time using CPU: {elapsed_time}\")\n\n# Backtesting\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 100,\n         initial_train_size = 90_000,\n         refit              = False,\n         verbose            = False\n     )\nstart_time = pd.Timestamp.now()\n_ = backtesting_forecaster(\n        forecaster = forecaster,\n        y          = data,\n        cv         = cv,\n        metric     = 'mean_absolute_error'\n\n    )\nelapsed_time = pd.Timestamp.now() - start_time\n\nprint(f\"Backtesting time using CPU: {elapsed_time}\")\n</pre> # Create and train forecaster with a RandomForestRegressor using CPU # ============================================================================== forecaster = ForecasterRecursive(                 RandomForestRegressor(n_estimators=200, max_depth=5),                 lags = 20              )  start_time = pd.Timestamp.now() forecaster.fit(y=data) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Training time using CPU: {elapsed_time}\")  # Predict # ============================================================================== start_time = pd.Timestamp.now() forecaster.predict(steps=100) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Prediction time using CPU: {elapsed_time}\")  # Backtesting # ============================================================================== cv = TimeSeriesFold(          steps              = 100,          initial_train_size = 90_000,          refit              = False,          verbose            = False      ) start_time = pd.Timestamp.now() _ = backtesting_forecaster(         forecaster = forecaster,         y          = data,         cv         = cv,         metric     = 'mean_absolute_error'      ) elapsed_time = pd.Timestamp.now() - start_time  print(f\"Backtesting time using CPU: {elapsed_time}\")"},{"location":"user_guides/skforecast-in-GPU.html#skforecast-in-gpu","title":"Skforecast in GPU\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#xgboost","title":"XGBoost\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#lightgbm","title":"LightGBM\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#catboost","title":"CatBoost\u00b6","text":""},{"location":"user_guides/skforecast-in-GPU.html#rapids-cuml","title":"RAPIDS cuML\u00b6","text":"<p>cuML is a library for executing machine learning algorithms on GPUs with an API that closely follows the scikit-learn API.</p> <p>To use cuML with skforecast, you need to install the RAPIDS cuML library. The installation process may vary depending on your environment and the version of CUDA you have installed. You can find detailed installation instructions in the RAPIDS documentation.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html","title":"Data transformation","text":"In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.recursive import ForecasterRecursiveMultiSeries\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import grid_search_forecaster\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from lightgbm import LGBMRegressor from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import MinMaxScaler from sklearn.preprocessing import PowerTransformer from sklearn.preprocessing import FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.recursive import ForecasterRecursiveMultiSeries from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import grid_search_forecaster In\u00a0[2]: Copied! <pre># Download data\n# ==============================================================================\ndata = fetch_dataset(\"h2o_exog\")\n</pre> # Download data # ============================================================================== data = fetch_dataset(\"h2o_exog\") <pre>h2o_exog\n--------\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008. Two additional variables (exog_1, exog_2) are\nsimulated.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice (3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,\nhttps://github.com/robjhyndman/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (195, 3)\n</pre> In\u00a0[3]: Copied! <pre># Data preprocessing\n# ==============================================================================\ndata.index.name = 'date'\n# Add an extra categorical variable\ndata['exog_3'] = ([\"A\"] * int(len(data) / 2)) + ([\"B\"] * (int(len(data) / 2) + 1))\ndata.head()\n</pre> # Data preprocessing # ============================================================================== data.index.name = 'date' # Add an extra categorical variable data['exog_3'] = ([\"A\"] * int(len(data) / 2)) + ([\"B\"] * (int(len(data) / 2) + 1)) data.head() Out[3]: y exog_1 exog_2 exog_3 date 1992-04-01 0.379808 0.958792 1.166029 A 1992-05-01 0.361801 0.951993 1.117859 A 1992-06-01 0.410534 0.952955 1.067942 A 1992-07-01 0.483389 0.958078 1.097376 A 1992-08-01 0.475463 0.956370 1.122199 A In\u00a0[4]: Copied! <pre># Create and fit forecaster that scales the input series\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = StandardScaler(),\n                 transformer_exog = None\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster that scales the input series # ============================================================================== forecaster = ForecasterRecursive(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = StandardScaler(),                  transformer_exog = None              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[4]: ForecasterRecursive General Information <ul> <li>Regressor: Ridge</li> <li>Lags: [1 2 3]</li> <li>Window features: None</li> <li>Window size: 3</li> <li>Series name: y</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:30:12</li> <li>Last fit date: 2025-08-07 19:30:12</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     exog_1, exog_2                 </ul> Data Transformations <ul> <li>Transformer for y: StandardScaler()</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[5]: Copied! <pre># Create and fit forecaster with same tranformation for all exogenous variables\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = None,\n                 transformer_exog = StandardScaler()\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster with same tranformation for all exogenous variables # ============================================================================== forecaster = ForecasterRecursive(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = None,                  transformer_exog = StandardScaler()              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[5]: ForecasterRecursive General Information <ul> <li>Regressor: Ridge</li> <li>Lags: [1 2 3]</li> <li>Window features: None</li> <li>Window size: 3</li> <li>Series name: y</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:30:12</li> <li>Last fit date: 2025-08-07 19:30:12</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     exog_1, exog_2                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: StandardScaler()</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>It is also possible to apply a different transformation to each exogenous variable making use of <code>ColumnTransformer</code>.</p> In\u00a0[6]: Copied! <pre># Create and fit forecaster with different transformations for each exog variable\n# ==============================================================================\ntransformer_exog = ColumnTransformer(\n                       [('scale_1', StandardScaler(), ['exog_1']),\n                        ('scale_2', StandardScaler(), ['exog_2']),\n                        ('onehot', OneHotEncoder(), ['exog_3']),\n                       ],\n                       remainder = 'passthrough',\n                       verbose_feature_names_out = False\n                   )\n\nforecaster = ForecasterRecursive(\n                 regressor        = Ridge(random_state=123),\n                 lags             = 3,\n                 transformer_y    = None,\n                 transformer_exog = transformer_exog\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']])\nforecaster\n</pre> # Create and fit forecaster with different transformations for each exog variable # ============================================================================== transformer_exog = ColumnTransformer(                        [('scale_1', StandardScaler(), ['exog_1']),                         ('scale_2', StandardScaler(), ['exog_2']),                         ('onehot', OneHotEncoder(), ['exog_3']),                        ],                        remainder = 'passthrough',                        verbose_feature_names_out = False                    )  forecaster = ForecasterRecursive(                  regressor        = Ridge(random_state=123),                  lags             = 3,                  transformer_y    = None,                  transformer_exog = transformer_exog              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']]) forecaster Out[6]: ForecasterRecursive General Information <ul> <li>Regressor: Ridge</li> <li>Lags: [1 2 3]</li> <li>Window features: None</li> <li>Window size: 3</li> <li>Series name: y</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:30:12</li> <li>Last fit date: 2025-08-07 19:30:12</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     exog_1, exog_2, exog_3                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: ColumnTransformer(remainder='passthrough',                   transformers=[('scale_1', StandardScaler(), ['exog_1']),                                 ('scale_2', StandardScaler(), ['exog_2']),                                 ('onehot', OneHotEncoder(), ['exog_3'])],                   verbose_feature_names_out=False)</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>It is possible to verify if the data transformation has been applied correctly by examining the training matrices. The training matrices should reflect the data transformation that was specified using the <code>transformer_y</code> or <code>transformer_exog</code> arguments.</p> In\u00a0[7]: Copied! <pre># Inspect training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(\n                       y    = data['y'],\n                       exog = data[['exog_1', 'exog_2', 'exog_3']]\n                   )\n</pre> # Inspect training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(                        y    = data['y'],                        exog = data[['exog_1', 'exog_2', 'exog_3']]                    ) In\u00a0[8]: Copied! <pre>X_train.head(4)\n</pre> X_train.head(4) Out[8]: lag_1 lag_2 lag_3 exog_1 exog_2 exog_3_A exog_3_B date 1992-07-01 0.410534 0.361801 0.379808 -2.119529 -2.135088 1.0 0.0 1992-08-01 0.483389 0.410534 0.361801 -2.131024 -1.996017 1.0 0.0 1992-09-01 0.475463 0.483389 0.410534 -2.109222 -1.822392 1.0 0.0 1992-10-01 0.534761 0.475463 0.483389 -2.132137 -1.590667 1.0 0.0 In\u00a0[9]: Copied! <pre>y_train.head(4)\n</pre> y_train.head(4) Out[9]: <pre>date\n1992-07-01    0.483389\n1992-08-01    0.475463\n1992-09-01    0.534761\n1992-10-01    0.568606\nFreq: MS, Name: y, dtype: float64</pre> <p> \ud83d\udca1 Tip </p> <p>To learn more about how to extract the training and prediction matrices, visit the following link: How to Extract Training and Prediction Matrices.</p> <p>Both <code>transformer_y</code> and <code>transformer_exog</code> accept scikit-learn pipelines. This allows for multiple transformations to be applied sequentially. The following example shows how to scale the target variable and apply a box-cox transformation.</p> In\u00a0[10]: Copied! <pre># Multiple transformations using pipelines and column transformers\n# ==============================================================================\ntransformer_y = Pipeline(\n                    steps = [\n                        ('scaler', StandardScaler()),\n                        ('power', PowerTransformer(method='yeo-johnson'))\n                    ]\n                )\n\ntransformer_exog = ColumnTransformer(\n                       [('scale_1', StandardScaler(), ['exog_1']),\n                        ('scale_2', StandardScaler(), ['exog_2']),\n                        ('onehot', OneHotEncoder(), ['exog_3']),\n                       ],\n                       remainder = 'passthrough',\n                       verbose_feature_names_out = False\n                   )\n\nforecaster = ForecasterRecursive(\n                    regressor        = Ridge(random_state=123),\n                    lags             = 3,\n                    transformer_y    = transformer_y,\n                    transformer_exog = transformer_exog\n                )\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']])\nforecaster\n</pre> # Multiple transformations using pipelines and column transformers # ============================================================================== transformer_y = Pipeline(                     steps = [                         ('scaler', StandardScaler()),                         ('power', PowerTransformer(method='yeo-johnson'))                     ]                 )  transformer_exog = ColumnTransformer(                        [('scale_1', StandardScaler(), ['exog_1']),                         ('scale_2', StandardScaler(), ['exog_2']),                         ('onehot', OneHotEncoder(), ['exog_3']),                        ],                        remainder = 'passthrough',                        verbose_feature_names_out = False                    )  forecaster = ForecasterRecursive(                     regressor        = Ridge(random_state=123),                     lags             = 3,                     transformer_y    = transformer_y,                     transformer_exog = transformer_exog                 ) forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2', 'exog_3']]) forecaster Out[10]: ForecasterRecursive General Information <ul> <li>Regressor: Ridge</li> <li>Lags: [1 2 3]</li> <li>Window features: None</li> <li>Window size: 3</li> <li>Series name: y</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:30:12</li> <li>Last fit date: 2025-08-07 19:30:12</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     exog_1, exog_2, exog_3                 </ul> Data Transformations <ul> <li>Transformer for y: Pipeline(steps=[('scaler', StandardScaler()), ('power', PowerTransformer())])</li> <li>Transformer for exog: ColumnTransformer(remainder='passthrough',                   transformers=[('scale_1', StandardScaler(), ['exog_1']),                                 ('scale_2', StandardScaler(), ['exog_2']),                                 ('onehot', OneHotEncoder(), ['exog_3'])],                   verbose_feature_names_out=False)</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>Using scikit-learn FunctionTransformer it is possible to include custom transformers in the forecaster object, for example, a logarithmic transformation.</p> <p>Scikit-learn's FunctionTransformer can be used to incorporate custom transformers, such as a logarithmic transformation, in the forecaster object. To implement this, a user-defined transformation function can be created and then passed to the <code>FunctionTransformer</code>. Detailed information on how to use FunctionTransformer can be found in the scikit-learn documentation.</p> <p> \u26a0 Warning </p> <p>For versions 1.1.0 &gt;= scikit-learn &lt;= 1.2.0 <code>sklearn.preprocessing.FunctionTransformer.inverse_transform</code> does not support DataFrames that are all numerical when <code>check_inverse=True</code>. It will raise an Exception which is fixed in scikit-learn 1.2.1.</p> <p>More info: https://scikit-learn.org/stable/whats_new/v1.2.html#version-1-2-1</p> In\u00a0[11]: Copied! <pre># Create custom transformer\n# =============================================================================\ndef log_transform(x):\n    \"\"\" \n    Calculate log adding 1 to avoid calculation errors if x is very close to 0.\n    \"\"\"\n    return np.log(x + 1)\n\n\ndef exp_transform(x):\n    \"\"\"\n    Inverse of log_transform.\n    \"\"\"\n    return np.exp(x) - 1\n\n\ntransformer_y = FunctionTransformer(func=log_transform, inverse_func=exp_transform)\n\n# Create and train forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor     = Ridge(random_state=123),\n                 lags          = 3,\n                 transformer_y = transformer_y\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create custom transformer # ============================================================================= def log_transform(x):     \"\"\"      Calculate log adding 1 to avoid calculation errors if x is very close to 0.     \"\"\"     return np.log(x + 1)   def exp_transform(x):     \"\"\"     Inverse of log_transform.     \"\"\"     return np.exp(x) - 1   transformer_y = FunctionTransformer(func=log_transform, inverse_func=exp_transform)  # Create and train forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor     = Ridge(random_state=123),                  lags          = 3,                  transformer_y = transformer_y              )  forecaster.fit(y=data['y']) <p>If the <code>FunctionTransformer</code> has an inverse function, the output of the predict method is automatically transformed back to the original scale.</p> In\u00a0[12]: Copied! <pre>forecaster.predict(steps=4)\n</pre> forecaster.predict(steps=4) Out[12]: <pre>2008-07-01    0.776206\n2008-08-01    0.775471\n2008-09-01    0.777200\n2008-10-01    0.777853\nFreq: MS, Name: pred, dtype: float64</pre> <p> \u26a0 Warning </p> <p>Skforecast allows the usage of scikit-learn pipelines as regressors. It is important to note that ColumnTransformer cannot be included in the pipeline; thus, the same transformation will be applied to both the modeled series and all exogenous variables. However, if the preprocessing transformations only apply to specific columns, then they need to be applied separately using <code>transformer_y</code> and <code>transformer_exog</code>.</p> In\u00a0[13]: Copied! <pre>pipe = Pipeline(steps=[('scaler', StandardScaler()), ('regressor', Ridge())])\npipe\n</pre> pipe = Pipeline(steps=[('scaler', StandardScaler()), ('regressor', Ridge())]) pipe Out[13]: <pre>Pipeline(steps=[('scaler', StandardScaler()), ('regressor', Ridge())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted Parameters steps\u00a0 [('scaler', ...), ('regressor', ...)] transform_input\u00a0 None memory\u00a0 None verbose\u00a0 False StandardScaler?Documentation for StandardScaler Parameters copy\u00a0 True with_mean\u00a0 True with_std\u00a0 True Ridge?Documentation for Ridge Parameters alpha\u00a0 1.0 fit_intercept\u00a0 True copy_X\u00a0 True max_iter\u00a0 None tol\u00a0 0.0001 solver\u00a0 'auto' positive\u00a0 False random_state\u00a0 None In\u00a0[14]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = pipe,\n                 lags      = 10\n             )\n\nforecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor = pipe,                  lags      = 10              )  forecaster.fit(y=data['y'], exog=data[['exog_1', 'exog_2']]) forecaster Out[14]: ForecasterRecursive General Information <ul> <li>Regressor: Pipeline</li> <li>Lags: [ 1  2  3  4  5  6  7  8  9 10]</li> <li>Window features: None</li> <li>Window size: 10</li> <li>Series name: y</li> <li>Exogenous included: True</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:30:12</li> <li>Last fit date: 2025-08-07 19:30:12</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     exog_1, exog_2                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1992-04-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'regressor__alpha': 1.0, 'regressor__copy_X': True, 'regressor__fit_intercept': True, 'regressor__max_iter': None, 'regressor__positive': False, 'regressor__random_state': None, 'regressor__solver': 'auto', 'regressor__tol': 0.0001}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> <p>When performing a grid search over a scikit-learn pipeline, the model's name precedes the parameters' name.</p> In\u00a0[15]: Copied! <pre># Hyperparameter grid search using a scikit-learn pipeline\n# ==============================================================================\npipe = Pipeline(steps=[('scaler', StandardScaler()), ('regressor', Ridge())])\nforecaster = ForecasterRecursive(\n                 regressor = pipe,\n                 lags = 10  # This value will be replaced in the grid search\n             )\n\n# Regressor's hyperparameters\nparam_grid = {'regressor__alpha': np.logspace(-3, 5, 10)}\n\n# Lags used as predictors\nlags_grid = [5, 24, [1, 2, 3, 23, 24]]\n\ncv = TimeSeriesFold(\n    steps=5, initial_train_size=len(data.loc[:'2000-04-01']), refit=False\n)\n\nresults_grid = grid_search_forecaster(\n                   forecaster         = forecaster,\n                   y                  = data['y'],\n                   exog               = data[['exog_1', 'exog_2']],\n                   param_grid         = param_grid,\n                   lags_grid          = lags_grid,\n                   cv                 = cv,\n                   metric             = 'mean_absolute_error'\n               )\n\nresults_grid.head(4)\n</pre> # Hyperparameter grid search using a scikit-learn pipeline # ============================================================================== pipe = Pipeline(steps=[('scaler', StandardScaler()), ('regressor', Ridge())]) forecaster = ForecasterRecursive(                  regressor = pipe,                  lags = 10  # This value will be replaced in the grid search              )  # Regressor's hyperparameters param_grid = {'regressor__alpha': np.logspace(-3, 5, 10)}  # Lags used as predictors lags_grid = [5, 24, [1, 2, 3, 23, 24]]  cv = TimeSeriesFold(     steps=5, initial_train_size=len(data.loc[:'2000-04-01']), refit=False )  results_grid = grid_search_forecaster(                    forecaster         = forecaster,                    y                  = data['y'],                    exog               = data[['exog_1', 'exog_2']],                    param_grid         = param_grid,                    lags_grid          = lags_grid,                    cv                 = cv,                    metric             = 'mean_absolute_error'                )  results_grid.head(4) <pre>lags grid:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [1 2 3 4 5] \n  Parameters: {'regressor__alpha': np.float64(0.001)}\n  Backtesting metric: 6.845311709559406e-05\n</pre> Out[15]: lags lags_label params mean_absolute_error regressor__alpha 0 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'regressor__alpha': 0.001} 0.000068 0.001000 1 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'regressor__alpha': 0.001} 0.000188 0.001000 2 [1, 2, 3, 4, 5] [1, 2, 3, 4, 5] {'regressor__alpha': 0.007742636826811269} 0.000526 0.007743 3 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'regressor__alpha': 0.007742636826811269} 0.001413 0.007743 In\u00a0[16]: Copied! <pre># Data download\n# ==============================================================================\ndata = fetch_dataset(name=\"items_sales\")\ndata.head()\n</pre> # Data download # ============================================================================== data = fetch_dataset(name=\"items_sales\") data.head() <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> Out[16]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[17]: Copied! <pre># Series transformation: same transformation for all series\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler(),\n                 transformer_exog   = None\n             )\nforecaster.fit(series=data)\n</pre> # Series transformation: same transformation for all series # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = 'ordinal',                  transformer_series = StandardScaler(),                  transformer_exog   = None              ) forecaster.fit(series=data) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <p>It is possible to access the fitted transformers for each series through the <code>transformers_series_</code> attribute. This allows verification that each transformer has been trained independently.</p> In\u00a0[18]: Copied! <pre># Mean and scale of the transformer for each series\n# ==============================================================================\nfor k, v in forecaster.transformer_series_.items():\n    print(f\"Series {k}: {v} mean={v.mean_}, scale={v.scale_}\")\n</pre> # Mean and scale of the transformer for each series # ============================================================================== for k, v in forecaster.transformer_series_.items():     print(f\"Series {k}: {v} mean={v.mean_}, scale={v.scale_}\") <pre>Series item_1: StandardScaler() mean=[22.37366364], scale=[2.54258317]\nSeries item_2: StandardScaler() mean=[16.26942518], scale=[4.89965692]\nSeries item_3: StandardScaler() mean=[17.19276546], scale=[5.43694388]\nSeries _unknown_level: StandardScaler() mean=[18.61195143], scale=[5.21803675]\n</pre> In\u00a0[19]: Copied! <pre># Series transformation: different transformation for each series\n# ==============================================================================\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 24,\n                 encoding           = 'ordinal',\n                 transformer_series = {'item_1': StandardScaler(), 'item_2': MinMaxScaler(), '_unknown_level': StandardScaler()},\n                 transformer_exog   = None\n             )\n\nforecaster.fit(series=data)\n</pre> # Series transformation: different transformation for each series # ============================================================================== forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 24,                  encoding           = 'ordinal',                  transformer_series = {'item_1': StandardScaler(), 'item_2': MinMaxScaler(), '_unknown_level': StandardScaler()},                  transformer_exog   = None              )  forecaster.fit(series=data) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 IgnoredArgumentWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 {'item_3'} not present in `transformer_series`. No transformation is applied to      \u2502\n\u2502 these series.                                                                        \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : IgnoredArgumentWarning                                                    \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:372                                                              \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=IgnoredArgumentWarning)          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[20]: Copied! <pre># Transformer trained for each series\n# ==============================================================================\nfor k, v in forecaster.transformer_series_.items():\n    if v is not None:\n        print(f\"Series {k}: {v.get_params()}\")\n    else:\n        print(f\"Series {k}: {v}\")\n</pre> # Transformer trained for each series # ============================================================================== for k, v in forecaster.transformer_series_.items():     if v is not None:         print(f\"Series {k}: {v.get_params()}\")     else:         print(f\"Series {k}: {v}\") <pre>Series item_1: {'copy': True, 'with_mean': True, 'with_std': True}\nSeries item_2: {'clip': False, 'copy': True, 'feature_range': (0, 1)}\nSeries item_3: None\nSeries _unknown_level: {'copy': True, 'with_mean': True, 'with_std': True}\n</pre>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#data-transformation-and-pipelines","title":"Data transformation and pipelines\u00b6","text":"<p>Skforecast has two arguments in all the forecasters that allow more detailed control over input data transformations. This feature is particularly useful as many machine learning models require specific data pre-processing transformations. For example, linear models may benefit from features being scaled, or categorical features being encoded into numerical values.</p> <ul> <li><p><code>transformer_y</code>: an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API with the methods: fit, transform, fit_transform and, inverse_transform. Scikit-learn ColumnTransformer is not allowed since they do not have the inverse_transform method. If multiple transformations are needed, a scikit-learn pipeline can be used. For example, scaling the target variable and applying a logarithmic transformation.</p> </li> <li><p><code>transformer_exog</code>: an instance of a transformer (preprocessor) compatible with the scikit-learn preprocessing API. Scikit-learn ColumnTransformer can be used if the preprocessing transformations only apply to some specific columns or if different transformations are needed for different columns. For example, scale numeric features and one hot encode categorical ones. It is also possible to use a scikit-learn pipeline to apply multiple transformations sequentially.</p> </li> </ul> <p>Transformations are learned and applied before training the forecaster and are automatically used when calling <code>predict</code>. The output of <code>predict</code> is always on the same scale as the original series y.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#libraries","title":"Libraries\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#data","title":"Data\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-input-series","title":"Transforming input series\u00b6","text":"<p>The following example shows how to include a transformer that scales the input series y.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-exogenous-variables","title":"Transforming exogenous variables\u00b6","text":"<p>The following example shows how to apply the same transformation (scaling) to all exogenous variables.</p>"},{"location":"user_guides/sklearn-transformers-and-pipeline.html#sklearn-pipelines-and-columntransformer","title":"Sklearn Pipelines and ColumnTransformer\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#custom-transformers","title":"Custom transformers\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#pipelines-as-regressors","title":"Pipelines as regressors\u00b6","text":""},{"location":"user_guides/sklearn-transformers-and-pipeline.html#transforming-multiple-input-series-in-global-models","title":"Transforming multiple input series in global models\u00b6","text":"<p>When using global forecasting models (ForecasterRecursiveMultiSeries or ForecasterDirectMultiVariate) the <code>transformer_series</code> argument replaces <code>transformer_y</code>. Three different options are available:</p> <ul> <li><p><code>transformer_series</code> is a single transformer: When a single transformer is provided, it is automatically cloned for each individual series. Each cloned transformer is then trained separately on one of the series.</p> </li> <li><p><code>transformer_series</code> is a dictionary: A different transformer can be specified for each series by passing a dictionary where the keys correspond to the series names and the values are the transformers. Each series is transformed according to its designated transformer. When this option is used, it is mandatory to include a transformer for unknown series, which is indicated by the key <code>'_unknown_level'</code>.</p> </li> <li><p><code>transformer_series</code> is None: no transformations are applied to any of the series.</p> </li> </ul> <p>Regardless of the configuration, each series is transformed independently. Even when using a single transformer, it is cloned internally and applied separately to each series.</p>"},{"location":"user_guides/sktime-pipelines.html","title":"Sktime pipelines","text":"<p>Sktime, a well-known forecasting library, provides functionality to apply transformations to both the target variable and exogenous variables using two distinct classes:</p> <ul> <li><p><code>TransformedTargetForecaster</code>: Applies the specified transformations to the target series.</p> </li> <li><p><code>ForecastingPipeline</code>: Applies the specified transformations to the exogenous variables before passing them to the forecaster.</p> </li> </ul> <p>Similarly, skforecast supports transformations for both the target variable and exogenous variables through the following arguments present in all forecasters:</p> <ul> <li><p><code>transformer_y</code>: Applies the specified transformations (single transformer or a sklearn pipeline with multiple transformers) to the target variable.</p> </li> <li><p><code>transformer_series</code>: Equivalent to <code>transformer_y</code> in multi-series forecasters.</p> </li> <li><p><code>transformer_exog</code>: Applies the specified transformations (single transformer or a sklearn pipeline with multiple transformers) to the exogenous variables.</p> </li> </ul> <p>The following document provides a side-by-side comparison of equivalent code in Sktime and Skforecast for applying transformations to the target variable and exogenous variables.</p> <p>Without exogenous variables</p> skforecast sktime <pre>from skforecast.recursive import ForecasterRecursive\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\n\nforecaster = ForecasterRecursive(\n                 regressor     = Ridge(random_state=951),\n                 lags          = 15,\n                 transformer_y = StandardScaler(),\n             )\nforecaster.fit(y=y)\npredictios = forecaster.predict(steps=10)\npredictios\n</pre> <pre>from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sktime.transformations.series.adapt import TabularToSeriesAdaptor\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.compose import (\n    make_reduction,\n    TransformedTargetForecaster,\n)\n\nregressor = make_reduction(Ridge(random_state=951), window_length=15, strategy=\"recursive\")\nforecaster = TransformedTargetForecaster(\n    steps=[\n        (\"boxcox\", TabularToSeriesAdaptor(StandardScaler())),\n        (\"regressor\", regressor),\n    ]\n)\nforecaster.fit(y=y)\nfh = ForecastingHorizon(np.arange(1, 11), is_relative=True)\npredictions = forecaster.predict(fh=fh)\npredictios\n</pre> <p>With exogenous variables</p> skforecast sktime <pre>from skforecast.recursive import ForecasterRecursive\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nforecaster = ForecasterRecursive(\n                 regressor        = Ridge(random_state=951),\n                 lags             = 15,\n                 transformer_y    = BoxCoxTransformer(),\n                 transformer_exog = StandardScaler()\n             )\nforecaster.fit(y=y)\npredictios = forecaster.predict(steps=10)\npredictios\n</pre> <pre>from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\nfrom sktime.transformations.series.adapt import TabularToSeriesAdaptor\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.compose import (\n    make_reduction,\n    TransformedTargetForecaster,\n    ForecastingPipeline,\n)\n\nregressor = make_reduction(Ridge(random_state=951), window_length=15, strategy=\"recursive\")\npipe_y = TransformedTargetForecaster(\n    steps=[\n        (\"boxcox\", BoxCoxTransformer()),\n        (\"regressor\", regressor),\n    ]\n)\npipe_X = ForecastingPipeline(\n    steps=[\n        (\"scaler\", TabularToSeriesAdaptor(StandardScaler())),\n        (\"forecaster\", pipe_y),\n    ]\n)\npipe_X.fit(y=y, X=exog)\nfh = ForecastingHorizon(np.arange(1, 11), is_relative=True)\npredictions = pipe_X.predict(fh=fh, X=exog_test)\npredictions\n</pre> <p> \u26a0 Warning </p> <p>When working with exogenous variables, both libraries apply the same transformations. However, the results differ because sktime incorporates the lagged values of the exogenous variables into the underlying training matrices, whereas skforecast does not. For example, if 3 lagged values are used and two exogenous variables are included, the underlying training matrices are as follows:</p> <ul> <li>skforecast: <code>lag_1</code>, <code>lag_2</code>, <code>lag_3</code>, <code>exog_1</code>, <code>exog_2</code></li> <li>sktime: <code>lag_1</code>, <code>lag_2</code>, <code>lag_3</code>, <code>exog_1_lag_1</code>, <code>exog_1_lag_2</code>, <code>exog_1_lag_3</code>, <code>exog_2_lag_1</code>, <code>exog_2_lag_2</code>, <code>exog_2_lag_3</code></li> </ul> In\u00a0[3]: Copied! <pre># Libraries\n# ======================================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.datasets import fetch_dataset\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\n\n# skforecast\nfrom skforecast.recursive import ForecasterRecursive\n\n# sktime\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.compose import (\n    make_reduction,\n    TransformedTargetForecaster,\n    ForecastingPipeline,\n)\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\nfrom sktime.transformations.series.adapt import TabularToSeriesAdaptor\n</pre> # Libraries # ====================================================================================== import numpy as np import pandas as pd from skforecast.datasets import fetch_dataset from sklearn.linear_model import Ridge from sklearn.preprocessing import StandardScaler  # skforecast from skforecast.recursive import ForecasterRecursive  # sktime from sktime.forecasting.base import ForecastingHorizon from sktime.forecasting.compose import (     make_reduction,     TransformedTargetForecaster,     ForecastingPipeline, ) from sktime.transformations.series.boxcox import BoxCoxTransformer from sktime.transformations.series.adapt import TabularToSeriesAdaptor In\u00a0[4]: Copied! <pre># Data\n# ======================================================================================\ndata = fetch_dataset(name='fuel_consumption')\ndata = data.rename(columns={'Gasolinas': 'litters'})\ndata = data.rename_axis('date')\ndata = data.loc[:'1990-01-01 00:00:00']\ndata = data[['litters']]\ndata['month'] = data.index.month\ndata['year'] = data.index.year\ndisplay(data.head(4))\n</pre> # Data # ====================================================================================== data = fetch_dataset(name='fuel_consumption') data = data.rename(columns={'Gasolinas': 'litters'}) data = data.rename_axis('date') data = data.loc[:'1990-01-01 00:00:00'] data = data[['litters']] data['month'] = data.index.month data['year'] = data.index.year display(data.head(4)) <pre>fuel_consumption\n----------------\nMonthly fuel consumption in Spain from 1969-01-01 to 2022-08-01.\nObtained from Corporaci\u00f3n de Reservas Estrat\u00e9gicas de Productos Petrol\u00edferos and\nCorporaci\u00f3n de Derecho P\u00fablico tutelada por el Ministerio para la Transici\u00f3n\nEcol\u00f3gica y el Reto Demogr\u00e1fico. https://www.cores.es/es/estadisticas\nShape of the dataset: (644, 5)\n</pre> litters month year date 1969-01-01 166875.2129 1 1969 1969-02-01 155466.8105 2 1969 1969-03-01 184983.6699 3 1969 1969-04-01 202319.8164 4 1969 In\u00a0[5]: Copied! <pre># Train-test dates\n# ======================================================================================\nend_train = '1980-01-01 23:59:59'\ndata_train = data.loc[:end_train]\ndata_test  = data.loc[end_train:]\n</pre> # Train-test dates # ====================================================================================== end_train = '1980-01-01 23:59:59' data_train = data.loc[:end_train] data_test  = data.loc[end_train:] In\u00a0[6]: Copied! <pre># Sktime pipeline\n# ======================================================================================\nregressor = make_reduction(Ridge(), window_length=15, strategy=\"recursive\")\npipe_y = TransformedTargetForecaster(\n    steps=[\n        (\"boxcox\", BoxCoxTransformer()),\n        (\"regressor\", regressor),\n    ]\n)\npipe_X = ForecastingPipeline(\n    steps=[\n        (\"scaler\", TabularToSeriesAdaptor(StandardScaler())),\n        (\"forecaster\", pipe_y),\n    ]\n)\npipe_X.fit(y=data_train['litters'], X=data_train[['month', 'year']])\nfh = ForecastingHorizon(np.arange(1, len(data_test) + 1), is_relative=True)\npredictions_sktime = pipe_X.predict(fh=fh, X=data_test[['month', 'year']])\npredictions_sktime\n</pre> # Sktime pipeline # ====================================================================================== regressor = make_reduction(Ridge(), window_length=15, strategy=\"recursive\") pipe_y = TransformedTargetForecaster(     steps=[         (\"boxcox\", BoxCoxTransformer()),         (\"regressor\", regressor),     ] ) pipe_X = ForecastingPipeline(     steps=[         (\"scaler\", TabularToSeriesAdaptor(StandardScaler())),         (\"forecaster\", pipe_y),     ] ) pipe_X.fit(y=data_train['litters'], X=data_train[['month', 'year']]) fh = ForecastingHorizon(np.arange(1, len(data_test) + 1), is_relative=True) predictions_sktime = pipe_X.predict(fh=fh, X=data_test[['month', 'year']]) predictions_sktime Out[6]: <pre>1980-02-01    430096.815068\n1980-03-01    472406.420587\n1980-04-01    509203.559184\n1980-05-01    495910.509282\n1980-06-01    518548.672893\n                  ...      \n1989-09-01    820033.569581\n1989-10-01    801291.145367\n1989-11-01    756075.962331\n1989-12-01    795345.389792\n1990-01-01    746317.734572\nFreq: MS, Name: litters, Length: 120, dtype: float64</pre> In\u00a0[7]: Copied! <pre># Skforecast with transformations\n# ======================================================================================\nforecaster = ForecasterRecursive(\n                 regressor        = Ridge(),\n                 lags             = 15,\n                 transformer_y    = BoxCoxTransformer(),\n                 transformer_exog = StandardScaler()\n             )\nforecaster.fit(y=data_train['litters'], exog=data_train[['month', 'year']])\n\npredictions_skforecast = forecaster.predict(steps=len(data_test), exog=data_test[['month', 'year']])\npredictions_skforecast\n</pre> # Skforecast with transformations # ====================================================================================== forecaster = ForecasterRecursive(                  regressor        = Ridge(),                  lags             = 15,                  transformer_y    = BoxCoxTransformer(),                  transformer_exog = StandardScaler()              ) forecaster.fit(y=data_train['litters'], exog=data_train[['month', 'year']])  predictions_skforecast = forecaster.predict(steps=len(data_test), exog=data_test[['month', 'year']]) predictions_skforecast Out[7]: <pre>1980-02-01    427508.153706\n1980-03-01    487904.492766\n1980-04-01    524565.943847\n1980-05-01    506245.770327\n1980-06-01    531938.860717\n                  ...      \n1989-09-01    770334.700792\n1989-10-01    753315.656399\n1989-11-01    787562.026285\n1989-12-01    743408.935078\n1990-01-01    682958.500996\nFreq: MS, Name: pred, Length: 120, dtype: float64</pre> In\u00a0[8]: Copied! <pre># Transformation results\n# ======================================================================================\nresults = pd.DataFrame({\n              'sktime': predictions_sktime,\n              'skforecast': predictions_skforecast,\n          })\nresults\n</pre> # Transformation results # ====================================================================================== results = pd.DataFrame({               'sktime': predictions_sktime,               'skforecast': predictions_skforecast,           }) results Out[8]: sktime skforecast 1980-02-01 430096.815068 427508.153706 1980-03-01 472406.420587 487904.492766 1980-04-01 509203.559184 524565.943847 1980-05-01 495910.509282 506245.770327 1980-06-01 518548.672893 531938.860717 ... ... ... 1989-09-01 820033.569581 770334.700792 1989-10-01 801291.145367 753315.656399 1989-11-01 756075.962331 787562.026285 1989-12-01 795345.389792 743408.935078 1990-01-01 746317.734572 682958.500996 <p>120 rows \u00d7 2 columns</p> <p>The following table shows the equivalent transformations in sktime and skforecast:</p> In\u00a0[9]: Copied! <pre># Box-Cox transformation\n# ======================================================================================\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\nfrom sklearn.preprocessing import PowerTransformer\n\n# sktime\ntransformer_sktime = BoxCoxTransformer()\ny_hat_sktime = transformer_sktime.fit_transform(data_train['litters'])\n\n# skforecast\ntransformer_skforecast = PowerTransformer(method='box-cox', standardize=False)\ny_hat_skforecast = transformer_skforecast.fit_transform(data_train[['litters']]).flatten()\n\nnp.testing.assert_allclose(y_hat_sktime, y_hat_skforecast)\n</pre> # Box-Cox transformation # ====================================================================================== from sktime.transformations.series.boxcox import BoxCoxTransformer from sklearn.preprocessing import PowerTransformer  # sktime transformer_sktime = BoxCoxTransformer() y_hat_sktime = transformer_sktime.fit_transform(data_train['litters'])  # skforecast transformer_skforecast = PowerTransformer(method='box-cox', standardize=False) y_hat_skforecast = transformer_skforecast.fit_transform(data_train[['litters']]).flatten()  np.testing.assert_allclose(y_hat_sktime, y_hat_skforecast) In\u00a0[10]: Copied! <pre># Differencing\n# ======================================================================================\nfrom sktime.transformations.series.difference import Differencer\nfrom skforecast.preprocessing import TimeSeriesDifferentiator\n\n# sktime\ntransformer_sktime = Differencer(lags=1)\ny_hat_sktime = transformer_sktime.fit_transform(data_train['litters'])[1:]\n\n# skforecast\ntransformer_skforecast = TimeSeriesDifferentiator(order=1)\ny_hat_skforecast = transformer_skforecast.fit_transform(data_train['litters'].to_numpy())[1:]\n\nnp.testing.assert_allclose(y_hat_sktime, y_hat_skforecast)\n</pre> # Differencing # ====================================================================================== from sktime.transformations.series.difference import Differencer from skforecast.preprocessing import TimeSeriesDifferentiator  # sktime transformer_sktime = Differencer(lags=1) y_hat_sktime = transformer_sktime.fit_transform(data_train['litters'])[1:]  # skforecast transformer_skforecast = TimeSeriesDifferentiator(order=1) y_hat_skforecast = transformer_skforecast.fit_transform(data_train['litters'].to_numpy())[1:]  np.testing.assert_allclose(y_hat_sktime, y_hat_skforecast) In\u00a0[11]: Copied! <pre># Log transformation\n# ======================================================================================\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sktime.transformations.series.boxcox import LogTransformer\n\n# sktime\ntransformer_sktime = LogTransformer(offset=1)\ny_hat_sktime = transformer_sktime.fit_transform(data_train['litters'])\n\n# skforecast\ntransformer_skforecast = FunctionTransformer(func=np.log1p, inverse_func=np.expm1, validate=True)\ny_hat_skforecast = transformer_skforecast.fit_transform(data_train[['litters']]).flatten()\n\nnp.testing.assert_allclose(y_hat_sktime, y_hat_skforecast)\n</pre> # Log transformation # ====================================================================================== from sklearn.preprocessing import FunctionTransformer from sktime.transformations.series.boxcox import LogTransformer  # sktime transformer_sktime = LogTransformer(offset=1) y_hat_sktime = transformer_sktime.fit_transform(data_train['litters'])  # skforecast transformer_skforecast = FunctionTransformer(func=np.log1p, inverse_func=np.expm1, validate=True) y_hat_skforecast = transformer_skforecast.fit_transform(data_train[['litters']]).flatten()  np.testing.assert_allclose(y_hat_sktime, y_hat_skforecast) <pre>/home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but FunctionTransformer was fitted with feature names\n  warnings.warn(\n</pre>"},{"location":"user_guides/sktime-pipelines.html#equivalent-pipelines-in-sktime-and-skforecast","title":"Equivalent Pipelines in sktime and skforecast\u00b6","text":""},{"location":"user_guides/sktime-pipelines.html#sktime","title":"Sktime\u00b6","text":""},{"location":"user_guides/sktime-pipelines.html#skforecast","title":"Skforecast\u00b6","text":""},{"location":"user_guides/sktime-pipelines.html#equivalent-transformations","title":"Equivalent transformations\u00b6","text":""},{"location":"user_guides/stacking-ensemble-models-forecasting.html","title":"Stacking multiple models","text":"<p>In machine learning, stacking is an ensemble technique that combines multiple models to reduce their biases and improve predictive performance. Specifically, the predictions of each model (base models) are stacked and used as input to a final model (metamodel) to compute the prediction.</p> <p>Stacking is effective because it leverages the strengths of different algorithms and attempts to mitigate their individual weaknesses. By combining multiple models, it can capture complex patterns in the data and improve prediction accuracy.</p> <p>However, stacking can be computationally expensive and requires careful tuning to avoid overfitting. To this end, it is highly recommended to train the final estimator through cross-validation. In addition, obtaining diverse and well-performing base models is critical to the success of the stacking technique.</p> <p>With scikit-learn, it is very easy to combine multiple regressors thanks to the StackingRegressor class. The <code>estimators</code> parameter corresponds to the list of the estimators (base learners) that will be stacked in parallel on the input data. The <code>final_estimator</code> (metamodel) will use the predictions of the estimators as input.</p> <p> \u270e Note </p> <p>See Stacking (ensemble) machine learning models to improve forecasting for a more detailed example of stacking models.</p> In\u00a0[1]: Copied! <pre># Data processing\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.model_selection import KFold\n\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold\nfrom skforecast.model_selection import backtesting_forecaster\nfrom skforecast.model_selection import grid_search_forecaster\n\n# Warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Data processing # ============================================================================== import numpy as np import pandas as pd  # Modelling and Forecasting # ============================================================================== from lightgbm import LGBMRegressor from sklearn.linear_model import Ridge from sklearn.ensemble import StackingRegressor from sklearn.model_selection import KFold  from skforecast.datasets import fetch_dataset from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold from skforecast.model_selection import backtesting_forecaster from skforecast.model_selection import grid_search_forecaster  # Warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre># Data\n# ==============================================================================\ndata = fetch_dataset(name = 'fuel_consumption')\ndata = data.loc[:\"2019-01-01\", ['Gasolinas']]\ndata = data.rename(columns = {'Gasolinas': 'consumption'})\ndata.index.name = 'date'\ndata['consumption'] = data['consumption'] / 100000\ndata.head(3)\n</pre> # Data # ============================================================================== data = fetch_dataset(name = 'fuel_consumption') data = data.loc[:\"2019-01-01\", ['Gasolinas']] data = data.rename(columns = {'Gasolinas': 'consumption'}) data.index.name = 'date' data['consumption'] = data['consumption'] / 100000 data.head(3) <pre>fuel_consumption\n----------------\nMonthly fuel consumption in Spain from 1969-01-01 to 2022-08-01.\nObtained from Corporaci\u00f3n de Reservas Estrat\u00e9gicas de Productos Petrol\u00edferos and\nCorporaci\u00f3n de Derecho P\u00fablico tutelada por el Ministerio para la Transici\u00f3n\nEcol\u00f3gica y el Reto Demogr\u00e1fico. https://www.cores.es/es/estadisticas\nShape of the dataset: (644, 5)\n</pre> Out[2]: consumption date 1969-01-01 1.668752 1969-02-01 1.554668 1969-03-01 1.849837 <p>In addition to the past values of the series (lags), an additional variable indicating the month of the year is added. This variable is included in the model to capture the seasonality of the series.</p> In\u00a0[3]: Copied! <pre># Calendar features\n# ==============================================================================\ndata['month_of_year'] = data.index.month\ndata.head(3)\n</pre> # Calendar features # ============================================================================== data['month_of_year'] = data.index.month data.head(3) Out[3]: consumption month_of_year date 1969-01-01 1.668752 1 1969-02-01 1.554668 2 1969-03-01 1.849837 3 <p>To facilitate the training of the models, the search for optimal hyperparameters, and the evaluation of their predictive accuracy, the data are divided into three separate sets: training, validation, and test.</p> In\u00a0[4]: Copied! <pre># Split train-validation-test\n# ==============================================================================\nend_train = '2007-12-01 23:59:00'\nend_validation = '2012-12-01 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_val   = data.loc[end_train:end_validation, :]\ndata_test  = data.loc[end_validation:, :]\n\nprint(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\nprint(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\nprint(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n</pre> # Split train-validation-test # ============================================================================== end_train = '2007-12-01 23:59:00' end_validation = '2012-12-01 23:59:00' data_train = data.loc[: end_train, :] data_val   = data.loc[end_train:end_validation, :] data_test  = data.loc[end_validation:, :]  print(f\"Dates train      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\") print(f\"Dates validation : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\") print(f\"Dates test       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\") <pre>Dates train      : 1969-01-01 00:00:00 --- 2007-12-01 00:00:00  (n=468)\nDates validation : 2008-01-01 00:00:00 --- 2012-12-01 00:00:00  (n=60)\nDates test       : 2013-01-01 00:00:00 --- 2019-01-01 00:00:00  (n=73)\n</pre> In\u00a0[5]: Copied! <pre># Create stacking regressor\n# ==============================================================================\nparams_ridge = {'alpha': 0.001}\nparams_lgbm = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'verbose': -1}\n\nestimators = [\n    ('ridge', Ridge(**params_ridge)),\n    ('lgbm', LGBMRegressor(random_state=42, **params_lgbm)),\n]\n\nstacking_regressor = StackingRegressor(\n                         estimators = estimators,\n                         final_estimator = Ridge(),\n                         cv = KFold(n_splits=5, shuffle=False)\n                     )\nstacking_regressor\n</pre> # Create stacking regressor # ============================================================================== params_ridge = {'alpha': 0.001} params_lgbm = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'verbose': -1}  estimators = [     ('ridge', Ridge(**params_ridge)),     ('lgbm', LGBMRegressor(random_state=42, **params_lgbm)), ]  stacking_regressor = StackingRegressor(                          estimators = estimators,                          final_estimator = Ridge(),                          cv = KFold(n_splits=5, shuffle=False)                      ) stacking_regressor Out[5]: <pre>StackingRegressor(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n                  estimators=[('ridge', Ridge(alpha=0.001)),\n                              ('lgbm',\n                               LGBMRegressor(max_depth=5, n_estimators=500,\n                                             random_state=42, verbose=-1))],\n                  final_estimator=Ridge())</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StackingRegressor?Documentation for StackingRegressoriNot fitted Parameters estimators\u00a0 [('ridge', ...), ('lgbm', ...)] final_estimator\u00a0 Ridge() cv\u00a0 KFold(n_split...shuffle=False) n_jobs\u00a0 None passthrough\u00a0 False verbose\u00a0 0 ridgeRidge?Documentation for Ridge Parameters alpha\u00a0 0.001 fit_intercept\u00a0 True copy_X\u00a0 True max_iter\u00a0 None tol\u00a0 0.0001 solver\u00a0 'auto' positive\u00a0 False random_state\u00a0 None lgbmLGBMRegressor Parameters boosting_type\u00a0 'gbdt' num_leaves\u00a0 31 max_depth\u00a0 5 learning_rate\u00a0 0.1 n_estimators\u00a0 500 subsample_for_bin\u00a0 200000 objective\u00a0 None class_weight\u00a0 None min_split_gain\u00a0 0.0 min_child_weight\u00a0 0.001 min_child_samples\u00a0 20 subsample\u00a0 1.0 subsample_freq\u00a0 0 colsample_bytree\u00a0 1.0 reg_alpha\u00a0 0.0 reg_lambda\u00a0 0.0 random_state\u00a0 42 n_jobs\u00a0 None importance_type\u00a0 'split' verbose\u00a0 -1 final_estimatorRidge?Documentation for Ridge Parameters alpha\u00a0 1.0 fit_intercept\u00a0 True copy_X\u00a0 True max_iter\u00a0 None tol\u00a0 0.0001 solver\u00a0 'auto' positive\u00a0 False random_state\u00a0 None In\u00a0[6]: Copied! <pre># Create forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor = stacking_regressor,\n                 lags      = 12  # Last 12 months used as predictors\n             )\n</pre> # Create forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor = stacking_regressor,                  lags      = 12  # Last 12 months used as predictors              ) In\u00a0[7]: Copied! <pre># Backtesting on test data\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 12,  # Forecast horizon\n         initial_train_size = len(data.loc[:end_validation]),\n         refit              = False, \n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data['consumption'],\n                          exog       = data['month_of_year'],\n                          cv         = cv,\n                          metric     = 'mean_squared_error',\n                          n_jobs     = 'auto',\n                          verbose    = False\n                      )        \n\nmetric\n</pre> # Backtesting on test data # ============================================================================== cv = TimeSeriesFold(          steps              = 12,  # Forecast horizon          initial_train_size = len(data.loc[:end_validation]),          refit              = False,       )  metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data['consumption'],                           exog       = data['month_of_year'],                           cv         = cv,                           metric     = 'mean_squared_error',                           n_jobs     = 'auto',                           verbose    = False                       )          metric <pre>  0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> Out[7]: mean_squared_error 0 0.053776 <p>When using <code>StackingRegressor</code>, the hyperparameters of each regressor must be preceded by the name of the regressor followed by two underscores. For example, the <code>alpha</code> hyperparameter of the ridge regressor must be specified as <code>ridge__alpha</code>. The hyperparameter of the final estimator must be specified with the prefix <code>final_estimator__</code>.</p> In\u00a0[8]: Copied! <pre># Grid search of hyperparameters and lags\n# ==============================================================================\nparam_grid = {\n    'ridge__alpha': [0.1, 1, 10],\n    'lgbm__n_estimators': [100, 500],\n    'lgbm__max_depth': [3, 5, 10],\n    'lgbm__learning_rate': [0.01, 0.1],\n    'final_estimator__alpha': [0.1, 1]\n}\n\n# Lags used as predictors\nlags_grid = [24]\n\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False, \n     )\n\nresults_grid = grid_search_forecaster(\n                   forecaster  = forecaster,\n                   y           = data['consumption'],\n                   exog        = data['month_of_year'],\n                   lags_grid   = lags_grid,\n                   param_grid  = param_grid,\n                   cv          = cv,\n                   metric      = 'mean_squared_error'\n               )\n\nresults_grid.head()\n</pre> # Grid search of hyperparameters and lags # ============================================================================== param_grid = {     'ridge__alpha': [0.1, 1, 10],     'lgbm__n_estimators': [100, 500],     'lgbm__max_depth': [3, 5, 10],     'lgbm__learning_rate': [0.01, 0.1],     'final_estimator__alpha': [0.1, 1] }  # Lags used as predictors lags_grid = [24]  cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data.loc[:end_train]),          refit              = False,       )  results_grid = grid_search_forecaster(                    forecaster  = forecaster,                    y           = data['consumption'],                    exog        = data['month_of_year'],                    lags_grid   = lags_grid,                    param_grid  = param_grid,                    cv          = cv,                    metric      = 'mean_squared_error'                )  results_grid.head() <pre>lags grid:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>params grid:   0%|          | 0/72 [00:00&lt;?, ?it/s]</pre> <pre>`Forecaster` refitted using the best-found lags and parameters, and the whole data set: \n  Lags: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] \n  Parameters: {'final_estimator__alpha': 1, 'lgbm__learning_rate': 0.01, 'lgbm__max_depth': 3, 'lgbm__n_estimators': 100, 'ridge__alpha': 0.1}\n  Backtesting metric: 0.0663542458488064\n</pre> Out[8]: lags lags_label params mean_squared_error final_estimator__alpha lgbm__learning_rate lgbm__max_depth lgbm__n_estimators ridge__alpha 0 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 1, 'lgbm__learning_... 0.066354 1.0 0.01 3.0 100.0 0.1 1 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 1, 'lgbm__learning_... 0.068780 1.0 0.01 3.0 100.0 1.0 2 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 0.1, 'lgbm__learnin... 0.069371 0.1 0.01 3.0 100.0 0.1 3 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 1, 'lgbm__learning_... 0.069450 1.0 0.10 10.0 500.0 0.1 4 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... {'final_estimator__alpha': 1, 'lgbm__learning_... 0.069464 1.0 0.01 5.0 100.0 0.1 <p>Once the best hyperparameters have been determined for each regressor in the ensemble, the test error is computed through backtesting.</p> In\u00a0[9]: Copied! <pre># Backtesting on test data\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data.loc[:end_validation]),\n         refit              = False, \n     )\n\nmetric, predictions = backtesting_forecaster(\n                          forecaster = forecaster,\n                          y          = data['consumption'],\n                          exog       = data['month_of_year'],\n                          cv         = cv,\n                          metric     = 'mean_squared_error'\n                      )        \n\nmetric\n</pre> # Backtesting on test data # ============================================================================== cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data.loc[:end_validation]),          refit              = False,       )  metric, predictions = backtesting_forecaster(                           forecaster = forecaster,                           y          = data['consumption'],                           exog       = data['month_of_year'],                           cv         = cv,                           metric     = 'mean_squared_error'                       )          metric <pre>  0%|          | 0/7 [00:00&lt;?, ?it/s]</pre> Out[9]: mean_squared_error 0 0.013506 <p>When a regressor of type <code>StackingRegressor</code> is used as a regressor in a predictor, its <code>get_feature_importances</code> method will not work. This is because objects of type <code>StackingRegressor</code> do not have either the <code>feature_importances</code> or <code>coef_</code> attribute. Instead, it is necessary to inspect each of the regressors that are part of the stacking.</p> In\u00a0[10]: Copied! <pre># Feature importances for each regressor in the stacking\n# ==============================================================================\nif forecaster.regressor.__class__.__name__ == 'StackingRegressor':\n    importancia_pred = []\n    for regressor in forecaster.regressor.estimators_:\n        try:\n            importancia = pd.DataFrame(\n                data = {\n                    'feature': forecaster.regressor.feature_names_in_,\n                    f'importance_{type(regressor).__name__}': regressor.coef_,\n                    f'importance_abs_{type(regressor).__name__}': np.abs(regressor.coef_)\n                }\n            ).set_index('feature')\n        except:\n            importancia = pd.DataFrame(\n                data = {\n                    'feature': forecaster.regressor.feature_names_in_,\n                    f'importance_{type(regressor).__name__}': regressor.feature_importances_,\n                    f'importance_abs_{type(regressor).__name__}': np.abs(regressor.feature_importances_)\n                }\n            ).set_index('feature')\n        importancia_pred.append(importancia)\n    \n    importancia_pred = pd.concat(importancia_pred, axis=1)\n    \nelse:\n    importancia_pred = forecaster.get_feature_importances()\n    importancia_pred['importance_abs'] = importancia_pred['importance'].abs()\n    importancia_pred = importancia_pred.sort_values(by='importance_abs', ascending=False)\n\nimportancia_pred.head(5)\n</pre> # Feature importances for each regressor in the stacking # ============================================================================== if forecaster.regressor.__class__.__name__ == 'StackingRegressor':     importancia_pred = []     for regressor in forecaster.regressor.estimators_:         try:             importancia = pd.DataFrame(                 data = {                     'feature': forecaster.regressor.feature_names_in_,                     f'importance_{type(regressor).__name__}': regressor.coef_,                     f'importance_abs_{type(regressor).__name__}': np.abs(regressor.coef_)                 }             ).set_index('feature')         except:             importancia = pd.DataFrame(                 data = {                     'feature': forecaster.regressor.feature_names_in_,                     f'importance_{type(regressor).__name__}': regressor.feature_importances_,                     f'importance_abs_{type(regressor).__name__}': np.abs(regressor.feature_importances_)                 }             ).set_index('feature')         importancia_pred.append(importancia)          importancia_pred = pd.concat(importancia_pred, axis=1)      else:     importancia_pred = forecaster.get_feature_importances()     importancia_pred['importance_abs'] = importancia_pred['importance'].abs()     importancia_pred = importancia_pred.sort_values(by='importance_abs', ascending=False)  importancia_pred.head(5) Out[10]: importance_Ridge importance_abs_Ridge importance_LGBMRegressor importance_abs_LGBMRegressor feature lag_1 0.020984 0.020984 59 59 lag_2 0.216998 0.216998 1 1 lag_3 0.188519 0.188519 0 0 lag_4 0.200916 0.200916 0 0 lag_5 0.106734 0.106734 0 0"},{"location":"user_guides/stacking-ensemble-models-forecasting.html#stacking-ensemble-machine-learning-models-to-improve-forecasting","title":"Stacking ensemble machine learning models to improve forecasting\u00b6","text":""},{"location":"user_guides/stacking-ensemble-models-forecasting.html#libraries-and-data","title":"Libraries and Data\u00b6","text":""},{"location":"user_guides/stacking-ensemble-models-forecasting.html#stackingregressor","title":"StackingRegressor\u00b6","text":"<p>With scikit-learn it is very easy to combine multiple regressors thanks to the StackingRegressor class.</p> <p>The <code>estimators</code> parameter corresponds to the list of the estimators (base learners) that will be stacked in parallel on the input data. It should be specified as a list of names and estimators. The <code>final_estimator</code> (metamodel) will use the predictions of the estimators as input.</p>"},{"location":"user_guides/stacking-ensemble-models-forecasting.html#hiperparameters-search-of-stackingregressor","title":"Hiperparameters search of StackingRegressor\u00b6","text":""},{"location":"user_guides/stacking-ensemble-models-forecasting.html#feature-importance-in-stackingregressor","title":"Feature importance in StackingRegressor\u00b6","text":""},{"location":"user_guides/table-of-contents.html","title":"Table of Contents","text":"<p>Welcome to the skforecast user guides! This comprehensive collection of guides is designed to help you navigate through the various features and functionalities of skforecast. Whether you are a beginner or an advanced user, you will find the necessary resources to master time series forecasting with skforecast. Below, you will find the user guides categorized by topic for easier navigation.</p> <p>Single series Forecasters</p> <ul> <li>Input data</li> <li>Recursive multi-step forecasting</li> <li>Direct multi-step forecasting</li> <li>ARIMA and SARIMAX forecasting</li> <li>Forecasting baseline</li> </ul> <p>Global Forecasters (multiple series)</p> <ul> <li>Independent multi-time series forecasting</li> <li>Series with different lengths and different exogenous variables</li> <li>Dependent multivariate series forecasting</li> <li>Deep learning Recurrent Neural Networks</li> </ul> <p>Feature Engineering</p> <ul> <li>Exogenous variables</li> <li>Window and custom features</li> <li>Categorical features</li> <li>Calendars features</li> <li>Data transformations</li> <li>Differentiation</li> <li>Feature selection</li> <li>Sktime pipelines</li> </ul> <p>Model Evaluation and Tuning</p> <ul> <li>Metrics</li> <li>Backtesting forecaster</li> <li>Hyperparameter tuning and lags selection</li> <li>Feature selection</li> </ul> <p>Probabilistic Forecasting</p> <ul> <li>Probabilistic forecasting</li> <li>Bootstrapped residuals</li> <li>Conformal predictions</li> <li>Conformal calibration</li> <li>Quantile forecasting</li> <li>Probabilistic global models</li> <li>Metrics in probabilistic forecasting</li> <li>Continuous Ranked Probability Score (CRPS)</li> </ul> <p>Model Explainability</p> <ul> <li>Model Explainability</li> </ul> <p>Model deployment</p> <ul> <li>Save and load forecaster</li> <li>Forecaster in production</li> </ul> <p>Plotting</p> <ul> <li>Plotting</li> </ul> <p>Datasets</p> <ul> <li>Datasets</li> </ul> <p>Additional Resources</p> <ul> <li>Skforecast 0.14 Migration guide</li> <li>Extract training and prediction matrices</li> <li>Weighted time series forecasting</li> <li>Stacking multiple models</li> <li>Forecasting with XGBoost and LightGBM</li> <li>Skforecast in GPU</li> </ul> <p>FAQ and forecasting tips</p> <ul> <li>Avoid negative predictions when forecasting</li> <li>Forecasting time series with missing values</li> <li>Forecasting with delayed historical data</li> <li>Backtesting vs One-step-ahead</li> <li>Cyclical features in time series</li> <li>Time series aggregation</li> <li>Parallelization in skforecast</li> <li>Profiling skforecast</li> </ul> <p>We hope you find these guides helpful. If you have any questions or need further assistance, please don't hesitate to reach out to the skforecast community. Remember to visit the FAQ and forecasting tips page for answers to frequently asked questions and forecasting tips.</p>"},{"location":"user_guides/time-series-differentiation.html","title":"Differentiation","text":"<p>Time series differentiation involves computing the differences between consecutive observations in the time series. When it comes to training forecasting models, differentiation offers the advantage of focusing on relative rates of change rather than directly attempting to model the absolute values. Once the predictions have been estimated, this transformation can be easily reversed to restore the values to their original scale.</p> <p>This methodology is very useful when dealing with time series that exhibit trends, as not all models can capture such trends properly. Among the many machine learning techniques, tree-based models, including decision trees, random forests, and gradient boosting machines (GBMs), stand out for their effectiveness and broad applicability to various domains. Nonetheless, these models are limited in their ability to extrapolate. Their inability to project values outside the observed range during training inevitably results in predicted values that deviate from the underlying trend.</p> <p>Skforecast introduces the <code>differentiation</code> parameter within its forecasters. This parameter indicates that a differentiation process must be applied before training the model, and this task is performed through the internal use of a new class named <code>TimeSeriesDifferentiator</code>. It is important to note that the entire differentiation process is automated and its effects are seamlessly reversed during the prediction phase. This ensures that the resulting forecast values are in the original scale of the time series data.</p> <p> \ud83d\udca1 Tip </p> <p>To learn more about modeling time series differentiation, visit our example: Modelling time series trend with tree based models.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nfrom skforecast.preprocessing import TimeSeriesDifferentiator\n</pre> # Libraries # ============================================================================== import numpy as np from skforecast.preprocessing import TimeSeriesDifferentiator In\u00a0[2]: Copied! <pre># Differentiation with TimeSeriesDifferentiator\n# ==============================================================================\ny = np.array([5, 8, 12, 10, 14, 17, 21, 19], dtype=float)\ndiffenciator = TimeSeriesDifferentiator()\ndiffenciator.fit(y)\ny_diff = diffenciator.transform(y)\n\nprint(f\"Original time series   : {y}\")\nprint(f\"Differenced time series: {y_diff}\")\n</pre> # Differentiation with TimeSeriesDifferentiator # ============================================================================== y = np.array([5, 8, 12, 10, 14, 17, 21, 19], dtype=float) diffenciator = TimeSeriesDifferentiator() diffenciator.fit(y) y_diff = diffenciator.transform(y)  print(f\"Original time series   : {y}\") print(f\"Differenced time series: {y_diff}\") <pre>Original time series   : [ 5.  8. 12. 10. 14. 17. 21. 19.]\nDifferenced time series: [nan  3.  4. -2.  4.  3.  4. -2.]\n</pre> <p>The process of differencing can be reversed (integration) using the <code>inverse_transform</code> method.</p> In\u00a0[3]: Copied! <pre>diffenciator.inverse_transform(y_diff)\n</pre> diffenciator.inverse_transform(y_diff) Out[3]: <pre>array([ 5.,  8., 12., 10., 14., 17., 21., 19.])</pre> <p> \u26a0 Warning </p> <p>The inverse transformation process, <code>inverse_transform</code>, is applicable only to the same time series that was previously differentiated using the same <code>TimeSeriesDifferentiator</code> object. This limitation arises from the need to use the initial n values of the time series (n equals the order of differentiation) to successfully reverse the differentiation. These values are stored when the <code>fit</code> method is executed.</p> <p> \u270e Note </p> <p>An additional method <code>inverse_transform_next_window</code> is available in the <code>TimeSeriesDifferentiator</code>. This method is designed to be used inside the Forecasters to reverse the differentiation of the predicted values. If the Forecaster regressor is trained with a differentiated time series, then the predicted values will be differentiated as well. The <code>inverse_transform_next_window</code> method allows to return the predictions to the original scale, with the assumption that they start immediately after the last values observed (<code>last_window</code>).</p> In\u00a0[4]: Copied! <pre># Data manipulation\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\n\n# Plots\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom skforecast.plot import set_dark_theme\n\n# Modelling and Forecasting\n# ==============================================================================\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.preprocessing import TimeSeriesDifferentiator\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\n</pre> # Data manipulation # ============================================================================== import numpy as np import pandas as pd  # Plots # ============================================================================== import matplotlib.pyplot as plt from skforecast.plot import set_dark_theme  # Modelling and Forecasting # ============================================================================== from xgboost import XGBRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error from sklearn.preprocessing import StandardScaler from skforecast.recursive import ForecasterRecursive from skforecast.preprocessing import TimeSeriesDifferentiator from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster In\u00a0[5]: Copied! <pre># Download data\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-learning-python/'\n    'master/data/AirPassengers.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y-%m')\ndata = data.set_index('Date')\ndata = data.asfreq('MS')\ndata = data['Passengers']\ndata = data.sort_index()\ndata.head(4)\n\n# Data partition train-test\n# ==============================================================================\nend_train = '1956-12-01 23:59:59'\nprint(\n    f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \" \n    f\"(n={len(data.loc[:end_train])})\")\nprint(\n    f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"\n    f\"(n={len(data.loc[end_train:])})\")\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 2.5))\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\nax.legend();\n</pre> # Download data # ============================================================================== url = (     'https://raw.githubusercontent.com/JoaquinAmatRodrigo/Estadistica-machine-learning-python/'     'master/data/AirPassengers.csv' ) data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['Date'] = pd.to_datetime(data['Date'], format='%Y-%m') data = data.set_index('Date') data = data.asfreq('MS') data = data['Passengers'] data = data.sort_index() data.head(4)  # Data partition train-test # ============================================================================== end_train = '1956-12-01 23:59:59' print(     f\"Train dates : {data.index.min()} --- {data.loc[:end_train].index.max()}  \"      f\"(n={len(data.loc[:end_train])})\") print(     f\"Test dates  : {data.loc[end_train:].index.min()} --- {data.index.max()}  \"     f\"(n={len(data.loc[end_train:])})\")  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 2.5)) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') ax.legend(); <pre>Train dates : 1949-01-01 00:00:00 --- 1956-12-01 00:00:00  (n=96)\nTest dates  : 1957-01-01 00:00:00 --- 1960-12-01 00:00:00  (n=48)\n</pre> <p>Two autoregressive forecasters are created, one with a scikit-learn <code>RandomForestRegressor</code> and the other with an <code>XGBoost</code>. Both are trained on data from 1949-01-01 to 1956-12-01 and produce forecasts for the next 48 months (4 years).</p> In\u00a0[6]: Copied! <pre># Forecasting without differentiation\n# ==============================================================================\nsteps = len(data.loc[end_train:])\n\n# Forecasters\nforecaster_rf = ForecasterRecursive(\n                    regressor = RandomForestRegressor(random_state=963),\n                    lags      = 12\n                )\nforecaster_gb = ForecasterRecursive(\n                    regressor = XGBRegressor(random_state=963),\n                    lags      = 12\n                )\n\n# Train\nforecaster_rf.fit(data.loc[:end_train])\nforecaster_gb.fit(data.loc[:end_train])\n\n# Predict\npredictions_rf = forecaster_rf.predict(steps=steps)\npredictions_gb = forecaster_gb.predict(steps=steps)\n\n# Error\nerror_rf = mean_absolute_error(data.loc[end_train:], predictions_rf)\nerror_gb = mean_absolute_error(data.loc[end_train:], predictions_gb)\nprint(f\"Error (MAE) Random Forest: {error_rf:.2f}\")\nprint(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True)\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions_rf.plot(ax=ax, label='Random Forest')\npredictions_gb.plot(ax=ax, label='Gradient Boosting')\nax.set_title('Forecasting without differentiation')\nax.set_xlabel('')\nax.legend();\n</pre> # Forecasting without differentiation # ============================================================================== steps = len(data.loc[end_train:])  # Forecasters forecaster_rf = ForecasterRecursive(                     regressor = RandomForestRegressor(random_state=963),                     lags      = 12                 ) forecaster_gb = ForecasterRecursive(                     regressor = XGBRegressor(random_state=963),                     lags      = 12                 )  # Train forecaster_rf.fit(data.loc[:end_train]) forecaster_gb.fit(data.loc[:end_train])  # Predict predictions_rf = forecaster_rf.predict(steps=steps) predictions_gb = forecaster_gb.predict(steps=steps)  # Error error_rf = mean_absolute_error(data.loc[end_train:], predictions_rf) error_gb = mean_absolute_error(data.loc[end_train:], predictions_gb) print(f\"Error (MAE) Random Forest: {error_rf:.2f}\") print(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")  # Plot fig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions_rf.plot(ax=ax, label='Random Forest') predictions_gb.plot(ax=ax, label='Gradient Boosting') ax.set_title('Forecasting without differentiation') ax.set_xlabel('') ax.legend(); <pre>Error (MAE) Random Forest: 54.79\nError (MAE) Gradient Boosting: 49.16\n</pre> <p>The plot shows that none of the models is capable of accurately predicting the trend. After a few steps, the predictions become nearly constant, close to the maximum values observed in the training data.</p> <p>Next, two new forecasters are trained using the same configuration, but with the argument <code>differentiation = 1</code>. This activates the internal process of differencing (order 1) the time series before training the model, and reverses the differentiation (also known as integration) for the predicted values.</p> In\u00a0[7]: Copied! <pre># Forecasting with differentiation\n# ==============================================================================\nsteps = len(data.loc[end_train:])\n\n# Forecasters\nforecaster_rf = ForecasterRecursive(\n                    regressor       = RandomForestRegressor(random_state=963),\n                    lags            = 12,\n                    differentiation = 1\n                )\nforecaster_gb = ForecasterRecursive(\n                    regressor       = XGBRegressor(random_state=963),\n                    lags            = 12,\n                    differentiation = 1\n                )\n\n# Train\nforecaster_rf.fit(data.loc[:end_train])\nforecaster_gb.fit(data.loc[:end_train])\n\n# Predict\npredictions_rf = forecaster_rf.predict(steps=steps)\npredictions_gb = forecaster_gb.predict(steps=steps)\n\n# Error\nerror_rf = mean_absolute_error(data.loc[end_train:], predictions_rf)\nerror_gb = mean_absolute_error(data.loc[end_train:], predictions_gb)\nprint(f\"Error (MAE) Random Forest: {error_rf:.2f}\")\nprint(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")\n\n# Plot\nfig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True)\ndata.loc[:end_train].plot(ax=ax, label='train')\ndata.loc[end_train:].plot(ax=ax, label='test')\npredictions_rf.plot(ax=ax, label='Random Forest')\npredictions_gb.plot(ax=ax, label='Gradient Boosting')\nax.set_title('Forecasting with differentiation')\nax.set_xlabel('')\nax.legend();\n</pre> # Forecasting with differentiation # ============================================================================== steps = len(data.loc[end_train:])  # Forecasters forecaster_rf = ForecasterRecursive(                     regressor       = RandomForestRegressor(random_state=963),                     lags            = 12,                     differentiation = 1                 ) forecaster_gb = ForecasterRecursive(                     regressor       = XGBRegressor(random_state=963),                     lags            = 12,                     differentiation = 1                 )  # Train forecaster_rf.fit(data.loc[:end_train]) forecaster_gb.fit(data.loc[:end_train])  # Predict predictions_rf = forecaster_rf.predict(steps=steps) predictions_gb = forecaster_gb.predict(steps=steps)  # Error error_rf = mean_absolute_error(data.loc[end_train:], predictions_rf) error_gb = mean_absolute_error(data.loc[end_train:], predictions_gb) print(f\"Error (MAE) Random Forest: {error_rf:.2f}\") print(f\"Error (MAE) Gradient Boosting: {error_gb:.2f}\")  # Plot fig, ax = plt.subplots(figsize=(7, 3), sharex=True, sharey=True) data.loc[:end_train].plot(ax=ax, label='train') data.loc[end_train:].plot(ax=ax, label='test') predictions_rf.plot(ax=ax, label='Random Forest') predictions_gb.plot(ax=ax, label='Gradient Boosting') ax.set_title('Forecasting with differentiation') ax.set_xlabel('') ax.legend(); <pre>Error (MAE) Random Forest: 24.91\nError (MAE) Gradient Boosting: 25.49\n</pre> <p>This time, both models are able to follow the trend in their predictions.</p> <p> \u26a0 Warning </p> <p>Derivation is a useful strategy for overcoming the problems of modeling trend when using a tree-based model, but it is not a magic bullet. By transforming the original time series values into the rate of change (first derivative), the model can extrapolate the original data once the transformation is reversed. Note, however, that the model is still limited to predicting rates of change within the range observed in the training data.</p> <p>For example, if the rate of change increases over time and the changes become larger and larger, the model will underestimate the increases and decreases, and the resulting predictions will deviate from the actual trend.</p> In\u00a0[8]: Copied! <pre># Time series differentiated by preprocessing before training\n# ==============================================================================\ndiferenciator = TimeSeriesDifferentiator(order=1)\ndata_diff = diferenciator.fit_transform(data.to_numpy())\ndata_diff = pd.Series(data_diff, index=data.index).dropna()\n\nforecaster = ForecasterRecursive(\n                 regressor = RandomForestRegressor(random_state=963),\n                 lags      = 15\n             )\nforecaster.fit(y=data_diff.loc[:end_train])\npredictions_diff = forecaster.predict(steps=steps)\n\n# Revert differentiation to obtain final predictions\nlast_value_train = data.loc[:end_train].iloc[[-1]]\npredictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:]\npredictions_1 = predictions_1.asfreq('MS')\npredictions_1.name = 'pred'\npredictions_1.head(5)\n</pre> # Time series differentiated by preprocessing before training # ============================================================================== diferenciator = TimeSeriesDifferentiator(order=1) data_diff = diferenciator.fit_transform(data.to_numpy()) data_diff = pd.Series(data_diff, index=data.index).dropna()  forecaster = ForecasterRecursive(                  regressor = RandomForestRegressor(random_state=963),                  lags      = 15              ) forecaster.fit(y=data_diff.loc[:end_train]) predictions_diff = forecaster.predict(steps=steps)  # Revert differentiation to obtain final predictions last_value_train = data.loc[:end_train].iloc[[-1]] predictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:] predictions_1 = predictions_1.asfreq('MS') predictions_1.name = 'pred' predictions_1.head(5) Out[8]: <pre>1957-01-01    312.00\n1957-02-01    302.93\n1957-03-01    341.61\n1957-04-01    338.03\n1957-05-01    341.97\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[9]: Copied! <pre># Time series differentiated internally by the forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = RandomForestRegressor(random_state=963),\n                 lags            = 15,\n                 differentiation = 1\n             )\n\nforecaster.fit(y=data.loc[:end_train])\npredictions_2 = forecaster.predict(steps=steps)\npredictions_2.head(5)\n</pre> # Time series differentiated internally by the forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = RandomForestRegressor(random_state=963),                  lags            = 15,                  differentiation = 1              )  forecaster.fit(y=data.loc[:end_train]) predictions_2 = forecaster.predict(steps=steps) predictions_2.head(5) Out[9]: <pre>1957-01-01    312.00\n1957-02-01    302.93\n1957-03-01    341.61\n1957-04-01    338.03\n1957-05-01    341.97\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[10]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_series_equal(predictions_1, predictions_2)\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_series_equal(predictions_1, predictions_2) <p>The predictions are the same for both approaches.</p> <p>The results are also equivalent when a transformation, such as standardization, is applied to the time series. In this case, the order of the steps is: transformation -&gt; differentiation -&gt; model fitting -&gt; prediction -&gt; inverse differentiation -&gt; inverse transformation.</p> In\u00a0[11]: Copied! <pre># Time series differentiated by preprocessing before training\n# ==============================================================================\nscaler = StandardScaler()\nscaler.fit(data.loc[:end_train].to_numpy().reshape(-1, 1))\ndata_scaled = scaler.transform(data.to_numpy().reshape(-1, 1))\ndata_scaled = pd.Series(data_scaled.flatten(), index=data.index)\ndata_scaled_diff = TimeSeriesDifferentiator(order=1).fit_transform(data_scaled.to_numpy())\ndata_scaled_diff = pd.Series(data_scaled_diff, index=data.index).dropna()\n\nforecaster = ForecasterRecursive(\n                 regressor     = RandomForestRegressor(random_state=963),\n                 lags          = 15,\n             )\nforecaster.fit(y=data_scaled_diff.loc[:end_train])\npredictions_diff = forecaster.predict(steps=steps)\n\n# Revert differentiation to obtain final predictions\nlast_value_train = data_scaled.loc[:end_train].iloc[[-1]]\npredictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:]\n# Revert the scaling\npredictions_1 = scaler.inverse_transform(predictions_1.to_numpy().reshape(-1, 1))\npredictions_1 = pd.Series(predictions_1.flatten(), index=data.loc[end_train:].index.to_numpy())\npredictions_1 = predictions_1.asfreq('MS')\npredictions_1.name = 'pred'\ndisplay(predictions_1.head(5))\n\n\n# Time series differentiated internally by the forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = RandomForestRegressor(random_state=963),\n                 lags            = 15,\n                 differentiation = 1,\n                 transformer_y = StandardScaler()\n             )\n\nforecaster.fit(y=data.loc[:end_train])\npredictions_2 = forecaster.predict(steps=steps)\ndisplay(predictions_2.head(5))\n</pre> # Time series differentiated by preprocessing before training # ============================================================================== scaler = StandardScaler() scaler.fit(data.loc[:end_train].to_numpy().reshape(-1, 1)) data_scaled = scaler.transform(data.to_numpy().reshape(-1, 1)) data_scaled = pd.Series(data_scaled.flatten(), index=data.index) data_scaled_diff = TimeSeriesDifferentiator(order=1).fit_transform(data_scaled.to_numpy()) data_scaled_diff = pd.Series(data_scaled_diff, index=data.index).dropna()  forecaster = ForecasterRecursive(                  regressor     = RandomForestRegressor(random_state=963),                  lags          = 15,              ) forecaster.fit(y=data_scaled_diff.loc[:end_train]) predictions_diff = forecaster.predict(steps=steps)  # Revert differentiation to obtain final predictions last_value_train = data_scaled.loc[:end_train].iloc[[-1]] predictions_1 = pd.concat([last_value_train, predictions_diff]).cumsum()[1:] # Revert the scaling predictions_1 = scaler.inverse_transform(predictions_1.to_numpy().reshape(-1, 1)) predictions_1 = pd.Series(predictions_1.flatten(), index=data.loc[end_train:].index.to_numpy()) predictions_1 = predictions_1.asfreq('MS') predictions_1.name = 'pred' display(predictions_1.head(5))   # Time series differentiated internally by the forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = RandomForestRegressor(random_state=963),                  lags            = 15,                  differentiation = 1,                  transformer_y = StandardScaler()              )  forecaster.fit(y=data.loc[:end_train]) predictions_2 = forecaster.predict(steps=steps) display(predictions_2.head(5)) <pre>1957-01-01    312.43\n1957-02-01    303.45\n1957-03-01    342.44\n1957-04-01    340.00\n1957-05-01    343.82\nFreq: MS, Name: pred, dtype: float64</pre> <pre>1957-01-01    312.43\n1957-02-01    303.45\n1957-03-01    342.44\n1957-04-01    340.00\n1957-05-01    343.82\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[12]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_series_equal(predictions_1, predictions_2)\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_series_equal(predictions_1, predictions_2) <p>Next, the outcomes of the backtesting process are subjected to a comparative analysis. This comparison is more complex than the previous one, as the process of undoing the differentiation must be performed separately for each backtesting fold.</p> In\u00a0[13]: Copied! <pre># Backtesting with the time series differentiated by preprocessing before training\n# ==============================================================================\nsteps = 5\nforecaster_1 = ForecasterRecursive(\n                   regressor = RandomForestRegressor(random_state=963),\n                   lags      = 15\n               )\n\ncv = TimeSeriesFold(\n         steps                 = steps,\n         initial_train_size    = len(data_diff.loc[:end_train]),\n         refit                 = True,\n         fixed_train_size      = False, \n         allow_incomplete_fold = True\n     )\n\n_, predictions_1 = backtesting_forecaster(\n                       forecaster    = forecaster_1,\n                       y             = data_diff,\n                       cv            = cv,\n                       metric        = 'mean_squared_error',\n                       n_jobs        = 'auto',\n                       verbose       = False,\n                       show_progress = True\n                   )\n\n# Revert differentiation of predictions. Predictions of each fold must be reverted\n# individually. An id is added to each prediction to identify the fold to which it belongs.\npredictions_1 = predictions_1.rename(columns={'pred': 'pred_diff'})\nfolds = len(predictions_1) / steps\nfolds = int(np.ceil(folds))\npredictions_1['backtesting_fold_id'] = np.repeat(range(folds), steps)[:len(predictions_1)]\n\n# Add the previously observed value of the time series (only to the first prediction of each fold)\nprevious_overved_values = data.shift(1).loc[predictions_1.index].iloc[::steps]\nprevious_overved_values.name = 'previous_overved_value'\npredictions_1 = predictions_1.merge(\n                    previous_overved_values,\n                    left_index  = True,\n                    right_index = True,\n                    how         = 'left'\n                )\npredictions_1 = predictions_1.fillna(0)\npredictions_1['summed_value'] = (\n    predictions_1['pred_diff'] + predictions_1['previous_overved_value']\n)\n\n# Revert differentiation using the cumulative sum by fold\npredictions_1['pred'] = (\n    predictions_1\n    .groupby('backtesting_fold_id')\n    .apply(lambda x: x['summed_value'].cumsum(), include_groups=False)\n    .to_numpy()\n)\n\npredictions_1.head(5)\n</pre> # Backtesting with the time series differentiated by preprocessing before training # ============================================================================== steps = 5 forecaster_1 = ForecasterRecursive(                    regressor = RandomForestRegressor(random_state=963),                    lags      = 15                )  cv = TimeSeriesFold(          steps                 = steps,          initial_train_size    = len(data_diff.loc[:end_train]),          refit                 = True,          fixed_train_size      = False,           allow_incomplete_fold = True      )  _, predictions_1 = backtesting_forecaster(                        forecaster    = forecaster_1,                        y             = data_diff,                        cv            = cv,                        metric        = 'mean_squared_error',                        n_jobs        = 'auto',                        verbose       = False,                        show_progress = True                    )  # Revert differentiation of predictions. Predictions of each fold must be reverted # individually. An id is added to each prediction to identify the fold to which it belongs. predictions_1 = predictions_1.rename(columns={'pred': 'pred_diff'}) folds = len(predictions_1) / steps folds = int(np.ceil(folds)) predictions_1['backtesting_fold_id'] = np.repeat(range(folds), steps)[:len(predictions_1)]  # Add the previously observed value of the time series (only to the first prediction of each fold) previous_overved_values = data.shift(1).loc[predictions_1.index].iloc[::steps] previous_overved_values.name = 'previous_overved_value' predictions_1 = predictions_1.merge(                     previous_overved_values,                     left_index  = True,                     right_index = True,                     how         = 'left'                 ) predictions_1 = predictions_1.fillna(0) predictions_1['summed_value'] = (     predictions_1['pred_diff'] + predictions_1['previous_overved_value'] )  # Revert differentiation using the cumulative sum by fold predictions_1['pred'] = (     predictions_1     .groupby('backtesting_fold_id')     .apply(lambda x: x['summed_value'].cumsum(), include_groups=False)     .to_numpy() )  predictions_1.head(5) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> Out[13]: pred_diff backtesting_fold_id previous_overved_value summed_value pred 1957-01-01 6.00 0 306.0 312.00 312.00 1957-02-01 -9.07 0 0.0 -9.07 302.93 1957-03-01 38.68 0 0.0 38.68 341.61 1957-04-01 -3.58 0 0.0 -3.58 338.03 1957-05-01 3.94 0 0.0 3.94 341.97 In\u00a0[14]: Copied! <pre># Backtesting with the time series differentiated internally\n# ==============================================================================\nforecaster_2 = ForecasterRecursive(\n                   regressor       = RandomForestRegressor(random_state=963),\n                   lags            = 15,\n                   differentiation = 1\n               )\n\ncv = TimeSeriesFold(\n         steps                 = steps,\n         initial_train_size    = len(data.loc[:end_train]),\n         refit                 = True,\n         fixed_train_size      = False, \n         allow_incomplete_fold = True,\n         differentiation       = 1  # Differentiation must be specified in the cv object\n     )\n\n_, predictions_2 = backtesting_forecaster(\n                       forecaster    = forecaster_2,\n                       y             = data,\n                       cv            = cv,\n                       metric        = 'mean_squared_error',\n                       n_jobs        = 'auto',\n                       verbose       = False,\n                       show_progress = True\n                   )\n\npredictions_2.head(5)\n</pre> # Backtesting with the time series differentiated internally # ============================================================================== forecaster_2 = ForecasterRecursive(                    regressor       = RandomForestRegressor(random_state=963),                    lags            = 15,                    differentiation = 1                )  cv = TimeSeriesFold(          steps                 = steps,          initial_train_size    = len(data.loc[:end_train]),          refit                 = True,          fixed_train_size      = False,           allow_incomplete_fold = True,          differentiation       = 1  # Differentiation must be specified in the cv object      )  _, predictions_2 = backtesting_forecaster(                        forecaster    = forecaster_2,                        y             = data,                        cv            = cv,                        metric        = 'mean_squared_error',                        n_jobs        = 'auto',                        verbose       = False,                        show_progress = True                    )  predictions_2.head(5) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> Out[14]: pred 1957-01-01 312.00 1957-02-01 302.93 1957-03-01 341.61 1957-04-01 338.03 1957-05-01 341.97 In\u00a0[15]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_series_equal(predictions_1['pred'], predictions_2['pred'])\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_series_equal(predictions_1['pred'], predictions_2['pred']) <p>Finally, the validation is also performed for the predictions obtained with <code>predict_boostrapping</code>.</p> In\u00a0[16]: Copied! <pre># Time series differentiated by preprocessing before training\n# ==============================================================================\nforecaster_1 = ForecasterRecursive(\n                   regressor = RandomForestRegressor(random_state=963),\n                   lags      = 15\n               )\n\nforecaster_1.fit(y=data_diff.loc[:end_train], store_in_sample_residuals=True)\nboot_predictions_diff = forecaster_1.predict_bootstrapping(\n                            steps  = steps,\n                            n_boot = 10\n                        )\n# Revert differentiation of predictions\nlast_value_train = data.loc[:end_train].iloc[[-1]]\nboot_predictions_1 = boot_predictions_diff.copy()\nboot_predictions_1.loc[last_value_train.index[0]] = last_value_train.values[0]\nboot_predictions_1 = boot_predictions_1.sort_index()\nboot_predictions_1 = boot_predictions_1.cumsum(axis=0).iloc[1:,]\nboot_predictions_1 = boot_predictions_1.asfreq('MS')\n\n\n# Time series differentiated internally by the forecaster\n# ==============================================================================\nforecaster_2 = ForecasterRecursive(\n                   regressor       = RandomForestRegressor(random_state=963),\n                   lags            = 15,\n                   differentiation = 1\n               )\n\nforecaster_2.fit(y=data.loc[:end_train], store_in_sample_residuals=True)\nboot_predictions_2 = forecaster_2.predict_bootstrapping(steps=steps, n_boot=10)\n</pre> # Time series differentiated by preprocessing before training # ============================================================================== forecaster_1 = ForecasterRecursive(                    regressor = RandomForestRegressor(random_state=963),                    lags      = 15                )  forecaster_1.fit(y=data_diff.loc[:end_train], store_in_sample_residuals=True) boot_predictions_diff = forecaster_1.predict_bootstrapping(                             steps  = steps,                             n_boot = 10                         ) # Revert differentiation of predictions last_value_train = data.loc[:end_train].iloc[[-1]] boot_predictions_1 = boot_predictions_diff.copy() boot_predictions_1.loc[last_value_train.index[0]] = last_value_train.values[0] boot_predictions_1 = boot_predictions_1.sort_index() boot_predictions_1 = boot_predictions_1.cumsum(axis=0).iloc[1:,] boot_predictions_1 = boot_predictions_1.asfreq('MS')   # Time series differentiated internally by the forecaster # ============================================================================== forecaster_2 = ForecasterRecursive(                    regressor       = RandomForestRegressor(random_state=963),                    lags            = 15,                    differentiation = 1                )  forecaster_2.fit(y=data.loc[:end_train], store_in_sample_residuals=True) boot_predictions_2 = forecaster_2.predict_bootstrapping(steps=steps, n_boot=10) In\u00a0[17]: Copied! <pre># Compare both predictions\n# ==============================================================================\npd.testing.assert_frame_equal(boot_predictions_1, boot_predictions_2)\n</pre> # Compare both predictions # ============================================================================== pd.testing.assert_frame_equal(boot_predictions_1, boot_predictions_2)"},{"location":"user_guides/time-series-differentiation.html#time-series-differentiation","title":"Time series differentiation\u00b6","text":""},{"location":"user_guides/time-series-differentiation.html#timeseriesdifferentiator","title":"TimeSeriesDifferentiator\u00b6","text":"<p><code>TimeSeriesDifferentiator</code> is a custom transformer that follows the preprocessing sklearn API. This means it has the method <code>fit</code>, <code>transform</code>, <code>fit_transform</code> and <code>inverse_transform</code>.</p>"},{"location":"user_guides/time-series-differentiation.html#forecasting-with-differentiation","title":"Forecasting with differentiation\u00b6","text":""},{"location":"user_guides/time-series-differentiation.html#internal-differentiation-vs-pre-processing","title":"Internal differentiation vs pre-processing\u00b6","text":"<p>Forecasters manage the differentiation process internally, so there is no need for additional pre-processing of the time series and post-processing of the predictions. This has several advantages, but before diving in, the results of both approaches are compared.</p>"},{"location":"user_guides/time-series-differentiation.html#conslusions","title":"Conslusions\u00b6","text":"<p>If, as demonstrated, the values are equivalent when differentiating the time series in a preprocessing step or when allowing the Forecaster to manage the differentiation internally, why the second alternative is better?</p> <ul> <li><p>Allowing the forecaster to manage all transformations internally guarantees that the same transformations are applied when the model is run on new data.</p> </li> <li><p>When the model is applied to new data that does not follow immediately after the training data (for example, if a model is not retrained for each prediction phase), the forecaster automatically increases the size of the last window needed to generate the predictors, as well as applying the differentiation to the incoming data and undoing it in the final predictions.</p> </li> </ul> <p>These transformations are non-trivial and very error-prone, so skforecast tries to avoid overcomplicating the already challenging task of forecasting time series.</p>"},{"location":"user_guides/training-and-prediction-matrices.html","title":"Extract training and prediction matrices","text":"<p> \u26a0 Warning </p> <p>If any data transformations and/or differentiation, are applied, they will affect the output matrices. Consequently, the predictions generated in this transformed scale may require additional steps to revert back to the original data scale.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.recursive import ForecasterRecursive, ForecasterRecursiveMultiSeries\nfrom skforecast.direct import ForecasterDirect, ForecasterDirectMultiVariate\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.preprocessing import RollingFeatures from skforecast.recursive import ForecasterRecursive, ForecasterRecursiveMultiSeries from skforecast.direct import ForecasterDirect, ForecasterDirectMultiVariate from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Download data single series\n# ==============================================================================\ndata = fetch_dataset(\n    name=\"h2o\", kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0}\n)\nprint(\"\")\n\ndata['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\ndata = data.set_index('datetime')\ndata = data.asfreq('MS')\ndata = data.sort_index()\n\n# Download data ForecasterRecursiveMultiSeries\n# ==============================================================================\ndata_multiseries = fetch_dataset(name=\"items_sales\")\nprint(\"\")\n\n# Download data ForecasterDirectMultiVariate\n# ==============================================================================\ndata_multivariate = fetch_dataset(name=\"air_quality_valencia_no_missing\")\n</pre> # Download data single series # ============================================================================== data = fetch_dataset(     name=\"h2o\", kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0} ) print(\"\")  data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d') data = data.set_index('datetime') data = data.asfreq('MS') data = data.sort_index()  # Download data ForecasterRecursiveMultiSeries # ============================================================================== data_multiseries = fetch_dataset(name=\"items_sales\") print(\"\")  # Download data ForecasterDirectMultiVariate # ============================================================================== data_multivariate = fetch_dataset(name=\"air_quality_valencia_no_missing\") <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 2)\n\nitems_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n\nair_quality_valencia_no_missing\n-------------------------------\nHourly measures of several air chemical pollutant at Valencia city (Avd.\nFrancia) from 2019-01-01 to 20213-12-31. Including the following variables:\npm2.5 (\u00b5g/m\u00b3), CO (mg/m\u00b3), NO (\u00b5g/m\u00b3), NO2 (\u00b5g/m\u00b3), PM10 (\u00b5g/m\u00b3), NOx (\u00b5g/m\u00b3),\nO3 (\u00b5g/m\u00b3), Veloc. (m/s), Direc. (degrees), SO2 (\u00b5g/m\u00b3). Missing values have\nbeen imputed using linear interpolation.\nRed de Vigilancia y Control de la Contaminaci\u00f3n Atmosf\u00e9rica, 46250047-Val\u00e8ncia -\nAv. Fran\u00e7a, https://mediambient.gva.es/es/web/calidad-ambiental/datos-\nhistoricos.\nShape of the dataset: (43824, 10)\n</pre> In\u00a0[3]: Copied! <pre># Data\n# ==============================================================================\ndisplay(data.head(3))\n\n# Plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(6, 3))\ndata['y'].plot(ax=ax)\nax.legend()\nplt.show()\n</pre> # Data # ============================================================================== display(data.head(3))  # Plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(6, 3)) data['y'].plot(ax=ax) ax.legend() plt.show() y datetime 1991-07-01 0.429795 1991-08-01 0.400906 1991-09-01 0.432159 In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'sum'],\n                      window_sizes = [5, 5]\n                  )\n\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 5,\n                 window_features = window_features\n             )\n\nforecaster.fit(y=data['y'])\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'sum'],                       window_sizes = [5, 5]                   )  forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 5,                  window_features = window_features              )  forecaster.fit(y=data['y']) forecaster Out[4]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3 4 5]</li> <li>Window features: ['roll_mean_5', 'roll_sum_5']</li> <li>Window size: 5</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:37:27</li> <li>Last fit date: 2025-08-07 19:37:27</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[5]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(y=data['y'])\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(y=data['y']) In\u00a0[6]: Copied! <pre># Predictors matrix\n# ==============================================================================\nX_train.head(3)\n</pre> # Predictors matrix # ============================================================================== X_train.head(3) Out[6]: lag_1 lag_2 lag_3 lag_4 lag_5 roll_mean_5 roll_sum_5 datetime 1991-12-01 0.502369 0.492543 0.432159 0.400906 0.429795 0.451554 2.257772 1992-01-01 0.602652 0.502369 0.492543 0.432159 0.400906 0.486126 2.430629 1992-02-01 0.660119 0.602652 0.502369 0.492543 0.432159 0.537968 2.689842 In\u00a0[7]: Copied! <pre># Target variable matrix\n# ==============================================================================\ny_train.head(3)\n</pre> # Target variable matrix # ============================================================================== y_train.head(3) Out[7]: <pre>datetime\n1991-12-01    0.602652\n1992-01-01    0.660119\n1992-02-01    0.336220\nFreq: MS, Name: y, dtype: float64</pre> <p>We can obtain the training predictions using the <code>predict</code> method of the regressor stored inside the forecaster object. By examining the predictions on the training data, analysts can get a better understanding of how the model is performing and make adjustments as necessary.</p> In\u00a0[8]: Copied! <pre># Training predictions using the internal regressor\n# ==============================================================================\npredictions_training = forecaster.regressor.predict(X_train)\npredictions_training[:4]\n</pre> # Training predictions using the internal regressor # ============================================================================== predictions_training = forecaster.regressor.predict(X_train) predictions_training[:4] Out[8]: <pre>array([0.49322601, 0.6376049 , 0.58531495, 0.44962278])</pre> <p>Skforecast provides the <code>create_predict_X</code> method to generate the matrices that the forecaster is using to make predictions. This method can be used to gain insight into the specific data manipulations that occur during the prediction process.</p> In\u00a0[9]: Copied! <pre># Create input matrix for predict method\n# ==============================================================================\nX_predict = forecaster.create_predict_X(steps=5)\nX_predict\n</pre> # Create input matrix for predict method # ============================================================================== X_predict = forecaster.create_predict_X(steps=5) X_predict Out[9]: lag_1 lag_2 lag_3 lag_4 lag_5 roll_mean_5 roll_sum_5 2008-07-01 0.762137 0.816255 0.827887 0.649435 0.761822 0.763507 3.817536 2008-08-01 0.865361 0.762137 0.816255 0.827887 0.649435 0.784215 3.921075 2008-09-01 0.878167 0.865361 0.762137 0.816255 0.827887 0.829961 4.149806 2008-10-01 0.806708 0.878167 0.865361 0.762137 0.816255 0.825726 4.128628 2008-11-01 0.873597 0.806708 0.878167 0.865361 0.762137 0.837194 4.185970 In\u00a0[10]: Copied! <pre># Predict using the internal regressor\n# ==============================================================================\npredictions = forecaster.regressor.predict(X_predict)\npredictions\n</pre> # Predict using the internal regressor # ============================================================================== predictions = forecaster.regressor.predict(X_predict) predictions Out[10]: <pre>array([0.86536052, 0.87816664, 0.80670845, 0.87359717, 0.96601636])</pre> In\u00a0[11]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'sum'],\n                      window_sizes = [5, 5]\n                  )\n\nforecaster = ForecasterDirect(\n                 regressor       = Ridge(random_state=123),\n                 steps           = 3,\n                 lags            = 5,\n                 window_features = window_features\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and fit forecaster # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'sum'],                       window_sizes = [5, 5]                   )  forecaster = ForecasterDirect(                  regressor       = Ridge(random_state=123),                  steps           = 3,                  lags            = 5,                  window_features = window_features              )  forecaster.fit(y=data['y']) <p>Two steps are required to extract the training matrices. One to create the entire training matrix and a second to subset the data needed for each model (step).</p> In\u00a0[12]: Copied! <pre># Create the whole train matrix\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(y=data['y'])\n\n# Extract X and y for step 1\nX_train_1, y_train_1 = forecaster.filter_train_X_y_for_step(\n                           step          = 1,\n                           X_train       = X_train,\n                           y_train       = y_train,\n                           remove_suffix = False\n                       )\n\nX_train_1.head(4)\n</pre> # Create the whole train matrix # ============================================================================== X_train, y_train = forecaster.create_train_X_y(y=data['y'])  # Extract X and y for step 1 X_train_1, y_train_1 = forecaster.filter_train_X_y_for_step(                            step          = 1,                            X_train       = X_train,                            y_train       = y_train,                            remove_suffix = False                        )  X_train_1.head(4) Out[12]: lag_1 lag_2 lag_3 lag_4 lag_5 roll_mean_5 roll_sum_5 datetime 1991-12-01 0.502369 0.492543 0.432159 0.400906 0.429795 0.451554 2.257772 1992-01-01 0.602652 0.502369 0.492543 0.432159 0.400906 0.486126 2.430629 1992-02-01 0.660119 0.602652 0.502369 0.492543 0.432159 0.537968 2.689842 1992-03-01 0.336220 0.660119 0.602652 0.502369 0.492543 0.518781 2.593903 In\u00a0[13]: Copied! <pre># Target variable matrix for step 1\n# ==============================================================================\ny_train_1.head(3)\n</pre> # Target variable matrix for step 1 # ============================================================================== y_train_1.head(3) Out[13]: <pre>datetime\n1991-12-01    0.602652\n1992-01-01    0.660119\n1992-02-01    0.336220\nFreq: MS, Name: y_step_1, dtype: float64</pre> In\u00a0[14]: Copied! <pre># Internal regressors {step: regressor}\n# ==============================================================================\nforecaster.regressors_\n</pre> # Internal regressors {step: regressor} # ============================================================================== forecaster.regressors_ Out[14]: <pre>{np.int64(1): Ridge(random_state=123),\n np.int64(2): Ridge(random_state=123),\n np.int64(3): Ridge(random_state=123)}</pre> In\u00a0[15]: Copied! <pre># Step 1 training predictions using the internal regressor\n# ==============================================================================\npredictions_training = forecaster.regressors_[1].predict(X_train_1)\npredictions_training[:4]\n</pre> # Step 1 training predictions using the internal regressor # ============================================================================== predictions_training = forecaster.regressors_[1].predict(X_train_1) predictions_training[:4] Out[15]: <pre>array([0.5960254 , 0.6592509 , 0.70209408, 0.50312286])</pre> In\u00a0[16]: Copied! <pre># Create input matrix for predict method\n# ==============================================================================\nX_predict = forecaster.create_predict_X(steps=None)  # All steps\nX_predict\n</pre> # Create input matrix for predict method # ============================================================================== X_predict = forecaster.create_predict_X(steps=None)  # All steps X_predict Out[16]: lag_1 lag_2 lag_3 lag_4 lag_5 roll_mean_5 roll_sum_5 2008-07-01 0.762137 0.816255 0.827887 0.649435 0.761822 0.763507 3.817536 2008-08-01 0.762137 0.816255 0.827887 0.649435 0.761822 0.763507 3.817536 2008-09-01 0.762137 0.816255 0.827887 0.649435 0.761822 0.763507 3.817536 In\u00a0[17]: Copied! <pre># Step 1 predictions using the internal regressor\n# ==============================================================================\npredictions = forecaster.regressors_[1].predict(X_predict)\npredictions\n</pre> # Step 1 predictions using the internal regressor # ============================================================================== predictions = forecaster.regressors_[1].predict(X_predict) predictions Out[17]: <pre>array([0.78198225, 0.78198225, 0.78198225])</pre> In\u00a0[18]: Copied! <pre># Create and fit ForecasterRecursive\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'sum'],\n                      window_sizes = [5, 5]\n                  )\n\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 5,\n                 window_features = window_features,\n                 transformer_y   = StandardScaler(),\n                 differentiation = 1\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and fit ForecasterRecursive # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'sum'],                       window_sizes = [5, 5]                   )  forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 5,                  window_features = window_features,                  transformer_y   = StandardScaler(),                  differentiation = 1              )  forecaster.fit(y=data['y']) In\u00a0[19]: Copied! <pre># Training predictions with transformations\n# ==============================================================================\nX_train_transformed, y_train_transformed = forecaster.create_train_X_y(y=data['y'])\n\n# Training predictions using the internal regressor\npredictions_transformed = forecaster.regressor.predict(X_train_transformed)\n\n# Revert differentiation (only if differentiation is not None)\npredictions_transformed = forecaster.differentiator.inverse_transform_training(predictions_transformed)\n\n# Revert transformation (only if transformer_y is not None)\npredictions_training = forecaster.transformer_y.inverse_transform(predictions_transformed.reshape(-1, 1))\npredictions_training.ravel()[:4]\n</pre> # Training predictions with transformations # ============================================================================== X_train_transformed, y_train_transformed = forecaster.create_train_X_y(y=data['y'])  # Training predictions using the internal regressor predictions_transformed = forecaster.regressor.predict(X_train_transformed)  # Revert differentiation (only if differentiation is not None) predictions_transformed = forecaster.differentiator.inverse_transform_training(predictions_transformed)  # Revert transformation (only if transformer_y is not None) predictions_training = forecaster.transformer_y.inverse_transform(predictions_transformed.reshape(-1, 1)) predictions_training.ravel()[:4] Out[19]: <pre>array([0.5547262 , 0.3597327 , 0.39960716, 0.42227145])</pre> In\u00a0[20]: Copied! <pre># Predict using the internal regressor with transformation\n# ==============================================================================\nX_predict_transformed = forecaster.create_predict_X(steps=5)\n\n# Predict using the internal regressor\npredictions_transformed = forecaster.regressor.predict(X_predict_transformed)\n\n# Revert differentiation (only if differentiation is not None)\npredictions_transformed = forecaster.differentiator.inverse_transform_next_window(predictions_transformed)\n\n# Revert transformation (only if transformer_y is not None)\npredictions = forecaster.transformer_y.inverse_transform(predictions_transformed.reshape(-1, 1))\npredictions.ravel()[:4]\n</pre> # Predict using the internal regressor with transformation # ============================================================================== X_predict_transformed = forecaster.create_predict_X(steps=5)  # Predict using the internal regressor predictions_transformed = forecaster.regressor.predict(X_predict_transformed)  # Revert differentiation (only if differentiation is not None) predictions_transformed = forecaster.differentiator.inverse_transform_next_window(predictions_transformed)  # Revert transformation (only if transformer_y is not None) predictions = forecaster.transformer_y.inverse_transform(predictions_transformed.reshape(-1, 1)) predictions.ravel()[:4] <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DataTransformationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The output matrix is in the transformed scale due to the inclusion of                \u2502\n\u2502 transformations or differentiation in the Forecaster. As a result, any predictions   \u2502\n\u2502 generated using this matrix will also be in the transformed scale. Please refer to   \u2502\n\u2502 the documentation for more details:                                                  \u2502\n\u2502 https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : DataTransformationWarning                                                 \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/recursive/_forecaster_recursive.py:1436                                         \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=DataTransformationWarning)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[20]: <pre>array([0.88563047, 0.62235217, 0.54433454, 0.56906843])</pre> <p>As before, when using a <code>ForecasterDirect</code>, two steps are required to extract the training matrices. One to create the entire training matrix and a second to subset the data needed for each model (step).</p> <p> \u26a0 Warning </p> <p>If the <code>ForecasterDirect</code> includes differentiation, the model in step 1 must be used if you want to reverse the differentiation of the training time series with the <code>inverse_transform_training</code> method.</p> In\u00a0[21]: Copied! <pre># Create and fit ForecasterDirect\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'sum'],\n                      window_sizes = [5, 5]\n                  )\n\nforecaster = ForecasterDirect(\n                 regressor       = Ridge(random_state=123),\n                 steps           = 3,\n                 lags            = 5,\n                 window_features = window_features,\n                 transformer_y   = StandardScaler(),\n                 differentiation = 1\n             )\n\nforecaster.fit(y=data['y'])\n</pre> # Create and fit ForecasterDirect # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'sum'],                       window_sizes = [5, 5]                   )  forecaster = ForecasterDirect(                  regressor       = Ridge(random_state=123),                  steps           = 3,                  lags            = 5,                  window_features = window_features,                  transformer_y   = StandardScaler(),                  differentiation = 1              )  forecaster.fit(y=data['y']) In\u00a0[22]: Copied! <pre># Training predictions with transformations\n# ==============================================================================\nX_train_transformed, y_train_transformed = forecaster.create_train_X_y(y=data['y'])\n\n# Extract X and y for step 1\nX_train_transformed_1, y_train_transformed_1 = forecaster.filter_train_X_y_for_step(\n                                                   step          = 1,\n                                                   X_train       = X_train_transformed,\n                                                   y_train       = y_train_transformed,\n                                                   remove_suffix = False\n                                               )\n\n# Training predictions using the internal regressor for step 1\npredictions_transformed = forecaster.regressors_[1].predict(X_train_transformed_1)\n\n# Revert differentiation (only if differentiation is not None)\npredictions_transformed = forecaster.differentiator.inverse_transform_training(predictions_transformed)\n\n# Revert transformation (only if transformer_y is not None)\npredictions_training = forecaster.transformer_y.inverse_transform(predictions_transformed.reshape(-1, 1))\npredictions_training.ravel()[:4]\n</pre> # Training predictions with transformations # ============================================================================== X_train_transformed, y_train_transformed = forecaster.create_train_X_y(y=data['y'])  # Extract X and y for step 1 X_train_transformed_1, y_train_transformed_1 = forecaster.filter_train_X_y_for_step(                                                    step          = 1,                                                    X_train       = X_train_transformed,                                                    y_train       = y_train_transformed,                                                    remove_suffix = False                                                )  # Training predictions using the internal regressor for step 1 predictions_transformed = forecaster.regressors_[1].predict(X_train_transformed_1)  # Revert differentiation (only if differentiation is not None) predictions_transformed = forecaster.differentiator.inverse_transform_training(predictions_transformed)  # Revert transformation (only if transformer_y is not None) predictions_training = forecaster.transformer_y.inverse_transform(predictions_transformed.reshape(-1, 1)) predictions_training.ravel()[:4] Out[22]: <pre>array([0.58659215, 0.55767068, 0.58243553, 0.58782361])</pre> In\u00a0[23]: Copied! <pre># Predict using the internal regressor with transformation\n# ==============================================================================\nX_predict_transformed = forecaster.create_predict_X(steps=None)  # All steps\n\n# Predict using the internal regressor for step 1\npredictions_transformed = forecaster.regressors_[1].predict(X_predict_transformed)\n\n# Revert differentiation (only if differentiation is not None)\npredictions_transformed = forecaster.differentiator.inverse_transform_next_window(predictions_transformed)\n\n# Revert transformation (only if transformer_y is not None)\npredictions = forecaster.transformer_y.inverse_transform(predictions_transformed.reshape(-1, 1))\npredictions.ravel()[:4]\n</pre> # Predict using the internal regressor with transformation # ============================================================================== X_predict_transformed = forecaster.create_predict_X(steps=None)  # All steps  # Predict using the internal regressor for step 1 predictions_transformed = forecaster.regressors_[1].predict(X_predict_transformed)  # Revert differentiation (only if differentiation is not None) predictions_transformed = forecaster.differentiator.inverse_transform_next_window(predictions_transformed)  # Revert transformation (only if transformer_y is not None) predictions = forecaster.transformer_y.inverse_transform(predictions_transformed.reshape(-1, 1)) predictions.ravel()[:4] <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DataTransformationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The output matrix is in the transformed scale due to the inclusion of                \u2502\n\u2502 transformations or differentiation in the Forecaster. As a result, any predictions   \u2502\n\u2502 generated using this matrix will also be in the transformed scale. Please refer to   \u2502\n\u2502 the documentation for more details:                                                  \u2502\n\u2502 https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : DataTransformationWarning                                                 \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/direct/_forecaster_direct.py:1583                                               \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=DataTransformationWarning)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[23]: <pre>array([0.85739057, 0.95264414, 1.04789772])</pre> <p> \ud83d\udca1 Tip </p> <p>To reverse the data transformation, you can also use one of these skforecast functions: <code>transform_numpy</code>, <code>transform_series</code>, <code>transform_dataframe</code>.</p> <pre>from skforecast.utils import transform_numpy\n\npredictions = transform_numpy(\n                  array             = predictions_transformed,\n                  transformer       = forecaster.transformer_y,\n                  fit               = False,\n                  inverse_transform = True\n              )\n</pre> In\u00a0[24]: Copied! <pre># Data\n# ==============================================================================\ndisplay(data_multiseries.head(3))\n\n# Plot\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 4), sharex=True)\n\nfor i, col in enumerate(data_multiseries.columns):\n    data_multiseries[col].plot(ax=axes[i])\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('sales')\n    axes[i].set_title(col)\n\nfig.tight_layout()\nplt.show();\n</pre> # Data # ============================================================================== display(data_multiseries.head(3))  # Plot # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 4), sharex=True)  for i, col in enumerate(data_multiseries.columns):     data_multiseries[col].plot(ax=axes[i])     axes[i].set_xlabel('')     axes[i].set_ylabel('sales')     axes[i].set_title(col)  fig.tight_layout() plt.show(); item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 In\u00a0[25]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'sum'],\n                      window_sizes = [5, 5]\n                  )\n\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 5,\n                 window_features = window_features,\n                 encoding        = 'ordinal'\n             )\n\nforecaster.fit(series=data_multiseries)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'sum'],                       window_sizes = [5, 5]                   )  forecaster = ForecasterRecursiveMultiSeries(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 5,                  window_features = window_features,                  encoding        = 'ordinal'              )  forecaster.fit(series=data_multiseries) forecaster <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[25]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3 4 5]</li> <li>Window features: ['roll_mean_5', 'roll_sum_5']</li> <li>Window size: 5</li> <li>Series encoding: ordinal</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Series weights: None</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:37:30</li> <li>Last fit date: 2025-08-07 19:37:31</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): item_1, item_2, item_3</li> <li>Training range: 'item_1': ['2012-01-01', '2015-01-01'], 'item_2': ['2012-01-01', '2015-01-01'], 'item_3': ['2012-01-01', '2015-01-01']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[26]: Copied! <pre># Create training matrices\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(series=data_multiseries)\n</pre> # Create training matrices # ============================================================================== X_train, y_train = forecaster.create_train_X_y(series=data_multiseries) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <p>Depending on the series encoding selected, the column(s) generated to identify the series to which the observations belong may be different. In this case, the column <code>_level_skforecast</code> is generated as <code>encoding = 'ordinal'</code>.</p> In\u00a0[27]: Copied! <pre># Predictors matrix\n# ==============================================================================\nX_train.head(3)\n</pre> # Predictors matrix # ============================================================================== X_train.head(3) Out[27]: lag_1 lag_2 lag_3 lag_4 lag_5 roll_mean_5 roll_sum_5 _level_skforecast 2012-01-06 21.379238 25.895533 27.549099 22.777826 8.253175 21.170974 105.854870 0 2012-01-07 21.106643 21.379238 25.895533 27.549099 22.777826 23.741668 118.708338 0 2012-01-08 20.533871 21.106643 21.379238 25.895533 27.549099 23.292877 116.464384 0 In\u00a0[28]: Copied! <pre># Target variable matrix\n# ==============================================================================\ny_train.head(3)\n</pre> # Target variable matrix # ============================================================================== y_train.head(3) Out[28]: <pre>2012-01-06    21.106643\n2012-01-07    20.533871\n2012-01-08    20.069327\nName: y, dtype: float64</pre> <p>We can obtain the training predictions using the <code>predict</code> method of the regressor stored inside the forecaster object. By examining the predictions on the training data, analysts can get a better understanding of how the model is performing and make adjustments as necessary.</p> In\u00a0[29]: Copied! <pre># Training predictions using the internal regressor\n# ==============================================================================\npredictions_training = forecaster.regressor.predict(X_train)\npredictions_training[:4]\n</pre> # Training predictions using the internal regressor # ============================================================================== predictions_training = forecaster.regressor.predict(X_train) predictions_training[:4] Out[29]: <pre>array([19.54628549, 22.29989602, 20.10135048, 20.97563208])</pre> <p>Skforecast provides the <code>create_predict_X</code> method to generate the matrices that the forecaster is using to make predictions. This method can be used to gain insight into the specific data manipulations that occur during the prediction process.</p> In\u00a0[30]: Copied! <pre># Create input matrix for predict method\n# ==============================================================================\nX_predict_dict = forecaster.create_predict_X(steps=5, levels=None)  # All levels\n\n# Check 'item_1' matrix\nX_predict_item_1 = X_predict_dict.query('level == \"item_1\"')\nX_predict_item_1.head()\n</pre> # Create input matrix for predict method # ============================================================================== X_predict_dict = forecaster.create_predict_X(steps=5, levels=None)  # All levels  # Check 'item_1' matrix X_predict_item_1 = X_predict_dict.query('level == \"item_1\"') X_predict_item_1.head() Out[30]: level lag_1 lag_2 lag_3 lag_4 lag_5 roll_mean_5 roll_sum_5 _level_skforecast 2015-01-02 item_1 10.496302 18.721223 18.857026 19.611623 17.329233 17.003081 85.015406 0.0 2015-01-03 item_1 13.614696 10.496302 18.721223 18.857026 19.611623 16.260174 81.300869 0.0 2015-01-04 item_1 14.526244 13.614696 10.496302 18.721223 18.857026 15.243098 76.215490 0.0 2015-01-05 item_1 16.802037 14.526244 13.614696 10.496302 18.721223 14.832100 74.160501 0.0 2015-01-06 item_1 13.888023 16.802037 14.526244 13.614696 10.496302 13.865460 69.327302 0.0 In\u00a0[31]: Copied! <pre># Predict 'item_1' using the internal regressor\n# ==============================================================================\npredictions_item_1 = forecaster.regressor.predict(X_predict_item_1.drop(columns='level'))\npredictions_item_1\n</pre> # Predict 'item_1' using the internal regressor # ============================================================================== predictions_item_1 = forecaster.regressor.predict(X_predict_item_1.drop(columns='level')) predictions_item_1 Out[31]: <pre>array([13.61469596, 14.5262436 , 16.80203691, 13.88802319, 15.13547167])</pre> In\u00a0[32]: Copied! <pre># Data\n# ==============================================================================\ndisplay(data_multivariate.head(3))\n\n# Plot\n# ==============================================================================\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 4), sharex=True)\n\nfor i, col in enumerate(data_multivariate.columns[:3]):\n    data_multivariate[col].plot(ax=axes[i])\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('sales')\n    axes[i].set_title(col)\n\nfig.tight_layout()\nplt.show();\n</pre> # Data # ============================================================================== display(data_multivariate.head(3))  # Plot # ============================================================================== fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 4), sharex=True)  for i, col in enumerate(data_multivariate.columns[:3]):     data_multivariate[col].plot(ax=axes[i])     axes[i].set_xlabel('')     axes[i].set_ylabel('sales')     axes[i].set_title(col)  fig.tight_layout() plt.show(); so2 co no no2 pm10 nox o3 veloc. direc. pm2.5 datetime 2019-01-01 00:00:00 8.0 0.2 3.0 36.0 22.0 40.0 16.0 0.5 262.0 19.0 2019-01-01 01:00:00 8.0 0.1 2.0 40.0 32.0 44.0 6.0 0.6 248.0 26.0 2019-01-01 02:00:00 8.0 0.1 11.0 42.0 36.0 58.0 3.0 0.3 224.0 31.0 In\u00a0[33]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'sum'],\n                      window_sizes = [5, 5]\n                  )\n\nforecaster = ForecasterDirectMultiVariate(\n                 regressor       = Ridge(random_state=123),\n                 level           = 'co',\n                 steps           = 3,\n                 lags            = 3,\n                 window_features = window_features\n             )\n\nforecaster.fit(series=data_multivariate)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'sum'],                       window_sizes = [5, 5]                   )  forecaster = ForecasterDirectMultiVariate(                  regressor       = Ridge(random_state=123),                  level           = 'co',                  steps           = 3,                  lags            = 3,                  window_features = window_features              )  forecaster.fit(series=data_multivariate) forecaster Out[33]: ForecasterDirectMultiVariate General Information <ul> <li>Regressor: Ridge</li> <li>Target series (level): co</li> <li>Lags: [1 2 3]</li> <li>Window features: ['roll_mean_5', 'roll_sum_5']</li> <li>Window size: 5</li> <li>Maximum steps to predict: 3</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:37:33</li> <li>Last fit date: 2025-08-07 19:37:33</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: StandardScaler()</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Target series (level): co</li> <li>Multivariate series: so2, co, no, no2, pm10, nox, o3, veloc., direc., pm2.5</li> <li>Training range: [Timestamp('2019-01-01 00:00:00'), Timestamp('2023-12-31 23:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: h</li> </ul> Regressor Parameters <ul>                     {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[34]: Copied! <pre># Create the whole train matrix\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(series=data_multivariate)\n\n# Extract X and y for step 1\nX_train_1, y_train_1 = forecaster.filter_train_X_y_for_step(\n                           step          = 1,\n                           X_train       = X_train,\n                           y_train       = y_train,\n                           remove_suffix = False\n                       )\n\nprint(\"Columns :\", list(X_train_1.columns))\nX_train_1.head(3)\n</pre> # Create the whole train matrix # ============================================================================== X_train, y_train = forecaster.create_train_X_y(series=data_multivariate)  # Extract X and y for step 1 X_train_1, y_train_1 = forecaster.filter_train_X_y_for_step(                            step          = 1,                            X_train       = X_train,                            y_train       = y_train,                            remove_suffix = False                        )  print(\"Columns :\", list(X_train_1.columns)) X_train_1.head(3) <pre>Columns : ['so2_lag_1', 'so2_lag_2', 'so2_lag_3', 'so2_roll_mean_5', 'so2_roll_sum_5', 'co_lag_1', 'co_lag_2', 'co_lag_3', 'co_roll_mean_5', 'co_roll_sum_5', 'no_lag_1', 'no_lag_2', 'no_lag_3', 'no_roll_mean_5', 'no_roll_sum_5', 'no2_lag_1', 'no2_lag_2', 'no2_lag_3', 'no2_roll_mean_5', 'no2_roll_sum_5', 'pm10_lag_1', 'pm10_lag_2', 'pm10_lag_3', 'pm10_roll_mean_5', 'pm10_roll_sum_5', 'nox_lag_1', 'nox_lag_2', 'nox_lag_3', 'nox_roll_mean_5', 'nox_roll_sum_5', 'o3_lag_1', 'o3_lag_2', 'o3_lag_3', 'o3_roll_mean_5', 'o3_roll_sum_5', 'veloc._lag_1', 'veloc._lag_2', 'veloc._lag_3', 'veloc._roll_mean_5', 'veloc._roll_sum_5', 'direc._lag_1', 'direc._lag_2', 'direc._lag_3', 'direc._roll_mean_5', 'direc._roll_sum_5', 'pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_roll_mean_5', 'pm2.5_roll_sum_5']\n</pre> Out[34]: so2_lag_1 so2_lag_2 so2_lag_3 so2_roll_mean_5 so2_roll_sum_5 co_lag_1 co_lag_2 co_lag_3 co_roll_mean_5 co_roll_sum_5 ... direc._lag_1 direc._lag_2 direc._lag_3 direc._roll_mean_5 direc._roll_sum_5 pm2.5_lag_1 pm2.5_lag_2 pm2.5_lag_3 pm2.5_roll_mean_5 pm2.5_roll_sum_5 datetime 2019-01-01 05:00:00 5.777693 4.969880 3.354255 4.162068 20.810338 -0.447961 -0.447961 -0.447961 -0.125168 -0.625841 ... 0.538214 0.528519 0.567299 0.673943 3.369715 2.493131 2.493131 2.613291 2.156683 10.783416 2019-01-01 06:00:00 4.969880 5.777693 4.969880 4.485193 22.425964 -0.447961 -0.447961 -0.447961 -0.447961 -2.239803 ... 0.644858 0.538214 0.528519 0.615774 3.078868 1.772172 2.493131 2.493131 2.276843 11.384215 2019-01-01 07:00:00 4.969880 4.969880 5.777693 4.808318 24.041589 -0.447961 -0.447961 -0.447961 -0.447961 -2.239803 ... 0.751502 0.644858 0.538214 0.606079 3.030393 0.570573 1.772172 2.493131 1.988459 9.942297 <p>3 rows \u00d7 50 columns</p> In\u00a0[35]: Copied! <pre># Target variable matrix for step 1\n# ==============================================================================\ny_train_1.head(3)\n</pre> # Target variable matrix for step 1 # ============================================================================== y_train_1.head(3) Out[35]: <pre>datetime\n2019-01-01 05:00:00   -0.447961\n2019-01-01 06:00:00   -0.447961\n2019-01-01 07:00:00   -0.447961\nFreq: h, Name: co_step_1, dtype: float64</pre> In\u00a0[36]: Copied! <pre># Internal regressors {step: regressor}\n# ==============================================================================\nforecaster.regressors_\n</pre> # Internal regressors {step: regressor} # ============================================================================== forecaster.regressors_ Out[36]: <pre>{np.int64(1): Ridge(random_state=123),\n np.int64(2): Ridge(random_state=123),\n np.int64(3): Ridge(random_state=123)}</pre> In\u00a0[37]: Copied! <pre># Step 1 training predictions using the internal regressor\n# ==============================================================================\npredictions_training = forecaster.regressors_[1].predict(X_train_1)\npredictions_training[:4]\n</pre> # Step 1 training predictions using the internal regressor # ============================================================================== predictions_training = forecaster.regressors_[1].predict(X_train_1) predictions_training[:4] Out[37]: <pre>array([-0.33195755, -0.41491613, -0.42473316, -0.26854783])</pre> In\u00a0[38]: Copied! <pre># Create input matrix for predict method\n# ==============================================================================\nX_predict = forecaster.create_predict_X(steps=None)  # All steps\nX_predict\n</pre> # Create input matrix for predict method # ============================================================================== X_predict = forecaster.create_predict_X(steps=None)  # All steps X_predict <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DataTransformationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The output matrix is in the transformed scale due to the inclusion of                \u2502\n\u2502 transformations or differentiation in the Forecaster. As a result, any predictions   \u2502\n\u2502 generated using this matrix will also be in the transformed scale. Please refer to   \u2502\n\u2502 the documentation for more details:                                                  \u2502\n\u2502 https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : DataTransformationWarning                                                 \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/direct/_forecaster_direct_multivariate.py:1948                                  \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=DataTransformationWarning)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[38]: level so2_lag_1 so2_lag_2 so2_lag_3 so2_roll_mean_5 so2_roll_sum_5 co_lag_1 co_lag_2 co_lag_3 co_roll_mean_5 ... direc._lag_1 direc._lag_2 direc._lag_3 direc._roll_mean_5 direc._roll_sum_5 pm2.5_lag_1 pm2.5_lag_2 pm2.5_lag_3 pm2.5_roll_mean_5 pm2.5_roll_sum_5 2024-01-01 00:00:00 co -0.684809 -0.684809 -0.684809 -0.684809 -3.424043 -0.447961 -0.447961 -0.447961 -0.447961 ... 0.790282 0.780587 0.790282 0.770892 3.85446 -0.631026 -0.510866 -0.631026 -0.486834 -2.43417 2024-01-01 01:00:00 co -0.684809 -0.684809 -0.684809 -0.684809 -3.424043 -0.447961 -0.447961 -0.447961 -0.447961 ... 0.790282 0.780587 0.790282 0.770892 3.85446 -0.631026 -0.510866 -0.631026 -0.486834 -2.43417 2024-01-01 02:00:00 co -0.684809 -0.684809 -0.684809 -0.684809 -3.424043 -0.447961 -0.447961 -0.447961 -0.447961 ... 0.790282 0.780587 0.790282 0.770892 3.85446 -0.631026 -0.510866 -0.631026 -0.486834 -2.43417 <p>3 rows \u00d7 51 columns</p> In\u00a0[39]: Copied! <pre># Step 1 predictions using the internal regressor\n# ==============================================================================\npredictions = forecaster.regressors_[1].predict(X_predict.drop(columns='level'))\npredictions\n</pre> # Step 1 predictions using the internal regressor # ============================================================================== predictions = forecaster.regressors_[1].predict(X_predict.drop(columns='level')) predictions Out[39]: <pre>array([-0.37588817, -0.37588817, -0.37588817])</pre> In\u00a0[40]: Copied! <pre># Create and fit forecaster ForecasterRecursiveMultiSeries\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'sum'],\n                      window_sizes = [5, 5]\n                  )\n\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor          = LGBMRegressor(random_state=123, verbose=-1),\n                 lags               = 5,\n                 window_features    = window_features,\n                 encoding           = 'ordinal',\n                 transformer_series = StandardScaler(),\n                 differentiation    = 1\n             )\n\nforecaster.fit(series=data_multiseries)\n</pre> # Create and fit forecaster ForecasterRecursiveMultiSeries # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'sum'],                       window_sizes = [5, 5]                   )  forecaster = ForecasterRecursiveMultiSeries(                  regressor          = LGBMRegressor(random_state=123, verbose=-1),                  lags               = 5,                  window_features    = window_features,                  encoding           = 'ordinal',                  transformer_series = StandardScaler(),                  differentiation    = 1              )  forecaster.fit(series=data_multiseries) <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> In\u00a0[41]: Copied! <pre># item_1 training predictions with transformations\n# ==============================================================================\nX_train_transformed, y_train_transformed = forecaster.create_train_X_y(series=data_multiseries)\n\n# Ordinal encoding mapping \nprint(\"Ordinal encoding mapping (_level_skforecast) :\", forecaster.encoding_mapping_)\n\n# Select `item_1` rows\nX_train_item_1_transformed = X_train_transformed[\n    X_train_transformed['_level_skforecast'] == forecaster.encoding_mapping_['item_1']\n]\n\n# Training predictions using the internal regressor\npredictions_item_1_transformed = forecaster.regressor.predict(X_train_item_1_transformed)\n\n# Revert differentiation (only if differentiation is not None)\npredictions_item_1_transformed = forecaster.differentiator_['item_1'].inverse_transform_training(\n    predictions_item_1_transformed\n)\n\n# Revert transformation (only if transformer_series is not None)\npredictions_item_1_training = forecaster.transformer_series_['item_1'].inverse_transform(\n    predictions_item_1_transformed.reshape(-1, 1)\n)\npredictions_item_1_training.ravel()[:4]\n</pre> # item_1 training predictions with transformations # ============================================================================== X_train_transformed, y_train_transformed = forecaster.create_train_X_y(series=data_multiseries)  # Ordinal encoding mapping  print(\"Ordinal encoding mapping (_level_skforecast) :\", forecaster.encoding_mapping_)  # Select `item_1` rows X_train_item_1_transformed = X_train_transformed[     X_train_transformed['_level_skforecast'] == forecaster.encoding_mapping_['item_1'] ]  # Training predictions using the internal regressor predictions_item_1_transformed = forecaster.regressor.predict(X_train_item_1_transformed)  # Revert differentiation (only if differentiation is not None) predictions_item_1_transformed = forecaster.differentiator_['item_1'].inverse_transform_training(     predictions_item_1_transformed )  # Revert transformation (only if transformer_series is not None) predictions_item_1_training = forecaster.transformer_series_['item_1'].inverse_transform(     predictions_item_1_transformed.reshape(-1, 1) ) predictions_item_1_training.ravel()[:4] <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>Ordinal encoding mapping (_level_skforecast) : {'item_1': 0, 'item_2': 1, 'item_3': 2}\n</pre> Out[41]: <pre>array([19.03003376, 19.87765376, 20.27301849, 21.50016429])</pre> In\u00a0[42]: Copied! <pre># Predict using the internal regressor with transformation\n# ==============================================================================\nX_predict_dict_transformed = forecaster.create_predict_X(steps=5, levels=None)  # All levels\n\n# Select 'item_1' matrix\nX_predict_item_1_transformed = X_predict_dict_transformed.query(\"level == 'item_1'\").drop(columns='level')\n\n# Predict 'item_1' using the internal regressor\npredictions_item_1_transformed = forecaster.regressor.predict(X_predict_item_1_transformed)\n\n# Revert differentiation (only if differentiation is not None)\npredictions_item_1_transformed = forecaster.differentiator_['item_1'].inverse_transform_next_window(\n    predictions_item_1_transformed\n)\n\n# Revert transformation (only if transformer_series is not None)\npredictions_item_1 = forecaster.transformer_series_['item_1'].inverse_transform(\n    predictions_item_1_transformed.reshape(-1, 1)\n)\npredictions_item_1.ravel()[:4]\n</pre> # Predict using the internal regressor with transformation # ============================================================================== X_predict_dict_transformed = forecaster.create_predict_X(steps=5, levels=None)  # All levels  # Select 'item_1' matrix X_predict_item_1_transformed = X_predict_dict_transformed.query(\"level == 'item_1'\").drop(columns='level')  # Predict 'item_1' using the internal regressor predictions_item_1_transformed = forecaster.regressor.predict(X_predict_item_1_transformed)  # Revert differentiation (only if differentiation is not None) predictions_item_1_transformed = forecaster.differentiator_['item_1'].inverse_transform_next_window(     predictions_item_1_transformed )  # Revert transformation (only if transformer_series is not None) predictions_item_1 = forecaster.transformer_series_['item_1'].inverse_transform(     predictions_item_1_transformed.reshape(-1, 1) ) predictions_item_1.ravel()[:4] <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DataTransformationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The output matrix is in the transformed scale due to the inclusion of                \u2502\n\u2502 transformations or differentiation in the Forecaster. As a result, any predictions   \u2502\n\u2502 generated using this matrix will also be in the transformed scale. Please refer to   \u2502\n\u2502 the documentation for more details:                                                  \u2502\n\u2502 https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : DataTransformationWarning                                                 \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/recursive/_forecaster_recursive_multiseries.py:2480                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=DataTransformationWarning)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[42]: <pre>array([11.9118058 , 13.90669023, 14.61048366, 14.68379556])</pre> <p>As before, when using a <code>ForecasterDirectMultiVariate</code>, two steps are required to extract the training matrices. One to create the entire training matrix and a second to subset the data needed for each model (step).</p> <p> \u26a0 Warning </p> <p>If the <code>ForecasterDirectMultiVariate</code> includes differentiation, the model in step 1 must be used if you want to reverse the differentiation of the training time series with the <code>inverse_transform_training</code> method.</p> In\u00a0[43]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'sum'],\n                      window_sizes = [5, 5]\n                  )\n\nforecaster = ForecasterDirectMultiVariate(\n                 regressor          = Ridge(random_state=123),\n                 level              = 'co',\n                 steps              = 3,\n                 lags               = 3,\n                 window_features    = window_features,\n                 transformer_series = StandardScaler(),\n                 differentiation    = 1\n             )\n\nforecaster.fit(series=data_multivariate)\n</pre> # Create and fit forecaster # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'sum'],                       window_sizes = [5, 5]                   )  forecaster = ForecasterDirectMultiVariate(                  regressor          = Ridge(random_state=123),                  level              = 'co',                  steps              = 3,                  lags               = 3,                  window_features    = window_features,                  transformer_series = StandardScaler(),                  differentiation    = 1              )  forecaster.fit(series=data_multivariate) In\u00a0[44]: Copied! <pre># Training predictions with transformations\n# ==============================================================================\nX_train_transformed, y_train_transformed = forecaster.create_train_X_y(series=data_multivariate)\n\n# Extract X and y for step 1\nX_train_transformed_1, y_train_transformed_1 = forecaster.filter_train_X_y_for_step(\n                                                   step          = 1,\n                                                   X_train       = X_train_transformed,\n                                                   y_train       = y_train_transformed,\n                                                   remove_suffix = False\n                                               )\n\n# Training predictions using the internal regressor for step 1\npredictions_transformed = forecaster.regressors_[1].predict(X_train_transformed_1)\n\n# Revert differentiation (only if differentiation is not None)\npredictions_transformed = forecaster.differentiator_['co'].inverse_transform_training(\n    predictions_transformed\n)\n\n# Revert transformation (only if transformer_series is not None)\npredictions_training = forecaster.transformer_series_['co'].inverse_transform(\n    predictions_transformed.reshape(-1, 1)\n)\npredictions_training.ravel()[:4]\n</pre> # Training predictions with transformations # ============================================================================== X_train_transformed, y_train_transformed = forecaster.create_train_X_y(series=data_multivariate)  # Extract X and y for step 1 X_train_transformed_1, y_train_transformed_1 = forecaster.filter_train_X_y_for_step(                                                    step          = 1,                                                    X_train       = X_train_transformed,                                                    y_train       = y_train_transformed,                                                    remove_suffix = False                                                )  # Training predictions using the internal regressor for step 1 predictions_transformed = forecaster.regressors_[1].predict(X_train_transformed_1)  # Revert differentiation (only if differentiation is not None) predictions_transformed = forecaster.differentiator_['co'].inverse_transform_training(     predictions_transformed )  # Revert transformation (only if transformer_series is not None) predictions_training = forecaster.transformer_series_['co'].inverse_transform(     predictions_transformed.reshape(-1, 1) ) predictions_training.ravel()[:4] Out[44]: <pre>array([0.09856102, 0.09112238, 0.09364988, 0.09022939])</pre> In\u00a0[45]: Copied! <pre># Predict using the internal regressor with transformation\n# ==============================================================================\nX_predict_transformed = forecaster.create_predict_X(steps=None)  # All steps\n\n# Predict using the internal regressor for step 1\npredictions_transformed = forecaster.regressors_[1].predict(X_predict_transformed.drop(columns='level'))\n\n# Revert differentiation (only if differentiation is not None)\npredictions_transformed = forecaster.differentiator_['co'].inverse_transform_next_window(\n    predictions_transformed\n)\n\n# Revert transformation (only if transformer_y is not None)\npredictions = forecaster.transformer_series_['co'].inverse_transform(\n    predictions_transformed.reshape(-1, 1)\n)\npredictions.ravel()[:4]\n</pre> # Predict using the internal regressor with transformation # ============================================================================== X_predict_transformed = forecaster.create_predict_X(steps=None)  # All steps  # Predict using the internal regressor for step 1 predictions_transformed = forecaster.regressors_[1].predict(X_predict_transformed.drop(columns='level'))  # Revert differentiation (only if differentiation is not None) predictions_transformed = forecaster.differentiator_['co'].inverse_transform_next_window(     predictions_transformed )  # Revert transformation (only if transformer_y is not None) predictions = forecaster.transformer_series_['co'].inverse_transform(     predictions_transformed.reshape(-1, 1) ) predictions.ravel()[:4] <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DataTransformationWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 The output matrix is in the transformed scale due to the inclusion of                \u2502\n\u2502 transformations or differentiation in the Forecaster. As a result, any predictions   \u2502\n\u2502 generated using this matrix will also be in the transformed scale. Please refer to   \u2502\n\u2502 the documentation for more details:                                                  \u2502\n\u2502 https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : DataTransformationWarning                                                 \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/direct/_forecaster_direct_multivariate.py:1948                                  \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=DataTransformationWarning)       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[45]: <pre>array([0.1004134 , 0.10082681, 0.10124021])</pre> <p> \ud83d\udca1 Tip </p> <p>To reverse the data transformation, you can also use one of these skforecast functions: <code>transform_numpy</code>, <code>transform_series</code>, <code>transform_dataframe</code>.</p> <pre>from skforecast.utils import transform_numpy\n\npredictions = transform_numpy(\n                  array             = predictions_transformed,\n                  transformer       = forecaster.transformer_series_['item_1'],\n                  fit               = False,\n                  inverse_transform = True\n              )\n</pre>"},{"location":"user_guides/training-and-prediction-matrices.html#how-to-extract-training-and-prediction-matrices","title":"How to Extract Training and Prediction Matrices\u00b6","text":"<p>While forecasting models are mainly used to predict future values, it's just as important to understand how the model is learning from the training data. Analyzing input and output matrices used during training, predictions on the training data or exploring the prediction matrices is crucial for assessing model performance and understanding areas for optimization. This process can reveal whether the model is overfitting, underfitting, or struggling with specific patterns in the data.</p> <p>Training matrices</p> <p>Training matrices contain the input features used by the model during the training process. These matrices are essential for understanding how the model interprets patterns and relationships within the data. They typically include the lagged variables, window features and exogenous variables. By extracting and analyzing these matrices, you can ensure that the input data is correctly structured and aligned with the model\u2019s requirements.</p> <p>Prediction matrices</p> <p>Prediction matrices are used to generate forecasts for future values. These matrices incorporate the features necessary for making predictions, such as recent observations (lags), window features and any exogenous variables. Understanding the structure of these matrices is important for debugging and for validating the model\u2019s future predictions.</p>"},{"location":"user_guides/training-and-prediction-matrices.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/training-and-prediction-matrices.html#forecasterrecursive","title":"ForecasterRecursive\u00b6","text":""},{"location":"user_guides/training-and-prediction-matrices.html#forecasterdirect","title":"ForecasterDirect\u00b6","text":""},{"location":"user_guides/training-and-prediction-matrices.html#creating-matrices-when-including-transformations","title":"Creating matrices when including transformations\u00b6","text":"<p>If any data transformations and/or differentiation, are applied, they will affect the output matrices. Consequently, the predictions generated in this transformed scale may require additional steps to revert back to the original data scale.</p>"},{"location":"user_guides/training-and-prediction-matrices.html#forecasterrecursivemultiseries","title":"ForecasterRecursiveMultiSeries\u00b6","text":""},{"location":"user_guides/training-and-prediction-matrices.html#forecasterdirectmultivariate","title":"ForecasterDirectMultiVariate\u00b6","text":""},{"location":"user_guides/training-and-prediction-matrices.html#creating-matrices-when-including-transformations-multiple-series","title":"Creating matrices when including transformations multiple series\u00b6","text":"<p>If any data transformations and/or differentiation, are applied, they will affect the output matrices. Consequently, the predictions generated in this transformed scale may require additional steps to revert back to the original data scale.</p>"},{"location":"user_guides/weighted-time-series-forecasting.html","title":"Weighted time series forecasting","text":"<p> \u270e Note </p> <p>The examples that follow demonstrate how a portion of the time series can be excluded from model training by assigning it a weight of zero. However, the use of weights extends beyond the inclusion or exclusion of observations and can also balance the degree of influence that each observation has on the forecasting model. For instance, an observation assigned a weight of 10 will have ten times more impact on the model training than an observation assigned a weight of 1.</p> <p> \u26a0 Warning </p> <p>In most gradient boosting implementations, such as LightGBM, XGBoost, and CatBoost, samples with zero weight are typically excluded when calculating gradients and Hessians. However, these samples are still taken into account when constructing the feature histograms, which can result in a model that differs from one trained without zero-weighted samples. For more information on this issue, please refer to this GitHub issue.</p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection import TimeSeriesFold, backtesting_forecaster\nfrom skforecast.plot import set_dark_theme\n</pre> # Libraries # ============================================================================== import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from skforecast.recursive import ForecasterRecursive from skforecast.model_selection import TimeSeriesFold, backtesting_forecaster from skforecast.plot import set_dark_theme In\u00a0[2]: Copied! <pre># Data download\n# ==============================================================================\nurl = (\n    'https://raw.githubusercontent.com/skforecast/skforecast-datasets/refs/heads/'\n    'main/data/energy_production_shutdown.csv'\n)\ndata = pd.read_csv(url, sep=',')\n\n# Data preprocessing\n# ==============================================================================\ndata['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\ndata = data.set_index('date')\ndata = data.asfreq('D')\ndata = data.sort_index()\ndata.head()\n</pre> # Data download # ============================================================================== url = (     'https://raw.githubusercontent.com/skforecast/skforecast-datasets/refs/heads/'     'main/data/energy_production_shutdown.csv' ) data = pd.read_csv(url, sep=',')  # Data preprocessing # ============================================================================== data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d') data = data.set_index('date') data = data.asfreq('D') data = data.sort_index() data.head() Out[2]: production date 2012-01-01 375.1 2012-01-02 474.5 2012-01-03 573.9 2012-01-04 539.5 2012-01-05 445.4 In\u00a0[3]: Copied! <pre># Split data into train-val-test\n# ==============================================================================\ndata = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00']\nend_train = '2013-12-31 23:59:00'\ndata_train = data.loc[: end_train, :]\ndata_test  = data.loc[end_train:, :]\n\nprint(\n    f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}   \"\n    f\"(n={len(data_train)})\"\n)\nprint(\n    f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}   \"\n    f\"(n={len(data_test)})\"\n)\n</pre> # Split data into train-val-test # ============================================================================== data = data.loc['2012-01-01 00:00:00': '2014-12-30 23:00:00'] end_train = '2013-12-31 23:59:00' data_train = data.loc[: end_train, :] data_test  = data.loc[end_train:, :]  print(     f\"Dates train : {data_train.index.min()} --- {data_train.index.max()}   \"     f\"(n={len(data_train)})\" ) print(     f\"Dates test  : {data_test.index.min()} --- {data_test.index.max()}   \"     f\"(n={len(data_test)})\" ) <pre>Dates train : 2012-01-01 00:00:00 --- 2013-12-31 00:00:00   (n=731)\nDates test  : 2014-01-01 00:00:00 --- 2014-12-30 00:00:00   (n=364)\n</pre> In\u00a0[4]: Copied! <pre># Time series plot\n# ==============================================================================\nset_dark_theme()\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_train['production'].plot(ax=ax, label='train', linewidth=1)\ndata_test['production'].plot(ax=ax, label='test', linewidth=1)\nax.axvspan(\n    pd.to_datetime('2012-06'),\n    pd.to_datetime('2012-10'), \n    label=\"Shutdown\",\n    color=\"gray\",\n    alpha=0.3\n)\nax.set_title('Energy production')\nax.set_xlabel(\"\")\nax.legend();\n</pre> # Time series plot # ============================================================================== set_dark_theme() fig, ax = plt.subplots(figsize=(7, 3)) data_train['production'].plot(ax=ax, label='train', linewidth=1) data_test['production'].plot(ax=ax, label='test', linewidth=1) ax.axvspan(     pd.to_datetime('2012-06'),     pd.to_datetime('2012-10'),      label=\"Shutdown\",     color=\"gray\",     alpha=0.3 ) ax.set_title('Energy production') ax.set_xlabel(\"\") ax.legend(); In\u00a0[5]: Copied! <pre># Custom function to create weights\n# ==============================================================================\ndef custom_weights(index):\n    \"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n</pre> # Custom function to create weights # ============================================================================== def custom_weights(index):     \"\"\"     Return 0 if index is between 2012-06-01 and 2012-10-21.     \"\"\"     weights = np.where(                   (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),                    0,                    1               )      return weights <p>A <code>ForecasterRecursive</code> is trained including the <code>custom_weights</code> function.</p> In\u00a0[6]: Copied! <pre># Create a recursive multi-step forecaster (ForecasterRecursive)\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor   = Ridge(random_state=123),\n                 lags        = 21,\n                 weight_func = custom_weights\n             )\n</pre> # Create a recursive multi-step forecaster (ForecasterRecursive) # ============================================================================== forecaster = ForecasterRecursive(                  regressor   = Ridge(random_state=123),                  lags        = 21,                  weight_func = custom_weights              ) <p> \u26a0 Warning </p> <p>If the regressor used in the model's fitting method does not support <code>sample_weight</code> within its <code>fit</code> method, the <code>weight_func</code> argument will be ignored.</p> <p>The <code>source_code_weight_func</code> argument stores the source code of the <code>weight_func</code> added to the forecaster.</p> In\u00a0[7]: Copied! <pre>print(forecaster.source_code_weight_func)\n</pre> print(forecaster.source_code_weight_func) <pre>def custom_weights(index):\n    \"\"\"\n    Return 0 if index is between 2012-06-01 and 2012-10-21.\n    \"\"\"\n    weights = np.where(\n                  (index &gt;= '2012-06-01') &amp; (index &lt;= '2012-10-21'),\n                   0,\n                   1\n              )\n\n    return weights\n\n</pre> <p>After creating the forecaster, a backtesting process is performed to simulate its behavior if the test set had been predicted in batches of 12 days.</p> In\u00a0[8]: Copied! <pre># Backtesting: predict batches of 12 days\n# ==============================================================================\ncv = TimeSeriesFold(\n         steps              = 12,\n         initial_train_size = len(data.loc[:end_train]),\n         refit              = False\n     )\n\nmetric, predictions_backtest = backtesting_forecaster(\n                                   forecaster = forecaster,\n                                   y          = data['production'],\n                                   cv         = cv,\n                                   metric     = 'mean_absolute_error'\n                               )\n\nmetric\n</pre> # Backtesting: predict batches of 12 days # ============================================================================== cv = TimeSeriesFold(          steps              = 12,          initial_train_size = len(data.loc[:end_train]),          refit              = False      )  metric, predictions_backtest = backtesting_forecaster(                                    forecaster = forecaster,                                    y          = data['production'],                                    cv         = cv,                                    metric     = 'mean_absolute_error'                                )  metric <pre>  0%|          | 0/31 [00:00&lt;?, ?it/s]</pre> Out[8]: mean_absolute_error 0 26.821189 In\u00a0[9]: Copied! <pre># Backtest predictions\n# ==============================================================================\npredictions_backtest.head()\n</pre> # Backtest predictions # ============================================================================== predictions_backtest.head() Out[9]: pred 2014-01-01 406.122211 2014-01-02 444.103631 2014-01-03 469.424876 2014-01-04 449.407001 2014-01-05 414.945674 In\u00a0[10]: Copied! <pre># Predictions plot\n# ==============================================================================\nfig, ax = plt.subplots(figsize=(7, 3))\ndata_test['production'].plot(ax=ax, label='test', linewidth=1)\npredictions_backtest.plot(ax=ax, label='predictions', linewidth=1)\nax.set_title('Energy production')\nax.set_xlabel(\"\")\nax.legend();\n</pre> # Predictions plot # ============================================================================== fig, ax = plt.subplots(figsize=(7, 3)) data_test['production'].plot(ax=ax, label='test', linewidth=1) predictions_backtest.plot(ax=ax, label='predictions', linewidth=1) ax.set_title('Energy production') ax.set_xlabel(\"\") ax.legend();"},{"location":"user_guides/weighted-time-series-forecasting.html#weighted-time-series-forecasting","title":"Weighted time series forecasting\u00b6","text":"<p>In many real-world scenarios, historical data is available for forecasting, but not all of it is reliable. For example, IoT sensors capture raw data from the physical world, but they are often prone to failure, malfunction, and attrition due to harsh deployment environments, leading to unusual or erroneous readings. Similarly, factories may shut down for maintenance, repair, or overhaul, resulting in gaps in the data. The Covid-19 pandemic has also affected population behavior, impacting many time series such as production, sales, and transportation.</p> <p>The presence of unreliable or unrepresentative values in the data history poses a significant challenge, as it hinders model learning. However, most forecasting algorithms require complete time series data, making it impossible to remove these observations. An alternative solution is to reduce the weight of the affected observations during model training. This document demonstrates how skforecast makes it easy to implement this strategy with two examples.</p>"},{"location":"user_guides/weighted-time-series-forecasting.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/weighted-time-series-forecasting.html#exclude-part-of-the-time-series","title":"Exclude part of the time series\u00b6","text":"<p>Between 2012-06-01 and 2012-09-30, the factory underwent a shutdown. To reduce the impact of these dates on the model, a custom function is created. This function assigns a value of 0 to any index date that falls within the shutdown period or up to 21 days later (lags used by the model), and a value of 1 to all other dates. Observations assigned a weight of 0 have no influence on the model training.</p>"},{"location":"user_guides/window-features-and-custom-features.html","title":"Window and custom features","text":"<p> \u26a0 Warning </p> <p>   This section focuses on window features and other user-defined features derived from past values of the time series being modelled. These features are different from exogenous variables. Exogenous variables are external to the time series and are added to the model as additional predictors. See the    Exogenous Variables section for more information on how to create window features and lags from exogenous variables. </p> In\u00a0[1]: Copied! <pre># Libraries\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom skforecast.datasets import fetch_dataset\nfrom skforecast.preprocessing import RollingFeatures\nfrom skforecast.recursive import ForecasterRecursive, ForecasterRecursiveMultiSeries\n</pre> # Libraries # ============================================================================== import numpy as np import pandas as pd from lightgbm import LGBMRegressor from skforecast.datasets import fetch_dataset from skforecast.preprocessing import RollingFeatures from skforecast.recursive import ForecasterRecursive, ForecasterRecursiveMultiSeries In\u00a0[2]: Copied! <pre># Data\n# ==============================================================================\ndata = fetch_dataset(name=\"h2o\", raw=False)\ndata.index.name = 'datetime'\ndata = data.rename(columns={'x': 'y'})\ny = data['y']\ny\n</pre> # Data # ============================================================================== data = fetch_dataset(name=\"h2o\", raw=False) data.index.name = 'datetime' data = data.rename(columns={'x': 'y'}) y = data['y'] y <pre>h2o\n---\nMonthly expenditure ($AUD) on corticosteroid drugs that the Australian health\nsystem had between 1991 and 2008.\nHyndman R (2023). fpp3: Data for Forecasting: Principles and Practice(3rd\nEdition). http://pkg.robjhyndman.com/fpp3package/,https://github.com/robjhyndman\n/fpp3package, http://OTexts.com/fpp3.\nShape of the dataset: (204, 1)\n</pre> Out[2]: <pre>datetime\n1991-07-01    0.429795\n1991-08-01    0.400906\n1991-09-01    0.432159\n1991-10-01    0.492543\n1991-11-01    0.502369\n                ...   \n2008-02-01    0.761822\n2008-03-01    0.649435\n2008-04-01    0.827887\n2008-05-01    0.816255\n2008-06-01    0.762137\nFreq: MS, Name: y, Length: 204, dtype: float64</pre> <p>The <code>RollingFeatures</code> class available in skforecast allows the creation of some of the most commonly used predictors:</p> <ul> <li>'mean': the mean of the previous n values.</li> <li>'std': the standard deviation of the previous n values.</li> <li>'min': the minimum of the previous n values.</li> <li>'max': the maximum of the previous n values.</li> <li>'sum': the sum of the previous n values.</li> <li>'median': the median of the previous n values.</li> <li>'ratio_min_max': the ratio between the minimum and maximum of the previous n values.</li> <li>'coef_variation': the coefficient of variation of the previous n values.</li> <li>'ewm': the exponentially weighted mean of the previous n values. The decay factor <code>alpha</code> can be set in the <code>kwargs_stats</code> argument, default is <code>{'ewm': {'alpha': 0.3}}</code>.</li> </ul> <p>The user can specify these predictors by passing a list of strings to the <code>stats</code> argument. The user can also specify the window size for each of these predictors by passing a <code>list</code> of integers to the <code>window_size</code> argument, if the value is the same for all the predictors, the user can pass a single <code>integer</code>.</p> <p>The following example demonstrates how to use the <code>RollingFeatures</code> class to include rolling statistics (mean, minimum, and maximum values). Here, the rolling mean is computed with a window size of 10 and 20, while the minimum and maximum values use a window size of 10.</p> In\u00a0[3]: Copied! <pre># Window features\n# ==============================================================================\nwindow_features = RollingFeatures(\n                      stats        = ['mean', 'mean', 'min', 'max'],\n                      window_sizes = [20, 10, 10, 10]\n                  )\n</pre> # Window features # ============================================================================== window_features = RollingFeatures(                       stats        = ['mean', 'mean', 'min', 'max'],                       window_sizes = [20, 10, 10, 10]                   ) In\u00a0[4]: Copied! <pre># Create and fit forecaster\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 3,\n                 window_features = window_features\n             )\n\nforecaster.fit(y=y)\nforecaster\n</pre> # Create and fit forecaster # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 3,                  window_features = window_features              )  forecaster.fit(y=y) forecaster Out[4]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3]</li> <li>Window features: ['roll_mean_20', 'roll_mean_10', 'roll_min_10', 'roll_max_10']</li> <li>Window size: 20</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:38:49</li> <li>Last fit date: 2025-08-07 19:38:49</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[5]: Copied! <pre># Predict\n# ==============================================================================\nsteps = 36\npredictions = forecaster.predict(steps=steps)\npredictions.head(3)\n</pre> # Predict # ============================================================================== steps = 36 predictions = forecaster.predict(steps=steps) predictions.head(3) Out[5]: <pre>2008-07-01    0.784687\n2008-08-01    0.907246\n2008-09-01    1.078849\nFreq: MS, Name: pred, dtype: float64</pre> <p>The window size needed by this Forecaster is the maximum of the window sizes between the lagged values and the rolling features. In this case, this value is 20.</p> In\u00a0[6]: Copied! <pre># Predict\n# ==============================================================================\nprint(\"Window size lags            : \", forecaster.max_lag)\nprint(\"Window size window features : \", forecaster.max_size_window_features)\nprint(\"Window size Forecaster      : \", forecaster.window_size)\n</pre> # Predict # ============================================================================== print(\"Window size lags            : \", forecaster.max_lag) print(\"Window size window features : \", forecaster.max_size_window_features) print(\"Window size Forecaster      : \", forecaster.window_size) <pre>Window size lags            :  3\nWindow size window features :  20\nWindow size Forecaster      :  20\n</pre> <p>Additional arguments can be passed to the <code>RollingFeatures</code>:</p> <ul> <li><p><code>features_names</code>: By default, feature names follow the pattern <code>roll_&lt;stat&gt;_&lt;window_size&gt;</code>. For instance, a rolling mean with a window size of 20 is named <code>roll_mean_20</code>. Users can also assign custom names to each feature using a <code>list</code> of strings.</p> </li> <li><p><code>min_periods</code>: allows specifying the minimum number of observations required to compute the statistics during the training matrix generation (same as the <code>min_periods</code> argument of pandas rolling). It can be a single integer or a list of integers, one for each statistic.</p> </li> <li><p><code>fill_na</code>: define the strategy for handling missing values during the training matrix generation. Available methods are: <code>'mean'</code>, <code>'median'</code>, <code>'ffill'</code>, <code>'bfill'</code>, or a <code>float</code> value.</p> </li> </ul> <p>By inspecting the training matrices, it is possible to check that the rolling features have been correctly included.</p> In\u00a0[7]: Copied! <pre># Training matrices used internally to fit the regressor\n# ==============================================================================\nX_train, y_train = forecaster.create_train_X_y(y=y)\nX_train\n</pre> # Training matrices used internally to fit the regressor # ============================================================================== X_train, y_train = forecaster.create_train_X_y(y=y) X_train Out[7]: lag_1 lag_2 lag_3 roll_mean_20 roll_mean_10 roll_min_10 roll_max_10 datetime 1993-03-01 0.387554 0.751503 0.771258 0.496401 0.534009 0.361801 0.771258 1993-04-01 0.427283 0.387554 0.751503 0.496275 0.540557 0.387554 0.771258 1993-05-01 0.413890 0.427283 0.387554 0.496924 0.540893 0.387554 0.771258 1993-06-01 0.428859 0.413890 0.427283 0.496759 0.535440 0.387554 0.771258 1993-07-01 0.470126 0.428859 0.413890 0.495638 0.534906 0.387554 0.771258 ... ... ... ... ... ... ... ... 2008-02-01 1.219941 1.176589 1.163534 0.980390 0.995834 0.561760 1.219941 2008-03-01 0.761822 1.219941 1.176589 0.978582 1.015840 0.745258 1.219941 2008-04-01 0.649435 0.761822 1.219941 0.966838 1.006258 0.649435 1.219941 2008-05-01 0.827887 0.649435 0.761822 0.955750 1.005253 0.649435 1.219941 2008-06-01 0.816255 0.827887 0.649435 0.946778 0.991464 0.649435 1.219941 <p>184 rows \u00d7 7 columns</p> <p>It is also possible to use the <code>RollingFeatures</code> class outside the forecaster to gain a deeper insight into its behaviour. The <code>transform</code> method computes the rolling features for a given numpy array, which is assumed to contain as many past observations as the maximum window size required to compute the features. The output is a numpy array with the rolling features.</p> In\u00a0[8]: Copied! <pre># Create rolling features from a given array\n# ==============================================================================\nx = np.arange(20)\nwindow_features.transform(X=x)\n</pre> # Create rolling features from a given array # ============================================================================== x = np.arange(20) window_features.transform(X=x) Out[8]: <pre>array([ 9.5, 14.5, 10. , 19. ])</pre> <p>The <code>transform_batch</code> method is designed to transform a whole pandas Series from which multiple rolling windows can be extracted. The output is a pandas DataFrame with the rolling features.</p> In\u00a0[9]: Copied! <pre># Create rolling features from a pandas series\n# ==============================================================================\nwindow_features.transform_batch(y).head(3)\n</pre> # Create rolling features from a pandas series # ============================================================================== window_features.transform_batch(y).head(3) Out[9]: roll_mean_20 roll_mean_10 roll_min_10 roll_max_10 datetime 1993-03-01 0.496401 0.534009 0.361801 0.771258 1993-04-01 0.496275 0.540557 0.387554 0.771258 1993-05-01 0.496924 0.540893 0.387554 0.771258 <p>The reason for these two different data transformation methods is that the first is used during prediction, where the forecaster only has access to the last window of the series. In contrast, the second is used during training, where the forecaster has access to the entire series.</p> <p> \ud83d\udca1 Tip </p> <p>If you have any doubt when creating your custom class, you can check the source code of the <code>RollingFeatures</code> class available in the API Reference.</p> In\u00a0[10]: Copied! <pre># Custom class to create rolling skewness features\n# ==============================================================================\nfrom scipy.stats import skew\n\n\nclass RollingSkewness():\n    \"\"\"\n    Custom class to create rolling skewness features.\n    \"\"\"\n\n    def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):\n        \n        if not isinstance(window_sizes, list):\n            window_sizes = [window_sizes]\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \n        rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')\n        rolling_skewness = rolling_obj.skew()\n        rolling_skewness = pd.DataFrame({\n                               self.features_names: rolling_skewness\n                           }).dropna()\n\n        return rolling_skewness\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \n        X = X[~np.isnan(X)]\n        if len(X) &gt; 0:\n            rolling_skewness = np.array([skew(X, bias=False)])\n        else:\n            rolling_skewness = np.array([np.nan])\n        \n        return rolling_skewness\n</pre> # Custom class to create rolling skewness features # ============================================================================== from scipy.stats import skew   class RollingSkewness():     \"\"\"     Custom class to create rolling skewness features.     \"\"\"      def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):                  if not isinstance(window_sizes, list):             window_sizes = [window_sizes]         self.window_sizes = window_sizes         self.features_names = features_names      def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:                  rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')         rolling_skewness = rolling_obj.skew()         rolling_skewness = pd.DataFrame({                                self.features_names: rolling_skewness                            }).dropna()          return rolling_skewness      def transform(self, X: np.ndarray) -&gt; np.ndarray:                  X = X[~np.isnan(X)]         if len(X) &gt; 0:             rolling_skewness = np.array([skew(X, bias=False)])         else:             rolling_skewness = np.array([np.nan])                  return rolling_skewness In\u00a0[11]: Copied! <pre># Transform batch\n# ==============================================================================\nwindow_features = RollingSkewness(window_sizes=3)\nwindow_features.transform_batch(y)\n</pre> # Transform batch # ============================================================================== window_features = RollingSkewness(window_sizes=3) window_features.transform_batch(y) Out[11]: rolling_skewness datetime 1991-10-01 -1.696160 1991-11-01 0.897261 1991-12-01 -1.602797 1992-01-01 1.681518 1992-02-01 -0.778727 ... ... 2008-02-01 1.359033 2008-03-01 -1.674974 2008-04-01 1.466482 2008-05-01 -0.747574 2008-06-01 -1.705640 <p>201 rows \u00d7 1 columns</p> In\u00a0[12]: Copied! <pre># Transform\n# ==============================================================================\nwindow_features.transform(X=np.array([6, 12, 8]))\n</pre> # Transform # ============================================================================== window_features.transform(X=np.array([6, 12, 8])) Out[12]: <pre>array([0.93521953])</pre> In\u00a0[13]: Copied! <pre># Forecaster with custom rolling features\n# ==============================================================================\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, n_jobs=-1, verbose=-1),\n                 lags            = 3,\n                 window_features = window_features\n             )\n\nforecaster.fit(y=y)\nforecaster.predict(steps=5)\n</pre> # Forecaster with custom rolling features # ============================================================================== forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, n_jobs=-1, verbose=-1),                  lags            = 3,                  window_features = window_features              )  forecaster.fit(y=y) forecaster.predict(steps=5) Out[13]: <pre>2008-07-01    0.769870\n2008-08-01    0.824683\n2008-09-01    0.819595\n2008-10-01    0.799849\n2008-11-01    0.802432\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[14]: Copied! <pre># Data download\n# ==============================================================================\ndata_multiseries = fetch_dataset(name=\"items_sales\")\ndata_multiseries.head()\n</pre> # Data download # ============================================================================== data_multiseries = fetch_dataset(name=\"items_sales\") data_multiseries.head() <pre>items_sales\n-----------\nSimulated time series for the sales of 3 different items.\nSimulated data.\nShape of the dataset: (1097, 3)\n</pre> Out[14]: item_1 item_2 item_3 date 2012-01-01 8.253175 21.047727 19.429739 2012-01-02 22.777826 26.578125 28.009863 2012-01-03 27.549099 31.751042 32.078922 2012-01-04 25.895533 24.567708 27.252276 2012-01-05 21.379238 18.191667 20.357737 In\u00a0[15]: Copied! <pre># Custom class to create rolling skewness features (multi-series)\n# ==============================================================================\nfrom scipy.stats import skew\n\n\nclass RollingSkewnessMultiSeries():\n    \"\"\"\n    Custom class to create rolling skewness features for multiple series.\n    \"\"\"\n\n    def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):\n        \n        if not isinstance(window_sizes, list):\n            window_sizes = [window_sizes]\n        self.window_sizes = window_sizes\n        self.features_names = features_names\n\n    def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:\n        \n        rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')\n        rolling_skewness = rolling_obj.skew()\n        rolling_skewness = pd.DataFrame({\n                               self.features_names: rolling_skewness\n                           }).dropna()\n\n        return rolling_skewness\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        \n        X_dim = X.ndim\n        if X_dim == 1:\n            n_series = 1  # Only one series\n            X = X.reshape(-1, 1)\n        else:\n            n_series = X.shape[1]  # Series (levels) to be predicted (present in last_window)\n        \n        n_stats = 1  # Only skewness is calculated\n        rolling_skewness = np.full(\n            shape=(n_series, n_stats), fill_value=np.nan, dtype=float\n        )\n        for i in range(n_series):\n            if len(X) &gt; 0:\n                rolling_skewness[i, :] = skew(X[:, i], bias=False)\n            else:\n                rolling_skewness[i, :] = np.nan      \n\n        if X_dim == 1:\n            rolling_skewness = rolling_skewness.flatten()  \n        \n        return rolling_skewness\n</pre> # Custom class to create rolling skewness features (multi-series) # ============================================================================== from scipy.stats import skew   class RollingSkewnessMultiSeries():     \"\"\"     Custom class to create rolling skewness features for multiple series.     \"\"\"      def __init__(self, window_sizes, features_names: list = 'rolling_skewness'):                  if not isinstance(window_sizes, list):             window_sizes = [window_sizes]         self.window_sizes = window_sizes         self.features_names = features_names      def transform_batch(self, X: pd.Series) -&gt; pd.DataFrame:                  rolling_obj = X.rolling(window=self.window_sizes[0], center=False, closed='left')         rolling_skewness = rolling_obj.skew()         rolling_skewness = pd.DataFrame({                                self.features_names: rolling_skewness                            }).dropna()          return rolling_skewness      def transform(self, X: np.ndarray) -&gt; np.ndarray:                  X_dim = X.ndim         if X_dim == 1:             n_series = 1  # Only one series             X = X.reshape(-1, 1)         else:             n_series = X.shape[1]  # Series (levels) to be predicted (present in last_window)                  n_stats = 1  # Only skewness is calculated         rolling_skewness = np.full(             shape=(n_series, n_stats), fill_value=np.nan, dtype=float         )         for i in range(n_series):             if len(X) &gt; 0:                 rolling_skewness[i, :] = skew(X[:, i], bias=False)             else:                 rolling_skewness[i, :] = np.nan                if X_dim == 1:             rolling_skewness = rolling_skewness.flatten()                    return rolling_skewness In\u00a0[16]: Copied! <pre># Forecaster with custom rolling features\n# ==============================================================================\nwindow_features = RollingSkewnessMultiSeries(window_sizes=3)\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 3,\n                 window_features = window_features,\n                 encoding        = 'ordinal'\n             )\n\nforecaster.fit(series=data_multiseries)\nforecaster.predict(steps=5, levels=None)  # Predict all levels\n</pre> # Forecaster with custom rolling features # ============================================================================== window_features = RollingSkewnessMultiSeries(window_sizes=3) forecaster = ForecasterRecursiveMultiSeries(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 3,                  window_features = window_features,                  encoding        = 'ordinal'              )  forecaster.fit(series=data_multiseries) forecaster.predict(steps=5, levels=None)  # Predict all levels <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[16]: level pred 2015-01-02 item_1 13.530901 2015-01-02 item_2 19.709656 2015-01-02 item_3 19.378026 2015-01-03 item_1 14.252429 2015-01-03 item_2 18.608582 2015-01-03 item_3 20.457204 2015-01-04 item_1 14.230324 2015-01-04 item_2 19.536894 2015-01-04 item_3 21.238047 2015-01-05 item_1 16.339439 2015-01-05 item_2 19.174911 2015-01-05 item_3 21.039226 2015-01-06 item_1 17.231169 2015-01-06 item_2 18.821718 2015-01-06 item_3 19.032785 In\u00a0[17]: Copied! <pre># Transform output multi-series shape (n_levels, n_stats)\n# ==============================================================================\nwindow_features.transform(pd.DataFrame(forecaster.last_window_).to_numpy())\n</pre> # Transform output multi-series shape (n_levels, n_stats) # ============================================================================== window_features.transform(pd.DataFrame(forecaster.last_window_).to_numpy()) Out[17]: <pre>array([[-1.7304836 ],\n       [ 1.69345527],\n       [ 0.27695088]])</pre> In\u00a0[18]: Copied! <pre># Forecaster with multiple window features\n# ==============================================================================\nwindow_features = [\n    RollingFeatures(stats=['mean', 'mean'], window_sizes=[20, 10]),\n    RollingSkewness(window_sizes=10)\n]\n\nforecaster = ForecasterRecursive(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 3,\n                 window_features = window_features\n             )\n\nforecaster.fit(y=y)\nforecaster\n</pre> # Forecaster with multiple window features # ============================================================================== window_features = [     RollingFeatures(stats=['mean', 'mean'], window_sizes=[20, 10]),     RollingSkewness(window_sizes=10) ]  forecaster = ForecasterRecursive(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 3,                  window_features = window_features              )  forecaster.fit(y=y) forecaster Out[18]: ForecasterRecursive General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3]</li> <li>Window features: ['roll_mean_20', 'roll_mean_10', 'rolling_skewness']</li> <li>Window size: 20</li> <li>Series name: y</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:38:52</li> <li>Last fit date: 2025-08-07 19:38:52</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for y: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')]</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: MS</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[19]: Copied! <pre># Inspect prediction matrix\n# ==============================================================================\nforecaster.create_predict_X(steps=5)\n</pre> # Inspect prediction matrix # ============================================================================== forecaster.create_predict_X(steps=5) Out[19]: lag_1 lag_2 lag_3 roll_mean_20 roll_mean_10 rolling_skewness 2008-07-01 0.762137 0.816255 0.827887 0.926472 0.959856 -0.130309 2008-08-01 0.953737 0.762137 0.816255 0.918757 0.944132 -0.053265 2008-09-01 1.027884 0.953737 0.762137 0.914148 0.935922 -0.024878 2008-10-01 0.963651 1.027884 0.953737 0.901165 0.915934 -0.022704 2008-11-01 1.096952 0.963651 1.027884 0.926125 0.907970 -0.173034 In\u00a0[20]: Copied! <pre># Predict\n# ==============================================================================\nforecaster.predict(steps=5)\n</pre> # Predict # ============================================================================== forecaster.predict(steps=5) Out[20]: <pre>2008-07-01    0.953737\n2008-08-01    1.027884\n2008-09-01    0.963651\n2008-10-01    1.096952\n2008-11-01    1.096672\nFreq: MS, Name: pred, dtype: float64</pre> In\u00a0[21]: Copied! <pre># Forecaster with multiple window features\n# ==============================================================================\nwindow_features = [\n    RollingFeatures(stats=['mean', 'mean'], window_sizes=[20, 10]),\n    RollingSkewnessMultiSeries(window_sizes=10)\n]\n\nforecaster = ForecasterRecursiveMultiSeries(\n                 regressor       = LGBMRegressor(random_state=123, verbose=-1),\n                 lags            = 3,\n                 window_features = window_features\n             )\n\nforecaster.fit(series=data_multiseries)\nforecaster\n</pre> # Forecaster with multiple window features # ============================================================================== window_features = [     RollingFeatures(stats=['mean', 'mean'], window_sizes=[20, 10]),     RollingSkewnessMultiSeries(window_sizes=10) ]  forecaster = ForecasterRecursiveMultiSeries(                  regressor       = LGBMRegressor(random_state=123, verbose=-1),                  lags            = 3,                  window_features = window_features              )  forecaster.fit(series=data_multiseries) forecaster <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 InputTypeWarning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Passing a DataFrame (either wide or long format) as `series` requires additional     \u2502\n\u2502 internal transformations, which can increase computational time. It is recommended   \u2502\n\u2502 to use a dictionary of pandas Series instead. For more details, see:                 \u2502\n\u2502 https://skforecast.org/latest/user_guides/independent-multi-time-series-forecasting. \u2502\n\u2502 html#input-data                                                                      \u2502\n\u2502                                                                                      \u2502\n\u2502 Category : InputTypeWarning                                                          \u2502\n\u2502 Location :                                                                           \u2502\n\u2502 /home/joaquin/miniconda3/envs/skforecast_17_py12/lib/python3.12/site-packages/skfore \u2502\n\u2502 cast/utils/utils.py:2350                                                             \u2502\n\u2502 Suppress : warnings.simplefilter('ignore', category=InputTypeWarning)                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> Out[21]: ForecasterRecursiveMultiSeries General Information <ul> <li>Regressor: LGBMRegressor</li> <li>Lags: [1 2 3]</li> <li>Window features: ['roll_mean_20', 'roll_mean_10', 'rolling_skewness']</li> <li>Window size: 20</li> <li>Series encoding: ordinal</li> <li>Exogenous included: False</li> <li>Weight function included: False</li> <li>Series weights: None</li> <li>Differentiation order: None</li> <li>Creation date: 2025-08-07 19:38:52</li> <li>Last fit date: 2025-08-07 19:38:52</li> <li>Skforecast version: 0.17.0</li> <li>Python version: 3.12.11</li> <li>Forecaster id: None</li> </ul> Exogenous Variables <ul>                     None                 </ul> Data Transformations <ul> <li>Transformer for series: None</li> <li>Transformer for exog: None</li> </ul> Training Information <ul> <li>Series names (levels): item_1, item_2, item_3</li> <li>Training range: 'item_1': ['2012-01-01', '2015-01-01'], 'item_2': ['2012-01-01', '2015-01-01'], 'item_3': ['2012-01-01', '2015-01-01']</li> <li>Training index type: DatetimeIndex</li> <li>Training index frequency: D</li> </ul> Regressor Parameters <ul>                     {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbose': -1}                 </ul> Fit Kwargs <ul>                     {}                 </ul> <p> \ud83d\udec8 API Reference \ud83d\uddce User Guide </p> In\u00a0[22]: Copied! <pre># Inspect prediction matrix for item_1\n# ==============================================================================\nforecaster.create_predict_X(steps=5).query(\"level == 'item_1'\")\n</pre> # Inspect prediction matrix for item_1 # ============================================================================== forecaster.create_predict_X(steps=5).query(\"level == 'item_1'\") Out[22]: level lag_1 lag_2 lag_3 roll_mean_20 roll_mean_10 rolling_skewness _level_skforecast 2015-01-02 item_1 10.496302 18.721223 18.857026 19.627230 18.109484 -1.596973 0.0 2015-01-03 item_1 18.861343 10.496302 18.721223 19.512458 17.676044 -1.504973 0.0 2015-01-04 item_1 17.778515 18.861343 10.496302 19.364411 17.349451 -1.337133 0.0 2015-01-05 item_1 19.272862 17.778515 18.861343 19.293172 17.434766 -1.284426 0.0 2015-01-06 item_1 20.198399 19.272862 17.778515 19.143477 17.800624 -1.469888 0.0 In\u00a0[23]: Copied! <pre># Inspect prediction matrix\n# ==============================================================================\nforecaster.predict(steps=5)\n</pre> # Inspect prediction matrix # ============================================================================== forecaster.predict(steps=5) Out[23]: level pred 2015-01-02 item_1 18.861343 2015-01-02 item_2 21.561617 2015-01-02 item_3 21.071428 2015-01-03 item_1 17.778515 2015-01-03 item_2 21.288322 2015-01-03 item_3 20.891229 2015-01-04 item_1 19.272862 2015-01-04 item_2 20.580463 2015-01-04 item_3 21.024649 2015-01-05 item_1 20.198399 2015-01-05 item_2 20.020874 2015-01-05 item_3 21.414145 2015-01-06 item_1 19.464618 2015-01-06 item_2 20.100328 2015-01-06 item_3 21.244314"},{"location":"user_guides/window-features-and-custom-features.html#window-and-custom-features","title":"Window and custom features\u00b6","text":"<p>When forecasting time series data, it may be useful to consider additional characteristics of the time series beyond just the lagged values. For example, the moving average of the previous n values may help to capture the trend in the series. The <code>window_features</code> argument allows the inclusion of additional predictors created with the previous values of the series.</p>"},{"location":"user_guides/window-features-and-custom-features.html#libraries-and-data","title":"Libraries and data\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#rollingfeatures","title":"RollingFeatures\u00b6","text":""},{"location":"user_guides/window-features-and-custom-features.html#create-your-custom-window-features","title":"Create your custom window features\u00b6","text":"<p><code>RollingFeatures</code> is very useful for including some of the most commonly used predictors.  However, users may need to include additional predictors that are not provided by this class. In such cases, users can create their own custom class to compute the desired features and include them in the forecaster.</p> <p>The custom class must have these 2 methods:</p> <ul> <li><p><code>transform_batch</code>: method to compute the features in batch from a pandas Series. This method will be used to compute the features during the training process. It must return a pandas DataFrame containing the rolling features.</p> </li> <li><p><code>transform</code>: method to compute the features from a numpy array. This method will be used to compute the features during the prediction process. It must return a numpy array containing the computed statistics.</p> </li> </ul> <p>and these 2 attributes:</p> <ul> <li><p><code>window_sizes</code>: size of the rolling window required to compute the features. It must be a list of integers.</p> </li> <li><p><code>features_names</code>: list with the names of the output features. It must be a list of strings.</p> </li> </ul> <p>The follwing example shows how to create a custom class to include the rolling skewness and kurtosis with a window size of 20.</p>"},{"location":"user_guides/window-features-and-custom-features.html#create-your-custom-window-features-forecasterrecursivemultiseries","title":"Create your custom window features ForecasterRecursiveMultiSeries\u00b6","text":"<p>When using a <code>ForecasterRecursiveMultiSeries</code>, the <code>transform</code> method must return a numpy array that calculates all the features for all the series contained in <code>last_window</code> at once.</p>"},{"location":"user_guides/window-features-and-custom-features.html#adding-multiple-window-features","title":"Adding multiple window features\u00b6","text":"<p>It is possible to include multiple window features in all forecasters. The <code>window_features</code> argument must be a list of instances of the classes that compute the desired features.</p>"}]}